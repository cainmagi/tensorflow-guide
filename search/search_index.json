{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"扉页 ¶ 摘要 Tensorflow总纲，写给初学者们。本页面将简述Tensorflow的基本原理，结构设计以及版本更新。读者亦可在本页找到本教程将提供给读者对Tensorflow怎样的认识。 注意 由于技术限制，现在中文搜索功能无法完善，请注意当搜索关键词的时候自行分词，例如： 开放源代码软件库 无法搜到任何内容。但可以搜索 开放 源代码 软件 库 即可确保索引到上述内容。 Tensorflow总说 ¶ Tensorflow官网 TensorFlow™ 是一个开放源代码软件库，用于进行高性能数值计算。借助其灵活的架构，用户可以轻松地将计算工作部署到多种平台（CPU、GPU、TPU）和设备（桌面设备、服务器集群、移动设备、边缘设备等）。TensorFlow™ 最初是由 Google Brain 团队（隶属于 Google 的 AI 部门）中的研究人员和工程师开发的，可为机器学习和深度学习提供强力支持，并且其灵活的数值计算核心广泛应用于许多其他科学领域。 Tensorflow治学 ¶ 写在所有内容之前，读者不得不看以下几个页面，本教程所述内容大略来自于对这些资料的研读。 Tensorflow官网(中/英文) : https://www.tensorflow.org/ Keras中文文档 : https://keras-zh.readthedocs.io/ Tensorflow 2.0前瞻(英文) : https://medium.com/tensorflow/effective-tensorflow-2-0-best-practices-and-whats-changed-a0ca48767aff Tensorflow 2.0前瞻(中文) : https://zhuanlan.zhihu.com/p/50049041 提示 特别值得注意的是，现在官方文档至少在教程部分， 已经支持中英双语 。相比官方文档，我们的教程更侧重于以搭建工程为导向的设计。然而现在官方文档的翔实可读程度，确实不可错过。另一方面，Keras的 官方中文文档 似乎已不再更新，上面提供的原官方中文文档译者现在正在更新、维护的版本。 大略来说，学习Tensorflow主要应当依赖于官方文档的介绍。敝人自r1.4版开始入门Tensorflow，即是通过学习官方文档的教程来快速上手。相比四处蒐集资料，官方文档能提供一个完整、系统、完全贴合时下最新API的例子，帮助用户建立一个与Tensorflow各个功能合宜的使用习惯。若是通过在Github上检索他人的project，固然是一个很好的上手办法，但是往往就会遗漏某些重要的功能而不学。例如，Tensorflow自带的网络存取和Tensorboard API，在时兴的一些project中，经常会因方便之故，以numpy的IO来代替。笔者以为，如此培养起来的使用习惯，可谓走入偏门。当然，从这一点来说，本教程也亦复如是，即使笔者写下这些内容的时候，参照的乃是时下最新的API，时间一久，难免会过时。倘若笔者懒惰一些，不再时时更新本教程，那么本教程也就入不足取之流了。 虽然如此，教程也有并不合宜的地方。最大的问题莫过于Tensorflow本身，它有着日新月异的变化，此时能圆转如意地使用的API，到了彼时也许就成了废案。这就导致Tensorflow的官方教程也不断更新。r1.4时，教程还主要集中在如何使用“底层”API上，到了r1.9.0，就已经变成基本围绕着 tf.keras 设计的思路了。最新消息显示，即将上线的Tensorflow 2.0，将会彻底抛弃过去的“中层”API以下的全部方法， tf.layers , tf.contrib 都将被移除，倘若长期墨守成规，局限在入门时候的一套技术上，迟早会被官方库如此迅速的更新所弃。这也是无可奈何之事。毕竟Tensorflow仍然处于不断改进之中。倘若希望追求一个更加稳定、长期可用的库，keras或许是一个不错的选择。 故而，本教程将基本基于Tensorflow现在的版本(r1.13)展开介绍。本教程既可以看作一个入门教程，也可以看作是本人重新自学新版Tensorflow规范化API的一个手记，故而命名本教程为“手札”。读者不妨跟着笔者的思路，有如与笔者共学一般读下来本教程。 Tensorflow原理 ¶ 一个标准的Tensorflow工作流可以表示成这样： graph TD st(开始) --> Sess[启动Session] Sess --> ConNet[构建网络] subgraph 构造流程 ioNet>读取网络参数] --> ConNet end subgraph 执行流程 ioDat>导入数据] --> Run ConNet --> Run[执行网络] Run --> ioNetS>保存网络参数] Run --> ioRes>导出结果] end ioRes --> SessCl[关闭Session] SessCl --> ed(结束) classDef styStart fill:#FAE6A9,stroke:#BA9132; classDef styIO fill:#cde498,stroke:#13540c; class st,ed styStart class ioNet,ioNetS,ioDat,ioRes styIO 与一般的计算库不同，Tensorflow的执行流程大体可以分为两步： 构造流程 : 在这一步，Tensorflow根据用户代码构造一个 数据流图(dataflow graph) 。所谓数据流图，指的是由一系列 张量(Tensor) 构成的符号运算集合。就如同一张流程图一样，在这一阶段，尽管用户定义了每一步的运算（从简单的加减乘除到复杂的网络单元），但是没有任何运算被执行。就像一个程序员撰写代码一样，Tensorflow在这一步，将用户的代码转换成它的“机器语言”，但是网络还没有进入被使用的阶段。 执行流程 : 在这一步，Tensorflow将计算用户指定的某个 Tensor 的输出结果。要得到一个Tensor的输出，则必须得计算它一系列的依赖变量。例如，我们已知 y = x_1 + x_2 y = x_1 + x_2 , x_2 = z_1 \\times z_2 x_2 = z_1 \\times z_2 。那么，如果Tensorflow要得到 y y 的结果，它就必须先计算 x_2 x_2 。这个过程被完全地封装起来，从用户看来，我们只需要调用 y y 的输出即可，不需要关心Tensorflow是怎样按照流图完成计算的。 因此，典型的Tensorflow式的语言风格也可以这样划分： 构造流程 : 用户定义整个网络的符号运算，指定网络各个节点的属性、输入和输出。这些代码往往被写成一个函数（例如 def construct (): ） 执行流程 : 创建一个Session，在Session内调用构造函数，然后输入数据，得到并保存输出结果。必要情况下，还需要导入导出网络参数。 这里提到 会话(Session) 。Session如同一个Tensorflow虚拟机，在一个Session打开的时候，设备的计算资源（GPU, CPU, 带宽等）才被加载。Session又如同一个工人，用户撰写的网络构造代码如同车间，输入的数据如同商品，“工人（Session）”利用预定义好的“车间（流图）”将可以快速、批量地生产这些“商品（数据）”。这样地设计保证了一些外围的代码，例如指定网络参数的名称、函数的属性等操作，不需要重复进行，从而确保处理数据的时候，总是执行必要的代码，加快运算速度。 另一方面，构造-执行的结构还封装了许多提高运算效率的特性，例如多线程。尽管用户定义网络构造的时候，所写的代码是按照逻辑顺序的、线性的。但实际运行的时候，Tensorflow可以自行发现网络的哪些部分可以被同时运行，从而利用多核系统的计算资源。这些过程也是被完全封装起来的，用户并不需要花费心思去专门进行这些调整。 须知 在Tensorflow已经转变为Keras导向的现在，Session的调用被Keras API封装了起来，从用户的角度来看，现在已经不再需要手动调用Session。 Tensorflow API架构 ¶ 下图显示了当前Tensorflow-API的组织形式 Tensorflow在多个平台上均有部署，包括Python, C++, Java, Javascript, Go等，未来还可能支持更多的语言。然而，不同平台上，API的使用方式和代码风格是大不相同的。例如，Python的语言风格目前已经朝向Keras转变，而Javascript仍倾向于使用中层API；C++和Java偏向于使用底层API编写程序。本教程只针对时兴Python的API编写，除非涉及到特定的专题，不会讨论其他语言上Tensorflow的用法。 一般而言，Tensorflow将API面向划分为三个层次： 高层API (High level) : 包括Estimators和Keras； 中层API (Mid level) : 包括layers, datasets, loss和metrics等具有功能性的函数，例如网络层的定义，Loss Function，对结果的测量函数等； 底层API (Low level) : 包括具体的加减乘除、具有解析式的数学函数、卷积、对Tensor属性的测量等。 从r1.4入门的用户，所接受的训练往往是从底层API开始，使用自己的代码风格构建对应中层API的函数，然后再用自己定义好的中层API构建网络。在这种代码风格的驱使下，用户除了需要定义各个网络层具体的表达式，还需要设计输入输出的接口，用来调用 优化器(optimizer) 的接口等操作。 另一种使用方式，是从中层API开始，直接使用预定义好的网络层构建网络，这样的代码风格会节省一部分时间，相当于几乎不再需要接触底层API，但是在输入输出、网络训练和测试等更高层的模块设计上，和底层API用户的代码风格相似。 Estimators是从r1.4就已经存在的API了，它可以被看成是已经集成在Tensorflow里的完整的网络。因此，Estimators往往适合分发、需求不高的应用，但是并不适合使用Tensorflow的研究者。尽管至今为止，Tensorflow仍然对Estimators倍加推崇，但本教程将完全不涉及这方面的内容。 另一个高层API是 Eager Execution ，从r1.9起，Tensorflow就加入了这种新的使用模式，并且在Tensorflow 2.0中，它仍然将会被保留。Eager摈弃了上述的构造-执行流程，任何对Tensor的定义会被立时计算、并可得到结果。对于需要进行小规模的调试、检查API功能的用户而言更加方便。但是，据一些用户的反响，现在Eager还没有做到和之前API的完全兼容，本教程将基本不涉及Eager的用法。 因此，Keras API将是本教程的重中之重。使用本教程的用户，可以对照目前的官方文档，跟进本教程的思路，自己逐步实现各个project的设计。使用Keras风格的设计，代码量会明显少于前面提到的各种风格（Estimators除外）。事实上， tf.keras 和底层API之间具有一定的兼容，这样的编写风格能让我们在满足自己设计的派生功能的基础上，尽可能使用规范化、预定义的API单元，从而减少代码出错的可能性。通过对本教程TF 1.x版的学习，对未来2.0版的上手也会带来莫大帮助。 金宇琛( @cainmagi )，2019年3月2日 Yuchen Jin( @cainmagi ), Mar. 2, 2019 教程导读 ¶ 接下来，本教程将会涉及 从线性问题入门 : 如何使用Tensorflow完成一个简单的线性分类设计，我们将从Hello World开始，逐步过渡到一个具有核函数的非线性问题project。 Hello world: 第一个Tensorflow程序。 线性分类: 一个简单的二分类问题。 线性回归: 一个同样简单的，线性拟合问题。 非线性回归: 拟合一个简单的，可以表达出解析式的非线性函数。 非线性分类: 使用线性分类器对非线性分布的数据进行分类。 训练与测试分立 : 从这一章开始，并试图解决几个更加复杂的实际问题。这里涉及到的project相对更大、更完整，训练往往需要一定时间才能完成。因此，在本章，所有项目的训练、测试环节都会分开，我们将从这里开始，使用TensorBoard跟踪我们的训练情况，并介绍如何存取神经网络。 Super Resolution: 使用神经网络进行图像像素插值。 Sparse Coding: 使用线性的词典学习(dictionary learning)来进行图像像素插值。 Generitive Model: 使用时兴的生成模型(Generitive Model)来完成图像风格的转换。 原生的数据管理 : 从这一章开始，我们将纳入Tensorflow自带的数据管理API。实际上，很多用户并不习惯使用这些API，往往倾向于自己完成数据的导入导出。虽然这并非一个复杂的工作，但据Tensorflow的文档，原生的数据管理内部实现了多线程，本身更适合用于提高数据IO的效率。读者可以酌情选择是否需要阅读本章。 使用MINIST数据集: 使用原生的数据管理来IO一个现成的MINIST数据集，这一节改自 官方文档教程 。 使用RNN处理文章: 通过LSTM模型来进行文本分类，并使用原生的数据管理来IO一个现成的IMDB数据集，这一节改自 官方文档教程 。 使用RBM生成音频: 通过限制玻尔兹曼机(RBM)来编写一个简单的音乐生成器，这一节改自 llSourcell/Music_Generator_Demo 。 其他的高级技巧 : 本章将介绍一些特殊的、利用底层API技巧，使得用户能更灵活地定制Tensorflow-keras，从而实现一些仅依靠原生库不能实现的功能。","text_tokens":["（","brain","所弃",">","要","定义","大体","用户","从而","组织","总说","一步","至今","而言","但本","虽然","层次","减少","手札","分为","基本原理","需求","metrics","创建","www","处理","重中之重","过于","培养",";","适合","表示","结构","numpy","更","表达","其他","小规模","两步","进入","的","得到","一套","更加","mar","iores","走入","9","多","原","包括","接口","提供","不合宜","代替","(","来自","已","调试","重新","毕竟","中文搜索","展开","边缘","机器语言","固然","不学","音乐","一方","文章","不可","=","关心","系统","自行","数据管理","手","总是","实际","方式","义","不断","需要","过程","这","部署","帮助","具有","导入","模型","应用","几乎","分发","project","转变","贴合","预定","自学","sesscl","多线程","中层","自","故而","为止","计算资源","提高","方法","下来","过去","and","分类","写成","难免会","属性","许多","zh","流程","对照","某些","莫过","遗漏","来看","日","神经","找到","加减","偏向","像素","不断更新","功能性","最初","io","这种","大不相同","核","fill","哪些","下图","流程图","转换成","日新月异","构设","被","会","estimators","双语","是从","网络单元","机器","2","笔者","符号","分类器","图像","来说","更新","角度","带来","上述","不同","50049041","学者","他人","不错","版本","相同","来","构成","输入输出","zhuanlan","深度","玻尔兹曼","除外","关键","线程","逻辑","处于","研究","从","朝向","根据","分布","亦可","长期","run","墨守","利用","库","去","大略","标准","无可奈何","迅速","，","必要","tpu","可能","开发","莫过于","仍然","生产","如此","zhihu","和","开始","命名","ed","readthedocs","集中","除非","源代码","再用","入门","事实","功能","重复","go","effective","下","、","部门","即","如是","ba9132","才","拟合","环节","分词","高性能","跟踪","ionets","写","end","又","low","保留","工程师","jin","当前","研究者","简单","侧重","dataflow","换成","存在","4","介绍","成规","虚拟","四处","隶属","没有","得","直接","官网","思路","[","接下","尽管","输入","就","简述","张量","程序","任何","抛弃","训练","+","生成器","@","resolution","时候","对于","带宽","即使","同时","现在","practices","译者","技巧","不合","表达式","各种","上面","时兴","桌面","索引","一些","先","过时","z","复杂","本原","名称","起来","c++","9.0","成器","一般","输出","工程","发现","快速","撰写","事","改","存取","有","以","乃是","依赖","们","规范","多核","一个","治学","亦复如是","有如","generator","还","达式","分立","并且","插值","结束","多种","python","yuchen","既","不妨","习惯","以为","iodat","保证","construct","程序员","效率",")","“","回归","classdef","model","loss","导向","execution","侧重于","玻尔","会话","变成","集群","针对","rbm","特殊","至少","llsourcell","_","模式","金宇琛","建立","一节","用来","相当于","蒐集","除了","对","不断改进","涉及","layers","代码","可用","以及","上线","具体","在","成","设计","划分","重于","构造","按照","相似","往往","多个","兹曼","方面","版","2019","性能","规范化","cde498","times","设备","办法","也许","这里","依赖于","当","导致","手动","ai","另一方","session","尽可","检查","图","从这一点","新版","构造函数","时间","底层","imdb","乘除","面向","依靠","自己","高性","车间","mid","多线","值得","久","方便","不足取","一张","试图","纳入","页面","y","写下","st","高层","基于","p","风格","eager","]","第一个","单元","扉页","提示","搜","敝","所","原理","广泛应用","广泛",",","如同","通过","流图","接下来","一定","之前","一系列","hello","则","跟进","规模","务器","读取","程度","导出","入","中","全部","读者","看","事实上","分开","迟早","加减乘除","因此","着","即可","当然","搜索","地","灵活","确保","值得注意","足取","level","13540c","前瞻","某个","必须","封装","项目","stystart","科学","其","集合","完全","情况","机","墨守成","讨论","生成","不","新","最新","可以","fae6a9","learning","指","best","重要","使得","a0ca48767aff","编写程序","解决","器","但","部分","：","medium","商品","https","变量","graph","实现","显示","流","数据流","语言","仍","变化","计算","这些","cainmagi","集","一","之中","def","前面","系列","由于","特别","迟早会","minist","有着","tensor","高级","等","软件","看作","所述","™","13","gpu","范化","摈弃","认识","keras","难免","局限","这方","这样","为","进行","强力","注意","java","彼时","-","维护","；","一点","量会","完整","实际上","义好","tensorboard","批量","内部","1","反响","问题","将会","围绕","启动","出错","偏门","年","本人","coding","教程","学","构建","接触","阅读","保存","开放","现成","可能性","看来","限制","#","模块","驱使","操作","出","词典","团队","初学","专门","3","都","时","一系","相当","将","而","无可","加载","怎样","彻底","不得不","如何","由","stroke","com","whats","本身","上","所有","尽可能","看成","¶","才能","datasets","super","合宜","使用","不能","满足","检索","推崇","它","另","这方面","非线性","赖于","了","sess","之","能"," ","领域","如果","倾向","0","倍加","工作","架构","式","顺序","移除","tf","英文","编写","并","人","无法","class","二","第一","懒惰","参照","最大","改进","经常","styio","org","特性","或许","基础","tensorflow","好","changed","请","起","会因","例如","一般而言","不会","/","做到","数学","结构设计","完善","能圆转","到","\\","内","function","少于","用法","我们","共","派生","神经网","是否","入门教程","本章","对应","仅","英双语","数据","轻松","外围","逐步","然后","手记","之间","一章","本页","”","subgraph","资料","运算","未来","原生","搭建","技术","这个","加快","里","选择","已经","导读","阶段","2.0","之故","）","节点","函数","消息","总纲","数值","须知","结果","移动","确实","错过","。","若","守成","文本","网络层","generitive","不再","ionet","明显","很多","此时","基本","另一方面","不足","调整","优化","资源","花费","是","定制","rnn","lstm","调用","内容","莫大","即将","world","支持","x","写给","中文","当于","让","初学者","所谓","心思","对本","不得","兼容","时时","已知","同样","例子","地方","新月","参数","典型","一方面","立时","音频","奈何","相比","自带","正在","学习","也","虚拟机","形式","测试","执行","转换","线性","很","运行","javascript","核心","contrib","关键词","以下","用于","给","过渡","酌情","r1",":","解析","神经网络","google","更大","希望","high","人员","但据","官方","文档","如意","提到","于","像","然而","读","并不需要","加入","optimizer","但是","sparse","借助","一样","一种",".","特定","几个","月","本","似乎","github","每","指定","只","music","墨守成规","倘若","均","稳定","隶属于","据","cpu","所写","打开","卷积","节省","一部","一部分","追求","集成","网络","不高","关闭","测量","服务","dictionary","管理","api","速度","整个","专题","研读","并非","完成","废案","服务器","demo","各个","平台","可","之流","接受","相对","三个","那么","与","connet","主要","可谓","跟着","最新消息","翔实","属于","应当","工人","td","目前","摘要"],"title":"扉页","title_tokens":["扉页"]},{"location":"#_1","text":"摘要 Tensorflow总纲，写给初学者们。本页面将简述Tensorflow的基本原理，结构设计以及版本更新。读者亦可在本页找到本教程将提供给读者对Tensorflow怎样的认识。 注意 由于技术限制，现在中文搜索功能无法完善，请注意当搜索关键词的时候自行分词，例如： 开放源代码软件库 无法搜到任何内容。但可以搜索 开放 源代码 软件 库 即可确保索引到上述内容。","text_tokens":["限制","技术","任何","版本","在","由于","设计","页面","功能","初学","中文搜索","无法","软件","时候","开放","现在","找到","可以","总纲","确保","将","上述","关键","tensorflow","怎样","自行","关键词","。","基本原理","认识","请","分词","搜","当","索引","给","但","原理","本原","构设","注意","例如","基本","：","亦可","结构设计","以及","结构","完善","到","库","内容","，","的","写给","中文","初学者","们","读者","对","教程"," ","更新","搜索","即可","提供","代码","简述","本页","摘要","源代码","学者","本"],"title":"扉页","title_tokens":["扉页"]},{"location":"#tensorflow","text":"Tensorflow官网 TensorFlow™ 是一个开放源代码软件库，用于进行高性能数值计算。借助其灵活的架构，用户可以轻松地将计算工作部署到多种平台（CPU、GPU、TPU）和设备（桌面设备、服务器集群、移动设备、边缘设备等）。TensorFlow™ 最初是由 Google Brain 团队（隶属于 Google 的 AI 部门）中的研究人员和工程师开发的，可为机器学习和深度学习提供强力支持，并且其灵活的数值计算核心广泛应用于许多其他科学领域。","text_tokens":["（","brain","用户","、","部门","tensorflow","高性能","广泛应用","广泛","工程师","到","其他","的","隶属","务器","官网","中","轻松","地","灵活","提供","科学","其","边缘","）","可以","数值","桌面","移动","。","部署","是","应用","工程","支持","一个","计算","并且","多种","许多","等","软件","学习","™","最初","gpu","核心","用于","为","进行","强力","集群","google","机器","人员","于","借助","代码","开放","团队","深度","隶属于","cpu","性能","将","设备","服务","研究","由","ai","库","服务器","架构","，","平台","tpu","开发","可","和","高性"," ","领域","属于","工作","源代码"],"title":"Tensorflow总说","title_tokens":["总说","tensorflow"]},{"location":"#tensorflow_1","text":"写在所有内容之前，读者不得不看以下几个页面，本教程所述内容大略来自于对这些资料的研读。 Tensorflow官网(中/英文) : https://www.tensorflow.org/ Keras中文文档 : https://keras-zh.readthedocs.io/ Tensorflow 2.0前瞻(英文) : https://medium.com/tensorflow/effective-tensorflow-2-0-best-practices-and-whats-changed-a0ca48767aff Tensorflow 2.0前瞻(中文) : https://zhuanlan.zhihu.com/p/50049041 提示 特别值得注意的是，现在官方文档至少在教程部分， 已经支持中英双语 。相比官方文档，我们的教程更侧重于以搭建工程为导向的设计。然而现在官方文档的翔实可读程度，确实不可错过。另一方面，Keras的 官方中文文档 似乎已不再更新，上面提供的原官方中文文档译者现在正在更新、维护的版本。 大略来说，学习Tensorflow主要应当依赖于官方文档的介绍。敝人自r1.4版开始入门Tensorflow，即是通过学习官方文档的教程来快速上手。相比四处蒐集资料，官方文档能提供一个完整、系统、完全贴合时下最新API的例子，帮助用户建立一个与Tensorflow各个功能合宜的使用习惯。若是通过在Github上检索他人的project，固然是一个很好的上手办法，但是往往就会遗漏某些重要的功能而不学。例如，Tensorflow自带的网络存取和Tensorboard API，在时兴的一些project中，经常会因方便之故，以numpy的IO来代替。笔者以为，如此培养起来的使用习惯，可谓走入偏门。当然，从这一点来说，本教程也亦复如是，即使笔者写下这些内容的时候，参照的乃是时下最新的API，时间一久，难免会过时。倘若笔者懒惰一些，不再时时更新本教程，那么本教程也就入不足取之流了。 虽然如此，教程也有并不合宜的地方。最大的问题莫过于Tensorflow本身，它有着日新月异的变化，此时能圆转如意地使用的API，到了彼时也许就成了废案。这就导致Tensorflow的官方教程也不断更新。r1.4时，教程还主要集中在如何使用“底层”API上，到了r1.9.0，就已经变成基本围绕着 tf.keras 设计的思路了。最新消息显示，即将上线的Tensorflow 2.0，将会彻底抛弃过去的“中层”API以下的全部方法， tf.layers , tf.contrib 都将被移除，倘若长期墨守成规，局限在入门时候的一套技术上，迟早会被官方库如此迅速的更新所弃。这也是无可奈何之事。毕竟Tensorflow仍然处于不断改进之中。倘若希望追求一个更加稳定、长期可用的库，keras或许是一个不错的选择。 故而，本教程将基本基于Tensorflow现在的版本(r1.13)展开介绍。本教程既可以看作一个入门教程，也可以看作是本人重新自学新版Tensorflow规范化API的一个手记，故而命名本教程为“手札”。读者不妨跟着笔者的思路，有如与笔者共学一般读下来本教程。","text_tokens":["所弃","用户","虽然","手札","www","过于","培养","numpy","更","的","一套","更加","走入","原","提供","不合宜","代替","(","来自","已","重新","毕竟","展开","固然","不学","一方","不可","系统","手","不断","这","帮助","project","贴合","自学","中层","自","故而","方法","下来","过去","and","难免会","zh","某些","莫过","遗漏","不断更新","io","日新月异","被","会","双语","2","笔者","来说","更新","50049041","他人","不错","版本","来","zhuanlan","处于","长期","墨守","库","大略","无可奈何","迅速","，","莫过于","仍然","如此","zhihu","开始","和","命名","readthedocs","集中","入门","功能","effective","下","、","即","如是","写","侧重","4","介绍","成规","四处","官网","思路","就","抛弃","时候","即使","现在","practices","译者","不合","上面","时兴","一些","过时","起来","9.0","一般","工程","快速","事","存取","有","以","乃是","依赖","规范","一个","亦复如是","有如","还","既","不妨","习惯","以为",")","“","导向","侧重于","变成","至少","建立","蒐集","对","不断改进","layers","可用","上线","在","成","设计","重于","往往","方面","版","规范化","办法","也许","依赖于","导致","另一方","从这一点","新版","时间","底层","不足取","久","值得","方便","页面","写下","基于","p","提示","敝",",","通过","之前","程度","入","中","全部","读者","看","迟早","着","当然","地","值得注意","足取","前瞻","完全","墨守成","最新","可以","best","重要","a0ca48767aff","部分","medium","https","显示","变化","这些","一","之中","特别","迟早会","有着","看作","所述","13","范化","keras","难免","局限","为","注意","彼时","-","维护","一点","完整","tensorboard","问题","将会","围绕","偏门","本人","教程","学","都","时","而","将","无可","彻底","不得不","如何","com","whats","本身","上","所有","合宜","使用","检索","它","赖于","了","之","能"," ","0","移除","tf","英文","并","人","懒惰","参照","最大","改进","经常","org","或许","tensorflow","好","changed","会因","例如","/","能圆转","到","我们","共","入门教程","英双语","手记","”","资料","搭建","技术","选择","已经","2.0","之故","消息","确实","错过","。","若","守成","不再","此时","另一方面","基本","不足","是","内容","即将","支持","中文","不得","时时","例子","地方","新月","一方面","奈何","相比","自带","正在","学习","也","很","contrib","以下","r1",":","希望","官方","文档","如意","于","然而","读","但是",".","几个","本","似乎","github","墨守成规","倘若","稳定","追求","网络","api","研读","废案","各个","可","之流","那么","与","主要","可谓","跟着","最新消息","翔实","应当"],"title":"Tensorflow治学","title_tokens":["tensorflow","治学"]},{"location":"#tensorflow_2","text":"一个标准的Tensorflow工作流可以表示成这样： graph TD st(开始) --> Sess[启动Session] Sess --> ConNet[构建网络] subgraph 构造流程 ioNet>读取网络参数] --> ConNet end subgraph 执行流程 ioDat>导入数据] --> Run ConNet --> Run[执行网络] Run --> ioNetS>保存网络参数] Run --> ioRes>导出结果] end ioRes --> SessCl[关闭Session] SessCl --> ed(结束) classDef styStart fill:#FAE6A9,stroke:#BA9132; classDef styIO fill:#cde498,stroke:#13540c; class st,ed styStart class ioNet,ioNetS,ioDat,ioRes styIO 与一般的计算库不同，Tensorflow的执行流程大体可以分为两步： 构造流程 : 在这一步，Tensorflow根据用户代码构造一个 数据流图(dataflow graph) 。所谓数据流图，指的是由一系列 张量(Tensor) 构成的符号运算集合。就如同一张流程图一样，在这一阶段，尽管用户定义了每一步的运算（从简单的加减乘除到复杂的网络单元），但是没有任何运算被执行。就像一个程序员撰写代码一样，Tensorflow在这一步，将用户的代码转换成它的“机器语言”，但是网络还没有进入被使用的阶段。 执行流程 : 在这一步，Tensorflow将计算用户指定的某个 Tensor 的输出结果。要得到一个Tensor的输出，则必须得计算它一系列的依赖变量。例如，我们已知 y = x_1 + x_2 y = x_1 + x_2 , x_2 = z_1 \\times z_2 x_2 = z_1 \\times z_2 。那么，如果Tensorflow要得到 y y 的结果，它就必须先计算 x_2 x_2 。这个过程被完全地封装起来，从用户看来，我们只需要调用 y y 的输出即可，不需要关心Tensorflow是怎样按照流图完成计算的。 因此，典型的Tensorflow式的语言风格也可以这样划分： 构造流程 : 用户定义整个网络的符号运算，指定网络各个节点的属性、输入和输出。这些代码往往被写成一个函数（例如 def construct (): ） 执行流程 : 创建一个Session，在Session内调用构造函数，然后输入数据，得到并保存输出结果。必要情况下，还需要导入导出网络参数。 这里提到 会话(Session) 。Session如同一个Tensorflow虚拟机，在一个Session打开的时候，设备的计算资源（GPU, CPU, 带宽等）才被加载。Session又如同一个工人，用户撰写的网络构造代码如同车间，输入的数据如同商品，“工人（Session）”利用预定义好的“车间（流图）”将可以快速、批量地生产这些“商品（数据）”。这样地设计保证了一些外围的代码，例如指定网络参数的名称、函数的属性等操作，不需要重复进行，从而确保处理数据的时候，总是执行必要的代码，加快运算速度。 另一方面，构造-执行的结构还封装了许多提高运算效率的特性，例如多线程。尽管用户定义网络构造的时候，所写的代码是按照逻辑顺序的、线性的。但实际运行的时候，Tensorflow可以自行发现网络的哪些部分可以被同时运行，从而利用多核系统的计算资源。这些过程也是被完全封装起来的，用户并不需要花费心思去专门进行这些调整。 须知 在Tensorflow已经转变为Keras导向的现在，Session的调用被Keras API封装了起来，从用户的角度来看，现在已经不再需要手动调用Session。","text_tokens":["（","y","重复","class","并",">","要","st","定义","下","大体","styio","用户","ba9132","、","从而","一步","风格","特性","才","]","tensorflow","单元","分为","ionets","end","创建","又","处理","例如",",",";","如同","表示","简单","流图","结构","dataflow","到","换成","\\","内","我们","两步","一系列","的","进入","得到","虚拟","则","没有","得","读取","导出","iores","数据","[","外围","加减乘除","因此","然后","尽管","即可","就","输入","地","确保","运算","subgraph","张量","程序","”","(","任何","这个","加快","13540c","某个","封装","必须","已经","stystart","+","阶段","集合","完全","情况","机器语言","时候","）","不","节点","函数","带宽","同时","现在","一方","可以","fae6a9","指","=","关心","结果","系统","自行","须知","先","。","一些","总是","实际","z","不再","ionet","复杂","但","部分","需要","过程","这","：","起来","商品","名称","另一方面","导入","资源","花费","是","调整","变量","一般","输出","调用","转变","graph","发现","预定","快速","sesscl","撰写","多线程","x","工人","流","数据流","所谓","语言","依赖","多核","心思","一个","计算","计算资源","提高","这些","一","def","已知","系列","还","写成","参数","典型","结束","属性","一方面","tensor","许多","等","流程","iodat","虚拟机","也","保证","construct","执行","程序员","转换","效率","线性","来看",")","加减","“","运行","classdef","gpu","导向","keras","fill","哪些","会话","这样","流程图","转换成","进行","为","被","-",":","网络单元","机器","义好","批量","2","符号","_","1","启动","td","提到","像","并不需要","但是","构建","一样","角度","代码","保存","不同","看来","#","在","成","设计","构成","每","指定","只","划分","操作","构造","按照","往往","专门","方面","cpu","所写","打开","cde498","一系","times","将","网络","设备","加载","顺序","关闭","怎样","线程","逻辑","这里","stroke","由","从","速度","api","整个","根据","手动","run","完成","另一方","session","使用","利用","库","去","标准","图","，","各个","必要","构造函数","乘除","它","与","那么","connet","生产","开始","和","了","sess","车间","ed"," ","如果","多线","一张","工作","式"],"title":"Tensorflow原理","title_tokens":["原理","tensorflow"]},{"location":"#tensorflow-api","text":"下图显示了当前Tensorflow-API的组织形式 Tensorflow在多个平台上均有部署，包括Python, C++, Java, Javascript, Go等，未来还可能支持更多的语言。然而，不同平台上，API的使用方式和代码风格是大不相同的。例如，Python的语言风格目前已经朝向Keras转变，而Javascript仍倾向于使用中层API；C++和Java偏向于使用底层API编写程序。本教程只针对时兴Python的API编写，除非涉及到特定的专题，不会讨论其他语言上Tensorflow的用法。 一般而言，Tensorflow将API面向划分为三个层次： 高层API (High level) : 包括Estimators和Keras； 中层API (Mid level) : 包括layers, datasets, loss和metrics等具有功能性的函数，例如网络层的定义，Loss Function，对结果的测量函数等； 底层API (Low level) : 包括具体的加减乘除、具有解析式的数学函数、卷积、对Tensor属性的测量等。 从r1.4入门的用户，所接受的训练往往是从底层API开始，使用自己的代码风格构建对应中层API的函数，然后再用自己定义好的中层API构建网络。在这种代码风格的驱使下，用户除了需要定义各个网络层具体的表达式，还需要设计输入输出的接口，用来调用 优化器(optimizer) 的接口等操作。 另一种使用方式，是从中层API开始，直接使用预定义好的网络层构建网络，这样的代码风格会节省一部分时间，相当于几乎不再需要接触底层API，但是在输入输出、网络训练和测试等更高层的模块设计上，和底层API用户的代码风格相似。 Estimators是从r1.4就已经存在的API了，它可以被看成是已经集成在Tensorflow里的完整的网络。因此，Estimators往往适合分发、需求不高的应用，但是并不适合使用Tensorflow的研究者。尽管至今为止，Tensorflow仍然对Estimators倍加推崇，但本教程将完全不涉及这方面的内容。 另一个高层API是 Eager Execution ，从r1.9起，Tensorflow就加入了这种新的使用模式，并且在Tensorflow 2.0中，它仍然将会被保留。Eager摈弃了上述的构造-执行流程，任何对Tensor的定义会被立时计算、并可得到结果。对于需要进行小规模的调试、检查API功能的用户而言更加方便。但是，据一些用户的反响，现在Eager还没有做到和之前API的完全兼容，本教程将基本不涉及Eager的用法。 因此，Keras API将是本教程的重中之重。使用本教程的用户，可以对照目前的官方文档，跟进本教程的思路，自己逐步实现各个project的设计。使用Keras风格的设计，代码量会明显少于前面提到的各种风格（Estimators除外）。事实上， tf.keras 和底层API之间具有一定的兼容，这样的编写风格能让我们在满足自己设计的派生功能的基础上，尽可能使用规范化、预定义的API单元，从而减少代码出错的可能性。通过对本教程TF 1.x版的学习，对未来2.0版的上手也会带来莫大帮助。 金宇琛( @cainmagi )，2019年3月2日 Yuchen Jin( @cainmagi ), Mar. 2, 2019","text_tokens":["（","定义","用户","组织","从而","至今","而言","但本","层次","减少","需求","metrics","重中之重","适合","更","表达","其他","小规模","的","得到","mar","更加","9","多","包括","接口","(","调试","手","方式","义","需要","部署","具有","帮助","应用","几乎","分发","转变","project","预定","中层","为止","属性","流程","对照","日","偏向","加减","功能性","这种","大不相同","下图","被","estimators","会","是从","2","带来","上述","不同","相同","输入输出","除外","研究","从","朝向","，","可能","仍然","开始","和","除非","再用","入门","事实","功能","go","下","、","low","保留","jin","当前","研究者","存在","4","没有","直接","思路","尽管","输入","就","程序","任何","训练","@","对于","现在","表达式","各种","时兴","一些","c++","一般","输出","有","规范","一个","还","达式","并且","python","yuchen",")","loss","execution","针对","模式","金宇琛","用来","相当于","除了","对","涉及","layers","代码","具体","在","设计","划分","构造","相似","往往","多个","方面","版","2019","规范化","尽可","检查","时间","底层","面向","乘除","自己","mid","方便","高层","风格","eager","单元","所",",","通过","一定","之前","跟进","规模","中","事实上","加减乘除","因此","level","完全","讨论","不","新","可以","编写程序","器","部分","：","实现","显示","语言","仍","计算","cainmagi","前面","tensor","等","范化","摈弃","keras","这方","这样","为","进行","java","；","-","量会","完整","义好","反响","1","将会","出错","年","教程","构建","接触","可能性","模块","驱使","操作","3","相当","而","将","上","尽可能","看成","datasets","使用","满足","推崇","它","另","这方面","了","能"," ","倾向","倍加","式","tf","编写","并","基础","tensorflow","好","起","例如","一般而言","不会","做到","数学","function","到","少于","派生","用法","我们","对应","逐步","然后","之间","未来","里","已经","2.0","）","函数","结果","。","网络层","不再","明显","基本","优化","是","调用","内容","莫大","支持","x","当于","让","对本","兼容","立时","学习","也","测试","形式","执行","javascript","r1",":","解析","high","官方","文档","提到","于","然而","加入","optimizer","但是","一种",".","特定","月","本","只","均","据","卷积","节省","一部","一部分","集成","网络","不高","测量","api","专题","各个","平台","可","接受","三个","目前"],"title":"Tensorflow API架构","title_tokens":["api"," ","tensorflow","架构"]},{"location":"#_2","text":"接下来，本教程将会涉及 从线性问题入门 : 如何使用Tensorflow完成一个简单的线性分类设计，我们将从Hello World开始，逐步过渡到一个具有核函数的非线性问题project。 Hello world: 第一个Tensorflow程序。 线性分类: 一个简单的二分类问题。 线性回归: 一个同样简单的，线性拟合问题。 非线性回归: 拟合一个简单的，可以表达出解析式的非线性函数。 非线性分类: 使用线性分类器对非线性分布的数据进行分类。 训练与测试分立 : 从这一章开始，并试图解决几个更加复杂的实际问题。这里涉及到的project相对更大、更完整，训练往往需要一定时间才能完成。因此，在本章，所有项目的训练、测试环节都会分开，我们将从这里开始，使用TensorBoard跟踪我们的训练情况，并介绍如何存取神经网络。 Super Resolution: 使用神经网络进行图像像素插值。 Sparse Coding: 使用线性的词典学习(dictionary learning)来进行图像像素插值。 Generitive Model: 使用时兴的生成模型(Generitive Model)来完成图像风格的转换。 原生的数据管理 : 从这一章开始，我们将纳入Tensorflow自带的数据管理API。实际上，很多用户并不习惯使用这些API，往往倾向于自己完成数据的导入导出。虽然这并非一个复杂的工作，但据Tensorflow的文档，原生的数据管理内部实现了多线程，本身更适合用于提高数据IO的效率。读者可以酌情选择是否需要阅读本章。 使用MINIST数据集: 使用原生的数据管理来IO一个现成的MINIST数据集，这一节改自 官方文档教程 。 使用RNN处理文章: 通过LSTM模型来进行文本分类，并使用原生的数据管理来IO一个现成的IMDB数据集，这一节改自 官方文档教程 。 使用RBM生成音频: 通过限制玻尔兹曼机(RBM)来编写一个简单的音乐生成器，这一节改自 llSourcell/Music_Generator_Demo 。 其他的高级技巧 : 本章将介绍一些特殊的、利用底层API技巧，使得用户能更灵活地定制Tensorflow-keras，从而实现一些仅依靠原生库不能实现的功能。","text_tokens":["入门","纳入","编写","功能","并","二","第一","、","用户","从而","风格","拟合","第一个","环节","虽然","tensorflow","跟踪","处理","适合","/","通过","简单","接下来","到","更","一定","神经网","表达","是否","我们","介绍","其他","的","hello","本章","更加","仅","导出","数据","分开","读者","接下","逐步","因此","一章","地","灵活","原生","程序","(","选择","训练","项目","生成器","情况","机","resolution","生成","不","函数","音乐","技巧","可以","文章","learning","时兴","使得","。","数据管理","文本","一些","实际","解决","generitive","复杂","很多","需要","这","模型","具有","导入","成器","rnn","lstm","定制","project","实现","多线程","改","world","存取","式","自","一个","提高","这些","集","generator","下来","同样","分类","分立","音频","插值","minist","自带","高级","习惯","学习","测试","转换","效率","线性","神经","像素",")","回归","model","io","keras","核","用于","玻尔","酌情","过渡","进行","特殊","rbm","-",":","会","解析","神经网络","完整","实际上","llsourcell","tensorboard","更大","内部","但据","_","问题","将会","分类器","官方","一节","图像","文档","于","sparse","coding","对","涉及","教程","阅读","几个","现成","本","限制","在","设计","来","出","词典","music","往往","玻尔兹曼","兹曼","都","将","网络","线程","dictionary","如何","这里","管理","从","api","分布","本身","所有","并非","才能","super","完成","使用","利用","不能","库","demo","，","时间","相对","imdb","底层","依靠","与","非线性","开始","了","自己","能"," ","倾向","多线","工作","试图"],"title":"教程导读","title_tokens":["教程","导读"]},{"location":"licenses/","text":"协议 (Licenses) ¶ 本站协议 (中文版) ¶ MIT 开源许可协议 版权所有 © 2019, 金宇琛 (cainmagi) 特此向任何得到本软件副本或相关文档的人授权：被授权人有权使用、复制、修改、 合并、出版、发布、散布、再授权和/或贩售软件及软件的副本，及授予被供应人 同等权利，只需服从以下义务： 在软件和软件的所有副本中都必须包含以上版权声明和本许可声明。 该软件是\"按原样\"提供的，没有任何形式的明示或暗示，包括但不限于为特定目的和 不侵权的适销性和适用性的保证担保。在任何情况下，作者或版权持有人，都无权要求 任何索赔，或有关损害赔偿的其他责任。无论在本软件的使用上或其他买卖交易中， 是否涉及合同，侵权或其他行为。 License of this website (English version) ¶ MIT License Copyright © 2019 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. 本站相关项目的协议 ¶ 下面介绍的诸多协议，原则上并无必要列在本条目中（例如MIT License）。列在下侧，主要是为了向诸位支持本文档的开发者致以真诚的谢意。亦将荣耀与在下的感谢致以高天之上的上帝，愿你保守这份文档，成为众人的帮助。 License of Material ¶ MIT License Copyright © 2016 - 2019 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of MkDocs ¶ BSD 2-Clause \"Simplified\" License Copyright © 2014, Tom Christie. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. License of Jieba3K ¶ The MIT License (MIT) Copyright © 2013 Sun Junyi Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of Simple Lightbox ¶ The MIT License (MIT) Copyright © 2018 Damir Brekalo Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of MathJax ¶ Apache License 2.0 See the full license here: MathJax license License of mermaid ¶ The MIT License (MIT) Copyright © 2014 - 2018 Knut Sveidqvist Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["持有人","（","consequential","列","as","met","之上","are","保守","special","materials","direct",";","行为","索赔","mit","granted","其他","的","得到","warranty","full","服从","if","供应","包括","提供","(","真诚","connection","贩售","致以","from","copyright","including","感谢","许可","帮助","distribution","redistribution","©","indirect","holder","无论","material","free","the","junyi","modification","and","brekalo","is","clause","reproduce","开源","作者","or","被","whether","2","tort","bsd","interruption","contract","advised","2014","an","by","publish","交易","simplified","services","有关","following","profits","rights","亦","谢意","买卖","possibility","复制","subject","even","，","必要","开发","be","该软件","和","donath","sell","on","责任","愿","下","、","文版","再","liability","jin","介绍","without","没有","权利","associated","holders","jieba3k","任何","为了","版权所有","成为","及","but","documentation","martin","merchantability","source","sun","noninfringement","a","person","in","yuchen","本站","do","mathjax","散布","damage","保证","限于","code",")","merge","loss","特此","list","files","modify","so","sveidqvist","strict","included","金宇琛","要求","涉及","conditions","在","reserved","2019","goods","redistributions","按","permission","distribute","damir","合并","deal","dealings","下面","2016","授权人","damages","同等","exemplary","copies","版权","however","shall","portions","\"",",","中文版","协议","substantial","fitness","中","liable","开发者","kind","目的","any","to","retain","must","必须","项目","情况","charge","lightbox","contributors","不","christie","众人","restriction","of","损害","action","义务","但","data","：","permit","mermaid","otherwise","下侧","obtaining","disclaimed","声明","incidental","性","cainmagi","荣耀","软件","use","other","无权","原则","that","negligence","way","为","-","procurement","明示","担保","caused","修改","侵权","english","disclaimer","apache","副本","无","limited","列在","都","2013","here","有权","向","将","包含","暗示","whom","provided","infringement","上","所有","¶","for","使用","authors","适销","诸多","需","适用"," ","以上","version","knut","相关","人","损害赔偿","并","上帝","all","warranties","发布","business","例如","/","有人","是否","furnished","above","诸位","implied","form","赔偿","limitation","2.0","license","）","simple","persons","。","see","tom","event","not","no","原样","是","这份","non","原则上","website","支持","claim","中文","notice","高天","arising","2018","条目","hereby","形式","particular","licenses","出版","such","out","binary","以下","适用性","mkdocs",":","permitted","copy","software","文档","合同","授权","express",".","特定","本","只","with","持有","substitute","授予","this","或","sublicense","你","purpose","forms","与","主要","theory"],"title":"协议","title_tokens":["协议"]},{"location":"licenses/#licenses","text":"","text_tokens":[],"title":"协议 (Licenses)","title_tokens":["(",")","协议"," ","licenses"]},{"location":"licenses/#_1","text":"MIT 开源许可协议 版权所有 © 2019, 金宇琛 (cainmagi) 特此向任何得到本软件副本或相关文档的人授权：被授权人有权使用、复制、修改、 合并、出版、发布、散布、再授权和/或贩售软件及软件的副本，及授予被供应人 同等权利，只需服从以下义务： 在软件和软件的所有副本中都必须包含以上版权声明和本许可声明。 该软件是\"按原样\"提供的，没有任何形式的明示或暗示，包括但不限于为特定目的和 不侵权的适销性和适用性的保证担保。在任何情况下，作者或版权持有人，都无权要求 任何索赔，或有关损害赔偿的其他责任。无论在本软件的使用上或其他买卖交易中， 是否涉及合同，侵权或其他行为。","text_tokens":["责任","持有人","相关","人","损害赔偿","版权","下","、","发布","\"","再",",","/","行为","有人","索赔","mit","是否","协议","其他","的","得到","没有","服从","中","权利","供应","包括","提供","赔偿","目的","(","任何","版权所有","贩售","必须","及","情况","不","。","损害","义务","但","许可","：","原样","是","©","无论","声明","性","cainmagi","散布","软件","保证","限于","形式","无权","出版",")","开源","作者","特此","以下","适用性","为","被","明示","担保","修改","金宇琛","文档","要求","侵权","合同","授权","涉及","特定","本","副本","在","只","持有","交易","2019","都","有权","授予","有关","向","包含","或","按","买卖","暗示","复制","上","所有","使用","，","适销","该软件","和","需","合并","适用","授权人"," ","以上","同等"],"title":"本站协议 (中文版)","title_tokens":["(","中文","文版",")","本站","中文版","协议"," "]},{"location":"licenses/#license-of-this-website-english-version","text":"MIT License Copyright © 2019 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["copies","as","all","shall","warranties","portions","\"",",","/","liability","jin","granted","mit","furnished","substantial","without","above","warranty","fitness","liable","associated","holders","implied","kind","limitation","any","to","(","connection","charge","but","license","documentation","from","restriction","persons","of","copyright","action","merchantability","including","event","permit","not","no","otherwise","obtaining","©","claim","noninfringement","notice","free","a","person","the","arising","cainmagi","in","and","hereby","is","yuchen","do","use","other","particular",")","out","merge","or",":","files","modify","so","whether","copy","software","tort","included","express","conditions","contract",".","an","publish","limited","with","2019","following","this","rights","sublicense","permission","whom","provided","purpose","subject","for","authors","distribute","be","deal","dealings"," ","sell","damages"],"title":"License of this website (English version)","title_tokens":["(",")","version","english","this","of"," ","license","website"]},{"location":"licenses/#_2","text":"下面介绍的诸多协议，原则上并无必要列在本条目中（例如MIT License）。列在下侧，主要是为了向诸位支持本文档的开发者致以真诚的谢意。亦将荣耀与在下的感谢致以高天之上的上帝，愿你保守这份文档，成为众人的帮助。","text_tokens":["无","为了","在","（","真诚","愿","成为","列","并","致以","上帝","license","列在","）","下","原则","众人","之上","向","将","亦","。","谢意","保守","你","感谢","例如","帮助","是","这份","mit","下侧","协议","介绍","的","必要","，","原则上","开发","支持","文档","诸多","与","中","主要","下面","诸位","高天"," ","开发者","条目","荣耀","本"],"title":"本站相关项目的协议","title_tokens":["项目","本站","相关","协议","的"]},{"location":"licenses/#license-of-material","text":"MIT License Copyright © 2016 - 2019 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["copies","as","all","shall","warranties","portions","\"",",","/","liability","granted","mit","furnished","substantial","without","above","warranty","fitness","liable","associated","holders","implied","kind","limitation","any","to","(","connection","charge","but","license","documentation","from","restriction","persons","of","martin","copyright","action","merchantability","including","event","permit","not","no","otherwise","non","obtaining","©","claim","notice","free","a","person","the","arising","in","and","hereby","is","do","use","other","particular",")","out","merge","or","-",":","files","modify","so","whether","copy","software","tort","included","express","conditions","contract",".","an","publish","limited","with","2019","following","this","rights","sublicense","permission","whom","provided","infringement","purpose","subject","for","authors","distribute","be","deal","dealings","2016","donath"," ","sell","damages"],"title":"License of Material","title_tokens":[" ","material","of","license"]},{"location":"licenses/#license-of-mkdocs","text":"BSD 2-Clause \"Simplified\" License Copyright © 2014, Tom Christie. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","text_tokens":["consequential","exemplary","as","however","all","met","shall","warranties","are","\"","special","materials","business",",","direct",";","/","liability","without","above","fitness","if","liable","holders","implied","form","any","to","retain","(","must","but","license","documentation","contributors","christie","of","copyright","merchantability","source","tom","including","event","data","not","no","distribution","otherwise","redistribution","©","indirect","holder","disclaimed","incidental","notice","a","the","arising","in","modification","and","is","clause","use","damage","other","that","particular","code","negligence","way",")","such","reproduce","binary","out","or","loss","list","-",":","procurement","permitted","whether","2","software","strict","caused","tort","bsd","disclaimer","interruption","express","conditions","contract","advised",".","2014","by","limited","with","reserved","substitute","simplified","services","goods","following","this","profits","rights","redistributions","possibility","provided","purpose","for","even","forms","be"," ","on","damages","theory"],"title":"License of MkDocs","title_tokens":[" ","mkdocs","of","license"]},{"location":"licenses/#license-of-jieba3k","text":"The MIT License (MIT) Copyright © 2013 Sun Junyi Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["copies","as","all","shall","warranties","portions","\"",",","/","liability","granted","mit","furnished","substantial","without","above","warranty","fitness","liable","associated","holders","implied","kind","limitation","any","to","(","connection","charge","but","license","documentation","from","restriction","persons","of","copyright","action","merchantability","including","event","permit","not","no","otherwise","sun","obtaining","©","claim","noninfringement","notice","free","a","person","the","arising","junyi","in","and","hereby","is","do","use","other","particular",")","out","merge","or",":","files","modify","so","whether","copy","software","tort","included","express","conditions","contract",".","an","publish","limited","with","2013","following","this","rights","sublicense","permission","whom","provided","purpose","subject","for","authors","distribute","be","deal","dealings"," ","sell","damages"],"title":"License of Jieba3K","title_tokens":["jieba3k"," ","of","license"]},{"location":"licenses/#license-of-simple-lightbox","text":"The MIT License (MIT) Copyright © 2018 Damir Brekalo Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["copies","as","all","shall","warranties","portions","\"",",","/","liability","granted","mit","furnished","substantial","without","above","warranty","fitness","liable","associated","holders","implied","kind","limitation","any","to","(","connection","charge","but","license","documentation","from","restriction","persons","of","copyright","action","merchantability","including","event","permit","not","no","otherwise","obtaining","©","claim","noninfringement","notice","free","a","person","the","arising","in","2018","and","brekalo","hereby","is","do","use","other","particular",")","out","merge","or",":","files","modify","so","whether","copy","software","tort","included","express","conditions","contract",".","an","publish","limited","with","following","this","rights","sublicense","permission","whom","provided","purpose","subject","for","authors","distribute","damir","be","deal","dealings"," ","sell","damages"],"title":"License of Simple Lightbox","title_tokens":["of"," ","license","lightbox","simple"]},{"location":"licenses/#license-of-mathjax","text":"Apache License 2.0 See the full license here: MathJax license","text_tokens":[":","full","mathjax","the","here"," ","2.0","license","see","apache"],"title":"License of MathJax","title_tokens":[" ","mathjax","of","license"]},{"location":"licenses/#license-of-mermaid","text":"The MIT License (MIT) Copyright © 2014 - 2018 Knut Sveidqvist Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["knut","copies","as","all","shall","warranties","portions","\"",",","/","liability","granted","mit","furnished","substantial","without","above","warranty","fitness","liable","associated","holders","implied","kind","limitation","any","to","(","connection","charge","but","license","documentation","from","restriction","persons","of","copyright","action","merchantability","including","event","permit","not","no","otherwise","obtaining","©","claim","noninfringement","notice","free","a","person","the","arising","in","2018","and","hereby","is","do","use","other","particular",")","out","merge","or","-",":","files","modify","so","sveidqvist","whether","copy","software","tort","included","express","conditions","contract",".","2014","an","publish","limited","with","following","this","rights","sublicense","permission","whom","provided","purpose","subject","for","authors","distribute","be","deal","dealings"," ","sell","damages"],"title":"License of mermaid","title_tokens":[" ","mermaid","of","license"]},{"location":"release-notes/","text":"更新记录 ¶ 大版本更新 ¶ 在此记录本文档的主要更新，读者可以在此确认经过更新后，本文档添加了哪些主要的内容、话题。 0.1 @ February 25, 2019 ¶ 正式立项，开始本文档的撰写工作。目前尚处于未完成状态，计划包括： Tensorflow 1.12 入门教程 从线性问题入门 80% 训练与测试分立 0% 原生的数据管理 0% 其他的高级技巧 0% Tensorflow 2.0 入门教程 0% Tensorflow 第三方扩展库 入门教程 0% 局部更新记录 ¶ 0.22 @ March 17, 2019 ¶ 完成“从线性问题入门”专题下的 非线性回归 ； 微调样式表(stylesheet)文件； 修正“从线性问题入门”专题下的 线性回归 的一些笔误； 将MathJax的默认渲染修改为HTML-CSS。 0.20 @ March 12, 2019 ¶ 完成“从线性问题入门”专题下的 线性回归 ； 微调样式表(stylesheet)文件； 修正前文的一些笔误。 0.18-r1.13 @ March 9, 2019 ¶ 由于Tensorflow的新版 r1.13 发行版预编译包开始支持CUDA 10，本文的内容全部根据 r1.13 版进行调整。特别注意这个星期是Tensorflow 2.0-alpha 横空出世的日子，可喜可贺，可喜可贺。 0.18 @ March 6, 2019 ¶ 完善“从线性问题入门”专题下的 线性分类 ，补充了一些概念便于初学者理解。 0.17 @ March 5, 2019 ¶ 完成“从线性问题入门”专题下的 线性分类 ； 修正前文的一些笔误。 0.15 @ March 4, 2019 ¶ 完成“从线性问题入门”专题下的 本章总说 和 Hello world! ； 微调图片链接， MathJax 的显示特性。 0.12 @ March 3, 2019 ¶ 补完扉页。未来可能会调整 教程导读 ； 修复 Arithmatex 对 MathJax 的引入； 引入 mermaid 库，用于绘制示意图。 0.11 @ February 25, 2019 ¶ 提交 Google Analytics 和 Google Search Console 的账户关联资料； 提交 Disqus 关联资料。 0.10 @ February 25, 2019 ¶ 正式立项，并撰写扉页的一部分。","text_tokens":["入门","0.22","17","stylesheet","并","发行版","下","未","引入","、","总说","特性","出世","添加","tensorflow","扉页","html",",","发行","完善","默认","4","其他","的","入门教程","本章","hello","局部","!","0.11","25","0.1","10","数据","全部","读者","9","链接","包括","”","原生","资料","未来","(","这个","0.15","0.12","立项","训练","文件","%","笔误","便于","导读","图片","@","february","2.0","第三","绘制","技巧","可以","关联","。","数据管理","一些","部分","补完","理解","0.10","正式","：","调整","mermaid","修正","是","记录本","修复","内容","0.17","march","撰写","账户","显示","world","支持","analytics","星期","cuda","6","尚","包","初学者","5","预","可喜可贺","计划","分类","由于","分立","特别","mathjax","高级","示意图","测试","三方","补充","前文","线性",")","“","此","回归","13","记录","1.12","提交","用于","哪些","为","进行","；","r1","编译","-","注意","会","状态","概念","意图","横空","css","横空出世","日子","google","大","渲染","0.18","问题","修改","样式","文档","空出","对","教程","更新","话题","经过",".","可喜","学者","本","版本","在","可贺","80","初学","版","2019","3","0.20","一部","一部分","第三方","12","将","后","处于","管理","确认","微调","示意","从","专题","根据","alpha","¶","完成","search","库","扩展","，","新版","可能","console","arithmatex","disqus","与","非线性","图片链接","开始","主要","了","和","本文"," ","0","样式表","工作","目前"],"title":"更新记录","title_tokens":["记录","更新"]},{"location":"release-notes/#_1","text":"","text_tokens":[],"title":"更新记录","title_tokens":["记录","更新"]},{"location":"release-notes/#_2","text":"在此记录本文档的主要更新，读者可以在此确认经过更新后，本文档添加了哪些主要的内容、话题。","text_tokens":["在","、","此","可以","添加","记录","后","。","确认","哪些","记录本","内容","的","，","文档","主要","了","读者","更新","话题","经过","本"],"title":"大版本更新","title_tokens":["版本","更新","大"]},{"location":"release-notes/#01-february-25-2019","text":"正式立项，开始本文档的撰写工作。目前尚处于未完成状态，计划包括： Tensorflow 1.12 入门教程 从线性问题入门 80% 训练与测试分立 0% 原生的数据管理 0% 其他的高级技巧 0% Tensorflow 2.0 入门教程 0% Tensorflow 第三方扩展库 入门教程 0%","text_tokens":["入门","分立","立项","训练","%","高级","80","2.0","测试","三方","未","第三","线性","技巧","第三方","tensorflow","1.12","处于","。","数据管理","管理","从","正式","：","状态","完成","库","其他","扩展","撰写","，","的","入门教程","问题","文档","与","尚","开始","数据","教程"," ","包括","0","工作","目前","原生","计划","本"],"title":"0.1 @ February 25, 2019","title_tokens":["25","0.1"," ","@","february","2019",","]},{"location":"release-notes/#_3","text":"","text_tokens":[],"title":"局部更新记录","title_tokens":["记录","更新","局部"]},{"location":"release-notes/#022-march-17-2019","text":"完成“从线性问题入门”专题下的 非线性回归 ； 微调样式表(stylesheet)文件； 修正“从线性问题入门”专题下的 线性回归 的一些笔误； 将MathJax的默认渲染修改为HTML-CSS。","text_tokens":["(","入门","文件","笔误","mathjax","stylesheet","下","线性",")","“","回归","将","一些","。","微调","html","从","为","；","专题","-","修正","完成","css","默认","渲染","的","问题","修改","样式","非线性"," ","样式表","”"],"title":"0.22 @ March 17, 2019","title_tokens":["0.22","17"," ","@","march","2019",","]},{"location":"release-notes/#020-march-12-2019","text":"完成“从线性问题入门”专题下的 线性回归 ； 微调样式表(stylesheet)文件； 修正前文的一些笔误。","text_tokens":["(","入门","文件","笔误","stylesheet","下","前文","线性",")","“","回归","一些","。","微调","从","；","专题","修正","完成","的","问题","样式"," ","样式表","”"],"title":"0.20 @ March 12, 2019","title_tokens":["0.20","12"," ","@","march","2019",","]},{"location":"release-notes/#018-r113-march-9-2019","text":"由于Tensorflow的新版 r1.13 发行版预编译包开始支持CUDA 10，本文的内容全部根据 r1.13 版进行调整。特别注意这个星期是Tensorflow 2.0-alpha 横空出世的日子，可喜可贺，可喜可贺。","text_tokens":["这个","由于","可贺","特别","2.0","发行版","版","13","出世","tensorflow","。","可喜可贺","进行","注意","编译","r1","根据","-","调整","alpha","发行","横空","横空出世","是","日子","内容","的","新版","，","支持","星期","cuda","空出","包","10","开始","全部","本文","预"," ",".","可喜"],"title":"0.18-r1.13 @ March 9, 2019","title_tokens":["13","0.18","9"," ","@","march",",",".","2019","r1","-"]},{"location":"release-notes/#018-march-6-2019","text":"完善“从线性问题入门”专题下的 线性分类 ，补充了一些概念便于初学者理解。","text_tokens":["入门","分类","便于","初学","补充","下","线性","“","一些","。","从","理解","专题","概念","完善","的","问题","，","初学者","了"," ","”","学者"],"title":"0.18 @ March 6, 2019","title_tokens":["6","0.18"," ","@","march","2019",","]},{"location":"release-notes/#017-march-5-2019","text":"完成“从线性问题入门”专题下的 线性分类 ； 修正前文的一些笔误。","text_tokens":["“","入门","分类","修正","完成","笔误","一些","。"," ","下","专题","的","问题","从","”","；","前文","线性"],"title":"0.17 @ March 5, 2019","title_tokens":["5","0.17"," ","@","march","2019",","]},{"location":"release-notes/#015-march-4-2019","text":"完成“从线性问题入门”专题下的 本章总说 和 Hello world! ； 微调图片链接， MathJax 的显示特性。","text_tokens":["入门","mathjax","图片","下","线性","“","总说","特性","。","微调","从","；","专题","完成","的","问题","本章","hello","world","，","显示","!","图片链接","和"," ","链接","”"],"title":"0.15 @ March 4, 2019","title_tokens":["0.15","4"," ","@","march","2019",","]},{"location":"release-notes/#012-march-3-2019","text":"补完扉页。未来可能会调整 教程导读 ； 修复 Arithmatex 对 MathJax 的引入； 引入 mermaid 库，用于绘制示意图。","text_tokens":["mathjax","导读","示意图","引入","绘制","扉页","。","用于","示意","补完","；","调整","会","意图","mermaid","修复","库","的","可能","，","arithmatex","对","教程"," ","未来"],"title":"0.12 @ March 3, 2019","title_tokens":["0.12"," ","@","march","3","2019",","]},{"location":"release-notes/#011-february-25-2019","text":"提交 Google Analytics 和 Google Search Console 的账户关联资料； 提交 Disqus 关联资料。","text_tokens":["和","关联","google","search","。","提交"," ","的","账户","console","；","analytics","资料","disqus"],"title":"0.11 @ February 25, 2019","title_tokens":["0.11","25"," ","@","february","2019",","]},{"location":"release-notes/#010-february-25-2019","text":"正式立项，并撰写扉页的一部分。","text_tokens":["一部","一部分","立项","扉页","并","。","撰写","，","的","部分","正式"],"title":"0.10 @ February 25, 2019","title_tokens":["25"," ","@","february",",","2019","0.10"]},{"location":"book-1-x/chapter-1/","text":"从线性问题入门 ¶ 摘要 本章将从线性问题入手，讨论最简单的分类与回归问题。我们将随机生成指定分布的数据，通过理论值，检查实验效果。这些简单的、解析的问题，可以提供读者一个直观的感受，并帮助读者快速上手Tensorflow的基本概念与运用。本章中，我们不会涉及任何数据存取、跟踪测度方面的概念，也不会将训练和测试分开，旨在使读者能集中心思到代码规范和Tensorflow的基本功能上。 漫谈线性问题 ¶ 在机器学习领域，线性问题既简单也不简单。目前神经网络主要是指多层、非凸的网络结构，常常用来解复杂的、难以推导的问题。但这并不意味着线性问题容易解决。相反，在很多情况下，线性问题是解不唯一的(undetermined)，解不稳定的(ill-posed/ill-conditioned)，条件的(constrained)。同时，为了得到一个快速收敛的、高质量的解，即使对线性问题，人们也在不断提出、改进解法。例如ISTA, AMP, PGD, LISTA, vAMP等算法，都用来解线性问题。在此不作详细展开。 我们已经知道，一个线性函数具有可加性，和一次齐次性，亦即 \\begin{align} f(x_1 + x_2) &= f(x_1) + f(x_2), \\\\ f(\\alpha x) &= \\alpha f(x). \\end{align} 因此，求解一个线性问题，我们需要将问题纯粹以 线性函数 进行描述。例如， \\begin{align} \\mathbf{y} \\sim \\mathbf{A}\\mathbf{x}. \\end{align} 具体而言， \\mathbf{x} \\mathbf{x} 是我们的已知数据， \\mathbf{y} \\mathbf{y} 是我们的未知量，我们需要找到一个合适的 \\mathbf{A} \\mathbf{A} 来确保 \\mathbf{x} \\mathbf{x} 能拟合到 \\mathbf{y} \\mathbf{y} 。如果此处 \\mathbf{y} \\in \\{0,~1\\}^p \\mathbf{y} \\in \\{0,~1\\}^p 是一个代表p类-分类的向量，那么这就是一个线性分类问题；相反，如果此处 \\mathbf{y} \\in \\mathbb{R}^p \\mathbf{y} \\in \\mathbb{R}^p 在连续p维空间取值，那么这就是一个线性回归问题。 线性问题与凸问题 ¶ 请注意，虽然我们在此处提到“ 线性问题 (Linear problem) ”，但我们指的并非“ 线性规划 (Linear programming) ”。虽然严格意义上，线性规划才是真正的线性问题，但我们在此处尚不讨论线性规划相关的内容，而是着眼于机器学习应用最普遍的两个领域， 分类 和 回归 上。实际上，这两种问题虽然求解的是线性函数，但本质上是凸问题。 例如，如果我们要求解回归问题，通常可以表述成 \\begin{equation} \\begin{aligned} \\arg \\min_{\\mathbf{A}}~& \\sum_{i=1}^N \\mathcal{L}(\\mathbf{A},~\\mathbf{x}_i,~\\mathbf{y}_i), \\\\ \\mathcal{L}(\\mathbf{A},~\\mathbf{x},~\\mathbf{y}) &= \\lVert \\mathbf{y} - \\mathbf{A}\\mathbf{x} \\rVert^2_2. \\end{aligned} \\end{equation} 虽然我们求解的模型 \\mathbf{A} \\mathbf{A} 是线性的，但我们优化的函数对象 \\mathcal{L} \\mathcal{L} 是一个 凸函数 (convex function) 。在此，我们可以将这个标量函数(同时也是凸函数)表述为 \\begin{align} \\forall~\\alpha,~\\beta,~\\mathbf{x}_1,~\\mathbf{x}_2,~\\mathcal{L}(\\alpha\\mathbf{x}_1 + \\beta\\mathbf{x}_2) \\leqslant \\alpha\\mathcal{L}(\\mathbf{x}_1) + \\beta\\mathcal{L}(\\mathbf{x}_2). \\end{align} 我们将这个问题表述为 最小二乘问题(Least-square problem) 。正是由于 \\mathbf{A} \\mathbf{A} 是线性的， \\mathcal{L} \\mathcal{L} 才能被确保为一个凸函数，进而，我们才能确保上述问题能得到精确的全局最小值解。 知悉Tensorflow ¶ 在本章接下来的内容里，我们将探讨Tensorflow如何求解一些简单的问题。对于初次上手的读者而言，在安装Tensorflow，亲自开始写一些project之前，笔者推荐你到这个游乐场“抢鲜体验”一番， Tensorflow Playground 在这个在线页面里，用户不需要有任何编程知识，可以通过直观的操作界面，建立一个简单的 多层感知机网络(有时也叫做Artificial Neural Network, ANN) ，并且实时观测网络的性能和测度。它提供了几个简单的二维数据集，供用户体验不同的数据集下，各种分类、回归问题的合宜解法。如果读者已经对神经网络的基本原理有所了解，相信能通过这个小小的实验场达到心有灵犀。我们在本章所做的project和demo，大体不跳出这个试验场的范畴，只是会略微复杂一点而已。 本章要点 ¶ 下图展示了通过本章学习，能了解到的概念： graph LR st(Hello world!) --> linclas(线性分类) linclas --> linreg(线性回归) linreg --> nonlinreg(非线性回归) nonlinreg --> ed(非线性分类) lp[感知机] --> linclas sigma[Logsitc回归] --> linclas ce[交叉熵] --> linclas opt[优化器] --> linreg argpar[项目选项] --> linreg para[参数回归] --> nonlinreg sdlayer[自定义层] --> nonlinreg kernel[核函数] --> ed classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,linclas,linreg,nonlinreg,ed styStart Hello world : 首先，在本节，读者将了解Tensorflow的安装方法，并编写通过第一个简单的Tensorflow程序。 线性分类 : 本节通过解一个简单的二分类问题，我们将引入单层感知机、Logistic回归和交叉熵的概念，并且实验结果进行可视化。 线性回归 : 本节通过解一个与上一节难度相仿的，简单的回归问题，另读者能对比不同优化器的性能、特点，并介绍如何在项目中进行arugument parsing(引入项目的可选项)。 非线性回归 : 本节将修改上一节回归问题的激活函数，将线性的回归问题推广到解析函数参数回归的范畴。同时介绍自行编写网络层(类API)的方法。 非线性分类 : 本节将通过上一节的参数回归，引入核函数的概念，将线性分类问题推广到非线性空间里。","text_tokens":["感知机",">","定义","大体","用户","标量","而言","lvert","虽然","基本原理","严格","lp","二维",";","推荐","理论值","最","结构","而是","跳出","的","得到","}","begin","!","线性规划","有所","vamp","提供","(","展开","=","自行","&","实际","lr","不断","需要","这","帮助","具有","模型","应用","project","kernel","logsitc","尚","equation","方法","下来","^","分类","实验场","ista","两个","神经","找到","摘要","核","fill","下图","被","普遍","会","可加性","机器","2","笔者","相信","上述","不同","使","来","ce","本节","本质","亦","从","自定","最小值","分布","连续","向量","相反","凸","代表","，","表述","展示","难度","开始","和","leqslant","小小的","亲自","ed","而已","集中","界面","随机","入门","forall","undetermined","求解","入手","功能","感受","首先","直观","下","、","即","ba9132","高质","才","拟合","mathcal","类","跟踪","知识","end","写","posed","可视化","简单","介绍","beta","意义","特点","constrained","对比","[","接下","算法","着眼","程序","任何","为了","此处","训练","+","sdlayer","对于","同时","即使","n","linclas","各种","一些","全局","复杂","本原","快速","齐次","只是","选项","存取","有","规范","以","要点","sum","小小","一个","a","in","并且","相仿","网络结构","既","artificial",")","“","此","回归","classdef","交叉","不作","arg","取值","略微","单层","_","可选","一节","建立","要求","用来","效果","problem","opt","对","涉及","r","测度","了解","代码","amp","具体","在","成","灵犀","ann","安装","一番","就是","方面","性能","可加","基本功能","非凸","上手","可选项","唯一","i","推广","para","检查","convex","容易","l","范畴","页面","y","st","未知量","引入","p","logistic","]","第一个","所","原理",",","通过","接下来","集下","之前","hello","解","中","规划","读者","分开","实时","因此","确保","lista","详细","真正","项目","stystart","抢鲜","align","情况","讨论","生成","不","两种","可以","精确","达到","fae6a9","指","programming","编程","描述","解决","收敛","器","但","：","aligned","推导","graph","在线","性","这些","集","正是","由于","理论","旨在","等","试验","操作界面","mathbb","为","进行","；","注意","-","一点","概念","进而","实际上","1","问题","熵","解法","修改","函数参数","高质量","sigma","argpar","心有灵犀","#","操作","都","network","叫做","解不","pgd","将","如何","conditioned","探讨","stroke","alpha","上","¶","才能","合宜","可视","意味","它","条件","非线性","另","了","min","能"," ","领域","如果","0","基本概念","编写","并","相关","游乐场","class","二","合适","第一","人们","未知","感知","知机","改进","自定义","tensorflow","arugument","请","着眼于","例如","不会","ill","/","function","到","\\","神经网","我们","本章","知道","~","一次","数据","最小","供","”","游乐","这个","里","已经","playground","函数","空间","结果","。","凸函数","网络层","很多","观测","基本","优化","linear","有时","是","内容","实验","parsing","world","x","心思","知悉","rvert","纯粹","已知","层","参数","意味着","对象","学习","也","测试","nonlinreg","线性","初次","激活",":","解析","神经网络","做","通常","mathbf","f","漫谈","运用","sim","提到","least",".","体验","几个","试验场","本","维空间","指定","基本功","节","稳定","难以","neural","网络","api","你","质量","linreg","{","并非","常常","提出","demo","二乘","那么","与","主要","square","目前","多层"],"title":"本章总说","title_tokens":["本章","总说"]},{"location":"book-1-x/chapter-1/#_1","text":"摘要 本章将从线性问题入手，讨论最简单的分类与回归问题。我们将随机生成指定分布的数据，通过理论值，检查实验效果。这些简单的、解析的问题，可以提供读者一个直观的感受，并帮助读者快速上手Tensorflow的基本概念与运用。本章中，我们不会涉及任何数据存取、跟踪测度方面的概念，也不会将训练和测试分开，旨在使读者能集中心思到代码规范和Tensorflow的基本功能上。","text_tokens":["任何","分类","理论","训练","使","入手","指定","基本概念","旨在","感受","并","功能","基本功","也","讨论","生成","直观","方面","测试","线性","、","可以","回归","将","tensorflow","基本功能","。","跟踪","从","分布","上手","不会","基本","上","帮助","通过","概念","理论值","最","简单","解析","到","我们","检查","实验","快速","，","问题","本章","的","存取","运用","规范","与","效果","心思","数据","中","读者","一个","和","分开","代码","涉及","能"," ","这些","测度","提供","集中","摘要","随机"],"title":"从线性问题入门","title_tokens":["从","入门","线性","问题"]},{"location":"book-1-x/chapter-1/#_2","text":"在机器学习领域，线性问题既简单也不简单。目前神经网络主要是指多层、非凸的网络结构，常常用来解复杂的、难以推导的问题。但这并不意味着线性问题容易解决。相反，在很多情况下，线性问题是解不唯一的(undetermined)，解不稳定的(ill-posed/ill-conditioned)，条件的(constrained)。同时，为了得到一个快速收敛的、高质量的解，即使对线性问题，人们也在不断提出、改进解法。例如ISTA, AMP, PGD, LISTA, vAMP等算法，都用来解线性问题。在此不作详细展开。 我们已经知道，一个线性函数具有可加性，和一次齐次性，亦即 \\begin{align} f(x_1 + x_2) &= f(x_1) + f(x_2), \\\\ f(\\alpha x) &= \\alpha f(x). \\end{align} 因此，求解一个线性问题，我们需要将问题纯粹以 线性函数 进行描述。例如， \\begin{align} \\mathbf{y} \\sim \\mathbf{A}\\mathbf{x}. \\end{align} 具体而言， \\mathbf{x} \\mathbf{x} 是我们的已知数据， \\mathbf{y} \\mathbf{y} 是我们的未知量，我们需要找到一个合适的 \\mathbf{A} \\mathbf{A} 来确保 \\mathbf{x} \\mathbf{x} 能拟合到 \\mathbf{y} \\mathbf{y} 。如果此处 \\mathbf{y} \\in \\{0,~1\\}^p \\mathbf{y} \\in \\{0,~1\\}^p 是一个代表p类-分类的向量，那么这就是一个线性分类问题；相反，如果此处 \\mathbf{y} \\in \\mathbb{R}^p \\mathbf{y} \\in \\mathbb{R}^p 在连续p维空间取值，那么这就是一个线性回归问题。","text_tokens":["undetermined","求解","y","并","合适","人们","未知","下","未知量","改进","、","即","p","高质","而言","拟合","类","end","posed","例如",",","ill","/","简单","结构","到","\\","神经网","我们","的","得到","解","知道","begin","}","~","一次","constrained","数据","vamp","因此","确保","lista","算法","(","详细","为了","此处","已经","+","展开","情况","align","不","同时","即使","函数","空间","指","=","&","。","描述","解决","收敛","复杂","但","很多","不断","需要","这","具有","是","推导","快速","齐次","x","以","一个","a","纯粹","性","in","已知","^","分类","网络结构","既","意味着","等","学习","也","ista","线性","神经",")","找到","此","回归","不作","mathbb","取值","进行","；","-","可加性","神经网络","机器","2","mathbf","_","1","f","问题","解法","用来","sim","高质量","对","r","amp",".","具体","在","维空间","来","就是","稳定","都","难以","可加","解不","pgd","将","网络","亦","conditioned","非凸","质量","连续","alpha","向量","相反","唯一","{","常常","提出","代表","，","意味","那么","条件","主要","和","能","领域","容易"," ","如果","0","目前","多层"],"title":"漫谈线性问题","title_tokens":["漫谈","线性","问题"]},{"location":"book-1-x/chapter-1/#_3","text":"请注意，虽然我们在此处提到“ 线性问题 (Linear problem) ”，但我们指的并非“ 线性规划 (Linear programming) ”。虽然严格意义上，线性规划才是真正的线性问题，但我们在此处尚不讨论线性规划相关的内容，而是着眼于机器学习应用最普遍的两个领域， 分类 和 回归 上。实际上，这两种问题虽然求解的是线性函数，但本质上是凸问题。 例如，如果我们要求解回归问题，通常可以表述成 \\begin{equation} \\begin{aligned} \\arg \\min_{\\mathbf{A}}~& \\sum_{i=1}^N \\mathcal{L}(\\mathbf{A},~\\mathbf{x}_i,~\\mathbf{y}_i), \\\\ \\mathcal{L}(\\mathbf{A},~\\mathbf{x},~\\mathbf{y}) &= \\lVert \\mathbf{y} - \\mathbf{A}\\mathbf{x} \\rVert^2_2. \\end{aligned} \\end{equation} 虽然我们求解的模型 \\mathbf{A} \\mathbf{A} 是线性的，但我们优化的函数对象 \\mathcal{L} \\mathcal{L} 是一个 凸函数 (convex function) 。在此，我们可以将这个标量函数(同时也是凸函数)表述为 \\begin{align} \\forall~\\alpha,~\\beta,~\\mathbf{x}_1,~\\mathbf{x}_2,~\\mathcal{L}(\\alpha\\mathbf{x}_1 + \\beta\\mathbf{x}_2) \\leqslant \\alpha\\mathcal{L}(\\mathbf{x}_1) + \\beta\\mathcal{L}(\\mathbf{x}_2). \\end{align} 我们将这个问题表述为 最小二乘问题(Least-square problem) 。正是由于 \\mathbf{A} \\mathbf{A} 是线性的， \\mathcal{L} \\mathcal{L} 才能被确保为一个凸函数，进而，我们才能确保上述问题能得到精确的全局最小值解。","text_tokens":["l","forall","求解","y","相关","标量","才","mathcal","lvert","虽然","请","严格","着眼于","end","例如",",","最","而是","function","\\","我们","的","beta","意义","解","得到","}","begin","线性规划","~","规划","最小","确保","”","着眼","(","这个","此处","真正","+","align","讨论","不","函数","同时","两种","n","可以","精确","指","=","programming","&","。","凸函数","实际","全局","但","这","模型","linear","优化","aligned","是","应用","内容","x","尚","sum","equation","一个","a","rvert","^","分类","正是","由于","对象","学习","也","两个","线性",")","“","此","回归","arg","为","注意","被","-","普遍","进而","实际上","机器","通常","mathbf","2","_","1","问题","要求","problem","提到","least",".","上述","在","成","将","本质","最小值","alpha","上","{","并非","i","才能","凸","表述","，","二乘","和","leqslant","min","convex"," ","领域","如果","square","能"],"title":"线性问题与凸问题","title_tokens":["与","凸","线性","问题"]},{"location":"book-1-x/chapter-1/#tensorflow","text":"在本章接下来的内容里，我们将探讨Tensorflow如何求解一些简单的问题。对于初次上手的读者而言，在安装Tensorflow，亲自开始写一些project之前，笔者推荐你到这个游乐场“抢鲜体验”一番， Tensorflow Playground 在这个在线页面里，用户不需要有任何编程知识，可以通过直观的操作界面，建立一个简单的 多层感知机网络(有时也叫做Artificial Neural Network, ANN) ，并且实时观测网络的性能和测度。它提供了几个简单的二维数据集，供用户体验不同的数据集下，各种分类、回归问题的合宜解法。如果读者已经对神经网络的基本原理有所了解，相信能通过这个小小的实验场达到心有灵犀。我们在本章所做的project和demo，大体不跳出这个试验场的范畴，只是会略微复杂一点而已。","text_tokens":["感知机","范畴","求解","页面","游乐场","直观","感知","知机","大体","用户","、","而言","tensorflow","基本原理","知识","所","写","二维","原理",",","推荐","通过","简单","接下来","集下","到","跳出","神经网","之前","我们","的","本章","有所","实时","数据","读者","接下","供","提供","”","游乐","这个","任何","(","里","已经","抢鲜","playground","对于","不","可以","达到","各种","一些","。","编程","观测","复杂","本原","需要","基本","有时","project","内容","实验","只是","有","在线","小小","一个","集","下来","分类","并且","实验场","artificial","也","神经",")","“","回归","试验","操作界面","初次","略微","会","一点","神经网络","做","笔者","问题","解法","建立","对","相信","测度","了解","心有灵犀","体验","几个","不同","试验场","在","灵犀","操作","ann","安装","一番","network","叫做","性能","neural","将","网络","如何","探讨","你","上手","合宜","demo","，","它","开始","和","了","小小的","亲自","而已","能"," ","如果","界面","多层"],"title":"知悉Tensorflow","title_tokens":["tensorflow","知悉"]},{"location":"book-1-x/chapter-1/#_4","text":"下图展示了通过本章学习，能了解到的概念： graph LR st(Hello world!) --> linclas(线性分类) linclas --> linreg(线性回归) linreg --> nonlinreg(非线性回归) nonlinreg --> ed(非线性分类) lp[感知机] --> linclas sigma[Logsitc回归] --> linclas ce[交叉熵] --> linclas opt[优化器] --> linreg argpar[项目选项] --> linreg para[参数回归] --> nonlinreg sdlayer[自定义层] --> nonlinreg kernel[核函数] --> ed classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,linclas,linreg,nonlinreg,ed styStart Hello world : 首先，在本节，读者将了解Tensorflow的安装方法，并编写通过第一个简单的Tensorflow程序。 线性分类 : 本节通过解一个简单的二分类问题，我们将引入单层感知机、Logistic回归和交叉熵的概念，并且实验结果进行可视化。 线性回归 : 本节通过解一个与上一节难度相仿的，简单的回归问题，另读者能对比不同优化器的性能、特点，并介绍如何在项目中进行arugument parsing(引入项目的可选项)。 非线性回归 : 本节将修改上一节回归问题的激活函数，将线性的回归问题推广到解析函数参数回归的范畴。同时介绍自行编写网络层(类API)的方法。 非线性分类 : 本节将通过上一节的参数回归，引入核函数的概念，将线性分类问题推广到非线性空间里。","text_tokens":["感知机","范畴","编写","class","并",">","二","st","首先","第一","感知","定义","知机","引入","、","ba9132","logistic","]","自定义","第一个","tensorflow","arugument","类","lp",",",";","可视化","通过","简单","到","我们","介绍","的","hello","本章","解","!","特点","对比","[","读者","中","程序","(","里","项目","stystart","sdlayer","函数","同时","linclas","fae6a9","空间","结果","自行","。","网络层","lr","器","：","优化","graph","kernel","实验","parsing","logsitc","选项","world","一个","方法","分类","层","参数","并且","相仿","学习","nonlinreg","线性",")","回归","classdef","交叉","核","fill","下图","进行","激活","-",":","单层","概念","解析","熵","问题","可选","修改","一节","函数参数","opt","sigma","了解","argpar","不同","本","#","在","安装","ce","节","性能","本节","将","网络","如何","stroke","自定","api","linreg","可选项","上","推广","para","可视","，","展示","难度","与","非线性","另","和","了","ed","能"," "],"title":"本章要点","title_tokens":["本章","要点"]},{"location":"book-1-x/chapter-1/hello-world/","text":"Hello world! ¶ 摘要 本节包含一个简易的安装指导，主要提供给Windows上的GPU用户。之后，用户可以按照本节指导编写第一个测试程序。 安装Tensorflow ¶ 本节针对Windows使用GPU的用户，提供一个安装最新版Tensorflow的方法。大致的步骤符合以下两个教程： 通过pip安装Tensorflow Tensorflow的GPU支持 但是，需要指出的是，官方教程里有些部分已经不符合当前用户的实际情况，笔者经过摸索，总结以下的安装过程。 总体而言，安装Tensorflow需要用户确保以下条件 NVIDIA驱动已经达到411.31及以上 CUDA 10.0 (10.1不确定能否正常使用) CUDNN已经达到7.3.1及以上 CUPTI版本与CUDA相配（不需要专门安装，在安装CUDA的同时已经自动集成） 除此之外，涉及多GPU支持的库(NCCL和TensorRT)，这里不考虑。因此，我们主要需要确保前三条正常安装。 提示 Tensorflow官方提供的安装包是基于CUDA 9.0，CUDNN >= 7.2的基础上编译的。然而现在NVIDIA已经不再提供CUDA 9.0了。Windows用户可以安装的版本包括过时的8.1和最新的10.1。在这种情况下，官方还提供一个令用户自行从源码编译Tensorflow库的方案，但是对于Windows用户而言过于繁琐，容易出错，这里不建议按照官方教程。 更新NVIDIA驱动 ¶ 首先，我们可以开始更新我们的NVIDIA驱动。任何载有NVIDIA-GPU显卡的电脑，都应当在系统里已经预装好了 GeForce Experience 。点击如下图标开启GeForce Experience（如果是低版本的GeForce Experience，图标可能会不同）， 如左图，在主界面上，可以观察到，当驱动版本不够高时，会自动出现更新提示，依照提示更新即可。建议在更新驱动时，关闭其他所有程序，以免造成干扰。 更新NVIDIA driver 检查更新结果 如果更新完成，如右图，可以在设置界面检查到当前的版本已经达到最新。在安装过程中，由于驱动更新，屏幕可能会闪烁数次。在看到更新提示完成后建议重启。 提示 如果GeForce Experience提示无法更新驱动，可以考虑将机器恢复出厂设置，可能可以解决这一问题。 安装CUDA ¶ 驱动更新并重启后，需要安装最新版的Visual Studio(VS)，这是因为CUDA库本身对VS具有一定的支持，如果先安装CUDA，则这部分支持无法安装到位。这对我们使用Tensorflow并不造成妨害，但是对CUDA的用户并不是一个推荐的做法，所以无论如何，建议在第一步，安装VS，下面提供VS社区版（免费）的链接： Visual Studio官网 接下来，可以开始安装CUDA。进入CUDA安装包的下载链接 下载最新版CUDA CUDA官方指南 提示 须知，并非最新版就是最适合的版本。实际能使用的CUDA版本取决于后续步骤中预编译包的支持版本。就笔者写到这里时，最新的CUDA 10.1已然不匹配最新版预编译包支持的CUDA 10.0，由于版本差距很小，不确定是否会由此引发问题，但读者可以自行尝试。 现在的安装包已经支持在线安装包和离线安装包两种模式，读者可以根据自己的实际情况选择对应的版本。注意，安装CUDA的时候，CUDA可能要求我们翻新驱动版本，但通过GeForce Experience安装的驱动一般应已达到最新，新于CUDA内部提供的驱动。因此，我们需要选择定制安装模式，根据我们需要对比版本号，将驱动更新的选项取消，如下图所示 安装CUDA后，建议重启。 危险 特别需要注意的是，如果你的机器无法通过GeForce Experience更新驱动，那么在这里选择安装驱动也是枉然。并且，这种强制更新驱动的行为很可能导致系统崩溃。因此特别建议在安装CUDA之前一定要解决驱动更新的问题，哪怕是通过恢复出厂设置。 安装CUDNN ¶ 安装CUDA的同时，已经自带安装上了CUPTI。但是CUDNN并没有包含在CUDA中，因此，我们需要到以下链接下载CUDNN并查看安装说明 下载最新版CUDNN CUDNN官方指南 不同的是，CUDNN没有安装包，是以压缩包的形式下载到本地。解压后，其中应当包含如下目录及文件 . └─ cuda/ ├─ bin/ # Binary library │ └─ cudnn64_7.dll ├─ include/ # C++ Include file │ └─ cudnn.h ├─ lib/ # C++ Lib file │ └─ x64/ │ └─ cudnn.lib └─ NVIDIA_SLA_cuDNN_Support.txt # Readme file 假设我们CUDA的安装目录在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，将上述解压的 bin , include 和 lib 三个文件夹，直接拷贝覆盖到该安装目录下，即可完成CUDNN的安装。 进行完上述步骤后，我们还需要确保几个环境变量正确设置： 存在 CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，是我们的安装目录。理论上在CUDA安装后，该目录已经自动设置好。 环境变量 PATH 中，存在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin , C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/extras/CUPTI/libx64 两个路径，由于CUDNN已经配置在CUDA安装目录下，我们不需要像官方文档那样，添加第三个CUDNN的安装路径。 安装Anaconda ¶ Windows用户建议使用Anaconda管理python环境。作为一个开源的项目，Anaconda发行版已经集成了我们所需要的大多数python包，其中有些包是我们自己难以安装上的，例如支持python3的PIL。我们选用python3版的Anaconda，下载路径如下： Anaconda 最新版下载链接 注意选择x64版的安装包。安装结束后，我们已经有python3.7的环境了。鉴于有些情况下我们可能需要使用更早的python版本，接下来我们可以安装python3.6的虚环境。 Anaconda可以安装在用户或系统目录下。注意，如果安装在系统目录下，如果不在虚环境下，安装任何包都需要管理员模式。不过这不影响本教程的示例，因为我们将安装虚环境。直接打开 Anaconda Prompt ，并键入以下命令： conda create -n py36 python = 3 .6 anaconda 按照引导流程安装，经过一段时间后，我们将得到python3.6版的Anaconda。以后我们可以直接从开始菜单 Anaconda Prompt (py36) 进入该虚环境，也可以在基环境中键入 activate py36 转换到虚环境中。 提示 如果用户愿意，还可以通过 conda install -c conda-forge opencv 安装 opencv 。opencv是一个强大的图像处理库，这里python版的opencv比C++版更容易上手。一般该命令安装的是conda库中最新版的opencv（截至笔者写到这里时是opencv3.4.4）。然而，这样安装的并非是支持GPU的版本。尽管conda还提供一个GPU的opencv版本，可以通过 conda install -c patricksnape opencv-gpu 安装 opencv2 ，但它的版本还停留在2.4.9，和opencv3的用法殊有不同，请读者注意这一点。 安装预编译好的Tensorflow ¶ 可以通过 pip install tensorflow-gpu == 查看Tensorflow是否有官方发行的新版。当然，使用GPU的用户要特别注意最新版是否和你预装的驱动匹配，尤其是CUDA是否匹配，否则Tensorflow可能无法正常工作。 在官方发行版不适合我们使用的时候，我们也可以查看如下第三方发行的项目： fo40225/tensorflow-windows-wheel 该项目的维护者在不断根据Tensorflow的更新，预编译出适合不同版本的Tensorflow安装包。截至笔者写到这里为止，最新支持到r1.12版。 原则上来讲，我们选择尽可能新的版本。有时候官方发行版对CUDA的支持滞后，因此我们可以选择第三方版。无论选择哪种发行方，要安装Tensorflow，我们需要选择对应的GPU版，并在虚环境下执行以下命令： 官方版 pip install --upgrade tensorflow-gpu 第三方版 pip install https://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.12.0/py36/GPU/cuda100cudnn73sse2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl 第三方版(CPU AVX2加强) curl -o tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.001 https://raw.githubusercontent.com/fo40225/tensorflow-windows-wheel/master/1.12.0/py36/GPU/cuda100cudnn73avx2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.001 curl -o tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.002 https://raw.githubusercontent.com/fo40225/tensorflow-windows-wheel/master/1.12.0/py36/GPU/cuda100cudnn73avx2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.002 \"C:/Program Files/7-Zip/7z.exe\" x tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.001 # Note that you need to specify where your 7-zip gets installed. pip install tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl 等待一段时间后，安装将会结束。 Hello world! 测试 ¶ 撰写如下代码，保存到 hello-world.py 文件， Python 1 2 3 4 5 6 7 8 9 if __name__ == '__main__' : import tensorflow as tf test_str = tf . constant ( 'Hello, world!' ) test_cal = tf . reduce_sum ( tf . random_normal ([ 1000 , 1000 ])) with tf . Session () as sess : print ( 'Current Tensorflow version is:' , tf . __version__ ) print ( 'Test string: ' , sess . run ( test_str )) print ( 'Test calculation:' , sess . run ( test_cal )) Output Current Tensorflow version is: 1 .13.1 Test string: b 'Hello, world!' Test calculation: 955 .5544 在保存目录下，执行 python hello-world.py 第一次运行可能需要等待较长的初始化时间（硬件配置）。如果能正常运行，我们将看到如下结果： 其中，显示结果之前的记录信息反映了GPU已经可以正常工作。而“Test calculation”显示的结果是 10^6 10^6 个服从 \\mathcal{N}(0,1) \\mathcal{N}(0,1) 分布的随机数之和。根据相互独立的随机正态分布互不相关的特性，我们可以推算出，该结果服从 \\mathcal{N}(0,10^3) \\mathcal{N}(0,10^3) 的正态分布。","text_tokens":["（","假设",">","要","as","用户","正确","一步","而言","完","是因为","干扰","─","重启","处理","大致","过于","推荐","适合","行为","py","以后","最","cp36m","枉然","更","所以","其他","pil","进入","的","得到","avx2","强大","文件夹","方案","}","pip","!","大多数","if","开始菜单","服从","总体而言","activate","路径","设置","多","9","包括","应已","提供","有时候","name","离线","main","b","where","反映","(","截至","互不","右图","键入","文件","虚","殊有","forge","=","系统","自行","新于","手","实际","观察","不断","示例","需要","过程","这","具有","机数","5544","件夹","cuda","无论","出现","├","为止","driver","5","x64","维护者","之外","方法","正态分布","下来","^","is","流程","tensorrt","随机数","两个","cp36","current","开源","readme","这种","1000","电脑","安装包","后续","编译","955","会","note","机器","强制","不够","2","笔者","如","图像","lib","opencv","拷贝","更新","上述","不同","数次","看到","版本","解压","免费","说明","尝试","版本号","opencv2","系统目录","段时间","确定","命令","等待","从","分布","根据","upgrade","run","库","这一","，","可能","步骤","master","崩溃","开始","和","驱动","环境","界面","随机","目录","string","繁琐","首先","下","mathcal","显卡","amd64","githubusercontent","写","测试程序","当前","做法","存在","4","fo40225","图像处理","sla","没有","直接","官网","包是","对比","[","接下","该项","尽管","就","来讲","程序","constant","引发","任何","左图","无论如何","及","摸索","时候","对于","压缩包","到位","同时","prompt","第三个","现在","nvidia","n","菜单","cudnn","先","愿意","种","过时","2.4","初始化","c++","9.0","一般","撰写","所示","选项","4.4","有","能否","以","6","包","sum","哪怕","一个","这部分","7","output","8","还","硬件","并且","python","结束","个","calculation","三方","鉴于",")","411.31","“","toolkit","记录","取决","针对","指导","考虑","files","取决于","初始","_","中预","模式","引导","要求","点击","cupti","主","对","涉及","代码","加强","└","在","令","o","str","安装","按照","就是","不是","版","配置","blob","win","studio","选用","这里","当","导致","file","session","不过","尽可","检查","正常","图","新版","时间","自己","下面","容易","管理员","一段时间","发行版","install","基于","第一个","添加","]","提示","\"","所","raw",",","通过","接下来","一定","cal","已然","一段","之前","独立","hello","环境变","相互","则","中","读者","取消","导致系统","库中","因此","覆盖","妨害","│","即可","最新版","链接","当然","random","确保","libx64","目的","whl","to","installed","项目","情况","extras","002","不","print","两种","新","最新","可以","达到","作为","解决","部分","但","：","wheel","v10","变量","https","gets","test","curl","较长","图标","include","显示","txt","在线","匹配","滞后","预","your","以免","低版本","__","由于","geforce","特别","理论","开启","program","that","原则","001","高时","那样","gpu","1.12","conda","这样","下载","进行","注意","-","维护","anaconda","一点","信息","三条","10.1","哪","'","内部","1","依照","need","问题","将会","出错","10.0","早","教程","经过","patricksnape","影响","保存","you","#","zip","出","官方版","该","符合","专门","experience","很小","都","3","时","c","第三方","将","而","包含","如何","com","基","本身","上","所有","visual","尽可能","¶","使用","大多","指出","总结","library","第一次","它","条件","了","sess","之","能"," ","如果","以上","opencv3","0","normal","工作","tf","version","第一步","编写","并","翻新","无法","相关","第一","nccl","屏幕","预装","特性","算出","基础","tensorflow","好","cuda100cudnn73avx2","闪烁","停留","请","自动","例如","/","发行","cudnn64","到","\\","是否","方","h","用法","我们","之后","除此之外","对应","一次","低版","10","”","推算出","8.1","bin","里","选择","已经","这部","）","载有","第三","恢复","环境变量","path","须知","造成","结果","py36","。","出厂","不再","7.2","查看","有时","是","定制","原则上","world","支持","建议","x","危险","cuda100cudnn73sse2","指南","有些","压缩","vs","比","7.3","自带","13.1","差距","也","测试","形式","转换","执行","import","很","运行","python3","binary","reduce","尤其","以下","7z","exe","由此","因为","包都","给","r1",":","简易","computing","官方","文档","像","然而","但是","dll",".","本地","几个","本","源码","推算","github","with","多数","节","support","cpu","难以","打开","集成","社区","否则","12","关闭","后","或","如下","管理","总体","你","{","并非","前","完成","相配","除此","三个","那么","与","主要","其中","create","windows","应当","摘要","specify"],"title":"Hello world!","title_tokens":["world"," ","!","hello"]},{"location":"book-1-x/chapter-1/hello-world/#hello-world","text":"摘要 本节包含一个简易的安装指导，主要提供给Windows上的GPU用户。之后，用户可以按照本节指导编写第一个测试程序。","text_tokens":["编写","安装","按照","节","第一","测试","用户","可以","第一个","gpu","包含","。","给","测试程序","简易","上","指导","之后","的","，","主要","一个","windows"," ","提供","摘要","程序","本"],"title":"Hello world!","title_tokens":["world"," ","!","hello"]},{"location":"book-1-x/chapter-1/hello-world/#tensorflow","text":"本节针对Windows使用GPU的用户，提供一个安装最新版Tensorflow的方法。大致的步骤符合以下两个教程： 通过pip安装Tensorflow Tensorflow的GPU支持 但是，需要指出的是，官方教程里有些部分已经不符合当前用户的实际情况，笔者经过摸索，总结以下的安装过程。 总体而言，安装Tensorflow需要用户确保以下条件 NVIDIA驱动已经达到411.31及以上 CUDA 10.0 (10.1不确定能否正常使用) CUDNN已经达到7.3.1及以上 CUPTI版本与CUDA相配（不需要专门安装，在安装CUDA的同时已经自动集成） 除此之外，涉及多GPU支持的库(NCCL和TensorRT)，这里不考虑。因此，我们主要需要确保前三条正常安装。 提示 Tensorflow官方提供的安装包是基于CUDA 9.0，CUDNN >= 7.2的基础上编译的。然而现在NVIDIA已经不再提供CUDA 9.0了。Windows用户可以安装的版本包括过时的8.1和最新的10.1。在这种情况下，官方还提供一个令用户自行从源码编译Tensorflow库的方案，但是对于Windows用户而言过于繁琐，容易出错，这里不建议按照官方教程。","text_tokens":["（","繁琐",">","nccl","下","基于","用户","而言","基础","tensorflow","提示","自动","大致","过于","通过","当前","我们","的","除此之外","方案","pip","总体而言","因此","多","最新版","包括","提供","确保","(","8.1","里","已经","及","情况","摸索","对于","不","）","同时","现在","nvidia","最新","达到","可以","=","自行","cudnn","。","实际","过时","不再","部分","需要","过程","7.2","：","是","9.0","支持","建议","cuda","能否","一个","有些","之外","方法","还","7.3","tensorrt","两个",")","411.31","gpu","这种","以下","安装包","针对","编译","考虑","10.1","三条","笔者","1","官方","出错","10.0","然而","cupti","但是","涉及","教程","经过",".","本","版本","在","源码","令","安装","按照","节","符合","专门","集成","确定","这里","从","总体","上","前","使用","相配","库","指出","总结","正常","，","新版","步骤","除此","条件","与","和","主要","驱动","了","windows"," ","容易","以上"],"title":"安装Tensorflow","title_tokens":["安装","tensorflow"]},{"location":"book-1-x/chapter-1/hello-world/#nvidia","text":"首先，我们可以开始更新我们的NVIDIA驱动。任何载有NVIDIA-GPU显卡的电脑，都应当在系统里已经预装好了 GeForce Experience 。点击如下图标开启GeForce Experience（如果是低版本的GeForce Experience，图标可能会不同）， 如左图，在主界面上，可以观察到，当驱动版本不够高时，会自动出现更新提示，依照提示更新即可。建议在更新驱动时，关闭其他所有程序，以免造成干扰。 更新NVIDIA driver 检查更新结果 如果更新完成，如右图，可以在设置界面检查到当前的版本已经达到最新。在安装过程中，由于驱动更新，屏幕可能会闪烁数次。在看到更新提示完成后建议重启。 提示 如果GeForce Experience提示无法更新驱动，可以考虑将机器恢复出厂设置，可能可以解决这一问题。","text_tokens":["（","无法","首先","屏幕","预装","好","提示","干扰","闪烁","显卡","自动","重启","当前","到","其他","我们","的","低版","中","设置","即可","程序","任何","左图","里","已经","右图","）","载有","nvidia","最新","可以","达到","恢复","造成","结果","系统","。","出厂","解决","观察","过程","是","图标","建议","出现","driver","以免","低版本","由于","geforce","开启","高时","gpu","电脑","-","会","考虑","机器","不够","依照","如","问题","点击","主","更新","不同","数次","看到","版本","在","安装","experience","都","时","将","关闭","后","如下","当","上","所有","完成","检查","这一","，","可能","开始","了","驱动"," ","如果","界面","应当"],"title":"更新NVIDIA驱动","title_tokens":["nvidia","驱动","更新"]},{"location":"book-1-x/chapter-1/hello-world/#cuda","text":"驱动更新并重启后，需要安装最新版的Visual Studio(VS)，这是因为CUDA库本身对VS具有一定的支持，如果先安装CUDA，则这部分支持无法安装到位。这对我们使用Tensorflow并不造成妨害，但是对CUDA的用户并不是一个推荐的做法，所以无论如何，建议在第一步，安装VS，下面提供VS社区版（免费）的链接： Visual Studio官网 接下来，可以开始安装CUDA。进入CUDA安装包的下载链接 下载最新版CUDA CUDA官方指南 提示 须知，并非最新版就是最适合的版本。实际能使用的CUDA版本取决于后续步骤中预编译包的支持版本。就笔者写到这里时，最新的CUDA 10.1已然不匹配最新版预编译包支持的CUDA 10.0，由于版本差距很小，不确定是否会由此引发问题，但读者可以自行尝试。 现在的安装包已经支持在线安装包和离线安装包两种模式，读者可以根据自己的实际情况选择对应的版本。注意，安装CUDA的时候，CUDA可能要求我们翻新驱动版本，但通过GeForce Experience安装的驱动一般应已达到最新，新于CUDA内部提供的驱动。因此，我们需要选择定制安装模式，根据我们需要对比版本号，将驱动更新的选项取消，如下图所示 安装CUDA后，建议重启。 危险 特别需要注意的是，如果你的机器无法通过GeForce Experience更新驱动，那么在这里选择安装驱动也是枉然。并且，这种强制更新驱动的行为很可能导致系统崩溃。因此特别建议在安装CUDA之前一定要解决驱动更新的问题，哪怕是通过恢复出厂设置。","text_tokens":["（","第一步","并","翻新","无法","要","第一","用户","一步","tensorflow","是因为","提示","重启","写","推荐","适合","通过","行为","做法","最","接下来","枉然","到","一定","是否","所以","已然","之前","我们","进入","的","则","对应","官网","对比","读者","取消","接下","导致系统","因此","妨害","设置","链接","最新版","就","应已","提供","离线","引发","(","无论如何","选择","已经","这部","情况","时候","不","到位","）","现在","两种","最新","可以","达到","恢复","须知","造成","系统","自行","先","。","新于","出厂","实际","解决","部分","但","需要","这","：","具有","是","定制","一般","所示","选项","支持","建议","危险","cuda","无论","包","在线","哪怕","一个","指南","这部分","匹配","预","下来","vs","由于","geforce","特别","并且","差距","也",")","很","这种","取决","由此","因为","安装包","下载","后续","编译","注意","会","10.1","取决于","机器","强制","笔者","内部","中预","问题","官方","模式","要求","10.0","但是","对","更新","版本","在","安装","experience","就是","不是","版","很小","免费","时","尝试","版本号","社区","将","studio","后","如何","这里","确定","如下","导致","你","根据","本身","visual","并非","使用","库","图","，","新版","可能","步骤","那么","崩溃","开始","和","驱动","自己","下面","能"," ","如果"],"title":"安装CUDA","title_tokens":["安装","cuda"]},{"location":"book-1-x/chapter-1/hello-world/#cudnn","text":"安装CUDA的同时，已经自带安装上了CUPTI。但是CUDNN并没有包含在CUDA中，因此，我们需要到以下链接下载CUDNN并查看安装说明 下载最新版CUDNN CUDNN官方指南 不同的是，CUDNN没有安装包，是以压缩包的形式下载到本地。解压后，其中应当包含如下目录及文件 . └─ cuda/ ├─ bin/ # Binary library │ └─ cudnn64_7.dll ├─ include/ # C++ Include file │ └─ cudnn.h ├─ lib/ # C++ Lib file │ └─ x64/ │ └─ cudnn.lib └─ NVIDIA_SLA_cuDNN_Support.txt # Readme file 假设我们CUDA的安装目录在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，将上述解压的 bin , include 和 lib 三个文件夹，直接拷贝覆盖到该安装目录下，即可完成CUDNN的安装。 进行完上述步骤后，我们还需要确保几个环境变量正确设置： 存在 CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，是我们的安装目录。理论上在CUDA安装后，该目录已经自动设置好。 环境变量 PATH 中，存在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin , C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/extras/CUPTI/libx64 两个路径，由于CUDNN已经配置在CUDA安装目录下，我们不需要像官方文档那样，添加第三个CUDNN的安装路径。","text_tokens":["目录","并","假设","下","正确","添加","完","好","─","自动",",","/","cudnn64","存在","到","h","我们","的","环境变","文件夹","sla","没有","直接","中","因此","覆盖","│","设置","链接","最新版","即可","路径","确保","libx64","bin","已经","文件","及","extras","压缩包","不","同时","第三","第三个","nvidia","最新","环境变量","path","=","cudnn","。","需要","查看","：","是","c++","v10","变量","件夹","include","txt","cuda","以","指南","├","7","压缩","x64","还","由于","理论","自带","program","形式","两个","那样","toolkit","binary","gpu","readme","以下","安装包","下载","进行",":","files","computing","_","官方","文档","lib","像","cupti","但是","拷贝","dll",".","└","本地","不同","上述","几个","#","在","安装","该","解压","support","说明","配置","c","将","包含","后","如下","file","上","完成","，","新版","步骤","三个","library","和","了","其中"," ","0","环境","应当"],"title":"安装CUDNN","title_tokens":["安装","cudnn"]},{"location":"book-1-x/chapter-1/hello-world/#anaconda","text":"Windows用户建议使用Anaconda管理python环境。作为一个开源的项目，Anaconda发行版已经集成了我们所需要的大多数python包，其中有些包是我们自己难以安装上的，例如支持python3的PIL。我们选用python3版的Anaconda，下载路径如下： Anaconda 最新版下载链接 注意选择x64版的安装包。安装结束后，我们已经有python3.7的环境了。鉴于有些情况下我们可能需要使用更早的python版本，接下来我们可以安装python3.6的虚环境。 Anaconda可以安装在用户或系统目录下。注意，如果安装在系统目录下，如果不在虚环境下，安装任何包都需要管理员模式。不过这不影响本教程的示例，因为我们将安装虚环境。直接打开 Anaconda Prompt ，并键入以下命令： conda create -n py36 python = 3 .6 anaconda 按照引导流程安装，经过一段时间后，我们将得到python3.6版的Anaconda。以后我们可以直接从开始菜单 Anaconda Prompt (py36) 进入该虚环境，也可以在基环境中键入 activate py36 转换到虚环境中。 提示 如果用户愿意，还可以通过 conda install -c conda-forge opencv 安装 opencv 。opencv是一个强大的图像处理库，这里python版的opencv比C++版更容易上手。一般该命令安装的是conda库中最新版的opencv（截至笔者写到这里时是opencv3.4.4）。然而，这样安装的并非是支持GPU的版本。尽管conda还提供一个GPU的opencv版本，可以通过 conda install -c patricksnape opencv-gpu 安装 opencv2 ，但它的版本还停留在2.4.9，和opencv3的用法殊有不同，请读者注意这一点。","text_tokens":["目录","一段时间","（","并","发行版","下","install","用户","提示","停留","请","所","写","处理","例如","通过","发行","以后","接下来","更","到","一段","用法","我们","pil","图像处理","进入","的","得到","强大","大多数","包是","直接","开始菜单","中","读者","接下","activate","库中","路径","尽管","链接","最新版","9","提供","任何","(","截至","选择","已经","键入","项目","虚","殊有","情况","不","）","菜单","prompt","n","最新","可以","forge","作为","=","py36","系统","愿意","。","手","但","2.4","示例","需要","这","：","是","c++","一般","建议","支持","有","4.4","6","包","一个","有些","7","x64","下来","还","比","python","结束","流程","也","转换","鉴于",")","python3","开源","gpu","conda","以下","安装包","因为","包都","这样","下载","注意","-","anaconda","一点","笔者","模式","引导","图像","早","然而","opencv","教程","经过","patricksnape","影响",".","不同","本","版本","在","安装","多数","按照","该","版","3","时","难以","打开","集成","c","opencv2","系统目录","将","选用","段时间","后","或","这里","管理","如下","命令","从","基","上","并非","使用","大多","不过","库","，","新版","可能","时间","它","开始","和","了","其中","自己","create","windows"," ","如果","容易","opencv3","环境","管理员"],"title":"安装Anaconda","title_tokens":["安装","anaconda"]},{"location":"book-1-x/chapter-1/hello-world/#tensorflow_1","text":"可以通过 pip install tensorflow-gpu == 查看Tensorflow是否有官方发行的新版。当然，使用GPU的用户要特别注意最新版是否和你预装的驱动匹配，尤其是CUDA是否匹配，否则Tensorflow可能无法正常工作。 在官方发行版不适合我们使用的时候，我们也可以查看如下第三方发行的项目： fo40225/tensorflow-windows-wheel 该项目的维护者在不断根据Tensorflow的更新，预编译出适合不同版本的Tensorflow安装包。截至笔者写到这里为止，最新支持到r1.12版。 原则上来讲，我们选择尽可能新的版本。有时候官方发行版对CUDA的支持滞后，因此我们可以选择第三方版。无论选择哪种发行方，要安装Tensorflow，我们需要选择对应的GPU版，并在虚环境下执行以下命令： 官方版 pip install --upgrade tensorflow-gpu 第三方版 pip install https://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.12.0/py36/GPU/cuda100cudnn73sse2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl 第三方版(CPU AVX2加强) curl -o tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.001 https://raw.githubusercontent.com/fo40225/tensorflow-windows-wheel/master/1.12.0/py36/GPU/cuda100cudnn73avx2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.001 curl -o tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.002 https://raw.githubusercontent.com/fo40225/tensorflow-windows-wheel/master/1.12.0/py36/GPU/cuda100cudnn73avx2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.002 \"C:/Program Files/7-Zip/7z.exe\" x tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.001 # Note that you need to specify where your 7-zip gets installed. pip install tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl 等待一段时间后，安装将会结束。","text_tokens":["一段时间","并","要","无法","install","发行版","下","用户","预装","tensorflow","cuda100cudnn73avx2","\"","amd64","githubusercontent","写","raw","适合","/","通过","发行","cp36m","到","是否","方","一段","fo40225","我们","的","avx2","对应","pip","该项","因此","当然","最新版","有时候","目的","来讲","where","whl","to","(","截至","installed","选择","项目","虚","时候","002","不","第三","新","最新","可以","=","py36","。","种","不断","查看","需要","：","有时","wheel","是","https","gets","curl","原则上","支持","x","有","cuda","无论","cuda100cudnn73sse2","为止","匹配","滞后","预","7","your","维护者","特别","结束","program","也","三方","执行","原则","that","cp36",")","001","gpu","尤其","1.12","以下","7z","exe","安装包","注意","编译","r1","-","维护",":","files","哪","note","笔者","_","need","将会","官方","对","更新",".","加强","不同","you","#","版本","在","zip","github","o","出","官方版","安装","版","cpu","blob","win","否则","c","第三方","12","段时间","后","这里","如下","命令","com","等待","你","根据","upgrade","尽可能","使用","尽可","正常","，","新版","可能","时间","master","和","驱动","windows"," ","环境","0","工作","specify"],"title":"安装预编译好的Tensorflow","title_tokens":["好","tensorflow","预","安装","的","编译"]},{"location":"book-1-x/chapter-1/hello-world/#hello-world_1","text":"撰写如下代码，保存到 hello-world.py 文件， Python 1 2 3 4 5 6 7 8 9 if __name__ == '__main__' : import tensorflow as tf test_str = tf . constant ( 'Hello, world!' ) test_cal = tf . reduce_sum ( tf . random_normal ([ 1000 , 1000 ])) with tf . Session () as sess : print ( 'Current Tensorflow version is:' , tf . __version__ ) print ( 'Test string: ' , sess . run ( test_str )) print ( 'Test calculation:' , sess . run ( test_cal )) Output Current Tensorflow version is: 1 .13.1 Test string: b 'Hello, world!' Test calculation: 955 .5544 在保存目录下，执行 python hello-world.py 第一次运行可能需要等待较长的初始化时间（硬件配置）。如果能正常运行，我们将看到如下结果： 其中，显示结果之前的记录信息反映了GPU已经可以正常工作。而“Test calculation”显示的结果是 10^6 10^6 个服从 \\mathcal{N}(0,1) \\mathcal{N}(0,1) 分布的随机数之和。根据相互独立的随机正态分布互不相关的特性，我们可以推算出，该结果服从 \\mathcal{N}(0,10^3) \\mathcal{N}(0,10^3) 的正态分布。","text_tokens":["目录","tf","version","string","（","相关","第一","as","下","特性","算出","]","mathcal","tensorflow",",","py","到","\\","cal","4","独立","之前","我们","hello","的","相互","}","!","一次","if","10","服从","[","name","9","random","main","反映","”","b","推算出","constant","(","互不","已经","文件","print","）","n","可以","=","结果","。","需要","：","初始化","是","机数","test","5544","较长","撰写","显示","world","6","sum","5","7","output","8","正态分布","__","^","硬件","python","is","13.1","个","calculation","随机数","import","执行",")","current","运行","“","reduce","gpu","记录","1000","-",":","955","信息","2","'","初始","_","1","代码",".","保存","看到","在","推算","with","str","该","3","配置","将","而","如下","等待","分布","根据","{","run","session","正常","，","可能","时间","第一次","和","其中","sess","了","之","能"," ","如果","0","normal","工作","随机"],"title":"Hello world! 测试","title_tokens":[" ","hello","测试","world","!"]},{"location":"book-1-x/chapter-1/linear-classification/","text":"线性分类 ¶ 摘要 本节介绍如何使用顺序模型(sequential model)来编写一个线性分类器，使用sigmoid函数激活，并验证其效果。在本节我们将编写第一个Project，并介绍一些基本概念、和一个推荐的Tensorflow Project的编写格式。 理论 ¶ 问题描述 ¶ 考虑我们有如下的二分类数据集 (\\mathbf{x},~y_i) \\in \\mathbb{D}_i (\\mathbf{x},~y_i) \\in \\mathbb{D}_i ，并且有一个未知的常数向量 \\mathbf{a}_i \\mathbf{a}_i 和未知的常数标量 c_i c_i ，使得： \\begin{equation} y_i = \\left\\{ \\begin{aligned} 0, && \\mathbf{a}_i^T \\mathbf{x} + c_i \\leqslant 0, \\\\ 1, && \\mathbf{a}_i^T \\mathbf{x} + c_i > 0. \\end{aligned} \\right. \\end{equation} 其中， \\mathbf{a} \\mathbf{a} 可以看成是某超平面的（未标准化的）法向量，那么 \\mathbf{a}^T \\mathbf{x} + c = 0 \\mathbf{a}^T \\mathbf{x} + c = 0 是该超平面的截距式定义，亦即该平面与 x_i x_i 轴的交点可以显式表述为 x_i^{(0)} = - \\frac{c}{a_i} x_i^{(0)} = - \\frac{c}{a_i} 。由此可知，式 (1) (1) 显式定义了一个点在超平面的哪一侧。特别地，若 \\mathbf{x} \\mathbf{x} 是一个二维向量，则该超平面退化为一维平面；若 \\mathbf{x} \\mathbf{x} 是一个标量，则该超平面退化为一条直线。 若我们定义 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} ，有 \\begin{equation} \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, ~~ \\mathbf{A} = \\begin{bmatrix} \\mathbf{a}^T_1 \\\\ \\mathbf{a}^T_2 \\\\ \\vdots \\\\ \\mathbf{a}^T_n \\end{bmatrix}, ~~ \\mathbf{c} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}. \\end{equation} 则我们可以认为 \\begin{align} \\mathbf{y} = \\left\\{ \\begin{bmatrix}\\hat{y}_1 > 0 & \\hat{y}_2 > 0 & \\cdots & \\hat{y}_n > 0\\end{bmatrix}^T, ~ \\left| ~ \\hat{\\mathbf{y}} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon} \\right. \\right\\}, \\end{align} 其中 \\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon} 是一个定义噪声的向量。 我们可以把向量 \\mathbf{y} \\mathbf{y} 的元素看成是互不相关的多个超平面对向量 \\mathbf{x} \\mathbf{x} 各自独立的分类结果。即 y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} 。由于每个超平面构成一个二分类，如果把每个二分类看作是向量是否属于这个类的测度，那么 \\mathbf{y} \\mathbf{y} 可以被看作是一个多分类的结果，尽管向量 \\mathbf{x} \\mathbf{x} 可能被同时分入多个类中。 假设我们的数据集 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} 符合 (3) (3) 定义的数据分布特征。我们的基本要求是，在我们不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 的情况下，使用大量 (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} 样本训练一个线性分类器，使得当我们给定任意一个新样本 \\mathbf{x} \\mathbf{x} 的时候，分类器能推断出其对应的 \\mathbf{y} \\mathbf{y} 来（亦即是否属于该分类）。 须知 请注意我们在这里说到“线性分类器”，虽然使用“线性”一词，但准确来说，这是一个仿射变换。因为线性变换要求有齐次性，即 f(x) = \\alpha f(x) f(x) = \\alpha f(x) ，但仿射变换允许我们引入一个平移向量 \\mathbf{b} \\mathbf{b} 。当然，我们的求解的线性问题本身也是一个仿射变换。 在这个问题里，我们虽然不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，但我们知道由 (3) (3) 确定的线性关系，因此，我们可以随机生成一组 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ，构建线性模型： \\begin{align} \\tilde{\\mathbf{y}} = \\sigma ( \\mathbf{W} \\mathbf{x} + \\mathbf{b} ). \\end{align} 其中，可微函数 \\sigma \\sigma 是一个将实数空间 \\mathbb{R}^n \\mathbb{R}^n 映射到有限范围的实数空间 [0,~1]^n [0,~1]^n 内的函数。特别地， \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 。因此，可以将 \\sigma \\sigma 看作是二分类布尔函数的插值函数。理论上，只要我们找到 \\mathbf{W}=\\mathbf{A} \\mathbf{W}=\\mathbf{A} ， \\mathbf{b}=\\mathbf{c} \\mathbf{b}=\\mathbf{c} ，则该线性分类器可以直接拟合出原分布来。 感知机 ¶ 我们将 (4) (4) 定义的线性模型称为 单层感知机 (Single-layer perceptron) 模型。它包含一个权重矩阵 \\mathbf{W} \\mathbf{W} 和一个偏置矩阵 \\mathbf{b} \\mathbf{b} 。事实上，可以将 (4) (4) 改写成如下形式 \\begin{align} \\tilde{\\mathbf{y}} = \\sigma \\left( \\begin{bmatrix} \\mathbf{W} & \\mathbf{b} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ 1 \\end{bmatrix} \\right). \\end{align} 可见偏置本身可以看成是输入向量多了一个常数元素的等价模型。 感知机是最早的神经网络形式，它非常孱弱，只能解线性问题，但却为神经网络后来的发展开了先河。在单层感知机里，我们视输入向量 \\mathbf{x} \\mathbf{x} 的每个元素为一个“神经元”，矩阵 \\mathbf{W} \\mathbf{W} 和偏置 \\mathbf{b} \\mathbf{b} 将我们的输入映射到输出层 \\mathbf{y} \\mathbf{y} ，输出层的每个元素也视为一个神经元。在这个过程中， W_{ij} W_{ij} 作为 i i 行 j j 列的元素，代表了连接两个神经元的权重。我们用红线代表正值，蓝线代表负值，感知机可以被图示为 线性感知机的输出也是输入的线性组合，但我们可以添加激活函数，即 \\sigma(\\cdot) \\sigma(\\cdot) 将其映射到非线性空间。这要求我们添加的激活函数是一个非线性函数。 事实上，将单层感知机层叠，前一层的输出作为后一层的输入，就构建出早期的神经网络。这种网络每一层都是全连接的（两个神经元之间总是有权重，尽管值可能为0），每一层都有激活函数。理论上，任意一个两层堆叠的感知机，只要神经元数目足够多，就可以拟合出任意一个非线性函数。然而，实际测试中，这一理论的效果并不尽如人意，因此又有陆续地改进，才有了后来的深度学习。饮水思源，鉴往知来，我们也将从这个简简单单的单层模型开始，走上学习“深度学习”之旅。 Sigmoid函数 ¶ 在上述介绍中，我们没有解决的两个问题是， 如何定义插值函数 \\sigma \\sigma ？ 如何找到合适的 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ？ 我们首先讨论第一个问题。一般地，多分类问题中，如果各个分类彼此并非相斥，且不一定要将结果分入任一类的话，我们可以用 Sigmoid 函数来定义 \\sigma \\sigma ，亦即 \\begin{align} \\sigma(\\mathbf{x}) = \\frac{1}{ 1 + e^{-\\mathbf{x}}}. \\end{align} 它同时满足 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 ，且是一个单调函数。以下代码向我们展示了这种函数的特性： Python 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import matplotlib.pyplot as plt def test_sigmoid (): x = np . linspace ( - 5 , 5 , 100 ) y = 1 / ( 1 + np . exp ( - x ) ) plt . plot ( x , y ) plt . xlabel ( 'x' ), plt . ylabel ( 'y' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () if __name__ == '__main__' : test_sigmoid () Output 使用sigmoid函数的一大好处是，它的导数求解非常简单，很适合用来做神经网络这样一个复杂模型的激活函数。注意虽然 \\sigma(\\mathbf{x}) \\sigma(\\mathbf{x}) 和 \\mathbf{x} \\mathbf{x} 都是向量，这意味着导数是Jacobian矩阵，但由于 \\sigma \\sigma 是一个对 \\mathbf{x} \\mathbf{x} 各元素独立的解析函数，这个Jacobian矩阵实际上是一个对角矩阵，对角线上第j个元素的值为 \\begin{align} \\left. \\frac{ \\partial \\sigma(x) }{ \\partial x } \\right|_{x=x_j} = \\left. - e^{-x} \\left( - \\frac{1}{\\left( 1+e^{-x} \\right)^2} \\right) \\right|_{x=x_j} = - \\sigma(x_j) ( 1 - \\sigma(x_j) ). \\end{align} 可见，该函数的导数和计算函数本身的复杂度相若，可以做到快速求导。 求解问题 ¶ 接下来，我们需要解决第二个问题，亦即找到 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} 。这一问题通常可以写成反问题的形式： \\begin{align} \\arg \\min_\\limits{\\mathbf{W},~\\mathbf{b}} \\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}^{(k)},~ \\sigma ( \\mathbf{W} \\mathbf{x}^{(k)} + \\mathbf{b} ) \\right). \\end{align} 最简单的情况下，我们可以把 损失函数(loss function) 定义为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = \\lVert \\mathbf{y} - \\tilde{\\mathbf{y}} \\rVert_2^2. \\end{align} 我们称 (8) (8) 为 逻辑斯蒂回归(logistic regression) 。有趣的是，虽然这个术语叫“回归”，但它解的其实是个分类问题。但是，既然这是一个分类问题，我们可以不使用这个损失函数，而是从概率论的角度看待这个问题。由此，我们引出一个新的损失函数：“交叉熵”。 交叉熵 ¶ 我们视sigmoid函数输出的值为一个概率，表示分类器对预测结果的确信程度，记 \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} ，则 \\begin{equation} \\begin{aligned} \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}), \\\\ \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) &= 1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}). \\end{aligned} \\end{equation} 注意这里的概率向量的含义是，其中第i个元素表明第i个超平面分类结果的 确信程度 。 然而，这个概率只是分类器对分类结果的确信程度，却并非是分类准确度的概率，实际上，分类 准确度 的概率，应当表述为 \\begin{equation} \\begin{aligned} \\mathbf{p}(\\mathbf{y}|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta})^{1-\\mathbf{y}}\\\\ &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\left(1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}) \\right)^{1-\\mathbf{y}}. \\end{aligned} \\end{equation} 注意 这里 \\mathbf{x}^{\\mathbf{y}} \\mathbf{x}^{\\mathbf{y}} 表示的是对每个元素一一求取指数，即函数第i个元素的返回值应当为 {x_i}^{y_i} {x_i}^{y_i} 。 我们使用真实值 \\mathbf{y} \\mathbf{y} 作为指数给概率向量加权。当 \\mathbf{y}=1 \\mathbf{y}=1 时，乘积项的第二个因子消去，该函数退化为以预测值为1的可信度 \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) ；反之则第一个因子消去，退化为以预测值为0的可信度 \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) 。这就是最大似然估计方法。至此，我们可以写出似然估计函数 \\begin{align} L(\\boldsymbol{\\Theta}) = \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}). \\end{align} 对似然估计函数取对数，则有 \\begin{equation} \\begin{aligned} l(\\boldsymbol{\\Theta}) &= \\sum_{k=1}^N \\log \\left( \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}) \\right) \\\\ &= \\sum_{k=1}^N \\mathbf{y}^{(k)} \\cdot \\log\\left(\\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right) + \\left(1 - \\mathbf{y}^{(k)} \\right) \\cdot \\log\\left(1 - \\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right). \\end{aligned} \\end{equation} 我们最终的目的是要最大化似然函数，亦即 \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) ，这等价于最小化 -l(\\boldsymbol{\\Theta}) -l(\\boldsymbol{\\Theta}) 。对比 (8) (8) 和 (9) (9) ，于是我们可以定义交叉熵为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = - \\mathrm{mean}\\left[ \\mathbf{y} \\cdot \\log\\left( \\tilde{\\mathbf{y}} \\right) + \\left(1 - \\mathbf{y} \\right) \\cdot \\log\\left(1 - \\tilde{\\mathbf{y}} \\right) \\right]. \\end{align} 注意这里我们使用 \\mathrm{mean}\\left[ \\cdot \\right] \\mathrm{mean}\\left[ \\cdot \\right] 表示求取一个向量所有元素的平均值。实际上，Tensorflow允许我们定义损失函数的输出为一个和输出向量维度相同的向量，Tensorflow自带的交叉熵也是这样定义的。实际应用时，Tensorflow会自动在向量维度上求均值，并压缩成上述 (14) (14) 的形式。 若我们记 \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) ，代入sigmoid函数，为了确保该损失函数的稳定性，我们可以将 (14) (14) 整理为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{x}} \\right) = \\mathrm{mean}\\left[ \\max(\\tilde{\\mathbf{x}}, \\mathbf{0}) - \\tilde{\\mathbf{x}} \\cdot \\mathbf{y} + \\log\\left(1 + e^{-|\\tilde{\\mathbf{x}}|} \\right) \\right]. \\end{align} 提示 这里交叉熵整理的推导过程参见 Tensorflow-API官方文档 。 实际情况下，我们使用 (15) (15) 来求取sigmoid函数激活下的交叉熵。 解线性多分类问题 ¶ 接下来，我们将开始实战上手，编写我们的第一个Project。虽然一个Project的格式并无定法，每个人按照自己的喜好会选择不同的风格，但一个从无受过训练的人，往往写出的Project几乎完全不具有可读性。实际上，学习任何语言， 变量命名规范 、 缩进规范 以及 模块化 、 面向对象 等都被认为是编写一个具有可读性的代码所不得不知的概念。本教程所推荐的代码格式，均具有统一的风格，读者在了解每个Project和其对应的教程时，会慢慢熟悉这种风格的特点。愿读者能从这样的风格中得到启发，得到 代码可读性 的神髓。 代码规范 ¶ 建立一个具有较强可读性的Tensorflow工程需要我们活用python的模块化设计。我们通常推荐以下的结构 . ├─ data/ # where we store our data │ └─ ... ├─ tools.py # codes for post-processing and analyzing records. ├─ extension.py # codes for extending the tensorflow model. ├─ dparser.py # data parser └─ main.py # main module where we define our tensorflow model. 除了保存数据的文件夹，我们应当有三个子模块。其中 tool : 用来处理、分析生成的数据，通常与Tensorflow无关； extension : 用来扩展tensorflow，例如在这里自定义网络层和操作符； dparser : 数据处理器，用来读取并预处理送入网络的数据； main : 主模块，只定义跟Tensorflow模型有关的内容，需要引用 extension 和 dparser 。 视情况可以灵活调整结构，但建议将定义Tensorflow模型的代码单独放在主模块里，和其他外围代码分离。 撰写各个模块时，建议使用类封装各组功能相同的函数。具有良好使用习惯的coder应当注意给各个面向用户的类、函数撰写（哪怕简短的）说明文字，在一些较长的函数、方法的定义中，适当注释各部分的功能，以便读者能正确理解代码意义。 另外，在对象命名上，python有如下必须遵守或不成文的规定，和C/C++用户熟悉的蛇形命名法不同，它大致包括 类与函数多用驼峰命名法，变量可以采用驼峰或蛇形命名法。 驼峰命名法指的是用大小写区分每个单词块，例如 alphaBetaFunction () ； 蛇形命名法指的是用下划线区分每个单词块，例如 alpha_beta_function = 10 ； 宏变量使用全字大写+蛇形命名法 函数/方法，还有模块均是首字母小写，但类的首字母大写。 用单下划线 _ 表示临时存储器，或省略参数，例如一个函数 func () 有两个返回值时，可以用 _ , b = func () 表示我们只需要第二个返回值；单下划线还可以与星号连用省略多个返回值； 以单下划线开头的方法，表示模块级的私有方法，在模块以外使用 import 导入类时，不会导入这些方法，例如 def _alphaBeta ( self ): ； 以单下划线结尾的对象，用来和python的关键字区分，例如 func ( x , class_ ) ; 以双下划线开头的方法，如果不以双下划线结尾，则表示类级的私有方法，只有类内部的方法能调用这些方法，在类外部、包括继承的子类里都原则上不能调用（但其实也有办法调用），例如 def _alphaBeta ( self ): ； 以双下划线同时开头和结尾的方法，一般是用来 重写 (override) 特殊功能，例如 def __getattribute__ (): 将重写获得类属性的方法。 Tensorflow的数据概念 ¶ 在Tensorflow中，我们把变量都称为“ 张量 (Tensor) ”。这是因为我们有零维的标量，一维的向量，二维的矩阵，更高维的我们都称为张量。作为一个更大的概念，张量当然也可以用来包括标量、向量和矩阵了。在Tensorflow中，有的张量是 可以训练 (trainable) 的，有的则不是。比如一个张量的形状（指各维大小），当然可以是一个 < tf . Tensor 'Shape:0' shape = ( 1 ,) dtype = int32 > 类型的张量，但它不是变量，当然就不可训练。我们也可以人为控制某些张量可以训练或不可以训练，但本节、乃至本章所介绍的凡是我们接触到的张量，都是可以训练的。 特别地，对于神经网络而言，在网络内计算（或者说流动、传播）的一个n维数据，通常按照以下形式组织： tensor [ batch , dim1 , dim2 , ... , dimn , channel ] 其中，第一个维度 batch 一定存在，它表示的是单个batch中的某一个样本。如果一个batch只有一个样本，那么 batch 只能取0。 从 dim1 到 dimn 指的是实际的n维数据的各个维度； channel 指的是数据的通道，例如，一个二维RGB图像，每种颜色代表一个通道，因此有三个通道。 channel 通常用在卷积网络里，我们经常需要在深度卷积网络里不断增大通道数的同时，缩小数据尺寸。 在某些特殊情况下， channel 维度可以不存在，例如我们使用的是全连接层而不是卷积网络， tf.keras.layer.Flatten 可以用来将一个有通道的张量压缩成一个没有通道的一维向量（但是注意 batch 维度仍然存在，不会被压缩）。 因此，我们知道一个n维的数据，在神经网络中通常被描述为一个n+2维的矩阵，而一个一维向量，在卷积网络里是三维的： vector [ batch , length , channel ] 但是在全连接网络里，是二维的： vector [ batch , channel ] 在本节，乃至本章里，我们还不讨论卷积网络，因此我们都是使用二维张量（一维向量组）作为我们的数据。 数据生成 ¶ 在本项目里，我们不需要扩展Tensorflow。但是，我们需要以随机生成数据代替数据集。因此，首先，通过以下代码定义数据生成器 dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class TestDataSet : ''' A generator of the data set for testing the linear model. ''' def __init__ ( self , scale_x , A , c ): ''' Initialize the data generator. scale_x: the scale of input vector. A, c: the linear transformation. ''' self . s_x = 2 * scale_x self . A = A self . c = c self . len_x = A . shape [ 0 ] self . config () def config ( self , train = True , batch = 100 , noise = 0.1 ): ''' Configuration train: a flag for controlling the iterator mode. batch: the number of samples in a batch noise: std. of the error added to the y. ''' self . train = bool ( train ) self . batch = batch self . noise = noise def next_train ( self ): ''' Get the next train batch: (x, y) ''' x = self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) y = np . matmul ( x , self . A ) + self . c if self . noise > 1e-3 : y = y + np . random . normal ( 0 , self . noise , size = y . shape ) y = np . greater ( y , 0.0 ) . astype ( np . float32 ) return x , y def next_test ( self ): ''' Get the next test batch x. ''' return self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) def __iter__ ( self ): while True : samp = self . __next__ () yield samp def __next__ ( self ): if self . train : return self . next_train () else : return self . next_test () 该生成器输入一组 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，以及相关配置，之后就可以通过 迭代器 (iterator) 或 方法 (method) 随机生成数据。这种数据集写法我们在后面还会用到， model . fit 允许我们不是馈入样本（或样本批次），而是馈入一个 生成器(generator) 。因此我们重写了 __iter__ 方法，并使其通过 yield 返回一个生成器。这样我们定义的数据集类就可以被Keras的训练函数 model . fit 使用。接下来，调用如下测试代码： dparser.py 1 2 3 4 5 6 7 8 9 10 def test_dataset (): A = np . random . normal ( 0 , 10 , [ 10 , 6 ]) c = np . random . uniform ( 1 , 3 , [ 1 , 6 ]) dataSet = TestDataSet ( 10 , A , c ) dIter = iter ( dataSet ) for i in range ( 10 ): x , y = next ( dIter ) print ( np . sum ( y , axis = 0 ) / 100 ) test_dataset () Output [ 0.47 0.57 0.58 0.56 0.5 0.38 ] [ 0.6 0.61 0.47 0.48 0.38 0.52 ] [ 0.5 0.61 0.49 0.42 0.45 0.53 ] [ 0.59 0.52 0.44 0.44 0.49 0.51 ] [ 0.54 0.59 0.48 0.5 0.51 0.47 ] [ 0.49 0.57 0.56 0.49 0.53 0.4 ] [ 0.5 0.61 0.51 0.54 0.51 0.52 ] [ 0.5 0.51 0.61 0.5 0.44 0.5 ] [ 0.44 0.46 0.53 0.45 0.56 0.52 ] [ 0.52 0.46 0.51 0.52 0.49 0.44 ] 我们随机生成了 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的数据，每组数据100个，并且测试了10组。输出结果是各组测试中， \\mathbf{y} \\mathbf{y} 在对应维度上分类为1的概率估计。结果基本都在0.5左右，说明我们的这种数据生成模式产生的点能均匀分布在各个超平面两侧，适合进行后续测试。 定义线性顺序模型 ¶ 顺序 (sequential) 模型是一个单输入单输出模型，网络结构较为简单，也不存在跨层短接（残差连接）。在大多数情况下，已经上手的Tensorflow用户不使用这个模型，故而作为我们入门的第一个project，我们姑且用之，但我们将不再使用顺序模型来实现后续的project。一个顺序模型大致可以描述为下图的模式： graph LR st(输<br/>入) --> l1[层<br/>1] l1 --> l2[层<br/>2] l2 --> l3[层<br/>3] l3 --> ldots[层<br/>...] ldots --> ed(输<br/>出) classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,ed styStart 由于我们完成的是一个线性分类器，故而我们使用单层的序列模型即可。 接下来，我们来定义一个类， class LinClsHandle : 。定义一个类的时候，我们通常需要定义的内容包括 在初始化方法 __init__ 里定义传入网络的固定参数，例如学习速率，存取路径等； 在方法 construct 里定义网络的构造和使用的优化器； 在方法 train 里定义训练网络的过程，主要需要调用 model . fit 。如果我们在数据集的定义非常完善，则这一环节不需要花费太多的功夫； 在方法 test 里定义测试网络的过程，主要需要调用 model . evaluate 。如果有必要，可以通过 model . predict 返回测试结果。 初始化方法 ¶ 首先，定义初始化方法： lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 def __init__ ( self , learning_rate = 0.01 , epoch = 10 , steppe = 30 ): ''' Initialization and pass fixed parameters. learning_rate: the learning rate for optimizer. epoch: training epochs. steppe: steps per epoch ''' self . lr = learning_rate self . epoch = epoch self . steppe = steppe 由于目前我们的project还非常简单，这里只需要有学习速率( learning_rate )，轮次数( epoch )和每轮迭代次数( steppe )即可。 构造方法 ¶ 接下来定义网络构造 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def construct ( self ): ''' Construct a linear model and set the optimizer as Adam ''' # Construction self . model = tf . keras . Sequential () self . dense1 = tf . keras . layers . Dense ( LABEL_SHAPE , use_bias = True , input_shape = ( INPUT_SHAPE ,), kernel_initializer = tf . keras . initializers . RandomNormal ( 0.0 , stddev = 10.0 ), bias_initializer = tf . keras . initializers . Constant ( 2 ), activation = tf . nn . sigmoid ) self . model . add ( self . dense1 ) # Set optimizer self . model . compile ( optimizer = tf . train . AdamOptimizer ( self . lr ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ self . accuracy , tf . keras . metrics . BinaryAccuracy ()] ) @staticmethod def accuracy ( y_true , y_pred ): return tf . keras . backend . mean ( tf . keras . backend . equal ( y_true , tf . keras . backend . round ( y_pred ))) 须知 这里 LABEL_SHAPE 和 INPUT_SHAPE 为两个宏变量，分别为输出和输入的向量维度。 我们使用 Dense 定义全连接层，它的用法请参照 这里 。由于我们已经知道 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 可能的取值范围，这里我们重定义了 \\mathbf{W} \\mathbf{W} 和 \\mathbf{b} \\mathbf{b} 的初始化方式。 信息: Dense API tf . keras . layers . Dense ( shape , ** kwargs ) 指全连接层，其输入一组已知形状的向量，输出一组形状为 shape 的向量。可用的API如下： shape : 正整数，输出空间维度。 activation : 激活函数。 若不指定，则不使用激活函数 (即，线性激活: a(\\mathbf{y}) = \\mathbf{y} a(\\mathbf{y}) = \\mathbf{y} )。该函数可以定义为任何元素级操作的Tensorflow函数。 use_bias : 布尔值，该层是否使用偏置向量。 True 则网络定义为 \\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} \\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} ， False 则定义为 \\mathbf{y} = \\mathbf{W}\\mathbf{x} \\mathbf{y} = \\mathbf{W}\\mathbf{x} 。 kernel_initializer : kernel 权值矩阵的初始化器，自定义的初始化器需要使用Keras后端API编写。 bias_initializer : 偏置向量的初始化器，同上。 kernel_regularizer : 运用到 kernel 权值矩阵的正则化函数，自定义的正则化函数需要使用Keras后端API编写。 bias_regularizer : 运用到偏置向的的正则化函数，同上。 activity_regularizer : 运用到层的输出的正则化函数，同上。 kernel_constraint : 运用到 kernel 权值矩阵的约束函数，只能使用Keras备选的几种方案，不能自定义。 bias_constraint : 运用到偏置向量的约束函数，同上。 信息: model.compile API model . compile ( optimizer , ** kwargs ) 在这里指的是顺序模型的编译函数，其可用的API如下： optimizer : 优化器，可以使用Tensorflow内置的优化器。 loss : 损失函数，也是目标函数。顺序模型只有一个输出，因此只能传入一个损失函数。可以使用形式为 func ( y_true , y_pred ) 的Tensorflow函数。 metrics : 测度函数，一般是一组函数，如果是一个函数则定义为 [ func ] 即可。自定义的测度函数目前还需要使用Keras后端API编写。 loss_weights : 损失的权重，顺序模型只有一个损失函数，因此只有一个权重，但要使用一维列表 [ value ] 定义。可以使用张量来控制可变权重。 sample_weight_mode : 按时间步采样权重，默认不提供。相比上面的损失权重，该选项会随着迭代次数使用不同的权重，因此输入的是二维列表。 weighted_metrics : 测度的权重，和损失权重类似，用来加给不同的测度函数。由于我们可以使用不只一个测度函数，这里的权重是个一维列表。 target_tensors : 默认情况下，Keras 将为模型的目标创建一个占位符，在训练过程中将使用目标数据。相反，如果你想使用自己的目标张量（反过来说，Keras在训练期间不会载入这些目标张量的外部 Numpy数据），您可以通过 target_tensors 参数指定它们。对于单输出的顺序模型，它应该是单个张量。 ** kwargs : 其他参量，会传递给 tf . Session . run 。 另外，注意我们这里构造网络的时候有如下技巧： Tensorflow在导入Keras模式以后，已经不再使用 (15) (15) 的形式定义 sigmoid交叉熵 ，而是采取更通用的定义 (14) (14) ； 我们使用Tensorflow重新封装过的类， 二分类交叉熵 ( BinaryCrossentropy ) 来作为Keras模型的损失函数 self.loss ，该函数与 多分类交叉熵 ( CategoricalCrossentropy ) 不同，乃是对两组对比张量的每个元素分别计算交叉熵，再求取均值，正符合本应用的需求； 我们通过静态方法，调用Keras的后端API，自己定义了预测准确度的测度函数 self.accuracy ，同时也使用另一个来自Tensorflow封装好的测度类 二分类准确度 ( BinaryAccuracy ) ，这是为了比照两个准确度的区别，以便我们更好理解该测度类； 我们将网络层的关键字 self.dense1 保留在了实例中，这是为了确保接下来我们能通过实例抽取该层的参数。 需要注意的是，由于 二分类交叉熵 ， 二分类准确度 和 多分类交叉熵 等都是类（从它们的定义都是大写字母开头也可以看出来），因此我们需要在使用的时候后面加上括号以实例化；由于这些类都定义了 __call__ 方法，我们可以像使用函数一样使用它们的实例。 训练和测试方法 ¶ 最后定义的式训练和测试方法。由于我们目前的project还比较简单，关于这两部分都直接调用现有的API即可。使用的API在之前已经说明。 model.fit 在没有额外设置的情况下，默认会返回一个 History回调器 ； model.evaluate 返回的是测试样本给出的损失函数和准确值测度。 model.predict 返回的是测试样本给出的网络输出。详情请参照 顺序模型API 。 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def train ( self , dataSet ): ''' Use a data set to train the network. ''' return self . model . fit ( dataSet , epochs = self . epoch , steps_per_epoch = self . steppe ) def test ( self , data , labels ): ''' Use (data, label) pairs to test the results. ''' loss , accu = self . model . evaluate ( data , labels ) print ( 'Evaluated loss =' , loss ) print ( 'Evaluated accuracy =' , accu ) return self . model . predict ( data ) 信息: model.fit API model . fit ( dataSet , ** kwargs ) 是训练函数，注意这个函数虽然支持输入一组 x,~y x,~y 用来代替 dataSet ，我们还是建议在任何情况下都用dataSet馈送数据，以免内存中数据集占用过多。 dataSet : 数据集，其本身应当是一个 tf.data.Dataset 类型的类，或者是一个能不断迭代产生新数据的生成器。数据的 batch 大小由 dataSet 本身决定。 epochs : 整数，终止训练时经历的世代(轮次)数，通常一个epoch表示遍历整个数据集一回。 verbose : 0, 1或2。日志显示模式。 0=安静模式, 1=进度条, 2=每轮一行。默认是1。 callbacks : 回调器，它是 tf.keras.callbacks 模块下的类，用来在训练中进行记录保存和数据检查点更新。默认是 tf.keras.callbacks.History 。 validation_split : 在 0 和 1 之间浮动。用作验证集的训练数据的比例。模型将分出一部分不会被训练的验证数据，并将在每一轮结束时评估这些验证数据的误差和任何其他模型指标。验证数据是混洗之前 x 和 y 数据的最后一部分样本中。 validation_data : 元组 (x_val，y_val) 或元组 (x_val，y_val，val_sample_weights) ，用来评估损失，以及在每轮结束时的任何模型度量指标。模型将不会在这个数据上进行训练。这个参数会覆盖 validation_split 。 shuffle : 布尔值（是否在每轮迭代之前混洗数据）。当 steps_per_epoch 非 None 时，这个参数无效。 class_weight : 可选的字典，用来映射类索引（整数）到权重（浮点）值，用于加权损失函数（仅在训练期间）。这可能有助于告诉模型 「更多关注」来自代表性不足的类的样本。 sample_weight : 用来给损失函数添加权重，作用类似 model.compile 的同一参数。 initial_epoch : 开始训练的轮次（有助于恢复之前的训练）。 steps_per_epoch : 在声明一个轮次完成并开始下一个轮次之前的总步数（样品批次）。使用TensorFlow数据张量等输入张量进行训练时，默认值 None 等于数据集中样本的数量除以 batch 的大小，如果无法确定，则为1。 validation_steps : 只有在指定了 steps_per_epoch 时才有用。停止前要验证的总步数（批次样本）。 该函数会返回 callbacks 定义的实例。 信息 model.evaluate API model . evaluate ( x , y , ** kwargs ) 是测试函数，需要传入 label 即 y 来验证性能。 x , y : Numpy 数组，分别是输入和输出的真实参照值。 batch_size : 计算的 batch 大小，该函数会将输入的数据组拆分成许多 batch 分别计算，并组合起来，这个设定值对效果不产生影响，只影响计算过程。 verbose : 0, 1。日志显示模式。0=安静模式, 1=进度条。默认是1。 sample_weight : 用来给损失函数添加权重，作用类似 model.compile 的同一参数。 steps : 整数或 None 。 声明评估结束之前的总步数（批次样本）。默认值 None 。 该函数会返回损失函数和测度（列表）。 信息 model.predict API model . predict ( x , ** kwargs ) 是预测函数，用在只知道输入 x 的场合。 x : Numpy 数组，输入值。 batch_size : 计算的 batch 大小，该函数会将输入的数据组拆分成许多 batch 分别计算，并组合起来，这个设定值对效果不产生影响，只影响计算过程。 verbose : 0, 1。日志显示模式。0=安静模式, 1=进度条。默认是1。 steps : 整数或 None 。 声明评估结束之前的总步数（批次样本）。默认值 None 。 该函数返回预测结果。 调试 ¶ 首先，训练网络。我们随机生成 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的仿射变换，并且设置好数据集，给定噪声扰动为 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 。设定20个epoch，每个epoch迭代500次，每次馈入32个样本构成的batch，然后开始训练： lin-cls.py 1 2 3 4 5 6 7 8 9 # Initialization A = np . random . normal ( 0 , 10 , [ INPUT_SHAPE , LABEL_SHAPE ]) c = np . random . uniform ( 1 , 3 , [ 1 , LABEL_SHAPE ]) dataSet = dp . TestDataSet ( 10 , A , c ) dataSet . config ( batch = 32 , noise = 0.1 ) # Construct the model and train it. h = LinClsHandle ( learning_rate = 0.01 , epoch = 20 , steppe = 500 ) h . construct () record = h . train ( iter ( dataSet )) Output Epoch 1 /20 500 /500 [==============================] - 1s 3ms/step - loss: 6 .3005 - accuracy: 0 .5884 - binary_accuracy: 0 .5884 Epoch 2 /20 500 /500 [==============================] - 1s 2ms/step - loss: 5 .4671 - accuracy: 0 .6407 - binary_accuracy: 0 .6407 Epoch 3 /20 500 /500 [==============================] - 1s 2ms/step - loss: 4 .5711 - accuracy: 0 .6957 - binary_accuracy: 0 .6957 Epoch 4 /20 500 /500 [==============================] - 1s 2ms/step - loss: 3 .6789 - accuracy: 0 .7519 - binary_accuracy: 0 .7519 Epoch 5 /20 500 /500 [==============================] - 1s 2ms/step - loss: 2 .7101 - accuracy: 0 .8127 - binary_accuracy: 0 .8127 Epoch 6 /20 500 /500 [==============================] - 1s 2ms/step - loss: 2 .0059 - accuracy: 0 .8627 - binary_accuracy: 0 .8627 Epoch 7 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .6403 - accuracy: 0 .8894 - binary_accuracy: 0 .8894 Epoch 8 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .3663 - accuracy: 0 .9066 - binary_accuracy: 0 .9066 Epoch 9 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .0466 - accuracy: 0 .9274 - binary_accuracy: 0 .9274 Epoch 10 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .8377 - accuracy: 0 .9418 - binary_accuracy: 0 .9418 Epoch 11 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .6465 - accuracy: 0 .9546 - binary_accuracy: 0 .9546 Epoch 12 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .4492 - accuracy: 0 .9667 - binary_accuracy: 0 .9667 Epoch 13 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .2795 - accuracy: 0 .9779 - binary_accuracy: 0 .9779 Epoch 14 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .1624 - accuracy: 0 .9861 - binary_accuracy: 0 .9861 Epoch 15 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0680 - accuracy: 0 .9926 - binary_accuracy: 0 .9926 Epoch 16 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0128 - accuracy: 0 .9971 - binary_accuracy: 0 .9971 Epoch 17 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0055 - accuracy: 0 .9981 - binary_accuracy: 0 .9981 Epoch 18 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0052 - accuracy: 0 .9986 - binary_accuracy: 0 .9986 Epoch 19 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0055 - accuracy: 0 .9981 - binary_accuracy: 0 .9981 Epoch 20 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0049 - accuracy: 0 .9985 - binary_accuracy: 0 .9985 接下来，从训练返回的 History 类型的回调器中抽取对loss和accuracy的记录。 lin-cls.py 1 2 3 4 5 # Show records plt . semilogy ( record . epoch , record . history [ 'loss' ]), plt . xlabel ( 'epoch' ), plt . ylabel ( 'Cross entropy' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () plt . plot ( record . epoch , record . history [ 'accuracy' ], label = 'self defined' ), plt . plot ( record . epoch , record . history [ 'binary_accuracy' ], label = 'from tensorflow' ), plt . xlabel ( 'epoch' ), plt . ylabel ( 'Accuracy' ), plt . legend () plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () Output 结果显示，我们自定义的准确度测度和Tensorflow内置的 二分类准确度 完全相同，这说明其本身的定义就是求取所有元素阈值化后，各自分类结果是否正确的平均值。这个实验也让我们对自定义测度函数有了一定的认识。 重新设定数据集的产生方式，变为每个batch含10个样本。使用这组重新随机生成的数据测试网络输出， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 12 # Generate a group of testing samples: dataSet . config ( batch = 10 ) x , y = next ( dataSet ) # Check the testing results yp = h . test ( x , y ) _ , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 ) ax1 . imshow ( y , interpolation = 'nearest' , aspect = 'auto' ) ax1 . set_title ( 'True class' ) ax2 . imshow ( yp , interpolation = 'nearest' , aspect = 'auto' ) ax2 . set_title ( 'Predicted class' ) plt . gcf () . set_size_inches ( 10 , 5 ), plt . show () Output Evaluated loss ( losses.BinaryCrossentropy ) = 0 .0023145806044340134 Evaluated accuracy ( self defined ) = 1 .0 Evaluated accuracy ( metrics.BinaryAccuracy ) = 1 .0 注意我们未对测量的结果阈值化，因此显示出来的测量结果和理想值略有差别，但从图可知，阈值化后则测量结果全部准确。 通过抽取 h.dense1 的参数，我们可以对比 \\mathbf{A} \\mathbf{A} 和 \\mathbf{W} \\mathbf{W} ，以及 \\mathbf{c} \\mathbf{c} 和 \\mathbf{b} \\mathbf{b} ， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 # Check the regressed values W , b = h . dense1 . get_weights () plt . imshow ( A , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'A' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . imshow ( W , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'W' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . plot ( c . T , label = 'c' ) plt . plot ( b . T , label = 'b' ) plt . legend () plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () Output 可以发现，虽然我们训练的分类器十分有效，但其权值和预期的 \\mathbf{A} \\mathbf{A} , \\mathbf{c} \\mathbf{c} 并不完全相同。这是由于sigmoid函数激活的特性，使得当预测值偏向最小或最大的情况下， |\\sigma(x)| \\rightarrow 1 |\\sigma(x)| \\rightarrow 1 ，根据 (7) (7) ，可知其梯度 |\\sigma(x)(1-\\sigma(x))| \\rightarrow 0 |\\sigma(x)(1-\\sigma(x))| \\rightarrow 0 ，因此那些分类结果已经比较确信的样本，其梯度消失，对训练网络的影响忽略不计（这是合理的，因为我们不希望极端样本干扰结果，更希望对分类结果不确切的样本进行训练）。故而，我们虽然可以求解出这个分类问题，但求解到的 \\mathbf{W} \\mathbf{W} , \\mathbf{b} \\mathbf{b} 不会回归到 \\mathbf{A} \\mathbf{A} , \\mathbf{c} \\mathbf{c} 上。关于回归问题，我们会在下一节进一步讨论。","text_tokens":["感知机","列","as","evaluate","用户","组织","虽然","是因为","干扰","*","0055","创建","tool","适合","py","最","参量","numpy","而是","方案","比例","48","非常","误差","乘积","9971","返回","where","(","调试","from","数据测试","binarycrossentropy","布尔","=","&","总是","方式","lr","verbose","理解","模型","应用","activity","equation","├","故而","除以","两个","2795","下图","关注","32","有助于","过","auto","正确理解","字母","分类器","initializers","角度","aspect","1s","相同","来","传播","left","有关","26","逻辑","高维","分布","0.44","根据","十分","开","run","组合","确信","去","代表","写字","权值","和","样品","get","随机","类型","diter","首先","较为简单","linclshandle","含义","下划线","end","又","度量","batch","预测值","没有","[","变换","接下","尽管","100","更好","regression","内置","生成器","第二","索引","有用","相若","发现","快速","齐次","撰写","iterator","功夫","t","规范","乃是","7","output","fixed","8","还","float32","对角线","网络结构","允许","our","2ms","model","列表","特殊","跟","初始","关于","建立","要求","代码","在","传入","成","混洗","后面","就是","增大","dimn","上手","24","集类","速率","自己","每种","exp","l","括号","取","过多","weighted","终止","right","单调","]","vector","跨层","pass","只能","0.48","一条","compile","浮点","独立","决定","之前","重写","解","yp","开头","│","目的","确保","元素","设定","显式","to","pred","none","且","完全","预处理","整理","matmul","next","predict","但","backend","「","0.56","graph","固定","较长","神髓","两层","变为","显示","结果显示","给定","第","首字母","call","仿射","title","集","测试方法","14","def","__","对角","缩小","linspace","看作","引出","测试代码","却","加","进度","3ms","详情请","应该","认识","mathbb","length","为","概念","信息","哪","内存","'","cls","1","定义数据","足够","占位","接触","保存","截距","transformation","出","该","initial","3","都","缩成","组","将","向","本身","正","看成","¶","助于","平移","使用","不能","42","后端","意味","analyzing","self","另","能"," ","出原","非常简单","tf","重","无法","二","第一","反过","改进","特性","项","tensorflow","例如","h","之后","对应","知道","~","10","records","有效","之间","加上","里","概率论","dim2","sigmoid","31","stddev","可见","级","theta","不足","samp","是","各","调用","支持","x","比如","标准化","压缩","已知","层","43","0.54","对象","信度","测试","import","连接","以下","给","数组","希望","良好","sim","于","分出","ylabel","epoch","6789",".","面向对象","称","k","entropy","数量","均","或","并非","前","称为","三个","与","其中","由此可知","initialize","应当","iter","目前","摘要","regularizer","平面","小写","假设","imshow","lvert","分入","参见","的","}","subplots","一回","name","路径","重新","互不","l1","7519","一轮","实际","target","不断","关系","具有","导入","project","generate","件夹","饮水","方法","41","分类","and","存储器","某些","37","偏向","extending","简短","fill","rate","semilogy","后续","编译","0.51","会","extension","处理器","序列","boldsymbol","零维","一组","training","划线","func","构成","放在","乃至","每轮","5711","mode","自定","27","向量","36","4671","frac","标准","表述","，","cdot","凡是","check","求解","一个二维","总","单","0.0","理器","拟合","post","构造方法","简单","彼此","默认","dim1","beta","术语","0.45","它们","这组","复杂度","特点","直接","进度条","对比","0.42","训练","+","11","相斥","采样","gca","0680","指全","技巧","callbacks","结尾","送入","熟悉","21","复杂","construction","实数","初始化","起来","controlling","c++","成器","dp","记","noise","可信度","哪怕","加权","一个","0.38","那些","并且","个","人意","construct","“","29","classdef","arg","取值","plt","以双","中将","单层","losses","目标","override","_","继承","权重","模式","两组","用来","0.6","predicted","除了","主","50","r","steps","└","以及","pairs","往往","多个","大小写","step","性能","常数","6403","当","化","静态","无定","i","session","每次","0.58","某","dataset","p","logistic","提示","采用","所","正整数","alphabeta","tools","反","均匀","任","则","全部","0.57","地","封装","世代","先河","其","align","情况","讨论","可以","we","使得","神经元","器","分离","：","data","30","aligned","rightarrow","split","语言","声明","同","以免","一","tensor","else","直线","13","任意","keras","nn","；","9667","规定","per","实际上","categoricalcrossentropy","问题","短接","randomnormal","极端","器中","构建","尽如人意","操作","时","parser","d","c","好处","含","如何","stroke","其实","两侧","检查点","布尔值","大化","for","模块化","默认值","0.49","min","经历","端","flatten","编写","关键字","0059","并","或者","合适","未知","知机","参照","经常","星号","不会","/","做到","sequential","全","注释","数据","外围","”","这个","预期","已经","图示","uniform","作用","馈入","等于","梯度","）","恢复","比较","合理","用单","传递","化后","results","成文","基本","调整","原则上","建议","可信","53","最后","连用","指数","不得","分析","e","500","约束","labels","学习","也","0.61","线性","文字","因为","adamoptimizer","激活","partial","做","通常","至此","false","f","module","dense1","9779","忽略不计","期间","迭代","每","np","一部分","可读性","bmatrix","20","后","测量","如下","std","完成","size","日志","主要","线性组合","可微","accuracy","属于","的话","单独","s","但类","（","输","」","0128","正确","add","record","而言","孱弱","metrics","二维","大致",";","差别","结构","想","轮","非","大多数","25","9","设置","包括","私有","提供","代替","nearest","来自","首字","无效","文件","分成","多用","设定值","备选","手","需要","统一","这","sample","0.5","38","？","kernel","略有","比较简单","5","the","^","shape","这种","interpolation","分别","7101","稳定性","向用","inches","既然","2","图像","引用","测试函数","1624","可读","姑且","更新","或者说","认为","操作符","equal","导数","8127","返回值","亦","确定","getattribute","相反","宏","可能","必要","下划","9981","另外","开始","指标","validation","愿","类中","功能","较强","ldots","l3","下","0.47","、","即","ba9132","才","环节","一类","54","4","rgb","介绍","gcf","plot","求导","&&","各自","张量","kwargs","constant","同时","n","上面","预测","尽如","一些","flag","define","通道","输出","mean","工程","只是","有","以","6","sum","a","weight","generator","流动","似然","第二个","线性关系","习惯","show","反过来","w",")","回归","进一步","loss","交叉","it","格式","双","人为","initialization","扰动","可选","一节","j","一大","对","范围","受过","测度","了解","样本","可用","定法","评估","把","构造","不是","场合","办法","残差","超平面","这里","行","尺寸","9986","检查","图","可变","时间","回调","0023145806044340134","面向","两","斯蒂","最小化","lin","堆叠","alphabetafunction","cdots","引入","9066","添加","各组","通过","<","一定","过来","mathrm","读取","程度","入","看","即可","当然","xlabel","灵活","大写","符","必须","项目","perceptron","round","staticmethod","不","新","反之","of","描述","正则","52","test","实现","次数","config","49","性","块","这些","均匀分布","外部","由于","一词","叫","仿射变换","等","use","8627","定性","这样","注意","-","len","实战","熵","10.0","sigma","教程","greater","影响","yield","无","正值","省略","法","指各维","符合","求取","适当","采取","步数","由","类似","上","大多","35","满足","同一","它","非线性","channel","log","之","如果","走上","顺序","17","基本概念","载入","class","未","最大","4492","自定义","好","jacobian","单词","请","视","额外","完善","到","整数","\\","思源","本章","仅","15","1e","然后","dtype","method","0049","活用","子","性关系","函数","馈送","只要","结果","9926","大量","网络层","不再","蛇形","临时","优化","linear","花费","9985","内容","实验","34","让","regressed","写出","比照","coder","rvert","不计","等价","相比","自带","查点","很","adam","evaluated",":","维度","varepsilon","18","解析","官方","0.53","max","input","运用","推断出","但是","true","形状","9861","只","指定","多数","安静","节","面向用户","稳定","卷积","网络","概率","写法","您","api","整个","colorbar","准确","因子","大小","axis","以单","那么","accu","39","added","tilde","理想",">","要","一行","值","16","定义","标量","一步","需求","─","有限","处理","推荐","6957","以后","表示","更","其他","error","得到","文件夹","轮次","begin","ax2","if","早期","多","main","b","对数","dparser","不以","single","fit","陆续","左右","不可","简简单单","过程","几乎","layer","获得","之旅","下来","写成","属性","set","许多","存储","元组","configuration","神经","找到","蓝线","只有","批次","被","静态方法","数","来说","时才","binaryaccuracy","limits","上述","不同","使","9418","value","深度","说明","区别","抽取","本节","defined","关键","从","产生","这一","9546","展示","估计","仍然","命名","leqslant","5884","ed","44","集中","activation","实例","大写字母","0.4","入门","事实","退化","samples","一层","history","线性变换","23","val","反过来说","mathcal","cross","类","while","法指","再","保留","二个","存在","pyplot","意义","0.1","不成文","输入","就","任何","为了","l2","最终","通用","return","喜好","@","时候","控制","对于","区分","一般","详情","选项","存取","用","in","告诉","插值","python","结束","46","matplotlib","22","遵守","噪声","负值","损失","记录","启发","平均值","三维","astype","缩进","考虑","label","weights","一个多","效果","类时","饮水思源","子类","layers","9274","设计","按照","28","配置","|","每组","按","有趣","给出","扩展","上求","还有","用作","调整结构","trainable","range","parameters","store","用到","一个点","y","单单","st","bool","次","推断","风格","数目","第一个",",","8894","点","接下来","40","3005","中","事实上","读者","因此","覆盖","random","6465","vdots","...","init","infty","代表性","停止","红线","stystart","均值","8377","确切","还会","有助","生成","print","fae6a9","steppe","一一","learning","作为","指","tensors","解决","部分","33","后来","完全相同","以便","最早","变量","推导","testing","前要","表明","计算","维","dense","0.59","理论","特别","45","原则","0.01","鉴往知来","该层","进行","mapsto","遍历","testdataset","epochs","shuffle","几种","内部","ij","可知","交点","#","模块","组拆","数据分布","initializer","network","而","包含","发展","codes","alpha","所有","number","一维","br","平均","占用","47","一侧","了","准确度","单个","values","忽略","特征","0","normal","式","字典","相关","人","感知","驼峰","颜色","自动","真实","ax1","于是","function","内","是否","神经网","用法","我们","int32","无定法","最小","选择","最大化","0.46","每个","偏置","步","空间","0466","须知","这是","。","若","以外","浮动","3663","legend","51","全字","19","bias","无关","出来","不知","阈值","矩阵","消","参数","意味着","看待","6407","消失","形式","压缩成","随着","binary","太多","scale","用于","由此","神经网络","group","mathbf","更大","映射","文档","0.52","改写","像","层叠","然而","0052","optimizer","一样","hat","本","说","还是","验证","轴","processing","一部","12","过程中将","现有","不成","代入","你","类级","慢慢","{","较为","train","各个","constraint","视为"],"title":"线性分类","title_tokens":["分类","线性"]},{"location":"book-1-x/chapter-1/linear-classification/#_1","text":"摘要 本节介绍如何使用顺序模型(sequential model)来编写一个线性分类器，使用sigmoid函数激活，并验证其效果。在本节我们将编写第一个Project，并介绍一些基本概念、和一个推荐的Tensorflow Project的编写格式。","text_tokens":["(","分类","在","来","编写","验证","其","并","基本概念","节","第一","sigmoid","函数","线性","、",")","本节","将","model","第一个","tensorflow","。","如何","一些","激活","基本","推荐","模型","格式","概念","使用","project","sequential","介绍","我们","，","的","分类器","效果","和","一个"," ","摘要","顺序","本"],"title":"线性分类","title_tokens":["分类","线性"]},{"location":"book-1-x/chapter-1/linear-classification/#_2","text":"","text_tokens":[],"title":"理论","title_tokens":["理论"]},{"location":"book-1-x/chapter-1/linear-classification/#_3","text":"考虑我们有如下的二分类数据集 (\\mathbf{x},~y_i) \\in \\mathbb{D}_i (\\mathbf{x},~y_i) \\in \\mathbb{D}_i ，并且有一个未知的常数向量 \\mathbf{a}_i \\mathbf{a}_i 和未知的常数标量 c_i c_i ，使得： \\begin{equation} y_i = \\left\\{ \\begin{aligned} 0, && \\mathbf{a}_i^T \\mathbf{x} + c_i \\leqslant 0, \\\\ 1, && \\mathbf{a}_i^T \\mathbf{x} + c_i > 0. \\end{aligned} \\right. \\end{equation} 其中， \\mathbf{a} \\mathbf{a} 可以看成是某超平面的（未标准化的）法向量，那么 \\mathbf{a}^T \\mathbf{x} + c = 0 \\mathbf{a}^T \\mathbf{x} + c = 0 是该超平面的截距式定义，亦即该平面与 x_i x_i 轴的交点可以显式表述为 x_i^{(0)} = - \\frac{c}{a_i} x_i^{(0)} = - \\frac{c}{a_i} 。由此可知，式 (1) (1) 显式定义了一个点在超平面的哪一侧。特别地，若 \\mathbf{x} \\mathbf{x} 是一个二维向量，则该超平面退化为一维平面；若 \\mathbf{x} \\mathbf{x} 是一个标量，则该超平面退化为一条直线。 若我们定义 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} ，有 \\begin{equation} \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, ~~ \\mathbf{A} = \\begin{bmatrix} \\mathbf{a}^T_1 \\\\ \\mathbf{a}^T_2 \\\\ \\vdots \\\\ \\mathbf{a}^T_n \\end{bmatrix}, ~~ \\mathbf{c} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}. \\end{equation} 则我们可以认为 \\begin{align} \\mathbf{y} = \\left\\{ \\begin{bmatrix}\\hat{y}_1 > 0 & \\hat{y}_2 > 0 & \\cdots & \\hat{y}_n > 0\\end{bmatrix}^T, ~ \\left| ~ \\hat{\\mathbf{y}} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon} \\right. \\right\\}, \\end{align} 其中 \\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon} 是一个定义噪声的向量。 我们可以把向量 \\mathbf{y} \\mathbf{y} 的元素看成是互不相关的多个超平面对向量 \\mathbf{x} \\mathbf{x} 各自独立的分类结果。即 y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} 。由于每个超平面构成一个二分类，如果把每个二分类看作是向量是否属于这个类的测度，那么 \\mathbf{y} \\mathbf{y} 可以被看作是一个多分类的结果，尽管向量 \\mathbf{x} \\mathbf{x} 可能被同时分入多个类中。 假设我们的数据集 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} 符合 (3) (3) 定义的数据分布特征。我们的基本要求是，在我们不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 的情况下，使用大量 (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} 样本训练一个线性分类器，使得当我们给定任意一个新样本 \\mathbf{x} \\mathbf{x} 的时候，分类器能推断出其对应的 \\mathbf{y} \\mathbf{y} 来（亦即是否属于该分类）。 须知 请注意我们在这里说到“线性分类器”，虽然使用“线性”一词，但准确来说，这是一个仿射变换。因为线性变换要求有齐次性，即 f(x) = \\alpha f(x) f(x) = \\alpha f(x) ，但仿射变换允许我们引入一个平移向量 \\mathbf{b} \\mathbf{b} 。当然，我们的求解的线性问题本身也是一个仿射变换。 在这个问题里，我们虽然不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，但我们知道由 (3) (3) 确定的线性关系，因此，我们可以随机生成一组 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ，构建线性模型： \\begin{align} \\tilde{\\mathbf{y}} = \\sigma ( \\mathbf{W} \\mathbf{x} + \\mathbf{b} ). \\end{align} 其中，可微函数 \\sigma \\sigma 是一个将实数空间 \\mathbb{R}^n \\mathbb{R}^n 映射到有限范围的实数空间 [0,~1]^n [0,~1]^n 内的函数。特别地， \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 。因此，可以将 \\sigma \\sigma 看作是二分类布尔函数的插值函数。理论上，只要我们找到 \\mathbf{W}=\\mathbf{A} \\mathbf{W}=\\mathbf{A} ， \\mathbf{b}=\\mathbf{c} \\mathbf{b}=\\mathbf{c} ，则该线性分类器可以直接拟合出原分布来。","text_tokens":["出原","（","求解","平面","退化","一个点","y","一个二维","类中","相关","假设",">","二","未知","定义","下","cdots","未","right","引入","标量","即","推断","线性变换","拟合","]","虽然","类","请","end","有限","二维","分入",",","一条","到","\\","独立","是否","内","我们","的","则","对应","}","begin","知道","~","直接","数据","[","变换","因此","尽管","当然","vdots","地","&&","元素","各自","”","b","infty","显式","(","这个","里","互不","训练","+","其","每个","align","情况","时候","生成","）","不","同时","性关系","函数","n","新","空间","可以","布尔","=","须知","只要","结果","使得","&","大量","。","若","这是","tilde","但","：","基本","关系","模型","实数","aligned","是","0.5","齐次","x","有","t","equation","一个","标准化","a","给定","性","仿射","集","in","^","分类","由于","一词","并且","特别","插值","理论","线性关系","仿射变换","看作","允许","也","直线","线性","w",")","“","找到","噪声","任意","由此","mathbb","因为","为","；","注意","被","-","varepsilon","考虑","哪","boldsymbol","mathbf","2","_","1","一组","f","问题","映射","分类器","一个多","要求","来说","推断出","可知","对","sigma","r","构建","范围","测度","样本","交点",".","hat","截距","说","在","来","构成","法","k","数据分布","轴","该","left","认为","把","多个","符合","3","|","d","c","将","bmatrix","常数","亦","超平面","这里","如下","由","确定","当","分布","alpha","向量","{","本身","i","看成","上","平移","一维","使用","准确","frac","标准","表述","，","可能","那么","与","一侧","和","leqslant","其中","了","某","可微","由此可知"," ","如果","特征","能","0","属于","随机","式"],"title":"问题描述","title_tokens":["描述","问题"]},{"location":"book-1-x/chapter-1/linear-classification/#_4","text":"我们将 (4) (4) 定义的线性模型称为 单层感知机 (Single-layer perceptron) 模型。它包含一个权重矩阵 \\mathbf{W} \\mathbf{W} 和一个偏置矩阵 \\mathbf{b} \\mathbf{b} 。事实上，可以将 (4) (4) 改写成如下形式 \\begin{align} \\tilde{\\mathbf{y}} = \\sigma \\left( \\begin{bmatrix} \\mathbf{W} & \\mathbf{b} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ 1 \\end{bmatrix} \\right). \\end{align} 可见偏置本身可以看成是输入向量多了一个常数元素的等价模型。 感知机是最早的神经网络形式，它非常孱弱，只能解线性问题，但却为神经网络后来的发展开了先河。在单层感知机里，我们视输入向量 \\mathbf{x} \\mathbf{x} 的每个元素为一个“神经元”，矩阵 \\mathbf{W} \\mathbf{W} 和偏置 \\mathbf{b} \\mathbf{b} 将我们的输入映射到输出层 \\mathbf{y} \\mathbf{y} ，输出层的每个元素也视为一个神经元。在这个过程中， W_{ij} W_{ij} 作为 i i 行 j j 列的元素，代表了连接两个神经元的权重。我们用红线代表正值，蓝线代表负值，感知机可以被图示为 线性感知机的输出也是输入的线性组合，但我们可以添加激活函数，即 \\sigma(\\cdot) \\sigma(\\cdot) 将其映射到非线性空间。这要求我们添加的激活函数是一个非线性函数。 事实上，将单层感知机层叠，前一层的输出作为后一层的输入，就构建出早期的神经网络。这种网络每一层都是全连接的（两个神经元之间总是有权重，尽管值可能为0），每一层都有激活函数。理论上，任意一个两层堆叠的感知机，只要神经元数目足够多，就可以拟合出任意一个非线性函数。然而，实际测试中，这一理论的效果并不尽如人意，因此又有陆续地改进，才有了后来的深度学习。饮水思源，鉴往知来，我们也将从这个简简单单的单层模型开始，走上学习“深度学习”之旅。","text_tokens":["（","感知机","事实","堆叠","y","列","并","单单","一层","值","感知","定义","知机","right","改进","即","数目","拟合","才","添加","孱弱","只能","视","end","又","简单","到","\\","思源","4","神经网","我们","全","的","解","}","begin","非常","中","事实上","因此","早期","之间","输入","多","就","尽管","地","元素","b","”","(","这个","里","红线","图示","single","perceptron","先河","其","每个","align","偏置","）","不","函数","陆续","空间","可以","只要","=","作为","&","可见","。","尽如","总是","神经元","实际","简简单单","但","过程","这","模型","后来","是","最早","输出","layer","两层","x","有","用","饮水","一个","之旅","矩阵","层","等价","理论","人意","学习","也","测试","形式","两个","线性","w","神经",")","却","“","连接","负值","这种","蓝线","任意","鉴往知来","为","激活","被","-","单层","神经网络","mathbf","_","1","映射","权重","问题","要求","j","改写","效果","ij","饮水思源","层叠","然而","sigma","足够","构建",".","尽如人意","正值","在","成","每","出","left","深度","都","将","网络","包含","bmatrix","常数","后","发展","如下","从","本身","向量","{","上","看成","开","i","行","组合","前","这一","代表","，","cdot","可能","称为","它","非线性","开始","和","了","视为","线性组合"," ","0","走上","tilde"],"title":"感知机","title_tokens":["感知","知机","感知机"]},{"location":"book-1-x/chapter-1/linear-classification/#sigmoid","text":"在上述介绍中，我们没有解决的两个问题是， 如何定义插值函数 \\sigma \\sigma ？ 如何找到合适的 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ？ 我们首先讨论第一个问题。一般地，多分类问题中，如果各个分类彼此并非相斥，且不一定要将结果分入任一类的话，我们可以用 Sigmoid 函数来定义 \\sigma \\sigma ，亦即 \\begin{align} \\sigma(\\mathbf{x}) = \\frac{1}{ 1 + e^{-\\mathbf{x}}}. \\end{align} 它同时满足 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 ，且是一个单调函数。以下代码向我们展示了这种函数的特性： Python 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import matplotlib.pyplot as plt def test_sigmoid (): x = np . linspace ( - 5 , 5 , 100 ) y = 1 / ( 1 + np . exp ( - x ) ) plt . plot ( x , y ) plt . xlabel ( 'x' ), plt . ylabel ( 'y' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () if __name__ == '__main__' : test_sigmoid () Output 使用sigmoid函数的一大好处是，它的导数求解非常简单，很适合用来做神经网络这样一个复杂模型的激活函数。注意虽然 \\sigma(\\mathbf{x}) \\sigma(\\mathbf{x}) 和 \\mathbf{x} \\mathbf{x} 都是向量，这意味着导数是Jacobian矩阵，但由于 \\sigma \\sigma 是一个对 \\mathbf{x} \\mathbf{x} 各元素独立的解析函数，这个Jacobian矩阵实际上是一个对角矩阵，对角线上第j个元素的值为 \\begin{align} \\left. \\frac{ \\partial \\sigma(x) }{ \\partial x } \\right|_{x=x_j} = \\left. - e^{-x} \\left( - \\frac{1}{\\left( 1+e^{-x} \\right)^2} \\right) \\right|_{x=x_j} = - \\sigma(x_j) ( 1 - \\sigma(x_j) ). \\end{align} 可见，该函数的导数和计算函数本身的复杂度相若，可以做到快速求导。","text_tokens":["非常简单","求解","y","要","合适","第一","首先","as","定义","值","right","即","特性","单调","第一个","虽然","jacobian","一类","end","分入",",","/","适合","做到","简单","彼此","numpy","\\","一定","4","pyplot","神经网","介绍","我们","gcf","任","的","plot","}","begin","复杂度","~","没有","if","10","非常","中","求导","name","9","多","xlabel","地","100","main","元素","b","infty","(","这个","+","且","align","11","相斥","讨论","sigmoid","不","函数","同时","可以","=","结果","可见","。","实际","解决","复杂","但","相若","这","：","模型","是","一般","0.5","test","各","？","快速","x","用","6","一个","第","计算","5","7","output","e","8","def","__","^","分类","矩阵","由于","对角","对角线","插值","python","linspace","set","个","意味着","show","matplotlib","两个","import","w","神经","找到",")","很","这种","以下","这样","为","注意","激活","-","plt",":","inches","解析","partial","做","神经网络","实际上","mathbf","2","'","_","1","问题","一大","用来","j","对","sigma","ylabel","代码",".","上述","在","来","np","该","left","3","都","|","独立","12","将","向","网络","好处","导数","亦","如何","本身","向量","{","上","并非","size","使用","frac","满足","，","各个","展示","意味","它","和","了","exp"," ","如果","0","的话"],"title":"Sigmoid函数","title_tokens":["sigmoid","函数"]},{"location":"book-1-x/chapter-1/linear-classification/#_5","text":"接下来，我们需要解决第二个问题，亦即找到 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} 。这一问题通常可以写成反问题的形式： \\begin{align} \\arg \\min_\\limits{\\mathbf{W},~\\mathbf{b}} \\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}^{(k)},~ \\sigma ( \\mathbf{W} \\mathbf{x}^{(k)} + \\mathbf{b} ) \\right). \\end{align} 最简单的情况下，我们可以把 损失函数(loss function) 定义为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = \\lVert \\mathbf{y} - \\tilde{\\mathbf{y}} \\rVert_2^2. \\end{align} 我们称 (8) (8) 为 逻辑斯蒂回归(logistic regression) 。有趣的是，虽然这个术语叫“回归”，但它解的其实是个分类问题。但是，既然这是一个分类问题，我们可以不使用这个损失函数，而是从概率论的角度看待这个问题。由此，我们引出一个新的损失函数：“交叉熵”。","text_tokens":["l","y","定义","下","right","即","logistic","mathcal","lvert","虽然","end",",","二个","最","简单","接下来","而是","function","\\","反","我们","的","术语","解","}","begin","~","接下","”","b","(","这个","+","regression","align","情况","概率论","不","第二","函数","n","新","可以","=","这是","。","解决","但","需要","：","是","x","sum","一个","rvert","8","下来","^","分类","写成","第二个","叫","个","看待","引出","形式","w","找到",")","“","回归","loss","交叉","损失","arg","由此","为","-","既然","通常","mathbf","2","_","1","问题","熵","但是","limits","sigma","角度",".","称","k","把","left","概率","逻辑","亦","其实","从","{","有趣","使用","这一","，","它","min","斯蒂"," ","tilde"],"title":"求解问题","title_tokens":["求解","问题"]},{"location":"book-1-x/chapter-1/linear-classification/#_6","text":"我们视sigmoid函数输出的值为一个概率，表示分类器对预测结果的确信程度，记 \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} ，则 \\begin{equation} \\begin{aligned} \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}), \\\\ \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) &= 1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}). \\end{aligned} \\end{equation} 注意这里的概率向量的含义是，其中第i个元素表明第i个超平面分类结果的 确信程度 。 然而，这个概率只是分类器对分类结果的确信程度，却并非是分类准确度的概率，实际上，分类 准确度 的概率，应当表述为 \\begin{equation} \\begin{aligned} \\mathbf{p}(\\mathbf{y}|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta})^{1-\\mathbf{y}}\\\\ &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\left(1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}) \\right)^{1-\\mathbf{y}}. \\end{aligned} \\end{equation} 注意 这里 \\mathbf{x}^{\\mathbf{y}} \\mathbf{x}^{\\mathbf{y}} 表示的是对每个元素一一求取指数，即函数第i个元素的返回值应当为 {x_i}^{y_i} {x_i}^{y_i} 。 我们使用真实值 \\mathbf{y} \\mathbf{y} 作为指数给概率向量加权。当 \\mathbf{y}=1 \\mathbf{y}=1 时，乘积项的第二个因子消去，该函数退化为以预测值为1的可信度 \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) ；反之则第一个因子消去，退化为以预测值为0的可信度 \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) 。这就是最大似然估计方法。至此，我们可以写出似然估计函数 \\begin{align} L(\\boldsymbol{\\Theta}) = \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}). \\end{align} 对似然估计函数取对数，则有 \\begin{equation} \\begin{aligned} l(\\boldsymbol{\\Theta}) &= \\sum_{k=1}^N \\log \\left( \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}) \\right) \\\\ &= \\sum_{k=1}^N \\mathbf{y}^{(k)} \\cdot \\log\\left(\\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right) + \\left(1 - \\mathbf{y}^{(k)} \\right) \\cdot \\log\\left(1 - \\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right). \\end{aligned} \\end{equation} 我们最终的目的是要最大化似然函数，亦即 \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) ，这等价于最小化 -l(\\boldsymbol{\\Theta}) -l(\\boldsymbol{\\Theta}) 。对比 (8) (8) 和 (9) (9) ，于是我们可以定义交叉熵为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = - \\mathrm{mean}\\left[ \\mathbf{y} \\cdot \\log\\left( \\tilde{\\mathbf{y}} \\right) + \\left(1 - \\mathbf{y} \\right) \\cdot \\log\\left(1 - \\tilde{\\mathbf{y}} \\right) \\right]. \\end{align} 注意这里我们使用 \\mathrm{mean}\\left[ \\cdot \\right] \\mathrm{mean}\\left[ \\cdot \\right] 表示求取一个向量所有元素的平均值。实际上，Tensorflow允许我们定义损失函数的输出为一个和输出向量维度相同的向量，Tensorflow自带的交叉熵也是这样定义的。实际应用时，Tensorflow会自动在向量维度上求均值，并压缩成上述 (14) (14) 的形式。 若我们记 \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) ，代入sigmoid函数，为了确保该损失函数的稳定性，我们可以将 (14) (14) 整理为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{x}} \\right) = \\mathrm{mean}\\left[ \\max(\\tilde{\\mathbf{x}}, \\mathbf{0}) - \\tilde{\\mathbf{x}} \\cdot \\mathbf{y} + \\log\\left(1 + e^{-|\\tilde{\\mathbf{x}}|} \\right) \\right]. \\end{align} 提示 这里交叉熵整理的推导过程参见 Tensorflow-API官方文档 。 实际情况下，我们使用 (15) (15) 来求取sigmoid函数激活下的交叉熵。","text_tokens":["l","平面","退化","取","y","并","要","值","第一","定义","下","最大","right","p","即","第一个","mathcal","项","]","tensorflow","提示","含义","自动","视","end","真实","参见",",",";","二个","于是","表示","\\","我们","mathrm","的","则","预测值","}","begin","~","程度","对比","[","15","9","最小","乘积","目的","确保","元素","b","返回","对数","(","这个","为了","最终","最大化","+","均值","每个","align","情况","sigmoid","函数","第二","n","可以","一一","整理","=","作为","结果","预测","反之","&","。","若","实际","theta","过程","这","aligned","是","应用","推导","输出","mean","记","只是","可信度","x","有","可信","以","sum","写出","equation","表明","一个","第","加权","指数","in","压缩","e","8","14","方法","^","分类","消","等价","似然","第二个","个","自带","允许","信度","也","形式","w","却",")","压缩成","交叉","损失","arg","平均值","稳定性","定性","给","为","这样","注意","；","激活","-","维度","会","实际上","boldsymbol","mathbf","至此","_","1","熵","分类器","官方","文档","max","于","然而","对","sigma","limits",".","上述","在","相同","来","k","该","left","就是","缩成","稳定","求取","时","|","将","概率","返回值","亦","这里","超平面","当","api","代入","向量","{","大化","i","并非","所有","确信","准确","使用","平均","去","因子","上求","表述","，","cdot","估计","和","其中","准确度","log","最小化"," ","0","应当","tilde"],"title":"交叉熵","title_tokens":["交叉","熵"]},{"location":"book-1-x/chapter-1/linear-classification/#_7","text":"接下来，我们将开始实战上手，编写我们的第一个Project。虽然一个Project的格式并无定法，每个人按照自己的喜好会选择不同的风格，但一个从无受过训练的人，往往写出的Project几乎完全不具有可读性。实际上，学习任何语言， 变量命名规范 、 缩进规范 以及 模块化 、 面向对象 等都被认为是编写一个具有可读性的代码所不得不知的概念。本教程所推荐的代码格式，均具有统一的风格，读者在了解每个Project和其对应的教程时，会慢慢熟悉这种风格的特点。愿读者能从这样的风格中得到启发，得到 代码可读性 的神髓。","text_tokens":["愿","编写","并","人","第一","、","风格","第一个","虽然","所","推荐","接下来","我们","的","得到","对应","特点","中","读者","接下","无定法","任何","选择","训练","其","喜好","每个","完全","不","。","手","实际","熟悉","但","统一","具有","是","几乎","变量","project","神髓","规范","语言","写出","一个","不知","不得","下来","对象","等","学习","这种","启发","这样","被","格式","会","缩进","概念","实际上","实战","可读","教程","受过","了解","代码","不同","以及","定法","本","无","模块","面向对象","在","认为","按照","往往","均","都","时","可读性","将","从","上","慢慢","无定","模块化","，","面向","命名","开始","和","自己","能"," "],"title":"解线性多分类问题","title_tokens":["分类","多","问题","解","线性"]},{"location":"book-1-x/chapter-1/linear-classification/#_8","text":"建立一个具有较强可读性的Tensorflow工程需要我们活用python的模块化设计。我们通常推荐以下的结构 . ├─ data/ # where we store our data │ └─ ... ├─ tools.py # codes for post-processing and analyzing records. ├─ extension.py # codes for extending the tensorflow model. ├─ dparser.py # data parser └─ main.py # main module where we define our tensorflow model. 除了保存数据的文件夹，我们应当有三个子模块。其中 tool : 用来处理、分析生成的数据，通常与Tensorflow无关； extension : 用来扩展tensorflow，例如在这里自定义网络层和操作符； dparser : 数据处理器，用来读取并预处理送入网络的数据； main : 主模块，只定义跟Tensorflow模型有关的内容，需要引用 extension 和 dparser 。 视情况可以灵活调整结构，但建议将定义Tensorflow模型的代码单独放在主模块里，和其他外围代码分离。 撰写各个模块时，建议使用类封装各组功能相同的函数。具有良好使用习惯的coder应当注意给各个面向用户的类、函数撰写（哪怕简短的）说明文字，在一些较长的函数、方法的定义中，适当注释各部分的功能，以便读者能正确理解代码意义。 另外，在对象命名上，python有如下必须遵守或不成文的规定，和C/C++用户熟悉的蛇形命名法不同，它大致包括 类与函数多用驼峰命名法，变量可以采用驼峰或蛇形命名法。 驼峰命名法指的是用大小写区分每个单词块，例如 alphaBetaFunction () ； 蛇形命名法指的是用下划线区分每个单词块，例如 alpha_beta_function = 10 ； 宏变量使用全字大写+蛇形命名法 函数/方法，还有模块均是首字母小写，但类的首字母大写。 用单下划线 _ 表示临时存储器，或省略参数，例如一个函数 func () 有两个返回值时，可以用 _ , b = func () 表示我们只需要第二个返回值；单下划线还可以与星号连用省略多个返回值； 以单下划线开头的方法，表示模块级的私有方法，在模块以外使用 import 导入类时，不会导入这些方法，例如 def _alphaBeta ( self ): ； 以单下划线结尾的对象，用来和python的关键字区分，例如 func ( x , class_ ) ; 以双下划线开头的方法，如果不以双下划线结尾，则表示类级的私有方法，只有类内部的方法能调用这些方法，在类外部、包括继承的子类里都原则上不能调用（但其实也有办法调用），例如 def _alphaBeta ( self ): ； 以双下划线同时开头和结尾的方法，一般是用来 重写 (override) 特殊功能，例如 def __getattribute__ (): 将重写获得类属性的方法。","text_tokens":["但类","（","小写","功能","关键字","较强","并","alphabetafunction","class","定义","单","、","用户","正确","理器","自定义","驼峰","post","tensorflow","类","单词","采用","─","视","法指","下划线","处理","各组","例如","大致","tool",",","推荐","/","星号","二个","tools","py","不会","表示",";","结构","function","其他","我们","的","beta","意义","重写","文件夹","则","注释","读取","10","数据","records","中","读者","外围","不成文","开头","│","main","包括","灵活","...","私有","大写","b","where","返回","(","里","封装","首字","dparser","必须","文件","+","alphabeta","不以","多用","每个","情况","活用","生成","子","预处理","）","函数","第二","同时","可以","=","we","区分","结尾","。","一些","define","用单","网络层","送入","熟悉","级","蛇形","以外","但","部分","需要","理解","分离","data","调整","模型","具有","以便","成文","临时","导入","c++","是","变量","一般","各","调用","内容","件夹","工程","较长","撰写","全字","原则上","建议","x","有","用","获得","无关","哪怕","一个","├","coder","首字母","the","块","连用","分析","这些","外部","方法","def","__","and","还","参数","存储器","python","属性","第二个","习惯","对象","存储","也","遵守","两个","import","原则","our","文字",")","model","extending","以下","简短","只有","向用","给","特殊","；","注意","-",":","规定","以双","跟","extension","双","处理器","正确理解","override","通常","_","字母","内部","继承","良好","建立","引用","用来","module","类时","可读","除了","主","划线","func","子类","代码",".","└","保存","不同","#","模块","在","相同","设计","操作","法","只","省略","放在","processing","均","面向用户","多个","操作符","都","大小写","时","说明","parser","适当","c","可读性","有关","将","网络","关键","办法","返回值","或","这里","如下","不成","其实","codes","自定","类级","alpha","上","getattribute","宏","for","使用","不能","模块化","大小","扩展","还有","，","各个","下划","三个","面向","analyzing","它","与","以单","self","另外","命名","和","其中","调整结构","能"," ","如果","store","应当","单独"],"title":"代码规范","title_tokens":["规范","代码"]},{"location":"book-1-x/chapter-1/linear-classification/#tensorflow","text":"在Tensorflow中，我们把变量都称为“ 张量 (Tensor) ”。这是因为我们有零维的标量，一维的向量，二维的矩阵，更高维的我们都称为张量。作为一个更大的概念，张量当然也可以用来包括标量、向量和矩阵了。在Tensorflow中，有的张量是 可以训练 (trainable) 的，有的则不是。比如一个张量的形状（指各维大小），当然可以是一个 < tf . Tensor 'Shape:0' shape = ( 1 ,) dtype = int32 > 类型的张量，但它不是变量，当然就不可训练。我们也可以人为控制某些张量可以训练或不可以训练，但本节、乃至本章所介绍的凡是我们接触到的张量，都是可以训练的。 特别地，对于神经网络而言，在网络内计算（或者说流动、传播）的一个n维数据，通常按照以下形式组织： tensor [ batch , dim1 , dim2 , ... , dimn , channel ] 其中，第一个维度 batch 一定存在，它表示的是单个batch中的某一个样本。如果一个batch只有一个样本，那么 batch 只能取0。 从 dim1 到 dimn 指的是实际的n维数据的各个维度； channel 指的是数据的通道，例如，一个二维RGB图像，每种颜色代表一个通道，因此有三个通道。 channel 通常用在卷积网络里，我们经常需要在深度卷积网络里不断增大通道数的同时，缩小数据尺寸。 在某些特殊情况下， channel 维度可以不存在，例如我们使用的是全连接层而不是卷积网络， tf.keras.layer.Flatten 可以用来将一个有通道的张量压缩成一个没有通道的一维向量（但是注意 batch 维度仍然存在，不会被压缩）。 因此，我们知道一个n维的数据，在神经网络中通常被描述为一个n+2维的矩阵，而一个一维向量，在卷积网络里是三维的： vector [ batch , length , channel ] 但是在全连接网络里，是二维的： vector [ batch , channel ] 在本节，乃至本章里，我们还不讨论卷积网络，因此我们都是使用二维张量（一维向量组）作为我们的数据。","text_tokens":["flatten","tf","（","取","一个二维",">","或者","第一","下","、","标量","组织","经常","而言","]","第一个","vector","tensorflow","颜色","是因为","只能","所","二维","例如",",","不会","表示","<","更","到","内","神经网","dim1","一定","我们","介绍","存在","rgb","的","全","本章","batch","则","知道","没有","int32","中","数据","[","因此","当然","就","包括","地","...","”","张量","(","dtype","里","训练","+","情况","dim2","控制","对于","）","不","同时","讨论","n","可以","作为","=","不可","指","。","描述","实际","通道","但","不断","需要","这","：","是","变量","layer","有","比如","用","一个","计算","维","压缩","流动","还","矩阵","层","特别","缩小","tensor","shape","某些","也","形式","神经",")","“","压缩成","连接","以下","keras","只有","因为","length","为","特殊","；","注意","被","三维",":","维度","概念","人为","神经网络","数","通常","零维","更大","'","2","1","图像","用来","但是","或者说",".","接触","样本","在","形状","传播","指各维","把","按照","深度","不是","都","乃至","缩成","卷积","本节","组","而","网络","将","或","高维","增大","从","dimn","向量","一维","尺寸","使用","大小","代表","，","各个","仍然","称为","三个","它","那么","和","了","其中","单个","channel","每种","某"," ","如果","trainable","0","凡是","类型"],"title":"Tensorflow的数据概念","title_tokens":["tensorflow","概念","的","数据"]},{"location":"book-1-x/chapter-1/linear-classification/#_9","text":"在本项目里，我们不需要扩展Tensorflow。但是，我们需要以随机生成数据代替数据集。因此，首先，通过以下代码定义数据生成器 dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class TestDataSet : ''' A generator of the data set for testing the linear model. ''' def __init__ ( self , scale_x , A , c ): ''' Initialize the data generator. scale_x: the scale of input vector. A, c: the linear transformation. ''' self . s_x = 2 * scale_x self . A = A self . c = c self . len_x = A . shape [ 0 ] self . config () def config ( self , train = True , batch = 100 , noise = 0.1 ): ''' Configuration train: a flag for controlling the iterator mode. batch: the number of samples in a batch noise: std. of the error added to the y. ''' self . train = bool ( train ) self . batch = batch self . noise = noise def next_train ( self ): ''' Get the next train batch: (x, y) ''' x = self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) y = np . matmul ( x , self . A ) + self . c if self . noise > 1e-3 : y = y + np . random . normal ( 0 , self . noise , size = y . shape ) y = np . greater ( y , 0.0 ) . astype ( np . float32 ) return x , y def next_test ( self ): ''' Get the next test batch x. ''' return self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) def __iter__ ( self ): while True : samp = self . __next__ () yield samp def __next__ ( self ): if self . train : return self . next_train () else : return self . next_test () 该生成器输入一组 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，以及相关配置，之后就可以通过 迭代器 (iterator) 或 方法 (method) 随机生成数据。这种数据集写法我们在后面还会用到， model . fit 允许我们不是馈入样本（或样本批次），而是馈入一个 生成器(generator) 。因此我们重写了 __iter__ 方法，并使其通过 yield 返回一个生成器。这样我们定义的数据集类就可以被Keras的训练函数 model . fit 使用。接下来，调用如下测试代码： dparser.py 1 2 3 4 5 6 7 8 9 10 def test_dataset (): A = np . random . normal ( 0 , 10 , [ 10 , 6 ]) c = np . random . uniform ( 1 , 3 , [ 1 , 6 ]) dataSet = TestDataSet ( 10 , A , c ) dIter = iter ( dataSet ) for i in range ( 10 ): x , y = next ( dIter ) print ( np . sum ( y , axis = 0 ) / 100 ) test_dataset () Output [ 0.47 0.57 0.58 0.56 0.5 0.38 ] [ 0.6 0.61 0.47 0.48 0.38 0.52 ] [ 0.5 0.61 0.49 0.42 0.45 0.53 ] [ 0.59 0.52 0.44 0.44 0.49 0.51 ] [ 0.54 0.59 0.48 0.5 0.51 0.47 ] [ 0.49 0.57 0.56 0.49 0.53 0.4 ] [ 0.5 0.61 0.51 0.54 0.51 0.52 ] [ 0.5 0.51 0.61 0.5 0.44 0.5 ] [ 0.44 0.46 0.53 0.45 0.56 0.52 ] [ 0.52 0.46 0.51 0.52 0.49 0.44 ] 我们随机生成了 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的数据，每组数据100个，并且测试了10组。输出结果是各组测试中， \\mathbf{y} \\mathbf{y} 在对应维度上分类为1的概率估计。结果基本都在0.5左右，说明我们的这种数据生成模式产生的点能均匀分布在各个超平面两侧，适合进行后续测试。","text_tokens":["s","0.4","（","用到","17","平面","y","class","相关",">","并","samples","diter","首先","16","定义","bool","0.47","0.0","23","]","vector","tensorflow","while","54","*","0.48","各组",",","/","适合","通过","py","点","接下来","40","而是","\\","4","我们","之后","均匀","error","重写","的","batch","0.45","对应","}","~","48","25","10","0.1","数据","[","if","中","接下","15","因此","1e","random","9","输入","就","0.57","100","0.42","init","代替","返回","to","(","里","method","dparser","训练","项目","+","return","uniform","生成器","馈入","其","11","0.46","还会","fit","生成","31","不","）","函数","print","可以","左右","=","结果","of","matmul","。","flag","next","21","器","33","需要","52","data","：","基本","30","linear","controlling","samp","normal","是","成器","0.5","38","test","调用","0.56","输出","51","noise","config","iterator","49","x","19","34","testing","以","6","53","sum","一个","a","5","7","the","output","0.38","集","in","均匀分布","generator","14","8","def","方法","0.59","__","41","float32","下来","^","分类","并且","set","43","0.54","个","else","shape","45","46","允许","22","测试","0.61","configuration","37","测试代码",")","29","13","model","这种","scale","以下","keras","mathbb","批次","这样","为","进行","mapsto","后续","被","-",":","32","testdataset","len","18","astype","0.51","维度","2","'","mathbf","_","1","一组","模式","0.52","input","0.53","0.6","但是","定义数据","50","true","r","代码",".","greater","yield","样本","以及","本","迭代","在","使","np","transformation","该","28","后面","3","不是","都","配置","说明","c","组","12","26","20","mode","写法","每组","概率","或","超平面","如下","两侧","27","std","0.44","分布","上","24","{","36","for","i","number","size","使用","35","42","产生","扩展","train","axis","，","各个","估计","0.58","self","0.49","47","集类","了","39","44","added"," ","get","dataset","range","0","能","initialize","iter","随机"],"title":"数据生成","title_tokens":["生成","数据"]},{"location":"book-1-x/chapter-1/linear-classification/#_10","text":"顺序 (sequential) 模型是一个单输入单输出模型，网络结构较为简单，也不存在跨层短接（残差连接）。在大多数情况下，已经上手的Tensorflow用户不使用这个模型，故而作为我们入门的第一个project，我们姑且用之，但我们将不再使用顺序模型来实现后续的project。一个顺序模型大致可以描述为下图的模式： graph LR st(输<br/>入) --> l1[层<br/>1] l1 --> l2[层<br/>2] l2 --> l3[层<br/>3] l3 --> ldots[层<br/>...] ldots --> ed(输<br/>出) classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,ed styStart 由于我们完成的是一个线性分类器，故而我们使用单层的序列模型即可。 接下来，我们来定义一个类， class LinClsHandle : 。定义一个类的时候，我们通常需要定义的内容包括 在初始化方法 __init__ 里定义传入网络的固定参数，例如学习速率，存取路径等； 在方法 construct 里定义网络的构造和使用的优化器； 在方法 train 里定义训练网络的过程，主要需要调用 model . fit 。如果我们在数据集的定义非常完善，则这一环节不需要花费太多的功夫； 在方法 test 里定义测试网络的过程，主要需要调用 model . evaluate 。如果有必要，可以通过 model . predict 返回测试结果。","text_tokens":["入门","（","输","class","ldots",">","第一","st","l3","下","定义","evaluate","单","较为简单","用户","ba9132","第一个","]","linclshandle","tensorflow","类","环节","跨层","大致","例如",",",";","/","通过","简单","接下来","结构","存在","<","完善","sequential","我们","的","则","大多数","非常","入","数据","[","接下","路径","输入","即可","包括","...","init","返回","(","这个","l2","里","已经","训练","stystart","l1","情况","fit","时候","不","）","可以","fae6a9","作为","结果","。","描述","predict","不再","lr","器","但","需要","过程","这","：","初始化","模型","优化","花费","是","输出","project","内容","实现","graph","固定","调用","test","功夫","存取","有","用","一个","故而","集","一","方法","下来","__","分类","由于","层","参数","网络结构","等","学习","也","测试","construct","线性",")","classdef","连接","model","太多","fill","下图","为","后续","；","-",":","单层","序列","通常","2","初始","1","分类器","模式","短接","姑且",".","#","在","传入","来","出","构造","多数","3","将","网络","残差","stroke","上手","较为","完成","br","大多","使用","train","，","必要","和","速率","主要","ed","之"," ","如果","顺序"],"title":"定义线性顺序模型","title_tokens":["定义","模型","顺序","线性"]},{"location":"book-1-x/chapter-1/linear-classification/#_11","text":"首先，定义初始化方法： lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 def __init__ ( self , learning_rate = 0.01 , epoch = 10 , steppe = 30 ): ''' Initialization and pass fixed parameters. learning_rate: the learning rate for optimizer. epoch: training epochs. steppe: steps per epoch ''' self . lr = learning_rate self . epoch = epoch self . steppe = steppe 由于目前我们的project还非常简单，这里只需要有学习速率( learning_rate )，轮次数( epoch )和每轮迭代次数( steppe )即可。","text_tokens":["非常简单","class","首先","定义","linclshandle","pass",",","py","简单","4","我们","轮","的","非常","10","9","即可","init","(","steppe","learning","=","。","lr","需要","：","初始化","30","project","次数","有","6","5","7","the","fixed","8","def","方法","__","and","还","由于","学习",")","0.01","rate","-",":","per","epochs","initialization","2","初始","cls","_","1","'","training","optimizer","epoch","steps",".","迭代","只","3","每轮","这里","for","，","self","和","速率"," ","lin","parameters","目前"],"title":"初始化方法","title_tokens":["初始","初始化","方法"]},{"location":"book-1-x/chapter-1/linear-classification/#_12","text":"接下来定义网络构造 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def construct ( self ): ''' Construct a linear model and set the optimizer as Adam ''' # Construction self . model = tf . keras . Sequential () self . dense1 = tf . keras . layers . Dense ( LABEL_SHAPE , use_bias = True , input_shape = ( INPUT_SHAPE ,), kernel_initializer = tf . keras . initializers . RandomNormal ( 0.0 , stddev = 10.0 ), bias_initializer = tf . keras . initializers . Constant ( 2 ), activation = tf . nn . sigmoid ) self . model . add ( self . dense1 ) # Set optimizer self . model . compile ( optimizer = tf . train . AdamOptimizer ( self . lr ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ self . accuracy , tf . keras . metrics . BinaryAccuracy ()] ) @staticmethod def accuracy ( y_true , y_pred ): return tf . keras . backend . mean ( tf . keras . backend . equal ( y_true , tf . keras . backend . round ( y_pred ))) 须知 这里 LABEL_SHAPE 和 INPUT_SHAPE 为两个宏变量，分别为输出和输入的向量维度。 我们使用 Dense 定义全连接层，它的用法请参照 这里 。由于我们已经知道 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 可能的取值范围，这里我们重定义了 \\mathbf{W} \\mathbf{W} 和 \\mathbf{b} \\mathbf{b} 的初始化方式。 信息: Dense API tf . keras . layers . Dense ( shape , ** kwargs ) 指全连接层，其输入一组已知形状的向量，输出一组形状为 shape 的向量。可用的API如下： shape : 正整数，输出空间维度。 activation : 激活函数。 若不指定，则不使用激活函数 (即，线性激活: a(\\mathbf{y}) = \\mathbf{y} a(\\mathbf{y}) = \\mathbf{y} )。该函数可以定义为任何元素级操作的Tensorflow函数。 use_bias : 布尔值，该层是否使用偏置向量。 True 则网络定义为 \\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} \\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} ， False 则定义为 \\mathbf{y} = \\mathbf{W}\\mathbf{x} \\mathbf{y} = \\mathbf{W}\\mathbf{x} 。 kernel_initializer : kernel 权值矩阵的初始化器，自定义的初始化器需要使用Keras后端API编写。 bias_initializer : 偏置向量的初始化器，同上。 kernel_regularizer : 运用到 kernel 权值矩阵的正则化函数，自定义的正则化函数需要使用Keras后端API编写。 bias_regularizer : 运用到偏置向的的正则化函数，同上。 activity_regularizer : 运用到层的输出的正则化函数，同上。 kernel_constraint : 运用到 kernel 权值矩阵的约束函数，只能使用Keras备选的几种方案，不能自定义。 bias_constraint : 运用到偏置向量的约束函数，同上。 信息: model.compile API model . compile ( optimizer , ** kwargs ) 在这里指的是顺序模型的编译函数，其可用的API如下： optimizer : 优化器，可以使用Tensorflow内置的优化器。 loss : 损失函数，也是目标函数。顺序模型只有一个输出，因此只能传入一个损失函数。可以使用形式为 func ( y_true , y_pred ) 的Tensorflow函数。 metrics : 测度函数，一般是一组函数，如果是一个函数则定义为 [ func ] 即可。自定义的测度函数目前还需要使用Keras后端API编写。 loss_weights : 损失的权重，顺序模型只有一个损失函数，因此只有一个权重，但要使用一维列表 [ value ] 定义。可以使用张量来控制可变权重。 sample_weight_mode : 按时间步采样权重，默认不提供。相比上面的损失权重，该选项会随着迭代次数使用不同的权重，因此输入的是二维列表。 weighted_metrics : 测度的权重，和损失权重类似，用来加给不同的测度函数。由于我们可以使用不只一个测度函数，这里的权重是个一维列表。 target_tensors : 默认情况下，Keras 将为模型的目标创建一个占位符，在训练过程中将使用目标数据。相反，如果你想使用自己的目标张量（反过来说，Keras在训练期间不会载入这些目标张量的外部 Numpy数据），您可以通过 target_tensors 参数指定它们。对于单输出的顺序模型，它应该是单个张量。 ** kwargs : 其他参量，会传递给 tf . Session . run 。 另外，注意我们这里构造网络的时候有如下技巧： Tensorflow在导入Keras模式以后，已经不再使用 (15) (15) 的形式定义 sigmoid交叉熵 ，而是采取更通用的定义 (14) (14) ； 我们使用Tensorflow重新封装过的类， 二分类交叉熵 ( BinaryCrossentropy ) 来作为Keras模型的损失函数 self.loss ，该函数与 多分类交叉熵 ( CategoricalCrossentropy ) 不同，乃是对两组对比张量的每个元素分别计算交叉熵，再求取均值，正符合本应用的需求； 我们通过静态方法，调用Keras的后端API，自己定义了预测准确度的测度函数 self.accuracy ，同时也使用另一个来自Tensorflow封装好的测度类 二分类准确度 ( BinaryAccuracy ) ，这是为了比照两个准确度的区别，以便我们更好理解该测度类； 我们将网络层的关键字 self.dense1 保留在了实例中，这是为了确保接下来我们能通过实例抽取该层的参数。 需要注意的是，由于 二分类交叉熵 ， 二分类准确度 和 多分类交叉熵 等都是类（从它们的定义都是大写字母开头也可以看出来），因此我们需要在使用的时候后面加上括号以实例化；由于这些类都定义了 __call__ 方法，我们可以像使用函数一样使用它们的实例。","text_tokens":["regularizer","（","要","16","as","定义","add","需求","metrics","*","创建","二维","py","以后","参量","numpy","而是","更","想","其他","的","方案","}","9","多","提供","b","(","来自","重新","binarycrossentropy","备选","布尔","=","方式","lr","target","需要","过程","理解","模型","sample","导入","应用","kernel","activity","5","the","方法","下来","and","分类","set","shape","两个","分别","只有","编译","静态方法","会","过","2","一组","字母","来说","binaryaccuracy","initializers","func","不同","来","value","区别","抽取","equal","关键","mode","自定","从","向量","相反","宏","run","，","可能","另外","权值","和","写字","activation","实例","大写字母","下","0.0","单","即","反过来说","linclshandle","类","再","保留","默认","4","它们","对比","[","接下","输入","更好","张量","kwargs","constant","任何","通用","为了","训练","+","return","内置","11","@","采样","控制","对于","时候","同时","指全","技巧","上面","预测","21","construction","初始化","一般","输出","mean","选项","有","乃是","6","以","一个","a","7","weight","8","还","个","22","construct","反过来","w",")","model","loss","交叉","损失","列表","取值","中将","losses","目标","label","初始","_","weights","权重","模式","两组","用来","对","范围","layers","测度","可用","在","传入","构造","后面","这里","按","化","静态","session","可变","时间","自己","lin","括号","y","weighted","]","只能","正整数",",","compile","通过","接下来","过来","则","中","开头","看","因此","即可","确保","元素","大写","符","pred","封装","均值","round","其","情况","staticmethod","不","可以","指","作为","tensors","器","但","backend","正则","：","以便","变量","次数","同","计算","call","这些","dense","14","外部","def","__","由于","等","use","加","13","应该","keras","该层","为","nn","注意","；","-","信息","categoricalcrossentropy","几种","cls","'","1","熵","randomnormal","10.0","占位","#","操作","该","initializer","符合","3","都","求取","c","向","将","采取","类似","布尔值","上","正","一维","使用","不能","后端","self","它","另","了","单个","准确度","能"," ","如果","顺序","端","tf","重","17","编写","载入","关键字","class","二","反过","参照","自定义","tensorflow","好","请","不会","到","\\","整数","是否","sequential","用法","我们","全","知道","10","数据","15","加上","已经","每个","偏置","sigmoid","）","函数","步","stddev","空间","须知","这是","。","若","网络层","级","传递","不再","优化","linear","是","调用","x","19","bias","比照","出来","已知","矩阵","层","参数","约束","相比","也","形式","线性","adam","随着","连接","给","adamoptimizer","激活",":","维度","18","mathbf","false","运用","input","dense1","像","optimizer","true","期间","一样",".","迭代","本","形状","指定","只","12","网络","过程中将","20","后","您","如下","api","你","{","准确","train","constraint","与","accuracy","目前"],"title":"构造方法","title_tokens":["构造","构造方法","方法"]},{"location":"book-1-x/chapter-1/linear-classification/#_13","text":"最后定义的式训练和测试方法。由于我们目前的project还比较简单，关于这两部分都直接调用现有的API即可。使用的API在之前已经说明。 model.fit 在没有额外设置的情况下，默认会返回一个 History回调器 ； model.evaluate 返回的是测试样本给出的损失函数和准确值测度。 model.predict 返回的是测试样本给出的网络输出。详情请参照 顺序模型API 。 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def train ( self , dataSet ): ''' Use a data set to train the network. ''' return self . model . fit ( dataSet , epochs = self . epoch , steps_per_epoch = self . steppe ) def test ( self , data , labels ): ''' Use (data, label) pairs to test the results. ''' loss , accu = self . model . evaluate ( data , labels ) print ( 'Evaluated loss =' , loss ) print ( 'Evaluated accuracy =' , accu ) return self . model . predict ( data ) 信息: model.fit API model . fit ( dataSet , ** kwargs ) 是训练函数，注意这个函数虽然支持输入一组 x,~y x,~y 用来代替 dataSet ，我们还是建议在任何情况下都用dataSet馈送数据，以免内存中数据集占用过多。 dataSet : 数据集，其本身应当是一个 tf.data.Dataset 类型的类，或者是一个能不断迭代产生新数据的生成器。数据的 batch 大小由 dataSet 本身决定。 epochs : 整数，终止训练时经历的世代(轮次)数，通常一个epoch表示遍历整个数据集一回。 verbose : 0, 1或2。日志显示模式。 0=安静模式, 1=进度条, 2=每轮一行。默认是1。 callbacks : 回调器，它是 tf.keras.callbacks 模块下的类，用来在训练中进行记录保存和数据检查点更新。默认是 tf.keras.callbacks.History 。 validation_split : 在 0 和 1 之间浮动。用作验证集的训练数据的比例。模型将分出一部分不会被训练的验证数据，并将在每一轮结束时评估这些验证数据的误差和任何其他模型指标。验证数据是混洗之前 x 和 y 数据的最后一部分样本中。 validation_data : 元组 (x_val，y_val) 或元组 (x_val，y_val，val_sample_weights) ，用来评估损失，以及在每轮结束时的任何模型度量指标。模型将不会在这个数据上进行训练。这个参数会覆盖 validation_split 。 shuffle : 布尔值（是否在每轮迭代之前混洗数据）。当 steps_per_epoch 非 None 时，这个参数无效。 class_weight : 可选的字典，用来映射类索引（整数）到权重（浮点）值，用于加权损失函数（仅在训练期间）。这可能有助于告诉模型 「更多关注」来自代表性不足的类的样本。 sample_weight : 用来给损失函数添加权重，作用类似 model.compile 的同一参数。 initial_epoch : 开始训练的轮次（有助于恢复之前的训练）。 steps_per_epoch : 在声明一个轮次完成并开始下一个轮次之前的总步数（样品批次）。使用TensorFlow数据张量等输入张量进行训练时，默认值 None 等于数据集中样本的数量除以 batch 的大小，如果无法确定，则为1。 validation_steps : 只有在指定了 steps_per_epoch 时才有用。停止前要验证的总步数（批次样本）。 该函数会返回 callbacks 定义的实例。 信息 model.evaluate API model . evaluate ( x , y , ** kwargs ) 是测试函数，需要传入 label 即 y 来验证性能。 x , y : Numpy 数组，分别是输入和输出的真实参照值。 batch_size : 计算的 batch 大小，该函数会将输入的数据组拆分成许多 batch 分别计算，并组合起来，这个设定值对效果不产生影响，只影响计算过程。 verbose : 0, 1。日志显示模式。0=安静模式, 1=进度条。默认是1。 sample_weight : 用来给损失函数添加权重，作用类似 model.compile 的同一参数。 steps : 整数或 None 。 声明评估结束之前的总步数（批次样本）。默认值 None 。 该函数会返回损失函数和测度（列表）。 信息 model.predict API model . predict ( x , ** kwargs ) 是预测函数，用在只知道输入 x 的场合。 x : Numpy 数组，输入值。 batch_size : 计算的 batch 大小，该函数会将输入的数据组拆分成许多 batch 分别计算，并组合起来，这个设定值对效果不产生影响，只影响计算过程。 verbose : 0, 1。日志显示模式。0=安静模式, 1=进度条。默认是1。 steps : 整数或 None 。 声明评估结束之前的总步数（批次样本）。默认值 None 。 该函数返回预测结果。","text_tokens":["（","」","一行","值","定义","evaluate","虽然","*","py","表示","numpy","更","其他","非","的","轮次","比例","一回","误差","设置","9","多","代替","返回","(","来自","无效","分成","fit","一轮","设定值","布尔","=","不断","verbose","需要","过程","这","模型","sample","project","比较简单","5","the","方法","set","除以","许多","元组","分别","只有","批次","被","关注","会","有助于","数","2","一组","测试函数","时才","更新","来","说明","每轮","确定","组合","产生","代表","，","可能","开始","和","指标","样品","validation","集中","实例","类型","总","history","下","即","val","linclshandle","类","简单","默认","4","度量","batch","没有","直接","进度条","输入","张量","kwargs","任何","训练","return","生成器","11","callbacks","预测","索引","有用","起来","成器","输出","详情","用","6","加权","一个","a","7","weight","8","告诉","还","结束",")","model","loss","损失","记录","列表","label","_","weights","权重","可选","关于","模式","用来","效果","对","测度","steps","样本","以及","在","传入","pairs","评估","混洗","场合","性能","当","给出","检查","用作","回调","两","dataset","lin","过多","y","终止","添加",",","compile","浮点","决定","之前","则","中","覆盖","即可","设定","to","代表性","停止","世代","none","其","情况","有助","生成","print","不","新","steppe","predict","器","部分","data","「","test","显示","split","前要","声明","计算","集","以免","这些","测试方法","14","def","由于","等","use","进度","详情请","13","keras","为","进行","；","注意","遍历","-","per","信息","epochs","内存","shuffle","cls","'","1","影响","保存","模块","组拆","initial","该","3","都","network","时","将","步数","由","类似","检查点","布尔值","本身","上","助于","使用","默认值","占用","同一","self","它","了","能"," ","如果","0","经历","式","顺序","tf","字典","class","并","无法","或者","参照","tensorflow","真实","不会","额外","到","整数","是否","我们","知道","~","仅","10","数据","之间","这个","已经","作用","等于","）","函数","馈送","恢复","比较","结果","。","results","浮动","不足","是","调用","支持","x","建议","最后","参数","labels","查点","测试","用于","给","evaluated",":","数组","通常","映射","分出","epoch","期间",".","迭代","还是","每","指定","只","验证","数量","安静","一部","一部分","12","网络","现有","或","api","整个","完成","size","准确","大小","train","日志","accu","accuracy","应当","目前"],"title":"训练和测试方法","title_tokens":["和","训练","测试","测试方法","方法"]},{"location":"book-1-x/chapter-1/linear-classification/#_14","text":"首先，训练网络。我们随机生成 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的仿射变换，并且设置好数据集，给定噪声扰动为 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 。设定20个epoch，每个epoch迭代500次，每次馈入32个样本构成的batch，然后开始训练： lin-cls.py 1 2 3 4 5 6 7 8 9 # Initialization A = np . random . normal ( 0 , 10 , [ INPUT_SHAPE , LABEL_SHAPE ]) c = np . random . uniform ( 1 , 3 , [ 1 , LABEL_SHAPE ]) dataSet = dp . TestDataSet ( 10 , A , c ) dataSet . config ( batch = 32 , noise = 0.1 ) # Construct the model and train it. h = LinClsHandle ( learning_rate = 0.01 , epoch = 20 , steppe = 500 ) h . construct () record = h . train ( iter ( dataSet )) Output Epoch 1 /20 500 /500 [==============================] - 1s 3ms/step - loss: 6 .3005 - accuracy: 0 .5884 - binary_accuracy: 0 .5884 Epoch 2 /20 500 /500 [==============================] - 1s 2ms/step - loss: 5 .4671 - accuracy: 0 .6407 - binary_accuracy: 0 .6407 Epoch 3 /20 500 /500 [==============================] - 1s 2ms/step - loss: 4 .5711 - accuracy: 0 .6957 - binary_accuracy: 0 .6957 Epoch 4 /20 500 /500 [==============================] - 1s 2ms/step - loss: 3 .6789 - accuracy: 0 .7519 - binary_accuracy: 0 .7519 Epoch 5 /20 500 /500 [==============================] - 1s 2ms/step - loss: 2 .7101 - accuracy: 0 .8127 - binary_accuracy: 0 .8127 Epoch 6 /20 500 /500 [==============================] - 1s 2ms/step - loss: 2 .0059 - accuracy: 0 .8627 - binary_accuracy: 0 .8627 Epoch 7 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .6403 - accuracy: 0 .8894 - binary_accuracy: 0 .8894 Epoch 8 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .3663 - accuracy: 0 .9066 - binary_accuracy: 0 .9066 Epoch 9 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .0466 - accuracy: 0 .9274 - binary_accuracy: 0 .9274 Epoch 10 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .8377 - accuracy: 0 .9418 - binary_accuracy: 0 .9418 Epoch 11 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .6465 - accuracy: 0 .9546 - binary_accuracy: 0 .9546 Epoch 12 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .4492 - accuracy: 0 .9667 - binary_accuracy: 0 .9667 Epoch 13 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .2795 - accuracy: 0 .9779 - binary_accuracy: 0 .9779 Epoch 14 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .1624 - accuracy: 0 .9861 - binary_accuracy: 0 .9861 Epoch 15 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0680 - accuracy: 0 .9926 - binary_accuracy: 0 .9926 Epoch 16 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0128 - accuracy: 0 .9971 - binary_accuracy: 0 .9971 Epoch 17 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0055 - accuracy: 0 .9981 - binary_accuracy: 0 .9981 Epoch 18 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0052 - accuracy: 0 .9986 - binary_accuracy: 0 .9986 Epoch 19 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0055 - accuracy: 0 .9981 - binary_accuracy: 0 .9981 Epoch 20 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0049 - accuracy: 0 .9985 - binary_accuracy: 0 .9985 接下来，从训练返回的 History 类型的回调器中抽取对loss和accuracy的记录。 lin-cls.py 1 2 3 4 5 # Show records plt . semilogy ( record . epoch , record . history [ 'loss' ]), plt . xlabel ( 'epoch' ), plt . ylabel ( 'Cross entropy' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () plt . plot ( record . epoch , record . history [ 'accuracy' ], label = 'self defined' ), plt . plot ( record . epoch , record . history [ 'binary_accuracy' ], label = 'from tensorflow' ), plt . xlabel ( 'epoch' ), plt . ylabel ( 'Accuracy' ), plt . legend () plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () Output 结果显示，我们自定义的准确度测度和Tensorflow内置的 二分类准确度 完全相同，这说明其本身的定义就是求取所有元素阈值化后，各自分类结果是否正确的平均值。这个实验也让我们对自定义测度函数有了一定的认识。 重新设定数据集的产生方式，变为每个batch含10个样本。使用这组重新随机生成的数据测试网络输出， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 12 # Generate a group of testing samples: dataSet . config ( batch = 10 ) x , y = next ( dataSet ) # Check the testing results yp = h . test ( x , y ) _ , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 ) ax1 . imshow ( y , interpolation = 'nearest' , aspect = 'auto' ) ax1 . set_title ( 'True class' ) ax2 . imshow ( yp , interpolation = 'nearest' , aspect = 'auto' ) ax2 . set_title ( 'Predicted class' ) plt . gcf () . set_size_inches ( 10 , 5 ), plt . show () Output Evaluated loss ( losses.BinaryCrossentropy ) = 0 .0023145806044340134 Evaluated accuracy ( self defined ) = 1 .0 Evaluated accuracy ( metrics.BinaryAccuracy ) = 1 .0 注意我们未对测量的结果阈值化，因此显示出来的测量结果和理想值略有差别，但从图可知，阈值化后则测量结果全部准确。 通过抽取 h.dense1 的参数，我们可以对比 \\mathbf{A} \\mathbf{A} 和 \\mathbf{W} \\mathbf{W} ，以及 \\mathbf{c} \\mathbf{c} 和 \\mathbf{b} \\mathbf{b} ， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 # Check the regressed values W , b = h . dense1 . get_weights () plt . imshow ( A , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'A' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . imshow ( W , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'W' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . plot ( c . T , label = 'c' ) plt . plot ( b . T , label = 'b' ) plt . legend () plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () Output 可以发现，虽然我们训练的分类器十分有效，但其权值和预期的 \\mathbf{A} \\mathbf{A} , \\mathbf{c} \\mathbf{c} 并不完全相同。这是由于sigmoid函数激活的特性，使得当预测值偏向最小或最大的情况下， |\\sigma(x)| \\rightarrow 1 |\\sigma(x)| \\rightarrow 1 ，根据 (7) (7) ，可知其梯度 |\\sigma(x)(1-\\sigma(x))| \\rightarrow 0 |\\sigma(x)(1-\\sigma(x))| \\rightarrow 0 ，因此那些分类结果已经比较确信的样本，其梯度消失，对训练网络的影响忽略不计（这是合理的，因为我们不希望极端样本干扰结果，更希望对分类结果不确切的样本进行训练）。故而，我们虽然可以求解出这个分类问题，但求解到的 \\mathbf{W} \\mathbf{W} , \\mathbf{b} \\mathbf{b} 不会回归到 \\mathbf{A} \\mathbf{A} , \\mathbf{c} \\mathbf{c} 上。关于回归问题，我们会在下一节进一步讨论。","text_tokens":["理想","（","值","16","0128","定义","正确","record","imshow","一步","虽然","干扰","metrics","0055","差别","6957","py","更","的","}","ax2","subplots","设置","9","9971","b","返回","nearest","(","重新","7519","from","数据测试","binarycrossentropy","=","方式","这","generate","略有","故而","5","the","下来","^","and","分类","set","shape","偏向","interpolation","7101","2795","rate","semilogy","32","inches","会","auto","boldsymbol","2","分类器","1624","binaryaccuracy","aspect","1s","相同","构成","9418","说明","抽取","5711","defined","8127","从","自定","根据","十分","确信","4671","产生","9546","，","9981","开始","和","权值","5884","get","随机","类型","check","求解","samples","首先","history","下","mathcal","cross","linclshandle","4","gcf","plot","batch","这组","预测值","0.1","对比","[","变换","接下","各自","训练","内置","11","gca","0680","n","预测","dp","输出","发现","noise","有","t","6","a","7","output","那些","8","并且","个","show","construct","w","2ms",")","回归","进一步","噪声","model","loss","记录","平均值","it","plt","losses","initialization","label","扰动","_","weights","关于","一节","predicted","对","r","测度","样本","以及","9274","在","就是","step","|","6403","当","化","每次","9986","图","回调","0023145806044340134","dataset","lin","y","次","9066","]",",","通过","8894","3005","接下来","一定","则","yp","全部","因此","6465","random","xlabel","设定","元素","均值","8377","其","确切","完全","情况","生成","讨论","不","可以","steppe","learning","of","使得","next","但","：","完全相同","test","变为","rightarrow","config","显示","结果显示","testing","给定","仿射","title","集","14","由于","仿射变换","8627","0.01","3ms","13","认识","mathbb","为","进行","9667","注意","mapsto","-","testdataset","cls","'","1","问题","极端","可知","sigma","器中","影响","#","出","3","求取","c","含","本身","上","所有","使用","平均","self","准确度","了","values","忽略"," ","0","normal","17","0059","class","并","二","未","最大","特性","4492","自定义","好","tensorflow","ax1","不会","/","到","\\","是否","h","我们","~","10","数据","records","15","然后","有效","最小","这个","预期","0049","已经","uniform","馈入","每个","梯度","sigmoid","）","函数","0466","9926","结果","这是","比较","合理","。","化后","results","3663","9985","legend","实验","x","19","让","regressed","出来","阈值","不计","参数","500","6407","消失","也","测试","binary","因为","evaluated","激活",":","varepsilon","18","group","mathbf","希望","input","dense1","sim","0052","9779","忽略不计","ylabel","epoch","true","6789",".","迭代","9861","np","entropy","12","网络","20","测量","或","{","colorbar","size","准确","train","accuracy","iter"],"title":"调试","title_tokens":["调试"]},{"location":"book-1-x/chapter-1/linear-regression/","text":"线性回归 ¶ 摘要 本节介绍如何使用类模型(Model)来编写一个线性回归器，以拟合出一个线性模型。本节将第一次介绍如何编写一个带用户参数(选项)的Project，并且允许用户选择不同的优化器、对比验证不同优化算法的效果。 理论 ¶ 一般回归问题 ¶ 设存在一个多输出方程 \\mathbf{y} = \\mathcal{F}(x) \\mathbf{y} = \\mathcal{F}(x) ，当然 \\mathcal{F} \\mathcal{F} 可以是非线性函数，那么我们可以考虑使用一个带可调参数的模型 \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}) \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}) 来模拟它，其中 \\boldsymbol{\\Theta} \\boldsymbol{\\Theta} 是可调的参数。于是，该问题可以被表述为 \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\boldsymbol{\\Theta}} &\\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}_k) \\right),\\\\ \\mathrm{s.t.}~&\\mathbf{y}_k = \\mathcal{F}(\\mathbf{x}_k). \\end{aligned} \\end{equation} 在我们不知道 \\mathcal{F} \\mathcal{F} 的情况下，我们的目的是使用大量的 \\mathbf{x}_k,~\\mathbf{y}_k \\mathbf{x}_k,~\\mathbf{y}_k 样本，来调整出一个最优的近似模型 \\mathbf{D}_{\\boldsymbol{\\Theta}} \\mathbf{D}_{\\boldsymbol{\\Theta}} 。由于 \\mathcal{F} \\mathcal{F} 是非线性的，这要求我们的 \\mathbf{D}_{\\boldsymbol{\\Theta}} \\mathbf{D}_{\\boldsymbol{\\Theta}} 也可以是非线性的。实际情况下，这样的问题往往不容易求解，尤其是信号的非线性性极强时，该问题很容易陷入局部最优解，从而对求得一个可接受的解造成很大的障碍。 这里 \\mathcal{L} \\mathcal{L} 是损失函数。在回归问题中，很多情况下我们都只能选择 均方误差 (Mean squared error, MSE) 作为损失函数，这是因为回归问题的目的是模拟出一组信号来，而这些信号的分布范围可能是任意的。在一些特别的应用里，例如，如果我们的信号全部为正值，那么我们可以考虑使用 信噪比 (Signal-to-noise ratio, SNR) 来作为我们的损失函数。 线性回归 ¶ 继上一节的学习，我们知道如何解一个定义为 \\begin{align} \\mathbf{y} = \\left\\{ \\begin{bmatrix}\\hat{y}_1 > 0 & \\hat{y}_2 > 0 & \\cdots & \\hat{y}_n > 0\\end{bmatrix}^T, ~ \\left| ~ \\hat{\\mathbf{y}} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon} \\right. \\right\\}, \\end{align} 的分类模型。在本节，让我们考虑一个更简单的模型： \\begin{align} \\mathbf{y} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon}. \\end{align} 现在， \\mathbf{y} \\mathbf{y} 是关乎 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 的一个仿射函数，并且我们仍然保留噪声函数 \\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon} 。由于这是一个线性模型，我们可以想象到，存在一个线性回归器， \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ，使得预测结果为 \\begin{align} \\tilde{\\mathbf{y}} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}. \\end{align} 类似上一节，假设我们的数据集 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} 符合 (3) (3) 定义的数据分布特征。我们的基本要求是，在我们不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 的情况下，使用大量 (\\mathbf{x}_k,~\\mathbf{y}_k) \\in \\mathbb{D} (\\mathbf{x}_k,~\\mathbf{y}_k) \\in \\mathbb{D} 样本训练一个线性分类器，使得当我们给定任意一个新样本 \\mathbf{x} \\mathbf{x} 的时候，分类器能推断出其对应的 \\mathbf{y} \\mathbf{y} 。 故而，该问题可以描述为 \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\mathbf{W},~\\mathbf{b}} &\\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathbf{W} \\mathbf{x}_k + \\mathbf{b} \\right), \\\\ \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) &= \\lVert \\mathbf{y} - \\tilde{\\mathbf{y}} \\rVert_2^2. \\end{aligned} \\end{equation} 在本例中， \\mathbf{y} \\mathbf{y} 有正有负，因此我们使用均方误差来作为损失函数。 求解问题 ¶ 作为线性问题，该问题实际上可以写出其解析解。未免读者感到过于突兀，我们先从一个简单的问题开始入手： 例子：一次函数的线性回归 如果我们的矩阵 \\mathbf{A} \\mathbf{A} 退化为标量 a a ，向量 \\mathbf{c} \\mathbf{c} 退化为标量c，那么 (3) (3) 可以重新写为： \\begin{align} y = a x + c + \\varepsilon. \\end{align} 考虑我们拥有N个样本点 (x_k,~y_k) (x_k,~y_k) ，上述问题实际上可以求得解析解。设由这N个点构成了样本向量 \\mathbf{x}_d,~\\mathbf{y}_d \\mathbf{x}_d,~\\mathbf{y}_d (注意与前述的向量区分开来)，则问题可以写成 \\begin{align} \\arg \\min_\\limits{a,~c} \\lVert \\mathbf{y}_d - a \\mathbf{x}_d - c \\mathbf{1} \\rVert^2_2. \\end{align} 这就是附图所示的，拟合到直线的一次函数回归问题。将该损失函数展开，有 \\begin{equation} \\begin{aligned} \\mathcal{L}(a,~c) &= ( \\mathbf{y}_d - a \\mathbf{x}_d - c \\mathbf{1} )^T ( \\mathbf{y}_d - a \\mathbf{x}_d - c \\mathbf{1} )\\\\ &= \\mathbf{y}_d^T\\mathbf{y}_d + a^2 \\mathbf{x}_d^T \\mathbf{x}_d + c^2 + 2ac \\mathbf{1}^T \\mathbf{x}_d - 2 a \\mathbf{y}_d^T \\mathbf{x}_d - 2c \\mathbf{1}^T \\mathbf{y}_d. \\end{aligned} \\end{equation} 令 \\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial a}=0,~\\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial c}=0 \\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial a}=0,~\\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial c}=0 ，则我们得到一组二元一次方程组 \\begin{equation} \\left\\{ \\begin{aligned} a \\mathbf{x}_d^T \\mathbf{x}_d + c \\mathbf{1}^T \\mathbf{x}_d &= \\mathbf{y}_d^T \\mathbf{x}_d. \\\\ c + a \\mathbf{1}^T \\mathbf{x}_d &= \\mathbf{1}^T \\mathbf{y}_d. \\end{aligned} \\right. \\end{equation} 解之，得 \\begin{equation} \\left\\{ \\begin{aligned} a &= \\frac{ \\mathbf{y}_d^T \\mathbf{x}_d - ( \\mathbf{1}^T \\mathbf{y}_d ) ( \\mathbf{1}^T \\mathbf{x}_d ) }{ \\mathbf{x}_d^T \\mathbf{x}_d - (\\mathbf{1}^T \\mathbf{x}_d)^2 } = \\frac{ \\sum_k x_k y_k - \\sum_k y_k \\sum_k x_k }{ \\sum_k (x_k)^2 - \\left(\\sum_k x_k\\right)^2 }. \\\\ c &= \\mathbf{1}^T \\mathbf{y}_d - a ( \\mathbf{1}^T \\mathbf{x}_d ) = \\sum_k y_k - a \\left( \\sum_k x_k \\right). \\end{aligned} \\right. \\end{equation} 这个式子在诸多教材上都会出现，作为学生解回归问题的入门话题。可见，我们在本节讨论的问题并不是一个陌生的问题，相反，我们过去非常熟悉的一个问题，是这个问题的退化到标量下的特殊情况。另，计算该问题的相关系数，我们常使用 \\begin{align} \\rho = \\frac{ \\sum_k \\left(x_k - \\overline{x}\\right) \\left(y_k - \\overline{y}\\right) }{ \\sqrt{ \\sum_k \\left(x_k - \\overline{x}\\right)^2 \\sum_k \\left(y_k - \\overline{y}\\right)^2 } }, \\end{align} 其中 \\overline{x} = \\sum_k x_k ,~ \\overline{y} = \\sum_k y_k \\overline{x} = \\sum_k x_k ,~ \\overline{y} = \\sum_k y_k 。 有了解上述例子的基础，我们自然可以写出， \\begin{equation} \\begin{aligned} \\mathcal{L}(\\mathbf{A},~\\mathbf{c}) &= \\sum_k ( \\mathbf{y}_k - \\mathbf{A} \\mathbf{x}_k - \\mathbf{c} )^T ( \\mathbf{y}_k - \\mathbf{A} \\mathbf{x}_k - \\mathbf{c} )\\\\ &= \\sum_k \\left[ \\mathbf{y}_k^T\\mathbf{y}_k + \\mathbf{x}_k^T \\mathbf{A}^T\\mathbf{A} \\mathbf{x}_k + \\mathbf{c}^T \\mathbf{c} + 2 \\mathbf{c}^T \\mathbf{A} \\mathbf{x}_k - 2 \\mathbf{y}_k^T \\mathbf{A} \\mathbf{x}_k - 2 \\mathbf{y}_k^T \\mathbf{c} \\right]. \\end{aligned} \\end{equation} 提示 接下来的求导主要涉及单值对矩阵求导（导数仍是矩阵），单值对向量求导（导数仍是向量）。可以参考 The Matrix Cookbook 查到对应情况下的求导结果。 同理，令 \\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{A}}=0,~\\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{c}}=0 \\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{A}}=0,~\\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{c}}=0 ，则我们得到一组二元一次方程组 \\begin{equation} \\left\\{ \\begin{aligned} \\sum_k \\left[ \\mathbf{A} \\mathbf{x}_k \\mathbf{x}_k^T + \\mathbf{c} \\mathbf{x}_k^T \\right] &= \\sum_k \\left[ \\mathbf{y}_k \\mathbf{x}_k^T \\right]. \\\\ \\sum_k \\left[ \\mathbf{c} + \\mathbf{A} \\mathbf{x}_k \\right] &= \\sum_k \\left[ \\mathbf{y}_k \\right]. \\end{aligned} \\right. \\end{equation} 解之，得 \\begin{equation} \\left\\{ \\begin{aligned} \\mathbf{A} &= \\left[ N \\sum_k \\left[ \\mathbf{y}_k \\mathbf{x}_k^T \\right] - \\sum_k \\left[ \\mathbf{y}_k \\right] \\sum_k \\left[ \\mathbf{x}_k^T \\right] \\right] \\left[ N \\sum_k \\left[ \\mathbf{x}_k \\mathbf{x}_k^T \\right] - \\sum_k \\left[ \\mathbf{x}_k \\right] \\sum_k \\left[ \\mathbf{x}_k^T \\right] \\right]^{-1} \\\\ \\mathbf{c} &= \\frac{1}{N} \\sum_k \\left[ \\mathbf{y}_k - \\mathbf{A} \\mathbf{x}_k \\right]. \\end{aligned} \\right. \\end{equation} 可见，当上式中的逆不存在时（即低秩的情况），该方程还是有可能解不唯一。 同时，相关系数的计算可以表示为 \\begin{align} \\rho = \\mathrm{mean} \\left[ \\frac{ \\sum_k \\left(\\mathbf{x}_k - \\overline{\\mathbf{x}}\\right) \\cdot \\left(\\mathbf{y}_k - \\overline{\\mathbf{y}}\\right) }{ \\sqrt{ \\sum_k \\left[ \\left(\\mathbf{x}_k - \\overline{\\mathbf{x}}\\right) \\cdot \\left(\\mathbf{x}_k - \\overline{\\mathbf{x}}\\right) \\right] \\sum_k \\left[ \\left(\\mathbf{y}_k - \\overline{\\mathbf{y}}\\right) \\cdot \\left(\\mathbf{y}_k - \\overline{\\mathbf{y}}\\right) \\right] } } \\right]. \\end{align} 这就是 皮尔森相关系数 (Pearson's correlation) 。其中 \\overline{\\mathbf{x}} = \\sum_k \\mathbf{x}_k ,~ \\overline{\\mathbf{y}} = \\sum_k \\mathbf{y}_k \\overline{\\mathbf{x}} = \\sum_k \\mathbf{x}_k ,~ \\overline{\\mathbf{y}} = \\sum_k \\mathbf{y}_k ， \\cdot \\cdot 表示的是两个向量按元素各自相乘。它是式 (11) (11) 在多变量问题上的推广。相当于对向量的每一个元素，分别从统计上求取皮尔森相关系数，然后对向量每个元素对应的皮尔森相关系数求取平均值。 优化算法 ¶ 接下来，我们要介绍几种最常见的优化算法。关于更多这方面的内容，可以查考Google团队编写的在线电子书 Deep Learning 。笔者打算在未来为此开辟专题写文，因此这里只是介绍几种常见的 一阶梯度下降 算法。传统优化领域里，单靠一阶梯度下降往往难以满足对准确度的需求，但深度学习(Deep learning)往往必须使用这些简单的一阶梯度下降算法，就连使用一阶梯度近似二阶梯度的算法 共轭梯度下降 ，在很多情况下都被认为是费用(cost)过高。这是由于一个深度网络，往往具有大量的参数需要训练，因此一个Model的参数少则数十MB，多则上GB。一阶梯度下降算法所需的计算量小，能确保我们一次迭代的过程能迅速完成，因而备受青睐。为了提升其性能，深度学习领域内也对其进行了诸多改进。 注意 其实，论到优化算法，往往不得不提到 反向传播 。不过实际上，一个Tensorflow的入门者，其实完全不需要学习如何推导反向传播的过程。下面我们的叙述也完全不会提及反向传播相关的内容。关于为何我们不需要了解反向传播，在下一节我们会论到。但是，在本教程后期，介绍高级技巧的时候，我们会详细展开。事实上，笔者认为，一个Tensorflow的用户，如果只是为了编写代码，反向传播与ta其实无关痛痒；但只有真正掌握反向传播，我们才算是真正入门了神经网络的理论。 我们在这里说到优化算法，是用在训练网络上的。事实上，只有几种个别的机器学习应用，需要我们在测试阶段执行 迭代算法 (iterative algorithm) 。一般来说，深度学习的训练过程可以被普遍地描述为：已知一个带可调参数 \\boldsymbol{\\Theta} \\boldsymbol{\\Theta} 的模型 \\mathcal{D}_{\\boldsymbol{\\Theta}} \\mathcal{D}_{\\boldsymbol{\\Theta}} ，已知一组数据集 (\\mathbf{x}_i,~\\mathbf{y}_i) \\in \\mathbb{D} (\\mathbf{x}_i,~\\mathbf{y}_i) \\in \\mathbb{D} ，则我们的训练目标为 \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\boldsymbol{\\Theta}} \\mathbb{E}_{(\\mathbf{x}_i,~\\mathbf{y}_i) \\in \\mathbb{D}} \\left[ \\mathcal{L} \\left( \\mathbf{y}_i,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_i) \\right) \\right]. \\end{aligned} \\end{equation} 实际情况下，一般用均值估计来代替上式的期望函数。联系我们上一节的优化问题 (1) (1) 和本节的优化问题 (5) (5) ，都可以描述成上式的形式。也就是说，线性分类/回归器，是神经网络在解线性问题时的特例。 引入动量的优化算法 ¶ 接下来，让我们看看第一个算法， 随机梯度下降 (stochastic gradient descent, SGD) 。 随机梯度下降 记学习率为 \\epsilon \\epsilon ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\mathbf{g} 。 注意学习率一般需要设为一个较小的值，视情况而定。 由于梯度的期望满足 \\begin{equation} \\begin{aligned} \\mathbb{E} \\left[ \\mathbf{g} \\right] &= \\frac{1}{m} \\sum\\limits_{k=1}^m \\mathbb{E} \\left[ \\nabla_{\\boldsymbol{\\Theta}} \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\right] \\\\ &= \\mathbb{E} \\left[ \\nabla_{\\boldsymbol{\\Theta}} \\mathcal{L} \\left( \\mathbf{y},~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}) \\right) \\right] = \\nabla_{\\boldsymbol{\\Theta}} \\mathbb{E} \\left[ \\mathcal{L} \\left( \\mathbf{y},~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}) \\right) \\right]. \\end{aligned} \\end{equation} 可知随机抽取m个样本计算的梯度，在统计学上的期望等于全局梯度的期望。因此，这是一个有效的算法。 随机梯度下降存在明显的弊端，就是在收敛到（全局或局部）最优解的前提下，全局梯度为0，但通过随机选取batch得到的梯度（一般）可能不为0；并且，迭代受到个别极端样本梯度的影响较大，因此，我们有了第一个改进，即 带动量的随机梯度下降 (SGD with momentum) 。 带动量的随机梯度下降 参考文献 提出该算法的文章，可以在这里参考： On the momentum term in gradient descent learning algorithms. Neural Networks 记学习率为 \\epsilon \\epsilon ，惯性常数为 \\alpha \\alpha ，初始化动量 \\mathbf{v}=\\mathbf{v}_0 \\mathbf{v}=\\mathbf{v}_0 （不考虑继续训练的情况下 \\mathbf{v}_0 = \\mathbf{0} \\mathbf{v}_0 = \\mathbf{0} ），则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 计算带动量的更新值 \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} + \\mathbf{v} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} + \\mathbf{v} 。 显然，我们不难计算出， \\begin{equation} \\begin{aligned} \\mathbb{E} \\left[ \\mathbf{v} \\right] &= \\alpha \\mathbb{E} \\left[ \\mathbf{v} \\right] - \\epsilon \\mathbb{E} \\left[ \\mathbf{g} \\right]. \\\\ \\mathbb{E} \\left[ \\mathbf{v} \\right] &= - \\frac{\\epsilon}{1 - \\alpha} \\mathbb{E} \\left[ \\mathbf{g} \\right]. \\end{aligned} \\end{equation} 注意惯性通常需要设为 \\alpha \\in (0,~1) \\alpha \\in (0,~1) 。 这种改进的带来的好处是， 每次更新梯度时，上一次的梯度都会以指数衰减的形式残留在本次迭代中，从而确保新的梯度会被旧的梯度部分中和，避免极端梯度对更新参数影响过大； 当求解得到的梯度陷入局部最优时，如果该局部最优处的曲率较小，可以依靠动量的惯性，越过该局部最优解。 附图说明了使用这种算法的好处。黑色路径为SGD的更新轨迹，而红色路径为本算法的更新轨迹，可以看出随着迭代次数的增加，算法收敛的效果强于SGD。 有人从Nesterov在1983年的论文得到启发，提出了一个修正后的带动量随机梯度下降法，即 带Nesterov动量的随机梯度下降 (SGD with Nesterov momentum) 。 带Nesterov动量的随机梯度下降 参考文献 提出该算法的文章，可以在这里参考： A method for unconstrained convex minimization problem with the rate of convergence o\\left( \\frac{1}{k_2} \\right) o\\left( \\frac{1}{k_2} \\right) 记学习率为 \\epsilon \\epsilon ，惯性常数为 \\alpha \\alpha ，初始化动量 \\mathbf{v}=\\mathbf{v}_0 \\mathbf{v}=\\mathbf{v}_0 （不考虑继续训练的情况下 \\mathbf{v}_0 = \\mathbf{0} \\mathbf{v}_0 = \\mathbf{0} ），则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算惯性目标点的位置： \\boldsymbol{\\Theta}^{\\dagger} \\leftarrow \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} \\boldsymbol{\\Theta}^{\\dagger} \\leftarrow \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} ； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}^{\\dagger}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}^{\\dagger}} (\\mathbf{x}_k) \\right) ； 计算带动量的更新值 \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta}^{\\dagger} - \\epsilon \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta}^{\\dagger} - \\epsilon \\mathbf{g} 。 其实，该方法的更新量期望与前一种方法一样， 显然，我们不难计算出， \\begin{equation} \\begin{aligned} \\mathbb{E} \\left[ \\mathbf{v} \\right] &= - \\frac{\\epsilon}{1 - \\alpha} \\mathbb{E} \\left[ \\mathbf{g} \\right]\\\\ &= \\frac{\\epsilon}{1 - \\alpha} \\nabla_{\\boldsymbol{\\Theta}} \\mathbb{E} \\left[ \\mathcal{L} \\left( \\mathbf{y},~ \\mathcal{D}_{\\boldsymbol{\\Theta} + \\alpha \\mathbf{v}} (\\mathbf{x}) \\right) \\right]. \\end{aligned} \\end{equation} 当收敛到最优解时， \\mathbf{v} \\rightarrow 0 \\mathbf{v} \\rightarrow 0 ，同时有 \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} \\rightarrow \\boldsymbol{\\Theta} \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} \\rightarrow \\boldsymbol{\\Theta} 。我们在此不展开证明这个算法是能收敛的。但Nesterov的文献表明，它能将上面提到的带动量梯度下降算法的误差从 O\\left(\\frac{1}{K}\\right) O\\left(\\frac{1}{K}\\right) 下降到 O\\left(\\frac{1}{K^2}\\right) O\\left(\\frac{1}{K^2}\\right) 。其中 K K 为迭代次数。下图展示了这种方法的改进原理。它的梯度是在更新动量的惯性部分之后才计算出来的，因此新的梯度和之前的惯性是首尾相接的。实际实现时，按照上面的算法，每次迭代需要更新两次参数，计算一次梯度。合理调整算法的计算次序，可以改进为每次迭代更新一次参数，计算一次梯度。 引入可变学习率的优化算法 ¶ 上述几种算法共同的特点是，具有一个“学习率”。实际上，这个学习率非常不好处理，值过小时，收敛速度很慢；值过大时，在最优解附近又难以收敛。为了解决这一思路，我们可以令学习率可变。最简单的思路是，将学习率设为指数衰减的（当然也可以设置下界），这样当开始学习的时候，学习率较大；而即将收敛时，学习率又会较小。 但是，以上做法不过是一些小小的花招(trick)罢了，接下来介绍的几种算法，是根据当前计算出的梯度来自适应调整学习率的。理论上，使用这种算法，用户不再需要特别关注学习率对训练的影响，我们尽可以设置一个偏大的学习率，在训练过程中，它能被自适应调整到一个合适的区间上。 首先，我们来介绍一种初步的改进， Adagrad (Adaptive Subgradient) ， Adgrad 参考文献 提出该算法的文章，可以在这里参考： Adaptive Subgradient Methods for Online Learning and Stochastic Optimization 记学习率为 \\epsilon \\epsilon ，小量 \\delta \\delta ，初始化学习率参数对角矩阵为 \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新学习率为： \\mathbf{r} \\leftarrow \\mathbf{r} + \\mathrm{diag}(\\mathbf{g})^2 \\mathbf{r} \\leftarrow \\mathbf{r} + \\mathrm{diag}(\\mathbf{g})^2 ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} 。 注意文献中常用向量点积 \\odot \\odot 来表示学习率，这样学习率就不是对角矩阵而是向量了。我们这里不定义额外的符号，以便不熟悉相关定义的读者理解。 这一方法的思想是，学习率随着梯度的累计增大而逐渐减小，类似我们使用指数衰减的策略。所不同的是，在梯度小的地方，我们认为梯度平缓，所以学习率减小得慢，以便算法迅速地通过这一片区域；在梯度大地地方，由于梯度陡峭，为了防止我们因为学习率过大漏过该区域，学习率减小得快，以适应梯度的大小。 这个方法没有从根本上解决迭代次数过多时，梯度过小的问题。不难看出该算法学习率以 O\\left(\\frac{1}{\\mathbf{g}^T\\mathbf{g}}\\right) O\\left(\\frac{1}{\\mathbf{g}^T\\mathbf{g}}\\right) 的比率衰减，经验指出，这个算法在很多情况下是不好用的，只能解决一些比较特定的模型。 在这里，我们依然不给出收敛性的证明（或许在未来我们会在专题中讨论这一问题）。读者不必为这些算法的原理感到压力，我们只需要对其有一个直观的了解就好。 考虑到Adagrad学习率减小的速度未免太快了，我们可以考虑它的改进， RMSprop (root mean square proportion) ，注意它是另一个算法Adadelta的特例，不过在本节我们不会讨论Adadelta，有兴趣的读者可以自己去寻找参考资料。 RMSprop 参考文献 提出该算法的文章，可以在这里参考： Overview of mini-batch gradient descent 记学习率为 \\epsilon \\epsilon ，小量 \\delta \\delta ，衰减参数 \\rho \\rho ，初始化学习率参数对角矩阵为 \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新学习率为： \\mathbf{r} \\leftarrow \\rho \\mathbf{r} + (1 - \\rho) \\mathrm{diag}(\\mathbf{g})^2 \\mathbf{r} \\leftarrow \\rho \\mathbf{r} + (1 - \\rho) \\mathrm{diag}(\\mathbf{g})^2 ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} 。 和上一个算法相比，它唯一的改变就是引入了一个衰减参数 \\rho \\rho ，以指数衰减将之前收集的学习率遗忘。如此就可以控制指数过大的问题，这个trick真是令人一言难尽。但是有趣的是，实际经验中，这个方法真的是卓有成效，是现在常用的神经网络优化算法之一。 最后让我们来介绍当前最实用的算法（之一）， Adam (adaptive momentum estimation) 。顾名思义，它的基本原理是基于对动量的可变估计。实际上，在上一节的Project中，我们选用的优化器就是Adam，Tensorflow的官方教程中，也将Adam作为默认推荐的优化器。 Adam 参考文献 提出该算法的文章，可以在这里参考： Adam: a Method for Stochastic Optimization 特别需要注意的是，Adam的收敛性证明已经被后来者推翻，指出其中存在一个错误。改正后的版本称为AMSGrad，Tensorflow的Keras API支持我们在设置Adam的时候开启AMSGrad模式。关于AMSGrad，我们不在此展开讨论，有兴趣的读者可以参考： On the Convergence of Adam and Beyond 记 k k 为迭代次数，学习率为 \\epsilon \\epsilon ，小量 \\delta \\delta ，衰减参数 \\rho_1,~\\rho_2 \\rho_1,~\\rho_2 ，初始化动量为 \\mathbf{s} = \\mathbf{0} \\mathbf{s} = \\mathbf{0} ，学习率参数对角矩阵为 \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新动量为： \\mathbf{s} \\leftarrow \\rho_1 \\mathbf{s} + (1 - \\rho_1) \\mathbf{g} \\mathbf{s} \\leftarrow \\rho_1 \\mathbf{s} + (1 - \\rho_1) \\mathbf{g} ； 更新学习率为： \\mathbf{r} \\leftarrow \\rho_2 \\mathbf{r} + (1 - \\rho_2) \\mathrm{diag}(\\mathbf{g})^2 \\mathbf{r} \\leftarrow \\rho_2 \\mathbf{r} + (1 - \\rho_2) \\mathrm{diag}(\\mathbf{g})^2 ； 调整参数大小： \\hat{\\mathbf{s}} \\leftarrow \\dfrac{\\mathbf{s}}{1 - \\rho_1^k} \\hat{\\mathbf{s}} \\leftarrow \\dfrac{\\mathbf{s}}{1 - \\rho_1^k} , \\hat{\\mathbf{r}} \\leftarrow \\dfrac{\\mathbf{r}}{1 - \\rho_2^k} \\hat{\\mathbf{r}} \\leftarrow \\dfrac{\\mathbf{r}}{1 - \\rho_2^k} ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\dfrac{ \\hat{\\mathbf{s}} }{\\delta + \\sqrt{\\hat{\\mathbf{r}}}} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\dfrac{ \\hat{\\mathbf{s}} }{\\delta + \\sqrt{\\hat{\\mathbf{r}}}} 。 Adam不仅估计了学习率的可变性，还引入了可变的动量。这是迄今为止，我们见到的第一个将动量和可变学习率结合起来的算法。我们当然期望它能带来双份的 快乐 好处，可是…… 为什么会这样呢？ ，已经有文献指出，Adam存在原理上的失误，并提出了改正的算法AMSGrad，这正是我们未来将要在专题中讨论的内容。现在读者只需要知道，Adam的思路其实就是结合动量和可变学习率就行了。 注意 无论是我们没提到的Adadelta还是提到的Adam，其实都引入了动量的概念。那么一个自然而然的idea就是，使用Nesterov动量代替普通的动量。当然，毫无意外的是，已经有人做过了。例如，Adam的Nesterov动量版本叫Nadam，有兴趣的读者不妨去了解一下。 解线性回归问题 ¶ 代码规范 ¶ 重申我们之前提到的，我们建议一个完整的工程应当包括 . ├─ data/ # where we store our data │ └─ ... ├─ tools.py # codes for post-processing and analyzing records. ├─ extension.py # codes for extending the tensorflow model. ├─ dparser.py # data parser └─ main.py # main module where we define our tensorflow model. 除了保存数据的文件夹，我们应当有三个子模块。其中 tool : 用来处理、分析生成的数据，通常与Tensorflow无关； extension : 用来扩展tensorflow，例如在这里自定义网络层和操作符； dparser : 数据处理器，用来读取并预处理送入网络的数据； main : 主模块，只定义跟Tensorflow模型有关的内容，需要引用 extension 和 dparser 。 视情况可以灵活调整结构，但建议将定义Tensorflow模型的代码单独放在主模块里，和其他外围代码分离。 在上一节中，我们没有定义 tool.py 和 extension.py ，这是因为我们的工程还很简单，不需要扩展Tensoflow模型，也不需要专门的数据处理代码。相应地，我们把数据的后处理代码直接集成在了主模块 lin-cls.py 里。在这一节，我们要开始构造一个真正严格按照这四部分分离的工程，并且在接下来的各个例子实现里，都会遵照这个模式，读者应当熟悉类似我们所推荐的、这样一个高度分离的模块化设计的思路。 扩展模块 ¶ 此次是我们第一次写扩展模块，编写扩展模块的目的是，提供一个更复杂的支持库，以便我们能轻松地使用Tensorflow。因此，扩展模块编写地原则应当包括： 可适用性 : 它应当与我们某一个Project完全无关，就像我们自己基于Tensorflow编写一个扩展库一样，以后我们在任何项目都应该可以使用同一个扩展模块文件； 低依赖性 : 它应当最低限度地需要依赖库。 tensorflow 库本身当然是需要的，而 numpy ， matplotlib 甚或是读写数据的模块，都不宜出现在这里，以确保我们的扩展模块被其他任何模块调用时，依赖关系都是树状的； 强一致性 : 它的使用风格，应当尽可能和Tensorflow本身的API一致，使得一个之前不怎么接触它的人，也能快速上手。 在这个工程里，我们扩展的内容其实很简单，就是允许模型调用一个指定的优化器。让我们直接看以下代码： extension.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class AdvNetworkBase : ''' Base object of the advanced network APIs. ''' @staticmethod def optimizer ( name = 'adam' , l_rate = 0.01 , decay = 0.0 ): ''' Define the optimizer by default parameters except learning rate. Note that most of optimizers do not suggest users to modify their speically designed parameters. name: the name of optimizer (default='adam') (available: 'adam', 'amsgrad', 'adamax', 'nadam', 'adadelta', 'rms', 'adagrad', 'nmoment', 'sgd') l_rate: learning rate (default=0.01) decay: decay ratio ('adadeltaDA' do not support this option) ''' name = name . casefold () if name == 'adam' : return tf . keras . optimizers . Adam ( l_rate , decay = decay ) elif name == 'amsgrad' : return tf . keras . optimizers . Adam ( l_rate , decay = decay , amsgrad = True ) elif name == 'adamax' : return tf . keras . optimizers . Adamax ( l_rate , decay = decay ) elif name == 'nadam' : return tf . keras . optimizers . Nadam ( l_rate , schedule_decay = decay ) elif name == 'adadelta' : return tf . keras . optimizers . Adadelta ( l_rate , decay = decay ) elif name == 'rms' : return tf . keras . optimizers . RMSprop ( l_rate , decay = decay ) elif name == 'adagrad' : return tf . keras . optimizers . Adagrad ( l_rate , decay = decay ) elif name == 'nmoment' : return tf . keras . optimizers . SGD ( lr = l_rate , momentum = 0.6 , decay = decay , nesterov = True ) else : return tf . keras . optimizers . SGD ( l_rate , decay = decay ) 我们在这里几乎罗列了所有可能使用的优化器，全部来自Keras API。但我们也可以使用Tensorflow旧版API定义的优化器。目前Tensorflow允许使用两种API中的任意一种来定义，但是实验发现，旧版API系列的优化器要么已经在Keras中能找到对应的版本，要么就水土不服，无法正常调用。因此，上文提到的几种优化器，我们基本上全部在这里用Keras API定义出来。 优化器的参数尽可能应当选择默认参数，并且应当封装起来，不宜让用户自行操作。尤其是Adadelta，Adam这些优化器的 \\rho \\rho 变量，在 Keras文档 中，建议我们遵从默认值。 任何继承该类的子类，都可以通过 self . optimizer ( self . optimizerName , self . learning_rate ) 来将封装好的优化器API调用到主模块中。 项目选项：argparse ¶ 本节将第一次引入 argparse 模块。该模块是python本身具有的原生模块，用来给代码提供启动选项。作为一个完整的Project，我们不希望为了调整参数而频繁地修改代码，因此 argparse 对我们是不可或缺的。在后面所有的Project中，我们都会通过 argparse 模块支持项目选项。 argparse 的官方文档可以在此查阅： argparse — Parser for command-line options, arguments and sub-commands 调用 argparse 的一开始，我们需要定义如下内容： Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import argparse def str2bool ( v ): if v . casefold () in ( 'yes' , 'true' , 't' , 'y' , '1' ): return True elif v . casefold () in ( 'no' , 'false' , 'f' , 'n' , '0' ): return False else : raise argparse . ArgumentTypeError ( 'Unsupported value encountered.' ) parser = argparse . ArgumentParser ( description = 'A demo for linear regression.' , formatter_class = argparse . ArgumentDefaultsHelpFormatter ) Output usage: tools.py [ -h ] A demo for linear regression. optional arguments: -h, --help show this help message and exit 我们首先定义了 str2bool 函数，用来支持用户提供布尔类型的选项；之后，我们初始化了 parser ，一般地初始化 parser 时，我们主要定义三个参数： description : 项目描述，展示在参数用法之前的一段字符串； formatter_class : 格式化器 ，我们一般调用的都是 ArgumentDefaultsHelpFormatter ，因为它能支持自动换行，并在每个参数用法后展示该参数的默认值； epilog : 后记 ，这一段说明文字出现在所有参数用法之后。我们一般不太需要这个功能，但是有时候我们可以使用该功能提供一些用法范例给用户。 现在，我们来介绍几种典型的 argparse 可以提供的参数类型。 字符串选项 1 2 3 4 5 6 7 parser . add_argument ( '-o' , '--optimizer' , default = 'adam' , metavar = 'str' , help = ''' \\ The optimizer we use to train the model (available: 'adam', 'amsgrad', 'adamax', 'nadam', 'adadelta', 'rms', 'adagrad', 'nmoment', 'sgd') ''' ) 在这里我们定义了一个字符串选项，这是最常用的一类选项。用户可以像 python codes.py -o amsgrad 或者 python codes.py --optimizer amsgrad 一样，通过添加参数来覆盖默认值(定义在 default 字段下)。 数值选项 1 2 3 4 5 6 parser . add_argument ( '-lr' , '--learningRate' , default = 0.001 , type = float , metavar = 'float' , help = ''' \\ The learning rate for training the model. ''' ) 这里添加的参数类型是一个浮点数，虽然用户在输入参数的时候输入的是一个字符串，但 metavar 字段告诉了用户应该输入浮点数， type 决定了用户输入的字符串会被自动转换为浮点数。类似地，将两个字段的 float 改为 int ，我们就能提供一个整数作为参数选项 布尔选项 1 2 3 4 5 6 parser . add_argument ( '-if' , '--importFlag' , type = str2bool , nargs = '?' , const = True , default = False , metavar = 'bool' , help = ''' \\ The flag of importing pre-trained model. ''' ) 这里添加的是一个二值选项，它的默认值是 False ，用户可以通过输入 ( 'yes' , 'true' , 't' , 'y' , '1' ) 中的任何一个来指定该选项为真，或通过 ( 'no' , 'false' , 'f' , 'n' , '0' ) 中的任何一个指定该选项为假，不区分大小写。该功能由我们之前定义的 str2bool 函数提供。 特别值得注意的是，这个布尔选项还可以有这样的用法，例如： python codes.py -if -o amsgrad 我们如果指派了 -if ，在不指定它任何值的情况下，该选项就会被开启（值为真）了；如果我们去掉这一行的 -if ，则该选项关闭（值为假）。 多值选项 1 2 3 4 5 6 parser . add_argument ( '-ml' , '--mergedLabel' , default = None , type = int , nargs = '+' , metavar = 'int' , help = ''' \\ The merged label settings. ''' ) 上面的设置提供了一个可以输入任意多个 int 型值的选项，用法如下： python codes.py -ml 1 3 4 0 2 -o amsgrad 上述的输入会被解析成一个值为 [ 1 , 3 , 4 , 0 , 2 ] 的列表。当然，我们也可以输入任意多的值，但是特别值得注意的是，由于在 nargs 字段指定了 + ，一旦我们指派该选项，就要至少输入一个值方可。 上面的几种范例，并不是每一种都需要用在Project中。实际设置选项的时候，应当参照实际情况来处理。例如，本例中，就只使用 字符串选项 和 数值选项 两种。更多关于 add_argument 的用法，请参阅官方文档： argparse — add_argument() 在所有参数都设置好后，调用 args = parser . parse_args () 即可使参数选项生效。用户输入的参数选项将返回到 args 中，例如，如果用户制定了 -o ( --optimizer )，那么我们可以调用 args.optimizer 来取出该字段的值。 数据生成 ¶ 本节的数据也是自动生成出来的。参考上一节的数据生成器，重新定义数据生成类的迭代器： dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class TestDataRegSet ( TestDataSet ): ''' A generator of the data set for testing the linear regression model. ''' def next_train ( self ): ''' Get the next train batch: (x, y) ''' x = self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) y = np . matmul ( x , self . A ) + self . c if self . noise > 1e-3 : y = y + np . random . normal ( 0 , self . noise , size = y . shape ) else : np . random . normal ( 0 , self . noise , size = y . shape ) return x , y 提示 这里我们在没有噪声的情况下，仍然调用随机噪声函数，这是为了确保噪声函数被调用，使得随机数无论开关噪声，都能保持一致性。 该生成器同样是输入一组 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，以及相关配置，之后就可以通过 迭代器 (iterator) 或 方法 (method) 随机生成数据。与上一节不同的是，我们在本节可以尝试更进一步，令 \\mathbf{A} \\mathbf{A} 的 SVD分解 写作如下形式 \\begin{align} \\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^T. \\end{align} 其中， \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma} 是一个对角矩阵，对角线上的元素顺次排列，对应为矩阵 \\mathbf{A} \\mathbf{A} 的各个特征值。Numpy的库已经集成了 SVD分解 。我们知道，一个 M \\times N M \\times N 的矩阵经过SVD分解后，应当有 \\mathbf{U}_{M \\times M} \\mathbf{U}_{M \\times M} 和 \\mathbf{V}^T_{N \\times N} \\mathbf{V}^T_{N \\times N} 两个方阵。故而，矩阵 \\boldsymbol{\\Sigma}_{M \\times N} \\boldsymbol{\\Sigma}_{M \\times N} 并非方阵。由于它只有对角线上有元素，所以必定有多出来的空行或空列。因此，若我们设 K = \\min(M,~N) K = \\min(M,~N) ，则我们可以知道，SVD分解其实不需要矩阵 \\mathbf{U} \\mathbf{U} 和 \\mathbf{V}^T \\mathbf{V}^T 两个方阵都是方阵，因为当我们取矩阵 \\boldsymbol{\\Sigma}_{K \\times K} \\boldsymbol{\\Sigma}_{K \\times K} 这一对角部分后，可以只取部分行/列构成的矩阵 \\mathbf{U}_{M \\times K} \\mathbf{U}_{M \\times K} 和 \\mathbf{V}^T_{K \\times N} \\mathbf{V}^T_{K \\times N} 。这相当于我们略去了 \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma} 上的空行/空列，但是SVD分解仍然能保证恢复出原矩阵来。 在本例中，我们保留 \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma} 中的前 r r 个特征值，其后的特征值都丢弃，我们把这样的做法称为矩阵的低秩近似，于是有 dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 def gen_lowrank ( A , r ): ''' Generate a low rank approximation to matrix A. A: input matrix. r: output rank. ''' sze = A . shape r_min = np . amin ( sze ) assert r <= r_min and r > 0 , 'r should in the range of [1, {0}]' . format ( r_min ) u , s , v = np . linalg . svd ( A , full_matrices = False ) s = np . diag ( s [: r ]) return np . matmul ( np . matmul ( u [:,: r ], s ), v [: r ,:]) 一个低秩近似的矩阵，其定义的仿射变换 (3) (3) 满足不同的 \\mathbf{x} \\mathbf{x} 对应同一个值 \\mathbf{y} \\mathbf{y} ；反之， \\mathbf{y} \\mathbf{y} 将会对应多个不同的解 \\mathbf{x} \\mathbf{x} 。如果我们训练的线性分类器模拟的是 (3) (3) 的逆过程，可能我们会无法模拟出合适的解来；但是，由于我们定义的 (4) (4) 仍是在拟合正过程，故而我们仍然可以把这个问题看成是有解的。在后续的内容中，我们会适当地讨论当问题 解不唯一 时，我们可以进行哪些工作来处理这类问题。 接下来，我们即可测试低秩近似的效果， dparser.py 1 2 3 4 5 6 7 8 9 def test_lowrank (): A = np . random . normal ( 0 , 10 , [ 10 , 6 ]) for r in range ( 1 , 7 ): A_ = gen_lowrank ( A , r ) RMS = np . sqrt ( np . mean ( np . square ( A - A_ ))) R = np . linalg . matrix_rank ( A_ ) print ( 'Rank = {0}, RMS={1}' . format ( R , RMS )) test_lowrank () Output Rank = 1 , RMS = 6.8600432267325955 Rank = 2 , RMS = 4.677152938185369 Rank = 3 , RMS = 3.216810970685858 Rank = 4 , RMS = 1.8380598782932136 Rank = 5 , RMS = 0.9348520972791058 Rank = 6 , RMS = 9.736224609164252e-15 可见，对于一个标准差为10的矩阵，低秩近似的残差仍然是不超过随机高斯矩阵本身的标准差的。这里的秩是我们在调用低秩近似函数后，使用 np.linalg.matrix_rank 测量的结果。 定义类模型 ¶ 类模型 (Model class) ，在官方文档中也称为函数式API，是Tensorflow-Keras的用户大多数情况下应当使用的模型。它支持一些灵活的操作，使得我们可以 多输入多输出 : 类模型的输入和输出层，都是通过函数定义的。类模型在构建的时候，只需要给定输入和输出即可； 跨层短接 : 由于类模型的各层都由函数定义，可以轻松将不同的层连接起来，通常通过 融合层 完成这一工作； 多优化器 : 可以通过复用同一层对应的对象，构建多个不同的类模型，并分别对它们使用不同的训练数据、损失函数、优化器，以实现多优化目标。 一个顺序模型大致可以描述为下图的模式： graph LR st1(输<br/>入<br/>1) --> l11[层<br/>1-1] l11 --> l21[层<br/>1-2] l21 --> l31[层<br/>1-3] l31 --> ldots1[层<br/>...] st2(输<br/>入<br/>2) --> l12[层<br/>2-1] l12 --> l22[层<br/>2-2] l22 --> l32[层<br/>2-3] l32 --> ldots2[层<br/>...] ldots1 --> l3[层<br/>3] ldots2 --> l3 l3 --> l4[层<br/>4] l4 --> ed1(输<br/>出<br/>1) l4 --> ed2(输<br/>出<br/>2) l22 --> ed3(输出3) l21 --> l3 classDef styStart fill:#FAE6A9,stroke:#BA9132; class st1,ed1,st2,ed2,ed3 styStart 在本节中，尽管我们开始使用类模型，但我们定义的仍然是一个单线路的线性回归模型，换言之，这样的模型完全可以通过 顺序模型 实现出来。我们从这一节开始，不再使用顺序模型，其一，是因为顺序模型都可以写成类模型的形式，其二，是希望读者能够熟悉、灵活运用类模型的优势。 我们定义一个继承自 extension.py 的类， class LinRegHandle ( ext . AdvNetworkBase ): 。与上一节的情况相若，这里我们不再赘述需要定义哪些方法。并且，我们也不会介绍一些改动不大、或者不重要的方法，详情请读者参阅源码。 初始化方法 ¶ 首先，定义初始化方法： lin-cls.py: class LinRegHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def __init__ ( self , learning_rate = 0.01 , epoch = 10 , steppe = 30 , optimizerName = 'adam' ): ''' Initialization and pass fixed parameters. learning_rate: the learning rate for optimizer. epoch: training epochs. steppe: steps per epoch optimizerName: the name of optimizer (available: 'adam', 'amsgrad', 'adamax', 'nadam', 'adadelta', 'rms', 'adagrad', 'nmoment', 'sgd') ''' self . lr = learning_rate self . epoch = epoch self . steppe = steppe self . optimizerName = optimizerName 与上一节相比，这里我们增加了一个参数， opmizerName ，用来指定我们选用的优化器名称，默认值为 adam 。 构造方法 ¶ 接下来定义网络构造 lin-cls.py: class LinRegHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def construct ( self ): ''' Construct a linear model and set the optimizer as Adam ''' # Construction input = tf . keras . Input ( shape = ( INPUT_SHAPE ,), dtype = tf . float32 ) dense1 = tf . keras . layers . Dense ( LABEL_SHAPE , use_bias = True , kernel_initializer = tf . keras . initializers . RandomNormal ( 0.0 , stddev = 10.0 ), bias_initializer = tf . keras . initializers . Constant ( 2 ), activation = None , name = 'dense1' )( input ) self . model = tf . keras . Model ( inputs = input , outputs = dense1 ) # Set optimizer self . model . compile ( optimizer = self . optimizer ( self . optimizerName , self . lr ), loss = tf . keras . losses . MeanSquaredError (), metrics = [ self . relation ] ) @staticmethod def relation ( y_true , y_pred ): m_y_true = tf . keras . backend . mean ( y_true , axis = 0 ) m_y_pred = tf . keras . backend . mean ( y_pred , axis = 0 ) s_y_true = tf . keras . backend . sqrt ( tf . keras . backend . mean ( tf . keras . backend . square ( y_true ), axis = 0 ) - tf . keras . backend . square ( m_y_true )) s_y_pred = tf . keras . backend . sqrt ( tf . keras . backend . mean ( tf . keras . backend . square ( y_pred ), axis = 0 ) - tf . keras . backend . square ( m_y_pred )) return tf . keras . backend . mean (( tf . keras . backend . mean ( y_true * y_pred , axis = 0 ) - m_y_true * m_y_pred ) / ( s_y_true * s_y_pred )) 使用类模型时，我们每定义一层，都调用对应的网络层函数，并返回层的输出结果。这就是为何它又叫“函数式API”。我们直接使用均方误差作为我们的损失函数，同时，我们还自行定义了一个评价函数， 皮尔森相关系数 ，该系数专门用来反映两组数据之间是否线性相关，上文我们已经叙述过它的定义。 注意 理想情况下，相关系数应当使用整个数据集来求取。但实际情况下做不到这一点，因此我们求取的相关系数只能看作是一个通过batch得到的估计。故此，我们可以发现，求相关系数要求我们每次输入的样本至少有2个。样本数目越多，相关系数的估计越准确。 注意 从式中可以发现，我们定义的皮尔森相关系数时，完全使用的时Tensorflow-Keras API，因此它当然可以用作我们的训练损失函数。但实际情况下，我们并不使用它。考虑一个反例，当两组数据的分布之间唯一的不同只是均值时，亦即 \\mathbf{y}_2 = \\mathbf{y}_1 + C \\mathbf{y}_2 = \\mathbf{y}_1 + C ，这种情况下皮尔森相关系数仍然为1。虽然我们可以考虑用 余弦相似度函数 (Cosine similarity) 来代替它，但经验显示，余弦相似度最大化到一定程度以后，其对应的均方误差反而上升。考虑另一个反例， \\mathbf{y}_2 = \\alpha \\mathbf{y}_1 \\mathbf{y}_2 = \\alpha \\mathbf{y}_1 ，显然 \\mathbf{y}_1 \\mathbf{y}_1 和 \\mathbf{y}_2 \\mathbf{y}_2 的余弦相似度是1。因此，实际应用中，无论是皮尔森相关系数还是余弦相似度，都适合用作评价函数而不是损失函数。 与上一节不同的是，由于这是一个线性回归器，我们不给它提供激活函数。 训练和测试方法 ¶ 类模型的 compile 、 fit 、 evaluate 、 predict 等API与顺序模型完全相同，详情请查看： Model类 (函数式API) - Keras中文文档 调试 ¶ 上一节中，我们每次训练后，就当场显示分析结果。在本节中，我们会“再进一步”。即使用 tools.py 专门进行实验结果分析（后处理）。相对地，训练后，我们会讲 原始输出 (raw output) 保存到文件里。这是一种编写代码的思想，是为了便于我们批量分析测试数据。在后面的Project中，我们会看到，我们既会编写当场显示分析结果的测试代码，也会编写保存输出后使用 tools.py 分析的代码。究竟使用哪种方式分析数据，视具体情况而定。一般地，测试少量数据时，我们当场分析；批量测试大量数据时，或者需要比较不同选项（例如不同噪声）对结果的影响时，我们在 tools.py 中分析。本实验的情况属于后者。 使实验结果可复现 ¶ 由于我们本次实验需要对比不同设置下的回归器性能，我们希望随机生成的矩阵 \\mathbf{A} \\mathbf{A} ，向量 \\mathbf{c} \\mathbf{c} 应当可复现；换言之，我们希望我们的结果是可复现的。 关于这一问题，Keras的文档给出的建议可以在这里查阅： 如何在 Keras 开发过程中获取可复现的结果？ - Keras中文文档 我们只需要使 argparse 添加一个选项 -sd ( --seed )，并通过该选项控制： 1 2 3 4 5 6 def setSeed ( seed ): np . random . seed ( seed ) random . seed ( seed + 12345 ) tf . set_random_seed ( seed + 1234 ) if args . seed is not None : # Set seed for reproductable results setSeed ( args . seed ) 其中， np.random.seed ， random.seed ， tf.set_random_seed 分别来自Numpy，python原生的random库，以及Tensorflow。将这三个库的 随机种子 (seed) 设为三个不同的值，即可保证我们每次指定 -sd 后，从程序运行开始，得到的所有随机数都是固定的随机序列。当然， Keras文档 指出，即使如此，我们还不能保证我们的结果完完全全是可复现的。因为多线程算法并发的先后顺序随机性、GPU运算带来的先后顺序随机性等干扰因素，均会导致我们每次得到的结果有细微的偏差。但这些因素对于本实验验证可复现数据的要求几乎没有什么影响。 使实验代码保存输出 ¶ 首先，训练网络。我们同样随机生成 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的仿射变换，将该变换中的线性变换矩阵采用秩为4的低秩近似，并且设置好数据集，给定噪声扰动由用户决定。默认值下，噪声为 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,10)^6 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,10)^6 ，epoch为20个，每个epoch迭代500次，每次馈入32个样本构成的batch。我们将上一节的主函数输出部分修改成如下形式，并进行不加参数的调试： lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # Initialization A = dp . gen_lowrank ( np . random . normal ( 0 , 10 , [ INPUT_SHAPE , LABEL_SHAPE ]), RANK ) c = np . random . uniform ( 1 , 3 , [ 1 , LABEL_SHAPE ]) dataSet = dp . TestDataRegSet ( 10 , A , c ) dataSet . config ( noise = args . noise ) # Generate a group of testing samples. if args . seed is not None : setSeed ( args . seed + 1000 ) dataSet . config ( batch = args . testBatchNum ) x , y = next ( dataSet ) # Set the data set for training. dataSet . config ( batch = args . trainBatchNum ) # Construct the model and train it. h = LinRegHandle ( learning_rate = args . learningRate , epoch = args . epoch , steppe = args . steppe , optimizerName = args . optimizer ) h . construct () print ( 'Begin to train:' ) print ( '---------------' ) record = h . train ( iter ( dataSet )) # Check the testing results print ( 'Begin to test:' ) print ( '---------------' ) yp , loss_p , corr_p = h . test ( x , y ) # Check the regressed values W , b = h . model . get_layer ( name = 'dense1' ) . get_weights () # Save if args . outputData is not None : np . savez_compressed ( args . outputData , epoch = record . epoch , loss = record . history [ 'loss' ], corr = record . history [ 'relation' ], test_x = x , test_y = y , pred_y = yp , pred_loss = loss_p , pred_corr = corr_p , W = W , b = b , A = A , c = c ) Output Begin to train: --------------- Epoch 1 /20 500 /500 [==============================] - 1s 2ms/step - loss: 29084 .6994 - relation: 0 .3472 Epoch 2 /20 500 /500 [==============================] - 1s 2ms/step - loss: 15669 .9579 - relation: 0 .5597 Epoch 3 /20 500 /500 [==============================] - 1s 2ms/step - loss: 8145 .8705 - relation: 0 .7134 Epoch 4 /20 500 /500 [==============================] - 1s 2ms/step - loss: 4000 .0838 - relation: 0 .8130 Epoch 5 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1856 .1477 - relation: 0 .8801 Epoch 6 /20 500 /500 [==============================] - 1s 2ms/step - loss: 799 .4556 - relation: 0 .9354 Epoch 7 /20 500 /500 [==============================] - 1s 2ms/step - loss: 336 .8600 - relation: 0 .9700 Epoch 8 /20 500 /500 [==============================] - 1s 2ms/step - loss: 166 .5899 - relation: 0 .9813 Epoch 9 /20 500 /500 [==============================] - 1s 2ms/step - loss: 113 .2465 - relation: 0 .9831 Epoch 10 /20 500 /500 [==============================] - 1s 2ms/step - loss: 102 .0431 - relation: 0 .9834 Epoch 11 /20 500 /500 [==============================] - 1s 2ms/step - loss: 99 .6678 - relation: 0 .9838 Epoch 12 /20 500 /500 [==============================] - 1s 2ms/step - loss: 99 .8547 - relation: 0 .9833 Epoch 13 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .1278 - relation: 0 .9834 Epoch 14 /20 500 /500 [==============================] - 1s 2ms/step - loss: 99 .6048 - relation: 0 .9835 Epoch 15 /20 500 /500 [==============================] - 1s 2ms/step - loss: 101 .1930 - relation: 0 .9832 Epoch 16 /20 500 /500 [==============================] - 1s 2ms/step - loss: 101 .6636 - relation: 0 .9835 Epoch 17 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .6665 - relation: 0 .9834 Epoch 18 /20 500 /500 [==============================] - 1s 2ms/step - loss: 101 .2459 - relation: 0 .9832 Epoch 19 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .9701 - relation: 0 .9836 Epoch 20 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .7719 - relation: 0 .9836 Begin to test: --------------- 10 /10 [==============================] - 0s 5ms/sample - loss: 94 .8883 - relation: 0 .9897 Evaluated loss ( losses.MeanSquaredError ) = 94 .88829040527344 Evaluated metric ( Pearson ' s correlation ) = 0 .9897396 以上结果是不加任何参数的前提下，直接以默认参数运行程序得到的。结果显明，MSE最后收敛在100左右，因为我们馈入的label添加了标准差为10的白噪声，对应的方差为100。可知，实验结果与预期一致。另一方面，我们可以看到，相关系数在这里可以充当类似准确度的作用，考虑到我们默认的噪声为10，这一相关系数的收敛结果是符合我们的预期的。 我们还可以注意到，这段代码中，生成测试集的代码被提前了，这是为了确保每次运行程序，只要指定了种子，生成的测试集总是一致的。 现在，我们可以导出生成数据了，首先，我们改变不同的优化器，其他参数全部一致，例如，学习率均为0.01（Adadelta除外，其初始参数一般推荐为1.0）。调用代码时的参数设置如下 python lin-reg.py -e 25 -sd 1 -do test/algorithm/ { optimizer } -o { optimizer } 其中我们用 {optimizer} 来指代我们选用的优化算法。同时，我们固定测试的epoch数量为25，这是因为有些算法的收敛速度不足以保证20个epoch收敛。 接下来，我们固定优化器为Adam，改变不同的噪声，分别令标准差为0, 1, 5, 10, 50, 100，产生多组结果。 python lin-reg.py -sd 1 -do test/noise/ { noise } -is { noise } 在 tools.py 中分析比较结果 ¶ 首先，在 tools.py 中定义数据解析函数 tools.py 1 2 3 4 5 6 7 8 9 10 11 12 def parseData ( path , keys ): keys_list = dict (( k , []) for k in keys ) name_list = [] for f in os . scandir ( path ): if f . is_file (): name , _ = os . path . splitext ( f . name ) name_list . append ( name . replace ( '_' , ' ' )) data = np . load ( os . path . join ( path , f . name )) for key in keys : keys_list [ key ] . append ( data [ key ]) epoch = data [ 'epoch' ] return name_list , epoch , keys_list 该函数的作用是，给定保存输出文件的文件夹路径，能够自动读取文件夹下所有数据文件，并将不同文件的结果列在列表的不同元素中。 keys 关键字能帮助我们指派我们关心的数据字段。 接下来，我们通过如下代码，对比不同优化器条件下的损失函数和测度函数，对比不同噪声条件下的损失函数和测度函数，输出的曲线反映了对训练过程的跟踪。 tools.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def showCurves ( path , prefix = '{0}' , converter = str ): ''' Show curves from different tests in a same folder. ''' name_list , epoch , keys_list = parseData ( path , [ 'loss' , 'corr' ]) loss_list = keys_list [ 'loss' ] corr_list = keys_list [ 'corr' ] if ( not loss_list ) or ( not corr_list ): raise FileExistsError ( 'No data found, could not draw curves.' ) for i in range ( len ( loss_list )): plt . semilogy ( loss_list [ i ], label = prefix . format ( converter ( name_list [ i ])), marker = MARKERS [ i % 9 ], markevery = 3 ) plt . legend () plt . xlabel ( 'epoch' ), plt . ylabel ( 'MSE' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . tight_layout (), plt . show () for i in range ( len ( corr_list )): plt . plot ( corr_list [ i ], label = prefix . format ( converter ( name_list [ i ])), marker = MARKERS [ i % 9 ], markevery = 3 ) plt . legend () plt . xlabel ( 'epoch' ), plt . ylabel ( 'Pearson \\' s correlation' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . tight_layout (), plt . show () showCurves ( './test/algorithm' ) showCurves ( './test/noise' , prefix = 'ε=N(0,{0})' , converter = int ) Output (优化器) 损失函数 (MSE) 测度函数 (相关系数) Output (噪声) 损失函数 (MSE) 测度函数 (相关系数) 可见，损失曲线反映了训练的进度，而测度曲线反映了当前的准确度。我们可以得到如下结论： 令人意外的是，SGD和Nesterov动量法收敛速度最快。这是由于这两种方法没有引入对学习率的调整。我们使用的损失函数初始点梯度非常大，这使得简单的方法，形如SGD和动量法在一开头就取得了非常迅速的下降；而对那些需要调整学习率的算法而言，初始梯度在很大的情况下，会导致初始学习率被降到较小的水准。这就是为何Adagrad几乎不收敛的原因，因为一开始这一算法的学习率就被大梯度抑制到将近0的水平了，导致训练无法为继； 在调整学习率的算法里，收敛速度有 RMSprop > Adam = NAdam > Adamax = AMSgrad > Adadelta。从AMSgrad以上的这些算法都可资利用，Adadelta的原理和RMSprop几乎相同但效果相差甚巨，这是由于参数不同引起的，我们虽然将Adadelta的学习率特地设为 1.0 ，仍然远远不如RMSprop，可见一个合适的参数对算法的重要性。 噪声的输出结果并不令人意外，所有噪声条件下的MSE最后都收敛到对应的噪声方差上。 为了检查测试集的情况，我们通过以下函数来绘制比较不同样本在不同优化器、不同噪声条件下的RMSE（均方根误差）， tools.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def showBars ( path , prefix = '{0}' , converter = str , ylim = None ): ''' Show bar graphs for RMSE for each result ''' name_list , epoch , keys_list = parseData ( path , [ 'test_y' , 'pred_y' ]) #print(keys_list) ytrue_list = keys_list [ 'test_y' ] ypred_list = keys_list [ 'pred_y' ] def RMSE ( y_true , y_pred ): return np . sqrt ( np . mean ( np . square ( y_true - y_pred ), axis = 1 )) N = ytrue_list [ 0 ] . shape [ 0 ] NG = len ( ytrue_list ) for i in range ( NG ): plt . bar ([ 0.6 + j + 0.8 * i / NG + 0.4 / NG for j in range ( - 1 , 9 , 1 )], RMSE ( ytrue_list [ i ], ypred_list [ i ]), width = 0.8 / NG , label = prefix . format ( converter ( name_list [ i ]))) plt . legend ( ncol = 5 ) plt . xlabel ( 'sample' ), plt . ylabel ( 'RMSE' ) if ylim is not None : plt . ylim ([ 0 , ylim ]) plt . gcf () . set_size_inches ( 12 , 5 ), plt . tight_layout (), plt . show () showBars ( './test/algorithm' , ylim = 70 ) showBars ( './test/noise' , prefix = 'ε=N(0,{0})' , converter = int ) Output (优化器) Output (噪声) 上述结果反映了 测试结果和训练情况相仿，这是由于我们的训练集和测试机完全独立同分布； Adadelta和Adagrad还没有训练好，它们的误差明显大于其他算法。且Adagrad已经无法收敛，可见这种算法不实用。 再接下来，我们要分别展示不同测试下的输出。下面列举的所有输出由该函数所产生： tools.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def saveResults ( path , opath , oprefix , datakeys , title = '' , xlabel = None , ylabel = None , onlyFirst = False , plot = False , prefix = ' ({0})' , converter = str ): ''' Save result graphs to a folder. ''' name_list , _ , data_list = parseData ( path , datakeys ) if plot : # show curves c_list = data_list [ 'c' ] b_list = data_list [ 'b' ] NG = len ( b_list ) for i in range ( NG ): plt . plot ( c_list [ i ] . T , label = 'c' ) plt . plot ( b_list [ i ] . T , label = 'b' ) plt . legend () plt . gca () . set_title ( title + prefix . format ( converter ( name_list [ i ]))) if xlabel is not None : plt . xlabel ( xlabel ) if ylabel is not None : plt . ylabel ( ylabel ) plt . gcf () . set_size_inches ( 5 , 5 ) if onlyFirst : formatName = '' else : formatName = name_list [ i ] . replace ( ' ' , '_' ) plt . savefig ( os . path . join ( opath , oprefix + '{0}.svg' . format ( formatName ))) plt . close ( plt . gcf ()) if onlyFirst : return else : # show images data_list = data_list [ datakeys [ 0 ]] NG = len ( data_list ) for i in range ( NG ): plt . imshow ( data_list [ i ], interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( title + prefix . format ( converter ( name_list [ i ]))) if xlabel is not None : plt . xlabel ( xlabel ) if ylabel is not None : plt . ylabel ( ylabel ) plt . gcf () . set_size_inches ( 6 , 5 ) if onlyFirst : formatName = '' else : formatName = name_list [ i ] . replace ( ' ' , '_' ) plt . savefig ( os . path . join ( opath , oprefix + '{0}.png' . format ( formatName ))) plt . close ( plt . gcf ()) if onlyFirst : return 测试代码 1 2 3 4 5 6 7 8 9 10 11 12 13 def saveAllResults (): saveResults ( './test/algorithm' , './record/algorithm' , 'alg-A-' , [ 'A' ], title = 'A' , prefix = '' , onlyFirst = True ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-yt-' , [ 'test_y' ], title = 'True values' , prefix = '' , onlyFirst = True ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-y-' , [ 'pred_y' ], title = 'Predicted values' ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-W-' , [ 'W' ], title = 'W' ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-cb-' , [ 'c' , 'b' ], title = 'Biases' , plot = True ) saveResults ( './test/noise' , './record/noise' , 'noi-A-' , [ 'A' ], title = 'A' , prefix = '' , onlyFirst = True ) saveResults ( './test/noise' , './record/noise' , 'noi-yt-' , [ 'test_y' ], title = 'True values' , prefix = ' (ε=N(0,{0}))' , converter = int ) saveResults ( './test/noise' , './record/noise' , 'noi-y-' , [ 'pred_y' ], title = 'Predicted values' , prefix = ' (ε=N(0,{0}))' , converter = int ) saveResults ( './test/noise' , './record/noise' , 'noi-W-' , [ 'W' ], title = 'W' , prefix = ' (ε=N(0,{0}))' , converter = int ) saveResults ( './test/noise' , './record/noise' , 'noi-cb-' , [ 'c' , 'b' ], title = 'Biases' , plot = True , prefix = ' (ε=N(0,{0}))' , converter = int ) saveAllResults () 首先考虑不同优化器的测试，在这些测试里，我们确保 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 对所有测试相同，且产生的随机数据真值 \\mathbf{y}_{\\mathrm{true}} \\mathbf{y}_{\\mathrm{true}} 对所有测试也相同，亦即： \\mathbf{A} \\mathbf{A} \\mathbf{y} \\mathbf{y} 的真实值 于是我们可得到所有的数据 优化器 \\mathbf{y} \\mathbf{y} 的预测值 \\mathbf{W} \\mathbf{W} \\mathbf{b} \\mathbf{b} 与 \\mathbf{c} \\mathbf{c} Adadelta Adagrad Adam Adamax AMSgrad Nesterov Adam Nesterov Moment RMSprop SGD 接下来考虑不同噪声的测试，在这些测试里，我们确保 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 对所有测试相同，但由于噪声大小的不同，随机数据真值 \\mathbf{y}_{\\mathrm{true}} \\mathbf{y}_{\\mathrm{true}} 会有所偏差： \\mathbf{A} \\mathbf{A} 于是我们可得到所有的数据 \\boldsymbol{\\varepsilon} \\sim N(0,~?) \\boldsymbol{\\varepsilon} \\sim N(0,~?) \\mathbf{y} \\mathbf{y} 的真实值 \\mathbf{y} \\mathbf{y} 的预测值 \\mathbf{W} \\mathbf{W} \\mathbf{b} \\mathbf{b} 与 \\mathbf{c} \\mathbf{c} 0 1 5 10 50 100 我们最为看重的，其实是是否拟合出 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 。一系列实验表明， \\mathbf{W} \\mathbf{W} 和 \\mathbf{b} \\mathbf{b} 的拟合效果甚好。由于我们建立的仿射变换模型和原始仿射变换模型有着完全一致的结构，优化结果反映这一问题的解相当准确。至此，我们已经掌握了一个完整的Project应当具有的模块结构，以及对不同的优化器有了理论和实际的体验。在后续的章节里，除非有特别的应用，我们不再探讨不同的优化器对结果的影响，在绝大多数情况下，我们都将使用AMSgrad。","text_tokens":["该类","opmizername","列","as","evaluate","用户","虽然","是因为","干扰","166","接起","参考文献","*","期望","tool","适合","py","最","而是","numpy","join","统计","曲率","局部","full","制定","非常","l31","4.677152938185369","误差","epsilon","后记","sqrt","较","where","返回","could","(","调试","字","svd","%","为何","from","一方","布尔","=","自行","&","总是","方式","lr","查考","理解","模型","应用","性极","equation","├","即使如此","故而","raise","epilog","降到","两个","6994","本例","高斯","下图","关注","32","继","过","auto","dfrac","分类器","commands","initializers","率均","带来","看到","aspect","1s","yt","相同","来","传播","前提","left","多则","有关","26","线程","分布","根据","模拟","利用","去","开发","去掉","和","get","评价","子书","on","同一个","随机","类型","备受","一片","首先","依赖性","上升","end","写","又","low","batch","预测值","得","没有","思路","有解","写文","[","变换","接下","可是","尽管","秩","失误","100","8883","二元","普通","regression","生成器","便于","l11","先","全局","相若","推翻","rms","发现","快速","os","iterator","秩是","常见","t","规范","依赖","amin","7","output","略去","fixed","水准","8","最为","还","float32","对角线","不如","do","允许","偏差","our","2ms","model","列表","特殊","跟","自然","成上","初始","小","均方","关于","收集","要求","建立","处","毫无","限度","lowrank","代码","在","l21","成","令","trained","偏大","相似","数据文件","adadelta","就是","后面","避免","增大","方阵","24","6636","799","正常","ytrue","help","余弦","水土","必定","自己","2ac","convex","多线","计算出来","113","marker","设","l","首尾","取","9838","right","alg","mb","]","强时","dict","跨层","pass","只能","不服","compile","浮点","方可","独立","决定","之前","解","取得","relation","yp","reg","开头","│","目的","确保","元素","to","详细","格式化","真正","最优","pred","展开讨论","none","且","完全","机","beyond","预处理","6.8600432267325955","最快","matmul","就要","next","各层","predict","testbatchnum","但","backend","电子书","102","message","固定","graph","绝大","减小","显示","parse","点数","给定","仿射","title","集","unconstrained","测试方法","14","def","ed3","__","对角","看作","测试代码","加","进度","详情请","应该","algorithms","mathbb","为","遵照","见到","概念","一下","哪","'","methods","cls","1","年","定义数据","树状","keys","ext","论","outputs","接触","保存","different","此次","出","该","3","都","解不","将","线性相关","本身","正","看成","¶","overview","使用","不能","42","痛痒","第一次","self","就是说","另","analyzing","能"," ","default","出原","tf","cosine","should","无法","第一","9835","改进","基础","模拟出","tensorflow","防止","率","例如","有人","共轭","h","之后","对应","知道","~","自然而然","10","绝大多数","records","有效","之间","里","共同","moment","31","stddev","0.001","造成","可见","明显","theta","不足","有时","修正","是","调用","8600","支持","x","matrices","中文","101","5ms","已知","陡峭","int","层","一方面","43","查阅","markers","对象","rho","测试","import","转换","取出","运行","大漏","连接","以下","给","实用","unsupported","splitext","更进一步","查","希望","不到","stochastic","inputs","sim","png","顾名","之一","ylabel","epoch","args","一种",".","故此","空列","称","初步","改为","k","with","弊端","均","数量","难以","g","集成","或","l32","load","并非","前","argumentparser","demo","相对","称为","三个","与","其中","corr","连接起来","策略","应当","iter","目前","迄今","摘要","度","小写","假设","从而","imshow","onlyfirst","lvert","严格","顺次","兴趣","为此","常用","所以","的","}","路径","name","9836","重新","轨迹","compressed","matrix","文章","casefold","关心","陌生","带动","不太","pearson","实际","ed2","关系","6048","具有","learningrate","机数","project","generate","件夹","多线程","15669","4556","方法","bar","过去","分类","and","41","ta","testdataregset","随机数","将要","看重","37","器有","真值","or","extending","fill","rate","semilogy","后续","普遍","会","extension","处理器","note","序列","机器","点积","boldsymbol","1234","方根","一组","率为","学生","94","training","ldots2","入门者","descent","版本","赘述","构成","放在","showcurves","继上","自定","27","讲","向量","networks","36","frac","标准","迅速","表述","，","cdot","check","求解","l4","选取","description","慢","甚巨","0.0","单","理器","拟合","post","构造方法","经验","linreghandle","root","简单","key","默认","1930","它们","特点","直接","费用","对比","算法","训练","+","11","gca","绘制","技巧","文献","1278","罢了","种","送入","熟悉","21","复杂","名称","construction","初始化","起来","tensoflow","u","成器","dp","noise","记","9834","原因","ldots1","一个","甚或","根本","那些","动量","并且","个","器为","construct","“","29","classdef","arg","rank","plt","advnetworkbase","losses","目标","可变性","附近","_","继承","模式","两组","同理","用来","saveallresults","nesterov","0.6","predicted","除了","主","50","r","steps","└","以及","2c","o","往往","多个","方面","双份","大小写","step","个别","丢弃","性能","times","常数","subgradient","当","负","唯一","i","另一方","99","不过","每次","而定","会论","依靠","某","images","dataset","容易","值得","特征值","型值","optional","merged","基于","p","提示","读写","采用","所","9.736224609164252","一致性","tools","append","一系列","则","0838","单靠","diag","outputdata","全部","地","值得注意","封装","重申","其","align","情况","讨论","其二","可以","最低","we","重要","使得","4000","收敛","范例","器","指派","7134","分离","：","data","30","aligned","settings","2465","rightarrow","没","同","一","正是","再进一步","else","直线","that","后期","13","任意","keras","关乎","cb","leftarrow","后者","；","tests","per","一点","ml","实际上","算是","完整","问题","suggest","短接","randomnormal","极端","要么","构建","话题","率以","9897396","操作","团队","专门","一对","时","parser","d","c","相当","好处","不得不","如何","stroke","其实","gb","大化","for","模块化","均会","默认值","不仅","their","只取","诸多","0431","min","参数设置","领域","强","原始","提及","从根本上","编写","关键字","并","exit","或者","合适","参照","频繁","算出","其一","或许","什么","不会","/","传统","方","approximation","optimization","dagger","数据","轻松","外围","远不如","”","原生","29084","这个","预期","已经","uniform","作用","馈入","等于","梯度","format","）","旧","恢复","种子","3472","比较","合理","9831","results","基本","调整","另一方面","users","no","感到","单值","建议","最后","指数","不得","分析","…","e","ylim","同样","500","学习","也","多值","线性","文字","可调","1477","尤其","常","因为","参阅","激活","partial","做","google","通常","至此","false","优势","f","无论是","module","收敛性","dense1","formatname","usage","随机噪声","特定","迭代","区域","每","np","88829040527344","由该","甚","neural","designed","写作","电子","this","bmatrix","20","后","测量","如下","完成","也就是说","顾名思义","size","看看","值过","就行了","主要","属于","单独","s","savez","（","输","率过","add","record","而言","基本原理","metrics","大致","过于",";","性相","70","minimization","结构","biases","12345","大多数","有所","拟出","25","设置","9","包括","提供","gen","代替","width","nearest","来自","文件","反向","测试数据","mini","手","9813","惯性","需要","这","帮助","sample","基本上","0.5","38","？","kernel","无论","令人","为止","5","the","红色","^","率设","花招","shape","式子","小时","后来者","这种","interpolation","分别","1000","哪些","inches","阶梯","2","求","符号","0s","区间","引用","黑色","先后顺序","更新","by","换言之","相接","认为","附图","操作符","尝试","除外","导数","亦","换行","argument","相反","无关痛痒","可资利用","从式","思想","怎么","可能","参考资料","momentum","开始","小小的","const","除非","st1","做过","计学","入手","功能","陷入","逐渐","l3","下","、","即","ba9132","才","保持","线路","一类","nargs","amsgrad","一次函数","4","介绍","gcf","plot","squared","大于","求导","过多时","fileexistserror","各自","constant","found","cost","同时","即使","现在","n","上面","预测","下界","一些","flag","define","本原","replace","输出","mean","工程","只是","二阶","有","卓有成效","以","6","sum","小小","a","generator","0.9348520972791058","assert","mse","v","相仿","show","w",")","此","回归","进一步","loss","一言难尽","it","比率","低","格式","至少","modify","initialization","扰动","一节","j","不难","因素","远远","白","解来","对","涉及","范围","了解","测度","样本","错误","encountered","具体","str","平缓","把","构造","不是","未免","5597","选用","残差","这里","available","行","推广","尽可","检查","可变","importflag","增加","lin","来者","为本","8705","cdots","引入","添加","累计","越","schedule","5899","raw","通过","指代","<","一定","一段","line","mathrm","cookbook","少量","读取","程度","入","看","当然","结合","即可","程序运行","灵活","青睐","algorithm","xlabel","开辟","数十","必须","上式","项目","staticmethod","不","新","段","of","反之","描述","次序","数据字","test","次数","实现","adadeltada","config","将近","这些","复现","改变","较大","系列","由于","高级","叫","证明","仿射变换","开启","等","use","未免太","oprefix","这四","svg","当上","这样","迄今为止","adagrad","注意","难看","求得","-","len","delta","开来","scandir","批量","不为","将会","启动","显然","10.0","9354","sigma","教程","意外","经过","影响","分解","反例","正值","savefig","罗列","法","信号","iterative","符合","求取","适当","一系","字符串","close","由","ed1","类似","上","尽可能","336","rmse","大多","35","指出","满足","同一","它","其后","非线性","这方面","条件","需","如果","以上","顺序","17","9579","class","最大","快","方程","自定义","好","视","setseed","signal","过高","得慢","额外","adgrad","到","整数","\\","15","然后","1e","运算","资料","未来","dtype","method","nabla","或缺","子","函数","高度","数值","只要","path","结果","st2","大量","curves","网络层","不再","曲线","优化","linear","l12","内容","实验","即将","34","让","当于","regressed","3.216810970685858","gradient","不足以","写出","sd","rvert","有些","例子","一旦","相比","完全一致","执行","很","下降","adam","9701","ncol","连","适用性","一言","有成","系数","受到","apis","尽","evaluated",":","varepsilon","18","解析","官方","相乘","input","运用","parsedata","推断出","draw","提到","并发","但是","true","只","指定","多数","节","当场","support","saveresults","皮尔森","网络","elif","方程组","api","信噪比","rmsprop","整个","colorbar","一般来说","准确","大小","提出","axis","接受","那么","假","39","1983","square","tilde","理想","完完全全","适应",">","要","一行","值","16","定义","标量","字段","不宜","一步","1.8380598782932136","需求","─","设由","处理","推荐","解时","以后","表示","更","其他","error","得到","文件夹","法在","begin","if","水平","command","多","有时候","main","反映","b","带","type","不加","dparser","展开","fit","folder","object","左右","不可","大时","speically","小量","过程","argparse","融合","几乎","layer","显明","meansquarederror","开关","出现","自","具体情况","遵从","后处理","标准差","save","下来","写成","秩为","is","set","sze","神经","找到","reproductable","越过","远远不如","markevery","只有","旧版","被","snr","笔者","引起","ng","来说","强于","角","limits","少则","上述","不同","足以","障碍","使","value","深度","灵活运用","说明","抽取","呢","nmoment","本节","关键","真的","从","库","产生","这一","前述","复用","估计","仍然","展示","一致","8801","想象","如此","教材","44","converter","集中","activation","真","不可或缺","0.4","入门","事实","退化","samples","一层","history","直观","拥有","线性变换","23","mathcal","逆","类","跟踪","改动","decay","再","保留","1856","为什么","先后","特地","当前","做法","存在","因而","?","形如","输入","就","程序","任何","为了","return","@","时候","控制","上文","对于","seed","区分","快乐","optimizers","convergence","参考","ypred","一般","超过","论文","位置","详情","所示","选项","用","细微","overline","estimation","寻找","特例","in","大地","datakeys","告诉","python","不妨","importing","46","保证","matplotlib","统计学","22","列举","each","噪声","损失","启发","多组","平均值","tight","float","list","考虑","proportion","correlation","不难看出","label","arguments","weights","值为","一个多","生效","效果","graphs","problem","相当于","8547","字符","子类","layers","反而","设计","按照","28","配置","|","充当","deep","按","过大","导致","file","结论","9897","有趣","给出","扩展","用作","真是","卓有","argumenttypeerror","下面","调整结构","—","range","parameters","store","随机性","中能","y","same","究竟","advanced","bool","次","推断","测试阶段","风格","数目","改正","第一个","1.0","继续","原理","str2bool",",","opath","能够","点","接下来","40","集来","导出","中","option","读者","事实上","因此","覆盖","random","9832","打算","...","看出","init","相差","排列","相关系数","stystart","均值","生成","print","两种","fae6a9","steppe","learning","作为","m","8145","该字","6678","写为","prefix","解决","部分","33","sgd","成效","后来","完全相同","以便","except","变量","空行","推导","pre","testing","similarity","仍","在线","trick","表明","计算","方差","dense","理论","特别","有着","45","原则","0.01","gpu","这方","数据处理","进行","两次","ratio","mapsto","testdataset","提升","epochs","大","几种","修改","metavar","不必","可知","请参阅","ε","noi","#","模块","不怎么","章节","数据分布","量","initializer","解之","network","掌握","个点","依然","相应","l22","而","探讨","codes","alpha","所有","获取","br","平均","既会","2459","trainbatchnum","了","准确度","values","抑制","适用","特征","0","normal","options","工作","式","相关","人","term","提前","8130","layout","浮点数","自动","真实","odot","showbars","于是","7719","内","是否","神经网","用法","我们","遗忘","首尾相接","本次","一次","好后","idea","突兀","formatter","选择","最大化","阶段","online","每个","这是","。","若","0.8","很多","optimizername","查看","not","legend","19","nadam","bias","无关","出来","mergedlabel","近似","adaptive","矩阵","地方","argumentdefaultshelpformatter","参数","典型","metric","9700","重要性","形式","随着","神经网络","group","most","mathbf","压力","文档","二值","像","然而","水土不服","optimizer","不好","adamax","一样","base","hat","体验","本","很大","还是","说","yes","源码","linalg","验证","processing","衰减","result","sub","联系","12","关闭","残留","9833","速度","专题","{","变性","train","各个","可","叙述","6665"],"title":"线性回归","title_tokens":["回归","线性"]},{"location":"book-1-x/chapter-1/linear-regression/#_1","text":"摘要 本节介绍如何使用类模型(Model)来编写一个线性回归器，以拟合出一个线性模型。本节将第一次介绍如何编写一个带用户参数(选项)的Project，并且允许用户选择不同的优化器、对比验证不同优化算法的效果。","text_tokens":["(","选择","参数","并且","来","编写","出","验证","节","第一","允许","线性","用户","、",")","本节","回归","拟合","model","将","类","。","如何","带","算法","器","模型","优化","使用","project","介绍","，","的","选项","第一次","以","一次","效果","对比","一个"," ","不同","摘要","本"],"title":"线性回归","title_tokens":["回归","线性"]},{"location":"book-1-x/chapter-1/linear-regression/#_2","text":"","text_tokens":[],"title":"理论","title_tokens":["理论"]},{"location":"book-1-x/chapter-1/linear-regression/#_3","text":"设存在一个多输出方程 \\mathbf{y} = \\mathcal{F}(x) \\mathbf{y} = \\mathcal{F}(x) ，当然 \\mathcal{F} \\mathcal{F} 可以是非线性函数，那么我们可以考虑使用一个带可调参数的模型 \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}) \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}) 来模拟它，其中 \\boldsymbol{\\Theta} \\boldsymbol{\\Theta} 是可调的参数。于是，该问题可以被表述为 \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\boldsymbol{\\Theta}} &\\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}_k) \\right),\\\\ \\mathrm{s.t.}~&\\mathbf{y}_k = \\mathcal{F}(\\mathbf{x}_k). \\end{aligned} \\end{equation} 在我们不知道 \\mathcal{F} \\mathcal{F} 的情况下，我们的目的是使用大量的 \\mathbf{x}_k,~\\mathbf{y}_k \\mathbf{x}_k,~\\mathbf{y}_k 样本，来调整出一个最优的近似模型 \\mathbf{D}_{\\boldsymbol{\\Theta}} \\mathbf{D}_{\\boldsymbol{\\Theta}} 。由于 \\mathcal{F} \\mathcal{F} 是非线性的，这要求我们的 \\mathbf{D}_{\\boldsymbol{\\Theta}} \\mathbf{D}_{\\boldsymbol{\\Theta}} 也可以是非线性的。实际情况下，这样的问题往往不容易求解，尤其是信号的非线性性极强时，该问题很容易陷入局部最优解，从而对求得一个可接受的解造成很大的障碍。 这里 \\mathcal{L} \\mathcal{L} 是损失函数。在回归问题中，很多情况下我们都只能选择 均方误差 (Mean squared error, MSE) 作为损失函数，这是因为回归问题的目的是模拟出一组信号来，而这些信号的分布范围可能是任意的。在一些特别的应用里，例如，如果我们的信号全部为正值，那么我们可以考虑使用 信噪比 (Signal-to-noise ratio, SNR) 来作为我们的损失函数。","text_tokens":["s","l","求解","y","陷入","下","right","从而","方程","mathcal","强时","模拟出","是因为","只能","end","signal","例如",",","于是","存在","\\","我们","mathrm","的","error","squared","解","}","begin","知道","~","局部","拟出","中","全部","误差","当然","目的","带","to","(","里","选择","最优","情况","不","函数","n","可以","=","作为","造成","&","大量","。","一些","实际","很多","theta","这","调整","模型","aligned","是","应用","输出","mean","性极","noise","x","t","sum","equation","一个","这些","^","近似","mse","由于","参数","特别","也","线性",")","很","回归","可调","损失","尤其","arg","任意","因为","这样","为","ratio","被","求得","-","考虑","boldsymbol","mathbf","snr","_","1","一组","f","问题","均方","一个多","要求","对","limits","范围","样本",".","障碍","很大","正值","在","来","k","出","信号","该","left","往往","都","d","而","这里","信噪比","分布","{","模拟","使用","表述","，","可能","可","接受","那么","它","非线性","其中","min"," ","容易","如果","设"],"title":"一般回归问题","title_tokens":["一般","回归","问题"]},{"location":"book-1-x/chapter-1/linear-regression/#_4","text":"继上一节的学习，我们知道如何解一个定义为 \\begin{align} \\mathbf{y} = \\left\\{ \\begin{bmatrix}\\hat{y}_1 > 0 & \\hat{y}_2 > 0 & \\cdots & \\hat{y}_n > 0\\end{bmatrix}^T, ~ \\left| ~ \\hat{\\mathbf{y}} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon} \\right. \\right\\}, \\end{align} 的分类模型。在本节，让我们考虑一个更简单的模型： \\begin{align} \\mathbf{y} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon}. \\end{align} 现在， \\mathbf{y} \\mathbf{y} 是关乎 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 的一个仿射函数，并且我们仍然保留噪声函数 \\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon} 。由于这是一个线性模型，我们可以想象到，存在一个线性回归器， \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ，使得预测结果为 \\begin{align} \\tilde{\\mathbf{y}} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}. \\end{align} 类似上一节，假设我们的数据集 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} 符合 (3) (3) 定义的数据分布特征。我们的基本要求是，在我们不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 的情况下，使用大量 (\\mathbf{x}_k,~\\mathbf{y}_k) \\in \\mathbb{D} (\\mathbf{x}_k,~\\mathbf{y}_k) \\in \\mathbb{D} 样本训练一个线性分类器，使得当我们给定任意一个新样本 \\mathbf{x} \\mathbf{x} 的时候，分类器能推断出其对应的 \\mathbf{y} \\mathbf{y} 。 故而，该问题可以描述为 \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\mathbf{W},~\\mathbf{b}} &\\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathbf{W} \\mathbf{x}_k + \\mathbf{b} \\right), \\\\ \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) &= \\lVert \\mathbf{y} - \\tilde{\\mathbf{y}} \\rVert_2^2. \\end{aligned} \\end{equation} 在本例中， \\mathbf{y} \\mathbf{y} 有正有负，因此我们使用均方误差来作为损失函数。","text_tokens":["l","y","假设",">","定义","下","cdots","right","推断","mathcal","lvert","end","保留",",","简单","存在","更","\\","到","我们","的","解","对应","}","知道","begin","~","数据","中","因此","误差","b","(","训练","+","其","align","情况","时候","不","函数","现在","n","新","可以","=","作为","结果","预测","这是","&","使得","。","大量","描述","器","：","基本","模型","aligned","是","x","t","有","让","sum","equation","一个","给定","a","故而","rvert","仿射","集","in","^","分类","由于","并且","学习","线性","w",")","回归","噪声","本例","损失","任意","arg","关乎","mathbb","为","-","varepsilon","考虑","boldsymbol","mathbf","2","_","1","问题","均方","分类器","一节","要求","推断出","limits","样本",".","hat","在","来","k","数据分布","该","left","符合","3","|","继上","d","c","本节","bmatrix","如何","类似","当","负","分布","上","{","正","使用","，","仍然","想象","和","min","特征"," ","能","0","tilde"],"title":"线性回归","title_tokens":["回归","线性"]},{"location":"book-1-x/chapter-1/linear-regression/#_5","text":"作为线性问题，该问题实际上可以写出其解析解。未免读者感到过于突兀，我们先从一个简单的问题开始入手： 例子：一次函数的线性回归 如果我们的矩阵 \\mathbf{A} \\mathbf{A} 退化为标量 a a ，向量 \\mathbf{c} \\mathbf{c} 退化为标量c，那么 (3) (3) 可以重新写为： \\begin{align} y = a x + c + \\varepsilon. \\end{align} 考虑我们拥有N个样本点 (x_k,~y_k) (x_k,~y_k) ，上述问题实际上可以求得解析解。设由这N个点构成了样本向量 \\mathbf{x}_d,~\\mathbf{y}_d \\mathbf{x}_d,~\\mathbf{y}_d (注意与前述的向量区分开来)，则问题可以写成 \\begin{align} \\arg \\min_\\limits{a,~c} \\lVert \\mathbf{y}_d - a \\mathbf{x}_d - c \\mathbf{1} \\rVert^2_2. \\end{align} 这就是附图所示的，拟合到直线的一次函数回归问题。将该损失函数展开，有 \\begin{equation} \\begin{aligned} \\mathcal{L}(a,~c) &= ( \\mathbf{y}_d - a \\mathbf{x}_d - c \\mathbf{1} )^T ( \\mathbf{y}_d - a \\mathbf{x}_d - c \\mathbf{1} )\\\\ &= \\mathbf{y}_d^T\\mathbf{y}_d + a^2 \\mathbf{x}_d^T \\mathbf{x}_d + c^2 + 2ac \\mathbf{1}^T \\mathbf{x}_d - 2 a \\mathbf{y}_d^T \\mathbf{x}_d - 2c \\mathbf{1}^T \\mathbf{y}_d. \\end{aligned} \\end{equation} 令 \\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial a}=0,~\\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial c}=0 \\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial a}=0,~\\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial c}=0 ，则我们得到一组二元一次方程组 \\begin{equation} \\left\\{ \\begin{aligned} a \\mathbf{x}_d^T \\mathbf{x}_d + c \\mathbf{1}^T \\mathbf{x}_d &= \\mathbf{y}_d^T \\mathbf{x}_d. \\\\ c + a \\mathbf{1}^T \\mathbf{x}_d &= \\mathbf{1}^T \\mathbf{y}_d. \\end{aligned} \\right. \\end{equation} 解之，得 \\begin{equation} \\left\\{ \\begin{aligned} a &= \\frac{ \\mathbf{y}_d^T \\mathbf{x}_d - ( \\mathbf{1}^T \\mathbf{y}_d ) ( \\mathbf{1}^T \\mathbf{x}_d ) }{ \\mathbf{x}_d^T \\mathbf{x}_d - (\\mathbf{1}^T \\mathbf{x}_d)^2 } = \\frac{ \\sum_k x_k y_k - \\sum_k y_k \\sum_k x_k }{ \\sum_k (x_k)^2 - \\left(\\sum_k x_k\\right)^2 }. \\\\ c &= \\mathbf{1}^T \\mathbf{y}_d - a ( \\mathbf{1}^T \\mathbf{x}_d ) = \\sum_k y_k - a \\left( \\sum_k x_k \\right). \\end{aligned} \\right. \\end{equation} 这个式子在诸多教材上都会出现，作为学生解回归问题的入门话题。可见，我们在本节讨论的问题并不是一个陌生的问题，相反，我们过去非常熟悉的一个问题，是这个问题的退化到标量下的特殊情况。另，计算该问题的相关系数，我们常使用 \\begin{align} \\rho = \\frac{ \\sum_k \\left(x_k - \\overline{x}\\right) \\left(y_k - \\overline{y}\\right) }{ \\sqrt{ \\sum_k \\left(x_k - \\overline{x}\\right)^2 \\sum_k \\left(y_k - \\overline{y}\\right)^2 } }, \\end{align} 其中 \\overline{x} = \\sum_k x_k ,~ \\overline{y} = \\sum_k y_k \\overline{x} = \\sum_k x_k ,~ \\overline{y} = \\sum_k y_k 。 有了解上述例子的基础，我们自然可以写出， \\begin{equation} \\begin{aligned} \\mathcal{L}(\\mathbf{A},~\\mathbf{c}) &= \\sum_k ( \\mathbf{y}_k - \\mathbf{A} \\mathbf{x}_k - \\mathbf{c} )^T ( \\mathbf{y}_k - \\mathbf{A} \\mathbf{x}_k - \\mathbf{c} )\\\\ &= \\sum_k \\left[ \\mathbf{y}_k^T\\mathbf{y}_k + \\mathbf{x}_k^T \\mathbf{A}^T\\mathbf{A} \\mathbf{x}_k + \\mathbf{c}^T \\mathbf{c} + 2 \\mathbf{c}^T \\mathbf{A} \\mathbf{x}_k - 2 \\mathbf{y}_k^T \\mathbf{A} \\mathbf{x}_k - 2 \\mathbf{y}_k^T \\mathbf{c} \\right]. \\end{aligned} \\end{equation} 提示 接下来的求导主要涉及单值对矩阵求导（导数仍是矩阵），单值对向量求导（导数仍是向量）。可以参考 The Matrix Cookbook 查到对应情况下的求导结果。 同理，令 \\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{A}}=0,~\\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{c}}=0 \\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{A}}=0,~\\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{c}}=0 ，则我们得到一组二元一次方程组 \\begin{equation} \\left\\{ \\begin{aligned} \\sum_k \\left[ \\mathbf{A} \\mathbf{x}_k \\mathbf{x}_k^T + \\mathbf{c} \\mathbf{x}_k^T \\right] &= \\sum_k \\left[ \\mathbf{y}_k \\mathbf{x}_k^T \\right]. \\\\ \\sum_k \\left[ \\mathbf{c} + \\mathbf{A} \\mathbf{x}_k \\right] &= \\sum_k \\left[ \\mathbf{y}_k \\right]. \\end{aligned} \\right. \\end{equation} 解之，得 \\begin{equation} \\left\\{ \\begin{aligned} \\mathbf{A} &= \\left[ N \\sum_k \\left[ \\mathbf{y}_k \\mathbf{x}_k^T \\right] - \\sum_k \\left[ \\mathbf{y}_k \\right] \\sum_k \\left[ \\mathbf{x}_k^T \\right] \\right] \\left[ N \\sum_k \\left[ \\mathbf{x}_k \\mathbf{x}_k^T \\right] - \\sum_k \\left[ \\mathbf{x}_k \\right] \\sum_k \\left[ \\mathbf{x}_k^T \\right] \\right]^{-1} \\\\ \\mathbf{c} &= \\frac{1}{N} \\sum_k \\left[ \\mathbf{y}_k - \\mathbf{A} \\mathbf{x}_k \\right]. \\end{aligned} \\right. \\end{equation} 可见，当上式中的逆不存在时（即低秩的情况），该方程还是有可能解不唯一。 同时，相关系数的计算可以表示为 \\begin{align} \\rho = \\mathrm{mean} \\left[ \\frac{ \\sum_k \\left(\\mathbf{x}_k - \\overline{\\mathbf{x}}\\right) \\cdot \\left(\\mathbf{y}_k - \\overline{\\mathbf{y}}\\right) }{ \\sqrt{ \\sum_k \\left[ \\left(\\mathbf{x}_k - \\overline{\\mathbf{x}}\\right) \\cdot \\left(\\mathbf{x}_k - \\overline{\\mathbf{x}}\\right) \\right] \\sum_k \\left[ \\left(\\mathbf{y}_k - \\overline{\\mathbf{y}}\\right) \\cdot \\left(\\mathbf{y}_k - \\overline{\\mathbf{y}}\\right) \\right] } } \\right]. \\end{align} 这就是 皮尔森相关系数 (Pearson's correlation) 。其中 \\overline{\\mathbf{x}} = \\sum_k \\mathbf{x}_k ,~ \\overline{\\mathbf{y}} = \\sum_k \\mathbf{y}_k \\overline{\\mathbf{x}} = \\sum_k \\mathbf{x}_k ,~ \\overline{\\mathbf{y}} = \\sum_k \\mathbf{y}_k ， \\cdot \\cdot 表示的是两个向量按元素各自相乘。它是式 (11) (11) 在多变量问题上的推广。相当于对向量的每一个元素，分别从统计上求取皮尔森相关系数，然后对向量每个元素对应的皮尔森相关系数求取平均值。","text_tokens":["s","l","入门","（","退化","入手","y","并","相关","下","right","标量","拥有","即","方程","拟合","mathcal","lvert","基础","]","逆","提示","设由","end",",","过于","一次函数","简单","点","接下来","表示","存在","到","\\","我们","mathrm","统计","的","得到","解","则","cookbook","}","begin","对应","~","得","一次","非常","中","[","读者","接下","然后","求导","多","秩","突兀","sqrt","元素","各自","(","这个","重新","相关系数","+","二元","均值","其","11","align","展开","情况","每个","讨论","matrix","）","不","函数","同时","n","可以","作为","=","陌生","结果","区分","&","先","。","写为","可见","pearson","实际","熟悉","参考","这","：","关系","感到","aligned","是","单值","变量","mean","所示","x","有","t","当于","仍","sum","出现","写出","equation","overline","一个","a","rvert","计算","the","下来","^","过去","例子","矩阵","写成","个","式子","直线","rho","两个","线性",")","回归","当上","损失","arg","分别","常","平均值","系数","为","特殊","注意","低","求得","-","会","varepsilon","考虑","解析","开来","partial","自然","实际上","correlation","mathbf","2","dfrac","_","1","一组","查","问题","'","学生","相乘","同理","相当于","对","limits","涉及","话题","了解","样本","上述",".","2c","还是","在","令","构成","每","k","该","left","附图","就是","3","解之","都","不是","时","求取","个点","未免","d","c","本节","解不","皮尔森","将","相当","导数","方程组","按","从","向量","{","上","相反","唯一","推广","使用","平均","frac","前述","，","可能","cdot","诸多","那么","与","另","它","开始","主要","了","min","其中","2ac","教材"," ","如果","0","式"],"title":"求解问题","title_tokens":["求解","问题"]},{"location":"book-1-x/chapter-1/linear-regression/#_6","text":"接下来，我们要介绍几种最常见的优化算法。关于更多这方面的内容，可以查考Google团队编写的在线电子书 Deep Learning 。笔者打算在未来为此开辟专题写文，因此这里只是介绍几种常见的 一阶梯度下降 算法。传统优化领域里，单靠一阶梯度下降往往难以满足对准确度的需求，但深度学习(Deep learning)往往必须使用这些简单的一阶梯度下降算法，就连使用一阶梯度近似二阶梯度的算法 共轭梯度下降 ，在很多情况下都被认为是费用(cost)过高。这是由于一个深度网络，往往具有大量的参数需要训练，因此一个Model的参数少则数十MB，多则上GB。一阶梯度下降算法所需的计算量小，能确保我们一次迭代的过程能迅速完成，因而备受青睐。为了提升其性能，深度学习领域内也对其进行了诸多改进。 注意 其实，论到优化算法，往往不得不提到 反向传播 。不过实际上，一个Tensorflow的入门者，其实完全不需要学习如何推导反向传播的过程。下面我们的叙述也完全不会提及反向传播相关的内容。关于为何我们不需要了解反向传播，在下一节我们会论到。但是，在本教程后期，介绍高级技巧的时候，我们会详细展开。事实上，笔者认为，一个Tensorflow的用户，如果只是为了编写代码，反向传播与ta其实无关痛痒；但只有真正掌握反向传播，我们才算是真正入门了神经网络的理论。 我们在这里说到优化算法，是用在训练网络上的。事实上，只有几种个别的机器学习应用，需要我们在测试阶段执行 迭代算法 (iterative algorithm) 。一般来说，深度学习的训练过程可以被普遍地描述为：已知一个带可调参数 \\boldsymbol{\\Theta} \\boldsymbol{\\Theta} 的模型 \\mathcal{D}_{\\boldsymbol{\\Theta}} \\mathcal{D}_{\\boldsymbol{\\Theta}} ，已知一组数据集 (\\mathbf{x}_i,~\\mathbf{y}_i) \\in \\mathbb{D} (\\mathbf{x}_i,~\\mathbf{y}_i) \\in \\mathbb{D} ，则我们的训练目标为 \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\boldsymbol{\\Theta}} \\mathbb{E}_{(\\mathbf{x}_i,~\\mathbf{y}_i) \\in \\mathbb{D}} \\left[ \\mathcal{L} \\left( \\mathbf{y}_i,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_i) \\right) \\right]. \\end{aligned} \\end{equation} 实际情况下，一般用均值估计来代替上式的期望函数。联系我们上一节的优化问题 (1) (1) 和本节的优化问题 (5) (5) ，都可以描述成上式的形式。也就是说，线性分类/回归器，是神经网络在解线性问题时的特例。","text_tokens":["入门","度","备受","提及","事实","l","编写","y","相关","要","下","right","改进","用户","测试阶段","mb","才","mathcal","]","tensorflow","需求","所","end","期望","为此","过高",",","不会","/","简单","最","接下来","共轭","传统","更","内","到","神经网","\\","因而","我们","介绍","的","解","则","}","begin","单靠","~","一次","费用","写文","数据","事实上","[","接下","因此","多","就","打算","地","青睐","确保","algorithm","算法","代替","带","未来","(","详细","为了","开辟","里","数十","必须","训练","真正","反向","上式","均值","cost","其","为何","阶段","完全","情况","展开","梯度","时候","不","函数","技巧","可以","learning","这是","大量","。","描述","实际","器","很多","但","查考","theta","需要","过程","：","电子书","优化","具有","模型","aligned","是","应用","一般","推导","内容","只是","常见","二阶","x","用","在线","无关","equation","一个","计算","5","特例","这些","不得","集","一","in","e","已知","下来","近似","分类","由于","ta","参数","理论","高级","学习","也","测试","形式","执行","线性","神经","下降",")","后期","回归","连","model","可调","arg","这方","只有","mathbb","为","进行","注意","；","被","普遍","会","提升","阶梯","神经网络","google","实际上","算是","机器","目标","几种","boldsymbol","mathbf","成上","笔者","小","_","一组","问题","1","关于","一节","来说","提到","但是","对","少则","教程","limits","论","了解","代码","入门者",".","迭代","本","说","在","来","传播","量","团队","认为","多则","往往","深度","iterative","left","就是","方面","都","时","个别","掌握","难以","性能","d","本节","联系","电子","网络","不得不","这里","deep","如何","其实","gb","专题","上","{","i","完成","无关痛痒","一般来说","使用","准确","不过","也就是说","满足","迅速","，","会论","估计","痛痒","诸多","就是说","与","这方面","和","准确度","需","了","min","下面","叙述","能"," ","领域","如果","子书","式"],"title":"优化算法","title_tokens":["优化","算法"]},{"location":"book-1-x/chapter-1/linear-regression/#_7","text":"接下来，让我们看看第一个算法， 随机梯度下降 (stochastic gradient descent, SGD) 。 随机梯度下降 记学习率为 \\epsilon \\epsilon ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\mathbf{g} 。 注意学习率一般需要设为一个较小的值，视情况而定。 由于梯度的期望满足 \\begin{equation} \\begin{aligned} \\mathbb{E} \\left[ \\mathbf{g} \\right] &= \\frac{1}{m} \\sum\\limits_{k=1}^m \\mathbb{E} \\left[ \\nabla_{\\boldsymbol{\\Theta}} \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\right] \\\\ &= \\mathbb{E} \\left[ \\nabla_{\\boldsymbol{\\Theta}} \\mathcal{L} \\left( \\mathbf{y},~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}) \\right) \\right] = \\nabla_{\\boldsymbol{\\Theta}} \\mathbb{E} \\left[ \\mathcal{L} \\left( \\mathbf{y},~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}) \\right) \\right]. \\end{aligned} \\end{equation} 可知随机抽取m个样本计算的梯度，在统计学上的期望等于全局梯度的期望。因此，这是一个有效的算法。 随机梯度下降存在明显的弊端，就是在收敛到（全局或局部）最优解的前提下，全局梯度为0，但通过随机选取batch得到的梯度（一般）可能不为0；并且，迭代受到个别极端样本梯度的影响较大，因此，我们有了第一个改进，即 带动量的随机梯度下降 (SGD with momentum) 。 带动量的随机梯度下降 参考文献 提出该算法的文章，可以在这里参考： On the momentum term in gradient descent learning algorithms. Neural Networks 记学习率为 \\epsilon \\epsilon ，惯性常数为 \\alpha \\alpha ，初始化动量 \\mathbf{v}=\\mathbf{v}_0 \\mathbf{v}=\\mathbf{v}_0 （不考虑继续训练的情况下 \\mathbf{v}_0 = \\mathbf{0} \\mathbf{v}_0 = \\mathbf{0} ），则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 计算带动量的更新值 \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} + \\mathbf{v} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} + \\mathbf{v} 。 显然，我们不难计算出， \\begin{equation} \\begin{aligned} \\mathbb{E} \\left[ \\mathbf{v} \\right] &= \\alpha \\mathbb{E} \\left[ \\mathbf{v} \\right] - \\epsilon \\mathbb{E} \\left[ \\mathbf{g} \\right]. \\\\ \\mathbb{E} \\left[ \\mathbf{v} \\right] &= - \\frac{\\epsilon}{1 - \\alpha} \\mathbb{E} \\left[ \\mathbf{g} \\right]. \\end{aligned} \\end{equation} 注意惯性通常需要设为 \\alpha \\in (0,~1) \\alpha \\in (0,~1) 。 这种改进的带来的好处是， 每次更新梯度时，上一次的梯度都会以指数衰减的形式残留在本次迭代中，从而确保新的梯度会被旧的梯度部分中和，避免极端梯度对更新参数影响过大； 当求解得到的梯度陷入局部最优时，如果该局部最优处的曲率较小，可以依靠动量的惯性，越过该局部最优解。 附图说明了使用这种算法的好处。黑色路径为SGD的更新轨迹，而红色路径为本算法的更新轨迹，可以看出随着迭代次数的增加，算法收敛的效果强于SGD。 有人从Nesterov在1983年的论文得到启发，提出了一个修正后的带动量随机梯度下降法，即 带Nesterov动量的随机梯度下降 (SGD with Nesterov momentum) 。 带Nesterov动量的随机梯度下降 参考文献 提出该算法的文章，可以在这里参考： A method for unconstrained convex minimization problem with the rate of convergence o\\left( \\frac{1}{k_2} \\right) o\\left( \\frac{1}{k_2} \\right) 记学习率为 \\epsilon \\epsilon ，惯性常数为 \\alpha \\alpha ，初始化动量 \\mathbf{v}=\\mathbf{v}_0 \\mathbf{v}=\\mathbf{v}_0 （不考虑继续训练的情况下 \\mathbf{v}_0 = \\mathbf{0} \\mathbf{v}_0 = \\mathbf{0} ），则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算惯性目标点的位置： \\boldsymbol{\\Theta}^{\\dagger} \\leftarrow \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} \\boldsymbol{\\Theta}^{\\dagger} \\leftarrow \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} ； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}^{\\dagger}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}^{\\dagger}} (\\mathbf{x}_k) \\right) ； 计算带动量的更新值 \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta}^{\\dagger} - \\epsilon \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta}^{\\dagger} - \\epsilon \\mathbf{g} 。 其实，该方法的更新量期望与前一种方法一样， 显然，我们不难计算出， \\begin{equation} \\begin{aligned} \\mathbb{E} \\left[ \\mathbf{v} \\right] &= - \\frac{\\epsilon}{1 - \\alpha} \\mathbb{E} \\left[ \\mathbf{g} \\right]\\\\ &= \\frac{\\epsilon}{1 - \\alpha} \\nabla_{\\boldsymbol{\\Theta}} \\mathbb{E} \\left[ \\mathcal{L} \\left( \\mathbf{y},~ \\mathcal{D}_{\\boldsymbol{\\Theta} + \\alpha \\mathbf{v}} (\\mathbf{x}) \\right) \\right]. \\end{aligned} \\end{equation} 当收敛到最优解时， \\mathbf{v} \\rightarrow 0 \\mathbf{v} \\rightarrow 0 ，同时有 \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} \\rightarrow \\boldsymbol{\\Theta} \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} \\rightarrow \\boldsymbol{\\Theta} 。我们在此不展开证明这个算法是能收敛的。但Nesterov的文献表明，它能将上面提到的带动量梯度下降算法的误差从 O\\left(\\frac{1}{K}\\right) O\\left(\\frac{1}{K}\\right) 下降到 O\\left(\\frac{1}{K^2}\\right) O\\left(\\frac{1}{K^2}\\right) 。其中 K K 为迭代次数。下图展示了这种方法的改进原理。它的梯度是在更新动量的惯性部分之后才计算出来的，因此新的梯度和之前的惯性是首尾相接的。实际实现时，按照上面的算法，每次迭代需要更新两次参数，计算一次梯度。合理调整算法的计算次序，可以改进为每次迭代更新一次参数，计算一次梯度。","text_tokens":["l","（","计学","首尾","求解","y","选取","为本","设","term","陷入","第一","值","下","right","改进","从而","即","算出","才","第一个","mathcal","]","继续","率","参考文献","视","期望","end","原理",",","解时","通过","minimization","点","有人","接下来","存在","到","\\","之前","我们","之后","首尾相接","统计","的","得到","本次","batch","则","解","}","begin","局部","~","曲率","一次","dagger","中","数据","[","接下","因此","有效","epsilon","路径","误差","确保","较","算法","看出","带","(","排列","这个","method","最优","训练","nabla","轨迹","+","展开","情况","等于","梯度","）","不","旧","同时","新","文章","文献","可以","learning","m","=","上面","of","这是","&","带动","。","合理","convergence","全局","收敛","实际","明显","但","theta","参考","需要","sgd","这","：","惯性","初始化","部分","调整","修正","aligned","是","一般","次数","实现","记","rightarrow","论文","位置","x","有","让","以","gradient","sum","equation","表明","一个","出来","计算","a","the","指数","in","动量","unconstrained","e","红色","方法","较大","下来","^","由于","参数","并且","v","个","证明","学习","统计学","形式","下降",")","此","随着","越过","这种","启发","algorithms","mathbb","rate","leftarrow","受到","为","下图","两次","；","注意","被","-","会","考虑","次序","目标","boldsymbol","mathbf","初始","通常","_","率为","1","小","2","不为","stochastic","显然","不难","极端","处","效果","nesterov","黑色","强于","年","problem","提到","可知","对","limits","一样","更新","带来","样本","descent",".","影响","一种","迭代","在","称","前提","相接","法","k","with","出","o","量","该","left","弊端","附图","按照","衰减","就是","都","时","个别","抽取","说明","d","g","neural","而","将","常数","好处","残留","后","或","这里","按","避免","过大","当","从","其实","alpha","上","{","networks","for","前","使用","每次","frac","而定","满足","提出","，","看看","可能","展示","momentum","依靠","与","它","和","了","其中","增加","convex"," ","如果","集中","on","0","1983","能","计算出来","随机","顺序"],"title":"引入动量的优化算法","title_tokens":["优化","的","动量","算法","引入"]},{"location":"book-1-x/chapter-1/linear-regression/#_8","text":"上述几种算法共同的特点是，具有一个“学习率”。实际上，这个学习率非常不好处理，值过小时，收敛速度很慢；值过大时，在最优解附近又难以收敛。为了解决这一思路，我们可以令学习率可变。最简单的思路是，将学习率设为指数衰减的（当然也可以设置下界），这样当开始学习的时候，学习率较大；而即将收敛时，学习率又会较小。 但是，以上做法不过是一些小小的花招(trick)罢了，接下来介绍的几种算法，是根据当前计算出的梯度来自适应调整学习率的。理论上，使用这种算法，用户不再需要特别关注学习率对训练的影响，我们尽可以设置一个偏大的学习率，在训练过程中，它能被自适应调整到一个合适的区间上。 首先，我们来介绍一种初步的改进， Adagrad (Adaptive Subgradient) ， Adgrad 参考文献 提出该算法的文章，可以在这里参考： Adaptive Subgradient Methods for Online Learning and Stochastic Optimization 记学习率为 \\epsilon \\epsilon ，小量 \\delta \\delta ，初始化学习率参数对角矩阵为 \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新学习率为： \\mathbf{r} \\leftarrow \\mathbf{r} + \\mathrm{diag}(\\mathbf{g})^2 \\mathbf{r} \\leftarrow \\mathbf{r} + \\mathrm{diag}(\\mathbf{g})^2 ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} 。 注意文献中常用向量点积 \\odot \\odot 来表示学习率，这样学习率就不是对角矩阵而是向量了。我们这里不定义额外的符号，以便不熟悉相关定义的读者理解。 这一方法的思想是，学习率随着梯度的累计增大而逐渐减小，类似我们使用指数衰减的策略。所不同的是，在梯度小的地方，我们认为梯度平缓，所以学习率减小得慢，以便算法迅速地通过这一片区域；在梯度大地地方，由于梯度陡峭，为了防止我们因为学习率过大漏过该区域，学习率减小得快，以适应梯度的大小。 这个方法没有从根本上解决迭代次数过多时，梯度过小的问题。不难看出该算法学习率以 O\\left(\\frac{1}{\\mathbf{g}^T\\mathbf{g}}\\right) O\\left(\\frac{1}{\\mathbf{g}^T\\mathbf{g}}\\right) 的比率衰减，经验指出，这个算法在很多情况下是不好用的，只能解决一些比较特定的模型。 在这里，我们依然不给出收敛性的证明（或许在未来我们会在专题中讨论这一问题）。读者不必为这些算法的原理感到压力，我们只需要对其有一个直观的了解就好。 考虑到Adagrad学习率减小的速度未免太快了，我们可以考虑它的改进， RMSprop (root mean square proportion) ，注意它是另一个算法Adadelta的特例，不过在本节我们不会讨论Adadelta，有兴趣的读者可以自己去寻找参考资料。 RMSprop 参考文献 提出该算法的文章，可以在这里参考： Overview of mini-batch gradient descent 记学习率为 \\epsilon \\epsilon ，小量 \\delta \\delta ，衰减参数 \\rho \\rho ，初始化学习率参数对角矩阵为 \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新学习率为： \\mathbf{r} \\leftarrow \\rho \\mathbf{r} + (1 - \\rho) \\mathrm{diag}(\\mathbf{g})^2 \\mathbf{r} \\leftarrow \\rho \\mathbf{r} + (1 - \\rho) \\mathrm{diag}(\\mathbf{g})^2 ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} 。 和上一个算法相比，它唯一的改变就是引入了一个衰减参数 \\rho \\rho ，以指数衰减将之前收集的学习率遗忘。如此就可以控制指数过大的问题，这个trick真是令人一言难尽。但是有趣的是，实际经验中，这个方法真的是卓有成效，是现在常用的神经网络优化算法之一。 最后让我们来介绍当前最实用的算法（之一）， Adam (adaptive momentum estimation) 。顾名思义，它的基本原理是基于对动量的可变估计。实际上，在上一节的Project中，我们选用的优化器就是Adam，Tensorflow的官方教程中，也将Adam作为默认推荐的优化器。 Adam 参考文献 提出该算法的文章，可以在这里参考： Adam: a Method for Stochastic Optimization 特别需要注意的是，Adam的收敛性证明已经被后来者推翻，指出其中存在一个错误。改正后的版本称为AMSGrad，Tensorflow的Keras API支持我们在设置Adam的时候开启AMSGrad模式。关于AMSGrad，我们不在此展开讨论，有兴趣的读者可以参考： On the Convergence of Adam and Beyond 记 k k 为迭代次数，学习率为 \\epsilon \\epsilon ，小量 \\delta \\delta ，衰减参数 \\rho_1,~\\rho_2 \\rho_1,~\\rho_2 ，初始化动量为 \\mathbf{s} = \\mathbf{0} \\mathbf{s} = \\mathbf{0} ，学习率参数对角矩阵为 \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新动量为： \\mathbf{s} \\leftarrow \\rho_1 \\mathbf{s} + (1 - \\rho_1) \\mathbf{g} \\mathbf{s} \\leftarrow \\rho_1 \\mathbf{s} + (1 - \\rho_1) \\mathbf{g} ； 更新学习率为： \\mathbf{r} \\leftarrow \\rho_2 \\mathbf{r} + (1 - \\rho_2) \\mathrm{diag}(\\mathbf{g})^2 \\mathbf{r} \\leftarrow \\rho_2 \\mathbf{r} + (1 - \\rho_2) \\mathrm{diag}(\\mathbf{g})^2 ； 调整参数大小： \\hat{\\mathbf{s}} \\leftarrow \\dfrac{\\mathbf{s}}{1 - \\rho_1^k} \\hat{\\mathbf{s}} \\leftarrow \\dfrac{\\mathbf{s}}{1 - \\rho_1^k} , \\hat{\\mathbf{r}} \\leftarrow \\dfrac{\\mathbf{r}}{1 - \\rho_2^k} \\hat{\\mathbf{r}} \\leftarrow \\dfrac{\\mathbf{r}}{1 - \\rho_2^k} ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\dfrac{ \\hat{\\mathbf{s}} }{\\delta + \\sqrt{\\hat{\\mathbf{r}}}} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\dfrac{ \\hat{\\mathbf{s}} }{\\delta + \\sqrt{\\hat{\\mathbf{r}}}} 。 Adam不仅估计了学习率的可变性，还引入了可变的动量。这是迄今为止，我们见到的第一个将动量和可变学习率结合起来的算法。我们当然期望它能带来双份的 快乐 好处，可是…… 为什么会这样呢？ ，已经有文献指出，Adam存在原理上的失误，并提出了改正的算法AMSGrad，这正是我们未来将要在专题中讨论的内容。现在读者只需要知道，Adam的思路其实就是结合动量和可变学习率就行了。 注意 无论是我们没提到的Adadelta还是提到的Adam，其实都引入了动量的概念。那么一个自然而然的idea就是，使用Nesterov动量代替普通的动量。当然，毫无意外的是，已经有人做过了。例如，Adam的Nesterov动量版本叫Nadam，有兴趣的读者不妨去了解一下。","text_tokens":["s","（","率过","适应","定义","用户","基本原理","参考文献","兴趣","期望","处理","推荐","常用","最","表示","而是","所以","的","}","非常","epsilon","设置","较","sqrt","代替","(","来自","展开","mini","文章","=","大时","小量","实际","需要","过程","这","理解","模型","具有","project","？","无论","自","令人","为止","the","方法","下来","^","and","率设","花招","小时","将要","神经","后来者","这种","关注","被","会","过","点积","boldsymbol","2","dfrac","符号","率为","区间","limits","更新","带来","上述","descent","不同","版本","来","认为","left","呢","抽取","本节","真的","从","根据","向量","frac","去","这一","思想","，","迅速","估计","参考资料","momentum","如此","开始","小小的","和","集中","on","随机","做过","一片","逐渐","首先","直观","下","慢","mathcal","经验","又","为什么","amsgrad","root","当前","做法","简单","存在","默认","介绍","batch","得","特点","没有","思路","接下","可是","就","过多时","失误","算法","为了","训练","+","普通","时候","控制","现在","文献","下界","罢了","一些","快乐","convergence","熟悉","参考","本原","推翻","初始化","起来","mean","记","t","有","用","卓有成效","以","sum","小小","一个","estimation","a","根本","特例","寻找","大地","动量","还","个","不妨",")","“","此","一言难尽","比率","考虑","自然","proportion","不难看出","可变性","初始","附近","_","小","一节","模式","收集","关于","不难","毫无","nesterov","对","r","了解","样本","错误","在","令","o","偏大","平缓","adadelta","就是","不是","双份","未免","选用","subgradient","这里","按","增大","当","过大","唯一","有趣","不过","每次","给出","可变","真是","卓有","自己","来者","l","y","right","引入","基于","改正","第一个","累计","只能","所","原理",",","通过","接下来","之前","mathrm","解","则","diag","中","读者","当然","结合","地","看出","排列","最优","展开讨论","其","情况","讨论","beyond","不","可以","learning","m","作为","of","收敛","解决","器","成效","：","后来","以便","次数","减小","没","trick","计算","这些","改变","较大","正是","由于","对角","理论","特别","叫","证明","开启","未免太","keras","leftarrow","这样","为","adagrad","迄今为止","；","注意","难看","-","见到","概念","delta","一下","实际上","几种","methods","1","问题","不必","教程","意外","影响","率以","出","该","都","时","依然","d","将","而","好处","其实","类似","上","for","overview","使用","指出","不仅","它","另","了","能"," ","以上","0","顺序","从根本上","相关","并","合适","第一","改进","快","或许","好","防止","tensorflow","率","什么","odot","例如","得慢","不会","额外","adgrad","有人","到","\\","神经网","我们","遗忘","知道","~","自然而然","optimization","数据","idea","”","资料","未来","这个","method","已经","nabla","共同","online","梯度","）","比较","这是","。","不再","很多","theta","基本","调整","优化","感到","是","内容","即将","支持","x","让","nadam","gradient","最后","指数","…","陡峭","adaptive","矩阵","地方","参数","相比","学习","也","rho","很","adam","随着","大漏","一言","因为","有成","尽",":","实用","神经网络","mathbf","无论是","官方","stochastic","压力","收敛性","提到","然而","顾名","不好","但是","之一","一种","hat","特定","迭代","还是","区域","称","初步","k","只","衰减","难以","g","网络","后","或","速度","api","rmsprop","专题","{","变性","顾名思义","大小","提出","值过","称为","那么","就行了","其中","策略","square","迄今"],"title":"引入可变学习率的优化算法","title_tokens":["优化","率","可变","的","学习","算法","引入"]},{"location":"book-1-x/chapter-1/linear-regression/#_9","text":"","text_tokens":[],"title":"解线性回归问题","title_tokens":["解","回归","线性","问题"]},{"location":"book-1-x/chapter-1/linear-regression/#_10","text":"重申我们之前提到的，我们建议一个完整的工程应当包括 . ├─ data/ # where we store our data │ └─ ... ├─ tools.py # codes for post-processing and analyzing records. ├─ extension.py # codes for extending the tensorflow model. ├─ dparser.py # data parser └─ main.py # main module where we define our tensorflow model. 除了保存数据的文件夹，我们应当有三个子模块。其中 tool : 用来处理、分析生成的数据，通常与Tensorflow无关； extension : 用来扩展tensorflow，例如在这里自定义网络层和操作符； dparser : 数据处理器，用来读取并预处理送入网络的数据； main : 主模块，只定义跟Tensorflow模型有关的内容，需要引用 extension 和 dparser 。 视情况可以灵活调整结构，但建议将定义Tensorflow模型的代码单独放在主模块里，和其他外围代码分离。 在上一节中，我们没有定义 tool.py 和 extension.py ，这是因为我们的工程还很简单，不需要扩展Tensoflow模型，也不需要专门的数据处理代码。相应地，我们把数据的后处理代码直接集成在了主模块 lin-cls.py 里。在这一节，我们要开始构造一个真正严格按照这四部分分离的工程，并且在接下来的各个例子实现里，都会遵照这个模式，读者应当熟悉类似我们所推荐的、这样一个高度分离的模块化设计的思路。","text_tokens":["并","要","定义","、","理器","自定义","post","tensorflow","是因为","严格","─","视","所","处理","例如","tool","/","推荐","tools","py","简单","接下来","结构","之前","我们","其他","的","文件夹","没有","读取","直接","思路","数据","records","中","读者","外围","接下","│","包括","main","灵活","...","地","where","这个","里","真正","dparser","文件","重申","情况","生成","子","预处理","不","可以","高度","we","。","define","网络层","送入","熟悉","但","部分","需要","分离","data","调整","模型","这","tensoflow","内容","件夹","工程","实现","建议","有","无关","一个","├","the","后处理","分析","下来","and","还","例子","并且","也","our","很","这四","model","extending","因为","数据处理","这样","；","-",":","遵照","会","跟","extension","处理器","完整","通常","cls","一节","模式","引用","用来","module","提到","除了","主","代码",".","└","保存","#","模块","在","操作","设计","只","把","构造","放在","processing","按照","专门","操作符","都","parser","相应","集成","有关","将","网络","这里","类似","codes","自定","上","for","模块化","扩展","，","各个","三个","analyzing","与","开始","和","其中","了","调整结构"," ","lin","store","应当","单独"],"title":"代码规范","title_tokens":["规范","代码"]},{"location":"book-1-x/chapter-1/linear-regression/#_11","text":"此次是我们第一次写扩展模块，编写扩展模块的目的是，提供一个更复杂的支持库，以便我们能轻松地使用Tensorflow。因此，扩展模块编写地原则应当包括： 可适用性 : 它应当与我们某一个Project完全无关，就像我们自己基于Tensorflow编写一个扩展库一样，以后我们在任何项目都应该可以使用同一个扩展模块文件； 低依赖性 : 它应当最低限度地需要依赖库。 tensorflow 库本身当然是需要的，而 numpy ， matplotlib 甚或是读写数据的模块，都不宜出现在这里，以确保我们的扩展模块被其他任何模块调用时，依赖关系都是树状的； 强一致性 : 它的使用风格，应当尽可能和Tensorflow本身的API一致，使得一个之前不怎么接触它的人，也能快速上手。 在这个工程里，我们扩展的内容其实很简单，就是允许模型调用一个指定的优化器。让我们直接看以下代码： extension.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class AdvNetworkBase : ''' Base object of the advanced network APIs. ''' @staticmethod def optimizer ( name = 'adam' , l_rate = 0.01 , decay = 0.0 ): ''' Define the optimizer by default parameters except learning rate. Note that most of optimizers do not suggest users to modify their speically designed parameters. name: the name of optimizer (default='adam') (available: 'adam', 'amsgrad', 'adamax', 'nadam', 'adadelta', 'rms', 'adagrad', 'nmoment', 'sgd') l_rate: learning rate (default=0.01) decay: decay ratio ('adadeltaDA' do not support this option) ''' name = name . casefold () if name == 'adam' : return tf . keras . optimizers . Adam ( l_rate , decay = decay ) elif name == 'amsgrad' : return tf . keras . optimizers . Adam ( l_rate , decay = decay , amsgrad = True ) elif name == 'adamax' : return tf . keras . optimizers . Adamax ( l_rate , decay = decay ) elif name == 'nadam' : return tf . keras . optimizers . Nadam ( l_rate , schedule_decay = decay ) elif name == 'adadelta' : return tf . keras . optimizers . Adadelta ( l_rate , decay = decay ) elif name == 'rms' : return tf . keras . optimizers . RMSprop ( l_rate , decay = decay ) elif name == 'adagrad' : return tf . keras . optimizers . Adagrad ( l_rate , decay = decay ) elif name == 'nmoment' : return tf . keras . optimizers . SGD ( lr = l_rate , momentum = 0.6 , decay = decay , nesterov = True ) else : return tf . keras . optimizers . SGD ( l_rate , decay = decay ) 我们在这里几乎罗列了所有可能使用的优化器，全部来自Keras API。但我们也可以使用Tensorflow旧版API定义的优化器。目前Tensorflow允许使用两种API中的任意一种来定义，但是实验发现，旧版API系列的优化器要么已经在Keras中能找到对应的版本，要么就水土不服，无法正常调用。因此，上文提到的几种优化器，我们基本上全部在这里用Keras API定义出来。 优化器的参数尽可能应当选择默认参数，并且应当封装起来，不宜让用户自行操作。尤其是Adadelta，Adam这些优化器的 \\rho \\rho 变量，在 Keras文档 中，建议我们遵从默认值。 任何继承该类的子类，都可以通过 self . optimizer ( self . optimizerName , self . learning_rate ) 来将封装好的优化器API调用到主模块中。","text_tokens":["l","tf","17","该类","中能","编写","class","人","无法","advanced","第一","16","定义","基于","0.0","依赖性","default","23","不宜","风格","用户","tensorflow","好","读写","decay","schedule","基本","写",",","不服","amsgrad","一致性","通过","py","以后","简单","numpy","更","默认","\\","4","到","其他","我们","之前","的","对应","一次","直接","25","10","if","数据","轻松","看","option","全部","中","15","因此","name","9","当然","就","包括","地","提供","确保","目的","to","任何","这个","(","来自","里","选择","已经","封装","项目","文件","return","11","完全","@","staticmethod","31","上文","object","两种","可以","最低","learning","=","casefold","of","使得","自行","speically","。","手","define","optimizers","21","lr","器","复杂","但","33","optimizername","需要","sgd","：","关系","模型","优化","以便","30","not","except","是","users","几乎","变量","基本上","rms","project","调用","内容","工程","adadeltada","快速","实验","发现","支持","建议","19","34","让","以","6","依赖","出现","nadam","无关","用","出来","一个","甚或","5","7","the","遵从","这些","8","14","def","系列","参数","并且","do","else","允许","也","matplotlib","22","rho","that","原则","起来","很",")","adam","0.01","找到","29","13","应该","任意","尤其","以下","适用性","keras","rate","apis","旧版","adagrad","；","低","被","ratio",":","32","18","extension","advnetworkbase","modify","note","most","几种","2","'","_","1","继承","suggest","文档","要么","限度","nesterov","像","0.6","提到","水土不服","optimizer","但是","树状","adamax","true","主","一样","子类","base","代码","接触",".","by","一种","模块","不怎么","在","版本","此次","来","罗列","操作","指定","28","adadelta","就是","3","都","network","时","support","designed","nmoment","12","而","this","将","26","20","elif","这里","available","其实","api","rmsprop","27","本身","上","24","尽可能","36","所有","使用","35","库","尽可","扩展","正常","默认值","，","可能","怎么","可","their","同一","第一次","它","与","一致","momentum","self","和","了","自己","水土","适用","某","能"," ","parameters","应当","目前","同一个","强"],"title":"扩展模块","title_tokens":["模块","扩展"]},{"location":"book-1-x/chapter-1/linear-regression/#argparse","text":"本节将第一次引入 argparse 模块。该模块是python本身具有的原生模块，用来给代码提供启动选项。作为一个完整的Project，我们不希望为了调整参数而频繁地修改代码，因此 argparse 对我们是不可或缺的。在后面所有的Project中，我们都会通过 argparse 模块支持项目选项。 argparse 的官方文档可以在此查阅： argparse — Parser for command-line options, arguments and sub-commands 调用 argparse 的一开始，我们需要定义如下内容： Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import argparse def str2bool ( v ): if v . casefold () in ( 'yes' , 'true' , 't' , 'y' , '1' ): return True elif v . casefold () in ( 'no' , 'false' , 'f' , 'n' , '0' ): return False else : raise argparse . ArgumentTypeError ( 'Unsupported value encountered.' ) parser = argparse . ArgumentParser ( description = 'A demo for linear regression.' , formatter_class = argparse . ArgumentDefaultsHelpFormatter ) Output usage: tools.py [ -h ] A demo for linear regression. optional arguments: -h, --help show this help message and exit 我们首先定义了 str2bool 函数，用来支持用户提供布尔类型的选项；之后，我们初始化了 parser ，一般地初始化 parser 时，我们主要定义三个参数： description : 项目描述，展示在参数用法之前的一段字符串； formatter_class : 格式化器 ，我们一般调用的都是 ArgumentDefaultsHelpFormatter ，因为它能支持自动换行，并在每个参数用法后展示该参数的默认值； epilog : 后记 ，这一段说明文字出现在所有参数用法之后。我们一般不太需要这个功能，但是有时候我们可以使用该功能提供一些用法范例给用户。 现在，我们来介绍几种典型的 argparse 可以提供的参数类型。 字符串选项 1 2 3 4 5 6 7 parser . add_argument ( '-o' , '--optimizer' , default = 'adam' , metavar = 'str' , help = ''' \\ The optimizer we use to train the model (available: 'adam', 'amsgrad', 'adamax', 'nadam', 'adadelta', 'rms', 'adagrad', 'nmoment', 'sgd') ''' ) 在这里我们定义了一个字符串选项，这是最常用的一类选项。用户可以像 python codes.py -o amsgrad 或者 python codes.py --optimizer amsgrad 一样，通过添加参数来覆盖默认值(定义在 default 字段下)。 数值选项 1 2 3 4 5 6 parser . add_argument ( '-lr' , '--learningRate' , default = 0.001 , type = float , metavar = 'float' , help = ''' \\ The learning rate for training the model. ''' ) 这里添加的参数类型是一个浮点数，虽然用户在输入参数的时候输入的是一个字符串，但 metavar 字段告诉了用户应该输入浮点数， type 决定了用户输入的字符串会被自动转换为浮点数。类似地，将两个字段的 float 改为 int ，我们就能提供一个整数作为参数选项 布尔选项 1 2 3 4 5 6 parser . add_argument ( '-if' , '--importFlag' , type = str2bool , nargs = '?' , const = True , default = False , metavar = 'bool' , help = ''' \\ The flag of importing pre-trained model. ''' ) 这里添加的是一个二值选项，它的默认值是 False ，用户可以通过输入 ( 'yes' , 'true' , 't' , 'y' , '1' ) 中的任何一个来指定该选项为真，或通过 ( 'no' , 'false' , 'f' , 'n' , '0' ) 中的任何一个指定该选项为假，不区分大小写。该功能由我们之前定义的 str2bool 函数提供。 特别值得注意的是，这个布尔选项还可以有这样的用法，例如： python codes.py -if -o amsgrad 我们如果指派了 -if ，在不指定它任何值的情况下，该选项就会被开启（值为真）了；如果我们去掉这一行的 -if ，则该选项关闭（值为假）。 多值选项 1 2 3 4 5 6 parser . add_argument ( '-ml' , '--mergedLabel' , default = None , type = int , nargs = '+' , metavar = 'int' , help = ''' \\ The merged label settings. ''' ) 上面的设置提供了一个可以输入任意多个 int 型值的选项，用法如下： python codes.py -ml 1 3 4 0 2 -o amsgrad 上述的输入会被解析成一个值为 [ 1 , 3 , 4 , 0 , 2 ] 的列表。当然，我们也可以输入任意多的值，但是特别值得注意的是，由于在 nargs 字段指定了 + ，一旦我们指派该选项，就要至少输入一个值方可。 上面的几种范例，并不是每一种都需要用在Project中。实际设置选项的时候，应当参照实际情况来处理。例如，本例中，就只使用 字符串选项 和 数值选项 两种。更多关于 add_argument 的用法，请参阅官方文档： argparse — add_argument() 在所有参数都设置好后，调用 args = parser . parse_args () 即可使参数选项生效。用户输入的参数选项将返回到 args 中，例如，如果用户制定了 -o ( --optimizer )，那么我们可以调用 args.optimizer 来取出该字段的值。","text_tokens":["（","小写","一行","值","定义","用户","add","字段","虽然","处理","py","常用","最","更","的","制定","if","command","9","后记","有时候","设置","提供","多","返回","(","type","字","casefold","布尔","不可","=","不太","实际","lr","需要","argparse","这","具有","learningrate","project","出现","5","the","raise","and","epilog","两个","本例","rate","被","会","2","training","commands","上述","来","使","value","说明","nmoment","本节","换行","argument","，","展示","开始","去掉","和","const","真","类型","不可或缺","功能","description","首先","下","一类","nargs","amsgrad","默认","4","介绍","?","[","输入","就","任何","为了","+","return","regression","11","时候","现在","n","上面","区分","一些","flag","初始化","一般","rms","选项","t","有","用","6","一个","a","7","output","in","8","告诉","还","python","v","importing","show",")","此","model","列表","float","格式","至少","label","arguments","初始","_","值为","关于","生效","用来","对","字符","代码","encountered","在","成","trained","o","str","多个","后面","adadelta","不是","大小写","这里","available","help","importflag","argumenttypeerror","—","值得","型值","optional","y","bool","merged","引入","]","添加","str2bool",",","浮点","通过","tools","方可","决定","一段","之前","line","则","中","因此","覆盖","当然","即可","地","值得注意","to","格式化","项目","none","情况","不","两种","可以","段","learning","作为","we","of","该字","就要","描述","范例","器","但","指派","sgd","：","settings","message","pre","parse","点数","一","14","def","由于","特别","else","开启","use","13","应该","任意","这样","为","adagrad","；","注意","-","ml","完整","几种","'","1","修改","启动","metavar","请参阅","模块","该","3","都","时","parser","字符串","将","而","由","类似","codes","本身","所有","for","使用","默认值","第一次","它","了","能"," ","如果","0","options","default","class","exit","并","或者","第一","参照","频繁","浮点数","自动","例如","到","\\","整数","h","用法","我们","之后","一次","10","好后","原生","formatter","这个","或缺","每个","）","函数","数值","0.001","。","调整","linear","no","有时","是","调用","内容","支持","nadam","mergedlabel","int","argumentdefaultshelpformatter","参数","典型","一旦","查阅","也","import","转换","多值","取出","文字","adam","因为","参阅","给",":","unsupported","解析","false","希望","f","官方","文档","二值","像","optimizer","但是","adamax","true","一样","usage","args","一种",".","yes","改为","指定","每","只","sub","12","this","elif","关闭","后","或","如下","argumentparser","大小","demo","train","三个","那么","主要","假","应当"],"title":"项目选项：argparse","title_tokens":["：","项目","选项","argparse"]},{"location":"book-1-x/chapter-1/linear-regression/#_12","text":"本节的数据也是自动生成出来的。参考上一节的数据生成器，重新定义数据生成类的迭代器： dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class TestDataRegSet ( TestDataSet ): ''' A generator of the data set for testing the linear regression model. ''' def next_train ( self ): ''' Get the next train batch: (x, y) ''' x = self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) y = np . matmul ( x , self . A ) + self . c if self . noise > 1e-3 : y = y + np . random . normal ( 0 , self . noise , size = y . shape ) else : np . random . normal ( 0 , self . noise , size = y . shape ) return x , y 提示 这里我们在没有噪声的情况下，仍然调用随机噪声函数，这是为了确保噪声函数被调用，使得随机数无论开关噪声，都能保持一致性。 该生成器同样是输入一组 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，以及相关配置，之后就可以通过 迭代器 (iterator) 或 方法 (method) 随机生成数据。与上一节不同的是，我们在本节可以尝试更进一步，令 \\mathbf{A} \\mathbf{A} 的 SVD分解 写作如下形式 \\begin{align} \\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^T. \\end{align} 其中， \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma} 是一个对角矩阵，对角线上的元素顺次排列，对应为矩阵 \\mathbf{A} \\mathbf{A} 的各个特征值。Numpy的库已经集成了 SVD分解 。我们知道，一个 M \\times N M \\times N 的矩阵经过SVD分解后，应当有 \\mathbf{U}_{M \\times M} \\mathbf{U}_{M \\times M} 和 \\mathbf{V}^T_{N \\times N} \\mathbf{V}^T_{N \\times N} 两个方阵。故而，矩阵 \\boldsymbol{\\Sigma}_{M \\times N} \\boldsymbol{\\Sigma}_{M \\times N} 并非方阵。由于它只有对角线上有元素，所以必定有多出来的空行或空列。因此，若我们设 K = \\min(M,~N) K = \\min(M,~N) ，则我们可以知道，SVD分解其实不需要矩阵 \\mathbf{U} \\mathbf{U} 和 \\mathbf{V}^T \\mathbf{V}^T 两个方阵都是方阵，因为当我们取矩阵 \\boldsymbol{\\Sigma}_{K \\times K} \\boldsymbol{\\Sigma}_{K \\times K} 这一对角部分后，可以只取部分行/列构成的矩阵 \\mathbf{U}_{M \\times K} \\mathbf{U}_{M \\times K} 和 \\mathbf{V}^T_{K \\times N} \\mathbf{V}^T_{K \\times N} 。这相当于我们略去了 \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma} 上的空行/空列，但是SVD分解仍然能保证恢复出原矩阵来。 在本例中，我们保留 \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma} 中的前 r r 个特征值，其后的特征值都丢弃，我们把这样的做法称为矩阵的低秩近似，于是有 dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 def gen_lowrank ( A , r ): ''' Generate a low rank approximation to matrix A. A: input matrix. r: output rank. ''' sze = A . shape r_min = np . amin ( sze ) assert r <= r_min and r > 0 , 'r should in the range of [1, {0}]' . format ( r_min ) u , s , v = np . linalg . svd ( A , full_matrices = False ) s = np . diag ( s [: r ]) return np . matmul ( np . matmul ( u [:,: r ], s ), v [: r ,:]) 一个低秩近似的矩阵，其定义的仿射变换 (3) (3) 满足不同的 \\mathbf{x} \\mathbf{x} 对应同一个值 \\mathbf{y} \\mathbf{y} ；反之， \\mathbf{y} \\mathbf{y} 将会对应多个不同的解 \\mathbf{x} \\mathbf{x} 。如果我们训练的线性分类器模拟的是 (3) (3) 的逆过程，可能我们会无法模拟出合适的解来；但是，由于我们定义的 (4) (4) 仍是在拟合正过程，故而我们仍然可以把这个问题看成是有解的。在后续的内容中，我们会适当地讨论当问题 解不唯一 时，我们可以进行哪些工作来处理这类问题。 接下来，我们即可测试低秩近似的效果， dparser.py 1 2 3 4 5 6 7 8 9 def test_lowrank (): A = np . random . normal ( 0 , 10 , [ 10 , 6 ]) for r in range ( 1 , 7 ): A_ = gen_lowrank ( A , r ) RMS = np . sqrt ( np . mean ( np . square ( A - A_ ))) R = np . linalg . matrix_rank ( A_ ) print ( 'Rank = {0}, RMS={1}' . format ( R , RMS )) test_lowrank () Output Rank = 1 , RMS = 6.8600432267325955 Rank = 2 , RMS = 4.677152938185369 Rank = 3 , RMS = 3.216810970685858 Rank = 4 , RMS = 1.8380598782932136 Rank = 5 , RMS = 0.9348520972791058 Rank = 6 , RMS = 9.736224609164252e-15 可见，对于一个标准差为10的矩阵，低秩近似的残差仍然是不超过随机高斯矩阵本身的标准差的。这里的秩是我们在调用低秩近似函数后，使用 np.linalg.matrix_rank 测量的结果。","text_tokens":["s","列",">","值","定义","一步","1.8380598782932136","顺次","*","处理","py","numpy","所以","的","}","begin","full","if","拟出","4.677152938185369","9","多","sqrt","gen","(","重新","dparser","svd","matrix","=","需要","过程","这","机数","0.5","generate","开关","无论","故而","5","the","标准差","方法","下来","^","and","分类","set","testdataregset","sze","shape","随机数","两个","本例","高斯","只有","哪些","后续","被","会","boldsymbol","2","一组","分类器","角","不同","来","构成","尝试","本节","模拟","库","标准","，","可能","仍然","一致","和","get","同一个","随机","下","拟合","逆","类","保持","end","low","保留","做法","4","batch","没有","有解","[","变换","接下","输入","就","秩","为了","训练","+","regression","return","生成器","11","对于","n","参考","u","成器","rms","超过","mean","noise","iterator","秩是","t","有","6","一个","amin","a","7","output","略去","in","generator","8","0.9348520972791058","assert","对角线","v","个","保证",")","进一步","model","噪声","rank","低","_","一节","效果","相当于","解来","lowrank","r","以及","在","令","把","多个","配置","丢弃","times","残差","这里","当","方阵","唯一","行","必定","range","特征值","设","取","y","]","提示",",","9.736224609164252","一致性","通过","接下来","<","解","则","diag","中","因此","random","即可","地","确保","元素","to","排列","其","align","情况","生成","讨论","不","print","6.8600432267325955","可以","m","of","使得","matmul","反之","next","器","部分","：","data","空行","test","testing","仍","仿射","14","def","由于","对角","else","仿射变换","13","这样","为","进行","；","-","testdataset","len","'","1","问题","将会","定义数据","sigma","经过","分解","该","3","都","一对","时","适当","c","相当","解不","其实","本身","上","正","看成","for","使用","满足","只取","同一","self","它","其后","了","min","能"," ","特征","如果","0","normal","工作","出原","should","class","相关","无法","合适","模拟出","自动","/","于是","\\","我们","之后","approximation","对应","知道","~","10","数据","15","1e","这个","method","已经","format","函数","恢复","结果","这是","可见","。","若","linear","是","调用","内容","x","matrices","当于","3.216810970685858","出来","e","同样","近似","矩阵","也","测试","形式","线性","因为",":","更进一步","mathbf","false","input","但是",".","随机噪声","迭代","空列","linalg","np","k","写作","集成","12","后","测量","或","如下","{","并非","前","size","train","各个","称为","与","其中","square","应当"],"title":"数据生成","title_tokens":["生成","数据"]},{"location":"book-1-x/chapter-1/linear-regression/#_13","text":"类模型 (Model class) ，在官方文档中也称为函数式API，是Tensorflow-Keras的用户大多数情况下应当使用的模型。它支持一些灵活的操作，使得我们可以 多输入多输出 : 类模型的输入和输出层，都是通过函数定义的。类模型在构建的时候，只需要给定输入和输出即可； 跨层短接 : 由于类模型的各层都由函数定义，可以轻松将不同的层连接起来，通常通过 融合层 完成这一工作； 多优化器 : 可以通过复用同一层对应的对象，构建多个不同的类模型，并分别对它们使用不同的训练数据、损失函数、优化器，以实现多优化目标。 一个顺序模型大致可以描述为下图的模式： graph LR st1(输<br/>入<br/>1) --> l11[层<br/>1-1] l11 --> l21[层<br/>1-2] l21 --> l31[层<br/>1-3] l31 --> ldots1[层<br/>...] st2(输<br/>入<br/>2) --> l12[层<br/>2-1] l12 --> l22[层<br/>2-2] l22 --> l32[层<br/>2-3] l32 --> ldots2[层<br/>...] ldots1 --> l3[层<br/>3] ldots2 --> l3 l3 --> l4[层<br/>4] l4 --> ed1(输<br/>出<br/>1) l4 --> ed2(输<br/>出<br/>2) l22 --> ed3(输出3) l21 --> l3 classDef styStart fill:#FAE6A9,stroke:#BA9132; class st1,ed1,st2,ed2,ed3 styStart 在本节中，尽管我们开始使用类模型，但我们定义的仍然是一个单线路的线性回归模型，换言之，这样的模型完全可以通过 顺序模型 实现出来。我们从这一节开始，不再使用顺序模型，其一，是因为顺序模型都可以写成类模型的形式，其二，是希望读者能够熟悉、灵活运用类模型的优势。 我们定义一个继承自 extension.py 的类， class LinRegHandle ( ext . AdvNetworkBase ): 。与上一节的情况相若，这里我们不再赘述需要定义哪些方法。并且，我们也不会介绍一些改动不大、或者不重要的方法，详情请读者参阅源码。","text_tokens":["输","l4","class","并",">","或者","一层","定义","下","l3","单","用户","、","ba9132","其一","]","类","tensorflow","线路","是因为","改动","跨层","接起","大致",",","linreghandle",";","/","能够","不会","通过","py","<","4","我们","介绍","的","它们","对应","大多数","入","中","轻松","数据","[","l31","读者","尽管","多","输入","即可","灵活","...","(","训练","stystart","完全","情况","时候","其二","不","函数","l11","可以","fae6a9","st2","使得","重要","。","一些","各层","描述","不再","lr","器","熟悉","但","相若","需要","这","：","起来","模型","优化","融合","ed2","l12","是","输出","实现","graph","详情","支持","ldots1","以","自","出来","同","给定","一个","方法","ed3","写成","由于","层","并且","对象","也","形式","线性",")","回归","classdef","model","连接","详情请","损失","分别","keras","fill","哪些","因为","下图","参阅","这样","为","；","-",":","extension","advnetworkbase","目标","大","通常","2","希望","1","优势","继承","官方","模式","文档","短接","一节","运用","对","ext","构建","ldots2",".","不同","换言之","#","在","l21","赘述","源码","操作","只","出","多数","多个","灵活运用","3","都","本节","顺序","将","l22","这里","由","l32","ed1","stroke","api","从","上","完成","br","大多","使用","这一","，","复用","仍然","称为","它","与","开始","和","连接起来"," ","应当","工作","st1","式"],"title":"定义类模型","title_tokens":["定义","模型","类"]},{"location":"book-1-x/chapter-1/linear-regression/#_14","text":"首先，定义初始化方法： lin-cls.py: class LinRegHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def __init__ ( self , learning_rate = 0.01 , epoch = 10 , steppe = 30 , optimizerName = 'adam' ): ''' Initialization and pass fixed parameters. learning_rate: the learning rate for optimizer. epoch: training epochs. steppe: steps per epoch optimizerName: the name of optimizer (available: 'adam', 'amsgrad', 'adamax', 'nadam', 'adadelta', 'rms', 'adagrad', 'nmoment', 'sgd') ''' self . lr = learning_rate self . epoch = epoch self . steppe = steppe self . optimizerName = optimizerName 与上一节相比，这里我们增加了一个参数， opmizerName ，用来指定我们选用的优化器名称，默认值为 adam 。","text_tokens":["opmizername","class","首先","定义","pass",",","linreghandle","amsgrad","py","默认","4","我们","的","10","name","9","init","(","11","steppe","learning","=","of","。","lr","器","optimizername","名称","sgd","：","初始化","优化","30","rms","nadam","6","一个","5","7","the","fixed","14","8","def","方法","__","and","参数","相比",")","0.01","adam","13","rate","为","adagrad","-",":","per","epochs","initialization","2","初始","cls","_","1","'","一节","用来","training","optimizer","adamax","epoch","steps",".","指定","adadelta","3","nmoment","12","选用","这里","available","上","for","默认值","，","self","与","了","增加"," ","lin","parameters"],"title":"初始化方法","title_tokens":["初始","初始化","方法"]},{"location":"book-1-x/chapter-1/linear-regression/#_15","text":"接下来定义网络构造 lin-cls.py: class LinRegHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def construct ( self ): ''' Construct a linear model and set the optimizer as Adam ''' # Construction input = tf . keras . Input ( shape = ( INPUT_SHAPE ,), dtype = tf . float32 ) dense1 = tf . keras . layers . Dense ( LABEL_SHAPE , use_bias = True , kernel_initializer = tf . keras . initializers . RandomNormal ( 0.0 , stddev = 10.0 ), bias_initializer = tf . keras . initializers . Constant ( 2 ), activation = None , name = 'dense1' )( input ) self . model = tf . keras . Model ( inputs = input , outputs = dense1 ) # Set optimizer self . model . compile ( optimizer = self . optimizer ( self . optimizerName , self . lr ), loss = tf . keras . losses . MeanSquaredError (), metrics = [ self . relation ] ) @staticmethod def relation ( y_true , y_pred ): m_y_true = tf . keras . backend . mean ( y_true , axis = 0 ) m_y_pred = tf . keras . backend . mean ( y_pred , axis = 0 ) s_y_true = tf . keras . backend . sqrt ( tf . keras . backend . mean ( tf . keras . backend . square ( y_true ), axis = 0 ) - tf . keras . backend . square ( m_y_true )) s_y_pred = tf . keras . backend . sqrt ( tf . keras . backend . mean ( tf . keras . backend . square ( y_pred ), axis = 0 ) - tf . keras . backend . square ( m_y_pred )) return tf . keras . backend . mean (( tf . keras . backend . mean ( y_true * y_pred , axis = 0 ) - m_y_true * m_y_pred ) / ( s_y_true * s_y_pred )) 使用类模型时，我们每定义一层，都调用对应的网络层函数，并返回层的输出结果。这就是为何它又叫“函数式API”。我们直接使用均方误差作为我们的损失函数，同时，我们还自行定义了一个评价函数， 皮尔森相关系数 ，该系数专门用来反映两组数据之间是否线性相关，上文我们已经叙述过它的定义。 注意 理想情况下，相关系数应当使用整个数据集来求取。但实际情况下做不到这一点，因此我们求取的相关系数只能看作是一个通过batch得到的估计。故此，我们可以发现，求相关系数要求我们每次输入的样本至少有2个。样本数目越多，相关系数的估计越准确。 注意 从式中可以发现，我们定义的皮尔森相关系数时，完全使用的时Tensorflow-Keras API，因此它当然可以用作我们的训练损失函数。但实际情况下，我们并不使用它。考虑一个反例，当两组数据的分布之间唯一的不同只是均值时，亦即 \\mathbf{y}_2 = \\mathbf{y}_1 + C \\mathbf{y}_2 = \\mathbf{y}_1 + C ，这种情况下皮尔森相关系数仍然为1。虽然我们可以考虑用 余弦相似度函数 (Cosine similarity) 来代替它，但经验显示，余弦相似度最大化到一定程度以后，其对应的均方误差反而上升。考虑另一个反例， \\mathbf{y}_2 = \\alpha \\mathbf{y}_1 \\mathbf{y}_2 = \\alpha \\mathbf{y}_1 ，显然 \\mathbf{y}_1 \\mathbf{y}_1 和 \\mathbf{y}_2 \\mathbf{y}_2 的余弦相似度是1。因此，实际应用中，无论是皮尔森相关系数还是余弦相似度，都适合用作评价函数而不是损失函数。 与上一节不同的是，由于这是一个线性回归器，我们不给它提供激活函数。","text_tokens":["s","tf","理想","度","cosine","17","y","class","并","相关","一层","16","as","定义","下","应当","最大","0.0","即","上升","23","数目","]","虽然","类","tensorflow","越","metrics","*","只能","经验","又",",","linreghandle","compile","/","性相","通过","适合","py","以后","接下来","到","\\","是否","4","一定","方","我们","的","集来","得到","batch","对应","}","relation","直接","25","10","程度","数据","[","中","接下","15","误差","因此","name","9","之间","输入","多","当然","提供","反映","sqrt","”","代替","返回","constant","(","dtype","pred","相关系数","已经","训练","最大化","+","return","none","均值","为何","其","11","完全","@","情况","staticmethod","上文","不","函数","同时","stddev","可以","=","m","作为","结果","自行","这是","。","网络层","21","实际","lr","器","但","optimizername","backend","construction","这","关系","模型","linear","是","应用","输出","调用","kernel","mean","发现","meansquarederror","显示","只是","有","19","用","similarity","6","bias","无论","一个","a","5","7","the","dense","14","8","def","下来","and","float32","还","由于","层","set","叫","个","shape","看作","use","22","construct","线性",")","“","adam","回归","13","model","loss","损失","这种","keras","系数","给","为","注意","激活","-",":","一点","至少","18","过","考虑","losses","做","label","2","cls","'","_","1","求","不到","mathbf","均方","无论是","两组","一节","randomnormal","input","10.0","用来","inputs","要求","dense1","显然","optimizer","initializers","true","layers","outputs","样本",".","不同","反而","故此","#","反例","还是","来","每","构造","该","相似","均","initializer","专门","就是","3","都","不是","时","求取","c","皮尔森","12","而","网络","26","20","亦","当","api","线性相关","整个","分布","alpha","上","24","唯一","{","大化","使用","准确","每次","从式","axis","，","用作","估计","仍然","self","它","余弦","另","与","和","了","叙述"," ","lin","square","0","activation","评价","式"],"title":"构造方法","title_tokens":["构造","构造方法","方法"]},{"location":"book-1-x/chapter-1/linear-regression/#_16","text":"类模型的 compile 、 fit 、 evaluate 、 predict 等API与顺序模型完全相同，详情请查看： Model类 (函数式API) - Keras中文文档","text_tokens":["(","相同","等","完全","fit","evaluate","函数","、",")","详情请","model","类","keras","predict","api","查看","-","compile","：","模型","完全相同","详情","的","，","文档","中文","与","式"," ","顺序"],"title":"训练和测试方法","title_tokens":["和","训练","测试","测试方法","方法"]},{"location":"book-1-x/chapter-1/linear-regression/#_17","text":"上一节中，我们每次训练后，就当场显示分析结果。在本节中，我们会“再进一步”。即使用 tools.py 专门进行实验结果分析（后处理）。相对地，训练后，我们会讲 原始输出 (raw output) 保存到文件里。这是一种编写代码的思想，是为了便于我们批量分析测试数据。在后面的Project中，我们会看到，我们既会编写当场显示分析结果的测试代码，也会编写保存输出后使用 tools.py 分析的代码。究竟使用哪种方式分析数据，视具体情况而定。一般地，测试少量数据时，我们当场分析；批量测试大量数据时，或者需要比较不同选项（例如不同噪声）对结果的影响时，我们在 tools.py 中分析。本实验的情况属于后者。","text_tokens":["原始","（","编写","究竟","或者","即","一步","视","raw","处理","例如","tools","py","到","我们","的","少量","中","数据","就","地","”","(","为了","里","训练","文件","便于","测试数据","情况","）","结果","比较","这是","大量","。","种","方式","需要","是","一般","输出","project","实验","显示","选项","具体情况","output","后处理","分析","再进一步","也","测试","测试代码",")","“","进一步","噪声","后者","进行","；","会","哪","批量","一节","对","代码","一种",".","保存","不同","影响","本","看到","具体","在","当场","专门","后面","时","本节","后","讲","上","使用","每次","而定","既会","思想","，","相对"," ","属于"],"title":"调试","title_tokens":["调试"]},{"location":"book-1-x/chapter-1/linear-regression/#_18","text":"由于我们本次实验需要对比不同设置下的回归器性能，我们希望随机生成的矩阵 \\mathbf{A} \\mathbf{A} ，向量 \\mathbf{c} \\mathbf{c} 应当可复现；换言之，我们希望我们的结果是可复现的。 关于这一问题，Keras的文档给出的建议可以在这里查阅： 如何在 Keras 开发过程中获取可复现的结果？ - Keras中文文档 我们只需要使 argparse 添加一个选项 -sd ( --seed )，并通过该选项控制： 1 2 3 4 5 6 def setSeed ( seed ): np . random . seed ( seed ) random . seed ( seed + 12345 ) tf . set_random_seed ( seed + 1234 ) if args . seed is not None : # Set seed for reproductable results setSeed ( args . seed ) 其中， np.random.seed ， random.seed ， tf.set_random_seed 分别来自Numpy，python原生的random库，以及Tensorflow。将这三个库的 随机种子 (seed) 设为三个不同的值，即可保证我们每次指定 -sd 后，从程序运行开始，得到的所有随机数都是固定的随机序列。当然， Keras文档 指出，即使如此，我们还不能保证我们的结果完完全全是可复现的。因为多线程算法并发的先后顺序随机性、GPU运算带来的先后顺序随机性等干扰因素，均会导致我们每次得到的结果有细微的偏差。但这些因素对于本实验验证可复现数据的要求几乎没有什么影响。","text_tokens":["tf","随机性","完完全全","并","设","值","下","、","添加","tensorflow","干扰","setseed","什么","先后","通过","numpy","\\","4","我们","本次","的","得到","}","12345","没有","if","对比","中","数据","random","设置","即可","程序运行","当然","算法","运算","原生","程序","(","来自","+","none","完全","生成","控制","对于","即使","seed","可以","种子","结果","。","器","但","results","需要","过程","argparse","：","这","not","是","机数","几乎","固定","？","实验","多线程","选项","建议","有","中文","6","细微","一个","sd","a","5","即使如此","这些","复现","def","还","矩阵","由于","is","set","python","查阅","等","保证","随机数","偏差",")","运行","回归","gpu","reproductable","分别","keras","因为","为","；","-",":","序列","2","mathbf","1234","希望","1","_","问题","关于","文档","要求","因素","并发","先后顺序","带来","args","影响",".","不同","换言之","以及","本","#","在","使","np","只","指定","验证","该","3","都","性能","c","将","后","线程","这里","如何","从","导致","向量","{","获取","所有","for","不能","库","每次","指出","给出","均会","这一","，","可","开发","三个","如此","开始","其中"," ","多线","应当","随机","顺序"],"title":"使实验结果可复现","title_tokens":["使","结果","实验","复现","可"]},{"location":"book-1-x/chapter-1/linear-regression/#_19","text":"首先，训练网络。我们同样随机生成 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的仿射变换，将该变换中的线性变换矩阵采用秩为4的低秩近似，并且设置好数据集，给定噪声扰动由用户决定。默认值下，噪声为 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,10)^6 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,10)^6 ，epoch为20个，每个epoch迭代500次，每次馈入32个样本构成的batch。我们将上一节的主函数输出部分修改成如下形式，并进行不加参数的调试： lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # Initialization A = dp . gen_lowrank ( np . random . normal ( 0 , 10 , [ INPUT_SHAPE , LABEL_SHAPE ]), RANK ) c = np . random . uniform ( 1 , 3 , [ 1 , LABEL_SHAPE ]) dataSet = dp . TestDataRegSet ( 10 , A , c ) dataSet . config ( noise = args . noise ) # Generate a group of testing samples. if args . seed is not None : setSeed ( args . seed + 1000 ) dataSet . config ( batch = args . testBatchNum ) x , y = next ( dataSet ) # Set the data set for training. dataSet . config ( batch = args . trainBatchNum ) # Construct the model and train it. h = LinRegHandle ( learning_rate = args . learningRate , epoch = args . epoch , steppe = args . steppe , optimizerName = args . optimizer ) h . construct () print ( 'Begin to train:' ) print ( '---------------' ) record = h . train ( iter ( dataSet )) # Check the testing results print ( 'Begin to test:' ) print ( '---------------' ) yp , loss_p , corr_p = h . test ( x , y ) # Check the regressed values W , b = h . model . get_layer ( name = 'dense1' ) . get_weights () # Save if args . outputData is not None : np . savez_compressed ( args . outputData , epoch = record . epoch , loss = record . history [ 'loss' ], corr = record . history [ 'relation' ], test_x = x , test_y = y , pred_y = yp , pred_loss = loss_p , pred_corr = corr_p , W = W , b = b , A = A , c = c ) Output Begin to train: --------------- Epoch 1 /20 500 /500 [==============================] - 1s 2ms/step - loss: 29084 .6994 - relation: 0 .3472 Epoch 2 /20 500 /500 [==============================] - 1s 2ms/step - loss: 15669 .9579 - relation: 0 .5597 Epoch 3 /20 500 /500 [==============================] - 1s 2ms/step - loss: 8145 .8705 - relation: 0 .7134 Epoch 4 /20 500 /500 [==============================] - 1s 2ms/step - loss: 4000 .0838 - relation: 0 .8130 Epoch 5 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1856 .1477 - relation: 0 .8801 Epoch 6 /20 500 /500 [==============================] - 1s 2ms/step - loss: 799 .4556 - relation: 0 .9354 Epoch 7 /20 500 /500 [==============================] - 1s 2ms/step - loss: 336 .8600 - relation: 0 .9700 Epoch 8 /20 500 /500 [==============================] - 1s 2ms/step - loss: 166 .5899 - relation: 0 .9813 Epoch 9 /20 500 /500 [==============================] - 1s 2ms/step - loss: 113 .2465 - relation: 0 .9831 Epoch 10 /20 500 /500 [==============================] - 1s 2ms/step - loss: 102 .0431 - relation: 0 .9834 Epoch 11 /20 500 /500 [==============================] - 1s 2ms/step - loss: 99 .6678 - relation: 0 .9838 Epoch 12 /20 500 /500 [==============================] - 1s 2ms/step - loss: 99 .8547 - relation: 0 .9833 Epoch 13 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .1278 - relation: 0 .9834 Epoch 14 /20 500 /500 [==============================] - 1s 2ms/step - loss: 99 .6048 - relation: 0 .9835 Epoch 15 /20 500 /500 [==============================] - 1s 2ms/step - loss: 101 .1930 - relation: 0 .9832 Epoch 16 /20 500 /500 [==============================] - 1s 2ms/step - loss: 101 .6636 - relation: 0 .9835 Epoch 17 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .6665 - relation: 0 .9834 Epoch 18 /20 500 /500 [==============================] - 1s 2ms/step - loss: 101 .2459 - relation: 0 .9832 Epoch 19 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .9701 - relation: 0 .9836 Epoch 20 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .7719 - relation: 0 .9836 Begin to test: --------------- 10 /10 [==============================] - 0s 5ms/sample - loss: 94 .8883 - relation: 0 .9897 Evaluated loss ( losses.MeanSquaredError ) = 94 .88829040527344 Evaluated metric ( Pearson ' s correlation ) = 0 .9897396 以上结果是不加任何参数的前提下，直接以默认参数运行程序得到的。结果显明，MSE最后收敛在100左右，因为我们馈入的label添加了标准差为10的白噪声，对应的方差为100。可知，实验结果与预期一致。另一方面，我们可以看到，相关系数在这里可以充当类似准确度的作用，考虑到我们默认的噪声为10，这一相关系数的收敛结果是符合我们的预期的。 我们还可以注意到，这段代码中，生成测试集的代码被提前了，这是为了确保每次运行程序，只要指定了种子，生成的测试集总是一致的。 现在，我们可以导出生成数据了，首先，我们改变不同的优化器，其他参数全部一致，例如，学习率均为0.01（Adadelta除外，其初始参数一般推荐为1.0）。调用代码时的参数设置如下 python lin-reg.py -e 25 -sd 1 -do test/algorithm/ { optimizer } -o { optimizer } 其中我们用 {optimizer} 来指代我们选用的优化算法。同时，我们固定测试的epoch数量为25，这是因为有些算法的收敛速度不足以保证20个epoch收敛。 接下来，我们固定优化器为Adam，改变不同的噪声，分别令标准差为0, 1, 5, 10, 50, 100，产生多组结果。 python lin-reg.py -sd 1 -do test/noise/ { noise } -is { noise }","text_tokens":["s","savez","（","16","用户","record","是因为","166","推荐","py","其他","的","得到","}","begin","25","if","name","设置","9","gen","b","9836","(","调试","不加","compressed","一方","左右","=","pearson","总是","9813","这","6048","关系","learningrate","sample","generate","layer","显明","meansquarederror","15669","5","4556","the","标准差","save","下来","^","and","秩为","is","set","testdataregset","shape","37","6994","1000","分别","rate","被","32","boldsymbol","2","0s","94","training","率均","不同","足以","看到","1s","前提","构成","来","除外","26","27","36","标准","产生","这一","，","一致","8801","get","随机","check","samples","首先","history","下","线性变换","23","mathcal","1856","linreghandle","默认","4","1930","batch","直接","[","变换","接下","秩","100","算法","程序","任何","为了","8883","训练","+","11","同时","seed","n","现在","1278","21","一般","dp","输出","noise","9834","用","以","6","a","7","output","8","还","mse","并且","python","do","个","器为","保证","22","construct","w","2ms",")","29","噪声","model","loss","多组","rank","it","低","考虑","losses","initialization","label","扰动","correlation","初始","_","weights","一节","白","8547","主","lowrank","r","50","样本","代码","在","成","令","o","28","adadelta","方面","step","充当","5597","选用","这里","24","6636","9897","另一方","99","799","每次","dataset","lin","113","y","9838","8705","p","次","1.0","]","添加","采用","5899",",","接下来","指代","决定","0838","relation","yp","导出","reg","outputdata","中","全部","random","9832","确保","algorithm","to","pred","相关系数","none","其","生成","不","print","可以","steppe","learning","段","8145","of","6678","next","4000","收敛","testbatchnum","器","部分","33","7134","：","data","30","102","test","2465","固定","config","testing","给定","仿射","方差","集","改变","14","仿射变换","加","0.01","13","mathbb","为","进行","mapsto","注意","-","cls","'","1","修改","可知","9354","9897396","#","该","符合","3","时","c","将","由","类似","上","for","336","35","默认值","2459","trainbatchnum","0431","了","准确度","values","参数设置"," ","以上","0","normal","17","9579","并","相关","9835","提前","8130","好","setseed","例如","/","7719","到","\\","h","我们","对应","~","10","数据","15","29084","预期","uniform","作用","馈入","每个","31","）","函数","种子","只要","3472","结果","这是","。","9831","optimizername","results","另一方面","优化","not","不足","是","调用","8600","实验","x","19","34","regressed","不足以","sd","最后","有些","101","e","5ms","同样","近似","矩阵","参数","metric","500","一方面","9700","学习","测试","形式","线性","运行","adam","9701","1477","因为","系数","evaluated",":","varepsilon","18","group","mathbf","input","dense1","sim","optimizer","epoch","args",".","迭代","np","指定","数量","88829040527344","12","网络","20","9833","如下","速度","{","准确","train","与","其中","corr","iter","6665"],"title":"使实验代码保存输出","title_tokens":["使","输出","实验","代码","保存"]},{"location":"book-1-x/chapter-1/linear-regression/#toolspy","text":"首先，在 tools.py 中定义数据解析函数 tools.py 1 2 3 4 5 6 7 8 9 10 11 12 def parseData ( path , keys ): keys_list = dict (( k , []) for k in keys ) name_list = [] for f in os . scandir ( path ): if f . is_file (): name , _ = os . path . splitext ( f . name ) name_list . append ( name . replace ( '_' , ' ' )) data = np . load ( os . path . join ( path , f . name )) for key in keys : keys_list [ key ] . append ( data [ key ]) epoch = data [ 'epoch' ] return name_list , epoch , keys_list 该函数的作用是，给定保存输出文件的文件夹路径，能够自动读取文件夹下所有数据文件，并将不同文件的结果列在列表的不同元素中。 keys 关键字能帮助我们指派我们关心的数据字段。 接下来，我们通过如下代码，对比不同优化器条件下的损失函数和测度函数，对比不同噪声条件下的损失函数和测度函数，输出的曲线反映了对训练过程的跟踪。 tools.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def showCurves ( path , prefix = '{0}' , converter = str ): ''' Show curves from different tests in a same folder. ''' name_list , epoch , keys_list = parseData ( path , [ 'loss' , 'corr' ]) loss_list = keys_list [ 'loss' ] corr_list = keys_list [ 'corr' ] if ( not loss_list ) or ( not corr_list ): raise FileExistsError ( 'No data found, could not draw curves.' ) for i in range ( len ( loss_list )): plt . semilogy ( loss_list [ i ], label = prefix . format ( converter ( name_list [ i ])), marker = MARKERS [ i % 9 ], markevery = 3 ) plt . legend () plt . xlabel ( 'epoch' ), plt . ylabel ( 'MSE' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . tight_layout (), plt . show () for i in range ( len ( corr_list )): plt . plot ( corr_list [ i ], label = prefix . format ( converter ( name_list [ i ])), marker = MARKERS [ i % 9 ], markevery = 3 ) plt . legend () plt . xlabel ( 'epoch' ), plt . ylabel ( 'Pearson \\' s correlation' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . tight_layout (), plt . show () showCurves ( './test/algorithm' ) showCurves ( './test/noise' , prefix = 'ε=N(0,{0})' , converter = int ) Output (优化器) 损失函数 (MSE) 测度函数 (相关系数) Output (噪声) 损失函数 (MSE) 测度函数 (相关系数) 可见，损失曲线反映了训练的进度，而测度曲线反映了当前的准确度。我们可以得到如下结论： 令人意外的是，SGD和Nesterov动量法收敛速度最快。这是由于这两种方法没有引入对学习率的调整。我们使用的损失函数初始点梯度非常大，这使得简单的方法，形如SGD和动量法在一开头就取得了非常迅速的下降；而对那些需要调整学习率的算法而言，初始梯度在很大的情况下，会导致初始学习率被降到较小的水准。这就是为何Adagrad几乎不收敛的原因，因为一开始这一算法的学习率就被大梯度抑制到将近0的水平了，导致训练无法为继； 在调整学习率的算法里，收敛速度有 RMSprop > Adam = NAdam > Adamax = AMSgrad > Adadelta。从AMSgrad以上的这些算法都可资利用，Adadelta的原理和RMSprop几乎相同但效果相差甚巨，这是由于参数不同引起的，我们虽然将Adadelta的学习率特地设为 1.0 ，仍然远远不如RMSprop，可见一个合适的参数对算法的重要性。 噪声的输出结果并不令人意外，所有噪声条件下的MSE最后都收敛到对应的噪声方差上。 为了检查测试集的情况，我们通过以下函数来绘制比较不同样本在不同优化器、不同噪声条件下的RMSE（均方根误差）， tools.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def showBars ( path , prefix = '{0}' , converter = str , ylim = None ): ''' Show bar graphs for RMSE for each result ''' name_list , epoch , keys_list = parseData ( path , [ 'test_y' , 'pred_y' ]) #print(keys_list) ytrue_list = keys_list [ 'test_y' ] ypred_list = keys_list [ 'pred_y' ] def RMSE ( y_true , y_pred ): return np . sqrt ( np . mean ( np . square ( y_true - y_pred ), axis = 1 )) N = ytrue_list [ 0 ] . shape [ 0 ] NG = len ( ytrue_list ) for i in range ( NG ): plt . bar ([ 0.6 + j + 0.8 * i / NG + 0.4 / NG for j in range ( - 1 , 9 , 1 )], RMSE ( ytrue_list [ i ], ypred_list [ i ]), width = 0.8 / NG , label = prefix . format ( converter ( name_list [ i ]))) plt . legend ( ncol = 5 ) plt . xlabel ( 'sample' ), plt . ylabel ( 'RMSE' ) if ylim is not None : plt . ylim ([ 0 , ylim ]) plt . gcf () . set_size_inches ( 12 , 5 ), plt . tight_layout (), plt . show () showBars ( './test/algorithm' , ylim = 70 ) showBars ( './test/noise' , prefix = 'ε=N(0,{0})' , converter = int ) Output (优化器) Output (噪声) 上述结果反映了 测试结果和训练情况相仿，这是由于我们的训练集和测试机完全独立同分布； Adadelta和Adagrad还没有训练好，它们的误差明显大于其他算法。且Adagrad已经无法收敛，可见这种算法不实用。 再接下来，我们要分别展示不同测试下的输出。下面列举的所有输出由该函数所产生： tools.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def saveResults ( path , opath , oprefix , datakeys , title = '' , xlabel = None , ylabel = None , onlyFirst = False , plot = False , prefix = ' ({0})' , converter = str ): ''' Save result graphs to a folder. ''' name_list , _ , data_list = parseData ( path , datakeys ) if plot : # show curves c_list = data_list [ 'c' ] b_list = data_list [ 'b' ] NG = len ( b_list ) for i in range ( NG ): plt . plot ( c_list [ i ] . T , label = 'c' ) plt . plot ( b_list [ i ] . T , label = 'b' ) plt . legend () plt . gca () . set_title ( title + prefix . format ( converter ( name_list [ i ]))) if xlabel is not None : plt . xlabel ( xlabel ) if ylabel is not None : plt . ylabel ( ylabel ) plt . gcf () . set_size_inches ( 5 , 5 ) if onlyFirst : formatName = '' else : formatName = name_list [ i ] . replace ( ' ' , '_' ) plt . savefig ( os . path . join ( opath , oprefix + '{0}.svg' . format ( formatName ))) plt . close ( plt . gcf ()) if onlyFirst : return else : # show images data_list = data_list [ datakeys [ 0 ]] NG = len ( data_list ) for i in range ( NG ): plt . imshow ( data_list [ i ], interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( title + prefix . format ( converter ( name_list [ i ]))) if xlabel is not None : plt . xlabel ( xlabel ) if ylabel is not None : plt . ylabel ( ylabel ) plt . gcf () . set_size_inches ( 6 , 5 ) if onlyFirst : formatName = '' else : formatName = name_list [ i ] . replace ( ' ' , '_' ) plt . savefig ( os . path . join ( opath , oprefix + '{0}.png' . format ( formatName ))) plt . close ( plt . gcf ()) if onlyFirst : return 测试代码 1 2 3 4 5 6 7 8 9 10 11 12 13 def saveAllResults (): saveResults ( './test/algorithm' , './record/algorithm' , 'alg-A-' , [ 'A' ], title = 'A' , prefix = '' , onlyFirst = True ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-yt-' , [ 'test_y' ], title = 'True values' , prefix = '' , onlyFirst = True ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-y-' , [ 'pred_y' ], title = 'Predicted values' ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-W-' , [ 'W' ], title = 'W' ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-cb-' , [ 'c' , 'b' ], title = 'Biases' , plot = True ) saveResults ( './test/noise' , './record/noise' , 'noi-A-' , [ 'A' ], title = 'A' , prefix = '' , onlyFirst = True ) saveResults ( './test/noise' , './record/noise' , 'noi-yt-' , [ 'test_y' ], title = 'True values' , prefix = ' (ε=N(0,{0}))' , converter = int ) saveResults ( './test/noise' , './record/noise' , 'noi-y-' , [ 'pred_y' ], title = 'Predicted values' , prefix = ' (ε=N(0,{0}))' , converter = int ) saveResults ( './test/noise' , './record/noise' , 'noi-W-' , [ 'W' ], title = 'W' , prefix = ' (ε=N(0,{0}))' , converter = int ) saveResults ( './test/noise' , './record/noise' , 'noi-cb-' , [ 'c' , 'b' ], title = 'Biases' , plot = True , prefix = ' (ε=N(0,{0}))' , converter = int ) saveAllResults () 首先考虑不同优化器的测试，在这些测试里，我们确保 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 对所有测试相同，且产生的随机数据真值 \\mathbf{y}_{\\mathrm{true}} \\mathbf{y}_{\\mathrm{true}} 对所有测试也相同，亦即： \\mathbf{A} \\mathbf{A} \\mathbf{y} \\mathbf{y} 的真实值 于是我们可得到所有的数据 优化器 \\mathbf{y} \\mathbf{y} 的预测值 \\mathbf{W} \\mathbf{W} \\mathbf{b} \\mathbf{b} 与 \\mathbf{c} \\mathbf{c} Adadelta Adagrad Adam Adamax AMSgrad Nesterov Adam Nesterov Moment RMSprop SGD 接下来考虑不同噪声的测试，在这些测试里，我们确保 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 对所有测试相同，但由于噪声大小的不同，随机数据真值 \\mathbf{y}_{\\mathrm{true}} \\mathbf{y}_{\\mathrm{true}} 会有所偏差： \\mathbf{A} \\mathbf{A} 于是我们可得到所有的数据 \\boldsymbol{\\varepsilon} \\sim N(0,~?) \\boldsymbol{\\varepsilon} \\sim N(0,~?) \\mathbf{y} \\mathbf{y} 的真实值 \\mathbf{y} \\mathbf{y} 的预测值 \\mathbf{W} \\mathbf{W} \\mathbf{b} \\mathbf{b} 与 \\mathbf{c} \\mathbf{c} 0 1 5 10 50 100 我们最为看重的，其实是是否拟合出 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 。一系列实验表明， \\mathbf{W} \\mathbf{W} 和 \\mathbf{b} \\mathbf{b} 的拟合效果甚好。由于我们建立的仿射变换模型和原始仿射变换模型有着完全一致的结构，优化结果反映这一问题的解相当准确。至此，我们已经掌握了一个完整的Project应当具有的模块结构，以及对不同的优化器有了理论和实际的体验。在后续的章节里，除非有特别的应用，我们不再探讨不同的优化器对结果的影响，在绝大多数情况下，我们都将使用AMSgrad。","text_tokens":["s","（","列",">","要","值","16","定义","record","imshow","onlyfirst","而言","虽然","*","70","py","结构","其他","biases","join","的","得到","文件夹","法在","}","有所","大多数","if","非常","25","水平","误差","name","9","路径","反映","较","sqrt","b","width","nearest","could","(","文件","%","为何","folder","from","=","关心","pearson","实际","需要","过程","这","关系","帮助","模型","具有","sample","应用","几乎","38","project","件夹","令人","5","save","方法","下来","raise","bar","41","is","set","shape","降到","看重","37","器有","真值","or","远远不如","markevery","这种","分别","interpolation","semilogy","后续","被","32","inches","会","继","auto","boldsymbol","2","方根","引起","ng","上述","不同","aspect","yt","相同","来","showcurves","关键","26","亦","从","27","分布","36","利用","可资利用","产生","迅速","这一","，","展示","仍然","一致","开始","和","44","converter","除非","随机","0.4","首先","下","甚巨","、","即","23","拟合","跟踪","再","特地","amsgrad","当前","简单","key","4","gcf","plot","它们","大于","预测值","?","没有","对比","[","形如","变换","接下","就","100","fileexistserror","算法","为了","训练","+","return","found","11","gca","n","绘制","预测","21","replace","ypred","输出","mean","noise","os","原因","有","t","6","一个","a","7","output","那些","in","动量","水准","datakeys","8","最为","还","mse","不如","相仿","46","show","22","列举","偏差","w",")","each","29","噪声","loss","损失","列表","tight","list","plt","考虑","label","correlation","初始","_","小","建立","j","saveallresults","效果","graphs","远远","nesterov","0.6","predicted","对","50","测度","代码","样本","以及","在","str","数据文件","28","adadelta","就是","file","导致","结论","24","i","检查","ytrue","下面","images","range","marker","设","same","y","alg","引入","1.0","]","dict","所","原理",",","opath","能够","通过","tools","点","接下来","40","append","独立","mathrm","一系列","解","取得","读取","中","开头","xlabel","algorithm","确保","元素","相差","to","相关系数","pred","none","且","完全","情况","机","不","print","两种","段","可以","最快","重要","使得","prefix","收敛","器","指派","但","33","sgd","data","：","数据字","30","test","绝大","将近","表明","同","给定","仿射","方差","title","这些","集","一","14","def","系列","由于","理论","特别","有着","else","仿射变换","45","测试代码","oprefix","进度","svg","13","cb","为","adagrad","；","tests","-","len","scandir","完整","大","'","1","问题","定义数据","ε","keys","noi","意外","影响","保存","#","模块","savefig","different","法","章节","出","该","3","都","掌握","一系","c","相当","将","而","close","探讨","其实","上","所有","for","rmse","使用","大多","35","42","条件","了","准确度","抑制","values","能"," ","以上","0","原始","17","关键字","并","相关","无法","合适","layout","好","率","自动","真实","showbars","/","于是","到","\\","是否","我们","对应","~","10","绝大多数","数据","15","远不如","里","已经","作用","moment","梯度","format","31","）","函数","path","结果","比较","这是","可见","curves","。","0.8","不再","明显","曲线","调整","优化","not","no","是","legend","实验","19","34","nadam","最后","ylim","int","参数","43","markers","重要性","学习","也","测试","完全一致","下降","adam","ncol","以下","因为","系数",":","实用","18","varepsilon","解析","splitext","至此","mathbf","false","f","parsedata","draw","sim","png","formatname","adamax","ylabel","epoch","true",".","体验","很大","np","k","多数","均","result","由该","甚","saveresults","12","20","如下","速度","rmsprop","{","load","colorbar","size","准确","大小","axis","可","与","corr","39","square","应当"],"title":"在tools.py中分析比较结果","title_tokens":["在","py","tools","中","比较","结果","分析","."]},{"location":"book-1-x/chapter-1/nonlinear-regression/","text":"非线性回归 ¶ 摘要 本节将讨论如何将一个解析的非线性的回归问题，表述成使用非线性函数激活的线性回归问题。特别地，我们将通过自己定义“激活层”来引入我们定义的解析的非线性函数。 理论 ¶ 一般回归问题 ¶ 回忆我们的多输出方程 \\mathbf{y} = \\mathcal{F}(x) \\mathbf{y} = \\mathcal{F}(x) ，其中 \\mathcal{F} \\mathcal{F} 可以是非线性函数，那么我们可以考虑使用一个带可调参数的模型 \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}) \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}) 来模拟它，其中 \\boldsymbol{\\Theta} \\boldsymbol{\\Theta} 是可调的参数。于是，该问题可以被表述为 \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\boldsymbol{\\Theta}} &\\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}_k) \\right),\\\\ \\mathrm{s.t.}~&\\mathbf{y}_k = \\mathcal{F}(\\mathbf{x}_k). \\end{aligned} \\end{equation} 其中， (\\mathbf{x}_k,~\\mathbf{y}_k) \\in \\mathbb{D} (\\mathbf{x}_k,~\\mathbf{y}_k) \\in \\mathbb{D} 来自由非线性函数 \\mathcal{F} \\mathcal{F} 产生的数据集。 非线性解析函数的分解 ¶ 对于一个解析的非线性函数，我们假设任何这样的函数都可以分解成多个复合函数 \\mathbf{f}_i \\mathbf{f}_i ，其中每个复合函数都只包含一个仿射变换 \\mathbf{h}_j = \\mathbf{W}_j \\cdot + \\mathbf{b}_j \\mathbf{h}_j = \\mathbf{W}_j \\cdot + \\mathbf{b}_j 和一个对在各元素操作的非线性激活函数 \\Lambda_j \\Lambda_j 。因此，复合函数可以写作 \\mathbf{f}_i = \\Lambda_j \\circ \\mathbf{h}_j \\mathbf{f}_i = \\Lambda_j \\circ \\mathbf{h}_j 。于是，整个非线性的函数可以表述为： \\begin{equation} \\begin{aligned} \\mathcal{F} = \\Lambda_M \\circ \\mathbf{h}_M \\circ \\Lambda_{M-1} \\circ \\mathbf{h}_{M-1} \\circ \\cdots \\Lambda_1 \\circ \\mathbf{h}_1. \\end{aligned} \\end{equation} 例如，对函数 \\begin{align} \\mathcal{F}(\\mathbf{x}) = \\exp( \\mathbf{A} \\log ( | \\mathbf{B} \\mathbf{x} + \\mathbf{c} | ) ). \\end{align} 可以分解为： \\mathbf{h}_1 (\\mathbf{x}) = \\mathbf{B} \\mathbf{x} + \\mathbf{c} \\mathbf{h}_1 (\\mathbf{x}) = \\mathbf{B} \\mathbf{x} + \\mathbf{c} ； \\Lambda_1 (\\mathbf{h}_1) = \\log ( | \\mathbf{h}_1 | ) \\Lambda_1 (\\mathbf{h}_1) = \\log ( | \\mathbf{h}_1 | ) ； \\mathbf{h}_2 (\\Lambda_1) = \\mathbf{A} \\Lambda_1 \\mathbf{h}_2 (\\Lambda_1) = \\mathbf{A} \\Lambda_1 ； \\Lambda_2 (\\mathbf{h}_2) = \\exp ( \\mathbf{h}_2 ) \\Lambda_2 (\\mathbf{h}_2) = \\exp ( \\mathbf{h}_2 ) 。 实际上，当然还存在更复杂的情况，例如，一个非线性函数 \\mathbf{f}_j \\mathbf{f}_j 是两个非线性函数的和、积、商，或是某函数导数的范数等……但原则上，这些函数都可以写作上述（多个）可分解复合函数的（联合）变换。本质上，函数中的任何参数，都可以看作是在参与一个仿射变换。因此，任何函数只要能写出解析式，理论上就能分解为（多个）上述的可分解函数的形式。 相信有一点功底的读者都可以看出， (2) (2) 其实就是一个神经网络的表达式。换言之，只要知道一个函数的解析式，我们就可以用一个或多个神经网络来为其建模。虽然我们可能不知道这个函数里具体的参数值，但通过对网络训练，我们可以让网络的参数回归到函数的参数上。 本节问题 ¶ 考虑一组三角函数的线性组合，使得列向量 \\mathbf{x} \\in \\mathbb{R}^T \\mathbf{x} \\in \\mathbb{R}^T 映射到列向量 \\mathbf{y} \\in \\mathbb{R}^T \\mathbf{y} \\in \\mathbb{R}^T ，其中 \\mathbf{x} \\mathbf{x} ， \\mathbf{y} \\mathbf{y} 均为时间域上的变量，则： \\begin{align} \\mathbf{y} = \\sum_{i=1}^N a_i \\cos ( \\omega_i \\mathbf{x} + \\varphi_i ). \\end{align} 如果我们将其写成矩阵的形式，应当有 \\begin{align} \\mathbf{y} = \\cos ( \\mathbf{x} \\boldsymbol{\\omega}^T + \\mathbf{1} \\boldsymbol{\\varphi}^T ) \\mathbf{a}. \\end{align} 设若我们有大量的样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，但我们不知道参数 \\boldsymbol{\\omega},~\\boldsymbol{\\varphi},~\\mathbf{a} \\boldsymbol{\\omega},~\\boldsymbol{\\varphi},~\\mathbf{a} 。若我们想在频率域上拟合出该模型的参数，则根据 (1) (1) ，该问题可以写作： \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\boldsymbol{\\omega},~\\boldsymbol{\\varphi},~\\mathbf{a}} &\\sum_{k=1}^K \\lVert \\mathrm{Re}\\{\\mathbf{Y}_k - \\hat{\\mathbf{Y}}_k \\} \\rVert_2^2 + \\lVert \\mathrm{Im}\\{\\mathbf{Y}_k - \\hat{\\mathbf{Y}}_k \\} \\rVert^2_2,\\\\ \\mathrm{s.t.}~&\\mathbf{Y}_k = \\mathrm{FFT}(\\mathbf{y}_k), \\\\ &\\hat{\\mathbf{Y}}_k = \\mathrm{FFT}(\\cos ( \\mathbf{x}_k \\boldsymbol{\\omega}^T + \\mathbf{1} \\boldsymbol{\\varphi}^T ) \\mathbf{a}). \\end{aligned} \\end{equation} 其中，FFT指 快速傅里叶变换 ，虽然FFT是一个线性变换，但显然，该问题是一个非线性问题，这是由于预测值的表达式 \\hat{\\mathbf{y}} \\hat{\\mathbf{y}} 的表达式 (5) (5) 是非线性的。 由于表达式 (5) (5) 是一个显式函数，相比上一节求取低秩近似的仿射变换，我们可以知道，即使该问题即使存在多个不同的 \\mathbf{x}_k \\mathbf{x}_k 对应同一个 \\mathbf{y}_k \\mathbf{y}_k ，也不影响我们对问题的求解（即训练得到的参数能和真实参数产生相同的输出）。然而，从这里的参数的定义可以看出，我们在这个问题中使用的参数是非常低秩的（所有的参数秩均为1），这将导致这个问题的解具有高度的不确定性，许多不同的参数组 \\boldsymbol{\\omega},~\\boldsymbol{\\varphi},~\\mathbf{a} \\boldsymbol{\\omega},~\\boldsymbol{\\varphi},~\\mathbf{a} 均能达到相同的效果。例如，我们已知三个参数向量是长度相同的，若我们选取三个向量各自的第 i i 个元素，和其对应的第 j j 个元素相互交换（例如 \\omega_i \\leftrightarrow \\omega_j \\omega_i \\leftrightarrow \\omega_j ），则根据 (4) (4) ，这两个不同的解均能产生相同的效果。另一个例子是，由于余弦函数具有周期性，对 \\boldsymbol{\\varphi} \\boldsymbol{\\varphi} 的任意元素 \\varphi_i \\varphi_i ，即使令 \\varphi_i = \\varphi_i + 2 \\pi \\varphi_i = \\varphi_i + 2 \\pi ，仍不影响拟合的效果。因此，通过 (6) (6) 求解的向量 \\boldsymbol{\\varphi} \\boldsymbol{\\varphi} 也具有不确定性。 根据我们前面提到的对非线性解析函数的分解方法，该问题的模型可以分解为： \\mathbf{h}_1 (\\mathbf{x})= \\mathbf{x} \\boldsymbol{\\omega}^T + \\mathbf{1} \\boldsymbol{\\varphi}^T \\mathbf{h}_1 (\\mathbf{x})= \\mathbf{x} \\boldsymbol{\\omega}^T + \\mathbf{1} \\boldsymbol{\\varphi}^T ； \\Lambda_1 (\\mathbf{h}_1) = \\cos ( \\mathbf{h}_1 ) \\Lambda_1 (\\mathbf{h}_1) = \\cos ( \\mathbf{h}_1 ) ； \\mathbf{h}_2 (\\Lambda_1) = \\Lambda_1 \\mathbf{a} \\mathbf{h}_2 (\\Lambda_1) = \\Lambda_1 \\mathbf{a} ； \\hat{\\mathbf{Y}} = \\Lambda_2 (\\mathbf{h}_2) = \\mathrm{FFT} ( \\mathbf{h}_2 ) \\hat{\\mathbf{Y}} = \\Lambda_2 (\\mathbf{h}_2) = \\mathrm{FFT} ( \\mathbf{h}_2 ) 。 解非线性回归问题 ¶ 我们已经知道，该问题可以建立成一个两层的模型，两层的变换函数和激活函数分别为 (\\mathbf{h}_1,~\\Lambda_1) (\\mathbf{h}_1,~\\Lambda_1) , (\\mathbf{h}_2,~\\Lambda_2) (\\mathbf{h}_2,~\\Lambda_2) 。然而，实现该模型仍然存在技术问题。即，神经网络中，并未定义 \\mathbf{h}_1,~\\mathbf{h}_2,~\\Lambda_2 \\mathbf{h}_1,~\\mathbf{h}_2,~\\Lambda_2 的层API，因此，我们必须自己来实现这些功能。 这一节讨论的内容更偏向于技术实现，而且对新入门的读者而言具有一定的难度。但本节讨论的技术，即自定义网络层，实在是非常广泛地应用在Keras API的用户中。例如，著名的Residual network和Inception network，在Tensorflow-Keras API中均未提供现成的API，需要读者自行设法构造。 熟悉旧版Tensorflow的用户，可能会发现，在实现自定义API的过程上，旧版API使用起来更容易上手；然而，Keras式的API也有其好处，那就是强制用户必须按照规范、统一的标准处理API的定义和接口，使得用户更容易建立规范的编写习惯。 自定义网络层 ¶ 自定义Keras层的方法可以参照： 编写你自己的Keras层 - Keras中文文档 编写好的层是一个类API，可以同时被顺序模型或类模型调用。 学习一个完全规范化的风格 ¶ 让我们观察Tensorflow-keras模型对最简单的层，全连接层 Dense 的定义（我们之前也分别在顺序模型和类模型中使用过该API）。下面的内容摘自 Tensorflow源码 ： import 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from tensorflow.python.eager import context from tensorflow.python.framework import common_shapes from tensorflow.python.framework import ops from tensorflow.python.framework import tensor_shape from tensorflow.python.keras import activations from tensorflow.python.keras import backend as K from tensorflow.python.keras import constraints from tensorflow.python.keras import initializers from tensorflow.python.keras import regularizers from tensorflow.python.keras.engine.base_layer import Layer from tensorflow.python.keras.engine.input_spec import InputSpec from tensorflow.python.keras.utils import conv_utils from tensorflow.python.keras.utils import generic_utils from tensorflow.python.keras.utils import tf_utils from tensorflow.python.ops import array_ops from tensorflow.python.ops import gen_math_ops from tensorflow.python.ops import math_ops from tensorflow.python.ops import nn from tensorflow.python.ops import nn_ops from tensorflow.python.ops import standard_ops from tensorflow.python.util.tf_export import tf_export class Dense 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @tf_export ( 'keras.layers.Dense' ) class Dense ( Layer ): def __init__ ( self , units , activation = None , use_bias = True , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) def build ( self , input_shape ) def call ( self , inputs ) def compute_output_shape ( self , input_shape ) def get_config ( self ) doc string 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class Dense ( Layer ): \"\"\"Just your regular densely-connected NN layer. `Dense` implements the operation: `output = activation(dot(input, kernel) + bias)` where `activation` is the element-wise activation function passed as the `activation` argument, `kernel` is a weights matrix created by the layer, and `bias` is a bias vector created by the layer (only applicable if `use_bias` is `True`). Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with `kernel`. Example: # as first layer in a sequential model: model = Sequential() model.add(Dense(32, input_shape=(16,))) # now the model will take as input arrays of shape (*, 16) # and output arrays of shape (*, 32) # after the first layer, you don't need to specify # the size of the input anymore: model.add(Dense(32)) Arguments: units: Positive integer, dimensionality of the output space. activation: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: `a(x) = x`). use_bias: Boolean, whether the layer uses a bias vector. kernel_initializer: Initializer for the `kernel` weights matrix. bias_initializer: Initializer for the bias vector. kernel_regularizer: Regularizer function applied to the `kernel` weights matrix. bias_regularizer: Regularizer function applied to the bias vector. activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\").. kernel_constraint: Constraint function applied to the `kernel` weights matrix. bias_constraint: Constraint function applied to the bias vector. Input shape: nD tensor with shape: `(batch_size, ..., input_dim)`. The most common situation would be a 2D input with shape `(batch_size, input_dim)`. Output shape: nD tensor with shape: `(batch_size, ..., units)`. For instance, for a 2D input with shape `(batch_size, input_dim)`, the output would have shape `(batch_size, units)`. \"\"\" Dense.__init__ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Dense ( Layer ): def __init__ ( self , units , activation = None , use_bias = True , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ): if 'input_shape' not in kwargs and 'input_dim' in kwargs : kwargs [ 'input_shape' ] = ( kwargs . pop ( 'input_dim' ),) super ( Dense , self ) . __init__ ( activity_regularizer = regularizers . get ( activity_regularizer ), ** kwargs ) self . units = int ( units ) self . activation = activations . get ( activation ) self . use_bias = use_bias self . kernel_initializer = initializers . get ( kernel_initializer ) self . bias_initializer = initializers . get ( bias_initializer ) self . kernel_regularizer = regularizers . get ( kernel_regularizer ) self . bias_regularizer = regularizers . get ( bias_regularizer ) self . kernel_constraint = constraints . get ( kernel_constraint ) self . bias_constraint = constraints . get ( bias_constraint ) self . supports_masking = True self . input_spec = InputSpec ( min_ndim = 2 ) Dense.build 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Dense ( Layer ): def build ( self , input_shape ): input_shape = tensor_shape . TensorShape ( input_shape ) if tensor_shape . dimension_value ( input_shape [ - 1 ]) is None : raise ValueError ( 'The last dimension of the inputs to `Dense` ' 'should be defined. Found `None`.' ) last_dim = tensor_shape . dimension_value ( input_shape [ - 1 ]) self . input_spec = InputSpec ( min_ndim = 2 , axes = { - 1 : last_dim }) self . kernel = self . add_weight ( 'kernel' , shape = [ last_dim , self . units ], initializer = self . kernel_initializer , regularizer = self . kernel_regularizer , constraint = self . kernel_constraint , dtype = self . dtype , trainable = True ) if self . use_bias : self . bias = self . add_weight ( 'bias' , shape = [ self . units ,], initializer = self . bias_initializer , regularizer = self . bias_regularizer , constraint = self . bias_constraint , dtype = self . dtype , trainable = True ) else : self . bias = None self . built = True Dense.call 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Dense ( Layer ): def call ( self , inputs ): inputs = ops . convert_to_tensor ( inputs ) rank = common_shapes . rank ( inputs ) if rank > 2 : # Broadcasting is required for the inputs. outputs = standard_ops . tensordot ( inputs , self . kernel , [[ rank - 1 ], [ 0 ]]) # Reshape the output back to the original ndim of the input. if not context . executing_eagerly (): shape = inputs . get_shape () . as_list () output_shape = shape [: - 1 ] + [ self . units ] outputs . set_shape ( output_shape ) else : outputs = gen_math_ops . mat_mul ( inputs , self . kernel ) if self . use_bias : outputs = nn . bias_add ( outputs , self . bias ) if self . activation is not None : return self . activation ( outputs ) # pylint: disable=not-callable return outputs Dense.compute_output_shape 1 2 3 4 5 6 7 8 9 class Dense ( Layer ): def compute_output_shape ( self , input_shape ): input_shape = tensor_shape . TensorShape ( input_shape ) input_shape = input_shape . with_rank_at_least ( 2 ) if tensor_shape . dimension_value ( input_shape [ - 1 ]) is None : raise ValueError ( 'The innermost dimension of input_shape must be defined, but saw: %s ' % input_shape ) return input_shape [: - 1 ] . concatenate ( self . units ) Dense.get_config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Dense ( Layer ): def get_config ( self ): config = { 'units' : self . units , 'activation' : activations . serialize ( self . activation ), 'use_bias' : self . use_bias , 'kernel_initializer' : initializers . serialize ( self . kernel_initializer ), 'bias_initializer' : initializers . serialize ( self . bias_initializer ), 'kernel_regularizer' : regularizers . serialize ( self . kernel_regularizer ), 'bias_regularizer' : regularizers . serialize ( self . bias_regularizer ), 'activity_regularizer' : regularizers . serialize ( self . activity_regularizer ), 'kernel_constraint' : constraints . serialize ( self . kernel_constraint ), 'bias_constraint' : constraints . serialize ( self . bias_constraint ) } base_config = super ( Dense , self ) . get_config () return dict ( list ( base_config . items ()) + list ( config . items ())) 为了便于读者阅读，我们将它按照重定义的方法分成了几片不同的代码。下面我们来分别观察不同代码里实现的内容。 初始化方法 ¶ 首先，在 __init__ 方法中，定义了用来初始化该层的所有可选参数。从这一段代码，可以观察到以下结论： units 是 Dense 的输出维度，是唯一一个必选参量，并用 int() 强制转换的方式确保输入的是整数。 除了布尔类型的输入，其他所有的输入都使用 tensorflow.python.keras 下的对应方法保护起来。例如， kernel_regularizer 的实现通过 keras.regularizers 方法初始化。这是为了确保用户使用该接口时，既可以使用字符串指定正则化器，也可以通过一个现成的正则化器实例来指定。 supports_masking 和 input_spec 这两个量由类本身决定，不受用户初始化参数的影响。 input_spec 用来限定输入网络的张量必须具有哪些属性，参见官方文档对 tf.keras.layers.InputSpec 的说明。 supports_masking 用来表示该输入是否支持 Masking 层，参见 Masking - Keras中文文档 对该层的介绍。它主要用来取消时序模型（一般是RNN/LSTM）中缺失的时间点数据对网络结果的影响。一般来说，一个与时序无关的（或者称为时不变(timeless)的）模型，直接设该值为True即可。 特别地， activity_regularizer 通过覆盖输入参量的默认值来实现。这是因为在 Dense 的父类 Layer 中，已经定义过 activity_regularizer 。其他的参量不能通过这种方式实现，是因为它们都跟新加入的参数有关。 构造方法 ¶ 接下来，让我们观察 build 方法。在该方法中，我们实现了网络中各参数的构造过程。 首先，通过 input_shape 来得到输入张量的形状。在本例中，特别检查了 input_shape 的规范性，确保输入数据的最后一个维度值为已知。因为， Dense 的API允许用户只通过输入来自上一层的张量，来推断全连接层的输入维度。特别地，如果我们的层有多个输入， input_shape 会是一个 list 类型。 接下来，通过 self.add_weight 或 self.add_variable 来添加参数。该方法的用法参见官方文档中的 Layers.add_weight 。它接受包括初始化器 initializer ，正则化器 regularizer 等一系列参数，这些参数都要求必须是具有回调属性的实例。这一条件我们已经在 __init__ 方法中满足了。 最后，设定 self.built 为 True 。事实上，Keras推荐我们使用类似 super ( Dense , self ) . build ( input_shape ) 的方式来完成这一设置，这种做法和显式地设定 self.built 等价。 回调方法 ¶ 接下来观察 call 方法。该方法接受的是该层实际输入的张量，同时也输出一个张量，该输出张量即使该层的输出结果。可以说，在该方法中，我们才正式开始实现层的实际功能。 在该范例中，该方法的实现是通过Tensorflow最底层的标准API。这些API对用户来说是封装起来的，一般情况下用户不需要使用它们。实际上，该方法实现的就是 \\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b} \\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b} 的过程。这种代码风格显得颇为繁琐，但是它提供了精密的参数检查和高效的API操作。我们不会讨论这段代码的具体实现细节，因为它既然是被封装的API，我们一般来说就不需要调用它们实现功能（除非我们需要修改使用同样被封装的底层API所编写的模块）。等价地，我们分别介绍Keras API和Tensorflow API如何做到相同的效果。 首先是Keras API版本的等效代码，它修改自 Keras中文文档 ： Dense.call 1 2 3 4 5 6 7 def call ( self , inputs ): res = K . dot ( inputs , self . kernel ) if self . use_bias : res = res + self . bias if self . activation is not None : res = self . activation ( res ) return res 接下来是Tensorflow API版本的等效代码， Dense.call 1 2 3 4 5 6 7 8 9 def call ( self , inputs ): inputs = tf . convert_to_tensor ( inputs ) rank = tf . rank ( inputs ) res = tf . tensordot ( inputs , self . kernel , [[ rank - 1 ], [ 0 ]]) if self . use_bias : res = tf . nn . bias_add ( inputs , self . bias ) if self . activation is not None : res = self . activation ( res ) return res 与Keras API的实现相比，Tensorflow API主要区别是使用 tf.tensordot 时需要指定执行矩阵计算的两轴。实际上，使用Tensorflow API对已经较为熟悉Tensorflow旧版API的用户是十分亲切的，它使我们找回了当初自己编写中层API的感觉。 输出形状方法 ¶ 接下来观察 compute_output_shape 方法。我们知道Tensorflow-Keras支持对每一层的输入输出作形状推断，而形状推断的具体实现就在这一步。 问题 为什么我们需要定义这个方法？难道我们不可以直接通过对输出张量计算 K.shape(output) 或 tf.shape(output) 来确定输出形状吗？ 这是由于，对Tensorflow-Keras而言，推断网络各层的形状和推断网络各层的张量是两码事。定义该方法能够确保我们在不调用任何一个 call 方法的前提下，推断出整个网络各层的输入、输出形状。 这里实现这一方法的过程仍然是调用Tensorflow的最底层API。事实上，Keras API对这一方法的输出并没有严格的要求，它可以是一个 tf.Shape ，可以是一个 list / tuple ，还可以是将两者元素混合在一起的 list 。我们不考虑对输入形状进行这些检查，那么，一个简单的，Keras风格的改写是 Dense.compute_output_shape 1 2 def compute_output_shape ( self , input_shape ): return ( * input_shape [:, - 1 ], self . units ) 或者我们可以更规范一点，使用Tensorflow API来确保该方法的输入、输出和具体操作都是对 tf.Shape 进行的 Dense.compute_output_shape 1 2 3 4 class Dense ( Layer ): def compute_output_shape ( self , input_shape ): input_shape = tensor_shape . TensorShape ( input_shape ) return input_shape [: - 1 ] . concatenate ( self . units ) 设置提取方法 ¶ 最后，在 get_config 中，我们将我们自定义的所有参数实例，通过 serialize 方法加入到该层的参数设置字典中。实现这一步是颇为重要的（但是在 Keras中文文档 中并未提及），它允许我们将我们自己定义的网络层编译成一个包含设置信息的字典，并允许我们通过该字典重构出具有相同参数设置的层来，参见： 关于Keras网络层 - Keras中文文档 例如，对一个 Dense 层，通过该方法重构的步骤是 1 2 3 layer = Dense ( 32 ) config = layer . get_config () reconstructed_layer = Dense . from_config ( config ) 综上，我们可以从官方代码如何定义 Dense 层学习到我们自己应该如何定义类似的层。事实上，我们建议读者在自定义任何层之前，先选择一个与我们要自定义的层形式相似的层，阅览官方代码，了解定义一个这样的层大致的步骤，然后再开始实际行动。 自定义第一层 ¶ 接下来，我们考虑来自己构造一个层API。该层的表达式为： \\begin{align} \\mathbf{y} = \\eta ( \\mathbf{x} \\boldsymbol{\\omega}^T + \\mathbf{1} \\boldsymbol{\\varphi}^T ), \\end{align} 这里 \\mathbf{x} \\mathbf{x} 为输入的列向量（但是注意在代码中它是行向量）， \\mathbf{1} \\mathbf{1} 是一个与 \\mathbf{x} \\mathbf{x} 形状相同的，值全为1的向量； \\boldsymbol{\\omega},~\\boldsymbol{\\varphi} \\boldsymbol{\\omega},~\\boldsymbol{\\varphi} 为可训练的参数，而 \\eta(\\cdot) \\eta(\\cdot) 是一个应用在元素级的激活函数。我们考虑实现以下功能： 该层输入一个形状为 [ N , L ] 的向量组，输出一个形状为 [ N , L , M ] 的矩阵组，其中 M 是参数 \\boldsymbol{\\omega} \\boldsymbol{\\omega} 的长度。因此，该层不需要获取输入向量的形状，但需要定义输出矩阵的列数 M 。换言之，该层的作用是将向量升维成低秩矩阵； 参数 \\boldsymbol{\\omega},~\\boldsymbol{\\varphi} \\boldsymbol{\\omega},~\\boldsymbol{\\varphi} 都可以指定初始化器、正则化器和限制条件，就像 Dense 层一样； 可以选择是否使用 \\boldsymbol{\\varphi} \\boldsymbol{\\varphi} ，就像在 Dense 层我们可以选择是否使用 biase 一样； 激活函数 \\eta(\\cdot) \\eta(\\cdot) 可以是一个任意的激活函数，并且允许我们为它添加正则化器。 综上，我们定义该层为 class UpDimAffine ( Layer ): UpDimAffine.__init__ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def __init__ ( self , units , activation = None , use_bias = True , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ): if 'input_shape' not in kwargs and 'input_dim' in kwargs : kwargs [ 'input_shape' ] = ( kwargs . pop ( 'input_dim' ),) super ( UpDimAffine , self ) . __init__ ( activity_regularizer = regularizers . get ( activity_regularizer ), ** kwargs ) self . units = int ( units ) self . activation = activations . get ( activation ) self . use_bias = use_bias self . kernel_initializer = initializers . get ( kernel_initializer ) self . bias_initializer = initializers . get ( bias_initializer ) self . kernel_regularizer = regularizers . get ( kernel_regularizer ) self . bias_regularizer = regularizers . get ( bias_regularizer ) self . kernel_constraint = constraints . get ( kernel_constraint ) self . bias_constraint = constraints . get ( bias_constraint ) self . supports_masking = True self . input_spec = InputSpec ( min_ndim = 2 ) UpDimAffine.build 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def build ( self , input_shape ): input_shape = tf . TensorShape ( input_shape ) self . kernel = self . add_weight ( 'kernel' , shape = [ 1 , self . units ], initializer = self . kernel_initializer , regularizer = self . kernel_regularizer , constraint = self . kernel_constraint , dtype = self . dtype , trainable = True ) if self . use_bias : self . bias = self . add_weight ( 'bias' , shape = [ 1 , self . units ], initializer = self . bias_initializer , regularizer = self . bias_regularizer , constraint = self . bias_constraint , dtype = self . dtype , trainable = True ) else : self . bias = None super ( UpDimAffine , self ) . build ( input_shape ) UpDimAffine.call 1 2 3 4 5 6 7 8 9 10 def call ( self , inputs ): inputs = tf . expand_dims ( tf . convert_to_tensor ( inputs ), - 1 ) rank = inputs . get_shape () . ndims res = tf . tensordot ( inputs , self . kernel , [[ rank - 1 ], [ 0 ]]) if self . use_bias : varbias = tf . tensordot ( tf . ones_like ( inputs ), self . bias , [[ rank - 1 ], [ 0 ]]) res = tf . add ( res , varbias ) if self . activation is not None : res = self . activation ( res ) return res UpDimAffine.compute_output_shape 1 2 3 def compute_output_shape ( self , input_shape ): input_shape = tensor_shape . TensorShape ( input_shape ) return input_shape . concatenate ( self . units ) UpDimAffine.get_config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def get_config ( self ): config = { 'units' : self . units , 'activation' : activations . serialize ( self . activation ), 'use_bias' : self . use_bias , 'kernel_initializer' : initializers . serialize ( self . kernel_initializer ), 'bias_initializer' : initializers . serialize ( self . bias_initializer ), 'kernel_regularizer' : regularizers . serialize ( self . kernel_regularizer ), 'bias_regularizer' : regularizers . serialize ( self . bias_regularizer ), 'activity_regularizer' : regularizers . serialize ( self . activity_regularizer ), 'kernel_constraint' : constraints . serialize ( self . kernel_constraint ), 'bias_constraint' : constraints . serialize ( self . bias_constraint ) } base_config = super ( UpDimAffine , self ) . get_config () return dict ( list ( base_config . items ()) + list ( config . items ())) 这组定义完全启发自 Dense 的定义，因此改动其实不多，与 Dense 相比，主要的区别是 call 方法的实现。特别地，与 Dense 相同的是，我们使用 tf.tensordot 来指定对两个高维度的张量，取前一个张量的最后一维和第二个张量的第一维来进行矩阵乘法。 自定义第二层 ¶ 第二层的表达式为： \\begin{equation} \\begin{aligned} \\mathbf{y}_1 = \\mathrm{Re}\\{ \\mathrm{FFT}( \\mathbf{x} \\mathbf{a} ) \\}, \\\\ \\mathbf{y}_2 = \\mathrm{Im}\\{ \\mathrm{FFT}( \\mathbf{x} \\mathbf{a} ) \\}, \\end{aligned} \\end{equation} 这里 \\mathbf{x} \\mathbf{x} 为输入的矩阵， \\mathrm{FFT}(\\cdot) \\mathrm{FFT}(\\cdot) 是快速傅里叶变换。我们使用 \\mathbf{y}_1 \\mathbf{y}_1 和 \\mathbf{y}_2 \\mathbf{y}_2 来表示输出是具有两个通道的向量。我们考虑实现以下功能： 该层输入一个形状为 [ N , L , M ] 的向量组，输出一个形状为 [ N , l , 2 ] 的矩阵组，其中 L 在 l 为奇数时，取 (L+1)/2 ；在 L 为偶数时，取 L/2+1 ； 2 是两个通道，分别表示傅里叶变换的实部值和虚部值，同时，从输出可以看出傅里叶变换将使信号长度折半； 参数只有 \\mathbf{a} \\mathbf{a} ，它可以指定初始化器、正则化器和限制条件，就像 Dense 层一样； 我们不使用两输出的形式，而是使用单输出、两通道的形式来定义该层，是为了方便处理后续的步骤（计算损失函数）。 这里我们介绍一种新的定义层的方法，即“使用层来定义层”。该层定义为 class FFTAffine ( Layer ): FFTAffine.__init__ 1 2 3 4 5 6 7 8 9 10 11 def __init__ ( self , kernel_initializer = 'glorot_uniform' , kernel_regularizer = None , kernel_constraint = None , ** kwargs ): if 'input_shape' not in kwargs and 'input_dim' in kwargs : kwargs [ 'input_shape' ] = ( kwargs . pop ( 'input_dim' ),) super ( FFTAffine , self ) . __init__ ( ** kwargs ) self . kernel_initializer = initializers . get ( kernel_initializer ) self . kernel_regularizer = regularizers . get ( kernel_regularizer ) self . kernel_constraint = constraints . get ( kernel_constraint ) self . supports_masking = True self . input_spec = InputSpec ( min_ndim = 3 ) FFTAffine.build 1 2 3 4 5 6 7 8 def build ( self , input_shape ): input_shape = tf . TensorShape ( input_shape ) self . layer_Dense = tf . keras . layers . Dense ( 1 , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = self . kernel_regularizer , kernel_constraint = self . kernel_constraint ) self . layer_Dense . build ( input_shape ) super ( FFTAffine , self ) . build ( input_shape ) FFTAffine.call 1 2 3 4 5 6 7 def call ( self , inputs ): res = tf . squeeze ( self . layer_Dense ( inputs ), - 1 ) res = tf . signal . rfft ( res ) res_r = tf . expand_dims ( tf . real ( res ), - 1 ) res_i = tf . expand_dims ( tf . imag ( res ), - 1 ) res = tf . concat ([ res_r , res_i ], - 1 ) return res FFTAffine.compute_output_shape 1 2 3 def compute_output_shape ( self , input_shape ): input_shape = tensor_shape . TensorShape ( input_shape ) return input_shape [: - 1 ] . concatenate ( 2 ) FFTAffine.get_config 1 2 3 4 5 6 7 8 def get_config ( self ): config = { 'kernel_initializer' : initializers . serialize ( self . kernel_initializer ), 'kernel_regularizer' : regularizers . serialize ( self . kernel_regularizer ), 'kernel_constraint' : constraints . serialize ( self . kernel_constraint ) } base_config = super ( FFTAffine , self ) . get_config () return dict ( list ( base_config . items ()) + list ( config . items ())) 首先，我们需要观察的是 __init__ 和 get_config 两个方法。由于该层只有一个参数 \\mathbf{a} \\mathbf{a} ，我们因此只需要为它指定初始化器、正则化器和限制条件即可。这一步和之前处理第一层的情况相似。 接下来，观察 build ，与定义第一层情况不同的是，在这里我们不是使用基本API (例如 add_weights )，而是来自 tf.keras.layers 的层API， Dense 。我们将初始化时构建的三个实例馈入 Dense 的参数中。并且，在这一阶段，我们不调用 Dense 处理张量，而是直接获取 Dense 的实例。 提示 注意我们在这里手动调用了 self.layer_Dense.build(input_shape) 。 在一些行内的人眼里，这一步是不可或缺的，因为只有调用了 build 方法，我们定义的 Dense 类才会实例化其内的参数。但事实不完全如此，即使我们去掉这一行，即不调用 Dense 的 build 方法，效果也完全一样。这是因为 Dense 的父类 Layer （当然也是我们继承的父类）具有检查 self.built 是否为 True 的能力，并在调用某些方法的时候，如果发现 self.built 为 False ，则自动调用 build 。这属于 build 方法的隐式调用。 虽然如此，我们仍然提倡用户一定要手动定义 build 。其一是因为，这种显式的定义在逻辑上是通顺、符合人的直觉的；其二是因为，我们不能完全确保自定义层里的所有子层的 build 方法一定会在任何情况下都能隐式触发。况且，这种做法是完全可行的，活用 compute_output_shape 或 tf.shape 等方法，我们可以做到手动触发一个有多个子层的自定义层中的所有子层的 build 方法。 提示 有些行内的人指出，在使用 build 方法时，应当显式地将子层的可训练、不可训练参数都反馈给自定义层的参数表（参见 StackOverflow的讨论串 ），具体的操作如下： self . _trainable_weights = self . layer_Dense . trainable_weights self . _non_trainable_variables = self . layer_Dense . non_trainable_variables 然而必须指出的是，这种做法是不正确的。因为观察 源代码 可以发现， trainable_weights 和 non_trainable_variables 都是封装好的属性方法。私有变量 _trainable_weights 和 _non_trainable_variables 与前者不同的是，这两个私有变量包含的是 直属于本层的可训练、不可训练变量 ；但前者的实现分别是 本层和本层的所有子层的所有可训练、不可训练变量 。因此，将子层的所有变量加诸自定义层的直属变量里，是多余、且容易造成误解的做法。 最后，观察 call 方法，在该方法里，我们首先将维度为 [ N , L , M ] 的参数通过无bias的全连接层映射到 [ N , L , 1 ] ，再压缩最后一维度，得到维度为 [ N , L ] 的向量，通过Tensorflow自带的实值FFT变换函数 tf.signal.rfft ，得到复数域的输出 [ N , l ] ，对该输出分别取实部和虚部，再将两实值化的结果以通道的形式并在一起，最终我们就得到两通道的输出 [ N , l , 2 ] 。其中，第一个通道是傅里叶变换的实部，第二个通道是傅里叶变换的虚部。 信息 实际上，Tensorflow的官方教程给出了一种自定义层的范例，参看 Custom layers 。在这一范例中，使用 tf.keras.Model 定义一个有多个子层的模型，且该模型的使用方法和 Layer 一样。从某种程度上，这种方法比我们使用的方法更简洁。然而，需要指出的是， Model类继承 - Keras中文文档 也提到了这种做法，但使用 Model 类继承会导致网络具体实现的细节变得不可追索，具体而言就是形如 Model.to_json 、 Model.to_yaml 、 Model.get_config 和 Model.save 等方法变得不可用。 我们的这种做法则不存在这个问题，因为我们在每个自定义层里都定义了 get_config 方法，从而使得我们可以像使用内置的层API一样来使用它们。 检测效果 ¶ 注意，在两个自定义层都定义好后，需要在两层定义的最后，加上如下代码： customObjects = { 'FFTAffine' : FFTAffine , 'UpDimAffine' : UpDimAffine } 该字典提供了一个索引表，将字符串形式的层名称映射到具体实现的类Object上。我们在任何涉及读取层的设置，例如 from_json 、 load_model 等方法中，都需要传入该索引表，确保Keras知道如何从配置文件里恢复出我们自定义的层。 如下代码提供了一个简单的两个自定义层叠加在一起的测试 test_layers 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np def test_layers (): # Set model and see the summary of the model model = tf . keras . models . Sequential ([ UpDimAffine ( 10 , use_bias = True , activation = tf . math . cos , input_shape = ( 5 ,)), FFTAffine ( trainable = False ) ]) model . compile ( optimizer = optimizers . Adam ( 0.01 ), loss = tf . keras . losses . mean_squared_error , metrics = [ tf . keras . metrics . mean_squared_error ] ) model . summary () model . save ( 'my_model.h5' ) # perform the test var_input = np . ones ([ 2 , 5 ]) var_output = model . predict ( var_input ) print ( var_input . shape , var_output . shape ) print ( var_output ) test_layers () Output _________________________________________________________________ Layer ( type ) Output Shape Param # ================================================================= up_dim_affine ( UpDimAffine ) ( None, 5 , 10 ) 20 _________________________________________________________________ fft_affine ( FFTAffine ) ( None, 3 , 2 ) 10 ================================================================= Total params: 30 Trainable params: 20 Non-trainable params: 10 _________________________________________________________________ ( 2 , 5 ) ( 2 , 3 , 2 ) [[[ -3.25903225e+00 5 .96046448e-08 ] [ 1 .05201025e-07 -5.80141695e-08 ] [ 3 .23036957e-08 -3.17865378e-08 ]] [[ -3.25903225e+00 5 .96046448e-08 ] [ 1 .05201025e-07 -5.80141695e-08 ] [ 3 .23036957e-08 -3.17865378e-08 ]]] Bug 注意，Tensorflow目前的版本(r1.13)仍然有不完善之处。在上述测试中，如果我们把 tf.keras.losses.mean_squared_error 替换成 tf.keras.MeanSquaredError ，虽然该测试能正常跑通，但接下来读取已保存的网络时则会报错。这是由于目前版本的Tensorflow使用了部分废旧的API来定义读取配置的函数，在Github上的某个 讨论串 ，有人已经给出了解决方案，但仍然需要等候被新版Tensorflow采纳才能生效。 该测试首先通过顺序模型，引入了我们自定义的两个层，然后通过 summary 显示模型的详细结构，并通过 save 保存整个网络的模型配置以及具体的参数值。接下来使用一个值全为1的，形状为 [ 2 , 5 ] 的输入来测试该模型，并记录测试结果，与我们的预期完全相符。 同时，我们在设置两层的时候，刻意地令第二层的参数不可训练，实际显示的结果表明，该设置是成功的。第二个函数的10个参量确实在模型的记录里显示为不可训练的。 信息 关于如何保存网络，我们会在下一章详细展开。 接下来，为了证明我们的自定义层能完全正常地工作，我们进行读取测试， test_read 1 2 3 4 5 6 7 8 9 10 11 import numpy as np def test_read (): customObjects [ 'cos' ] = tf . math . cos new_model = tf . keras . models . load_model ( 'my_model.h5' , custom_objects = customObjects ) new_model . summary () var_input = np . ones ([ 2 , 5 ]) var_output = new_model . predict ( var_input ) print ( var_input . shape , var_output . shape ) print ( var_output ) test_read () Output _________________________________________________________________ Layer ( type ) Output Shape Param # ================================================================= up_dim_affine ( UpDimAffine ) ( None, 5 , 10 ) 20 _________________________________________________________________ fft_affine ( FFTAffine ) ( None, 3 , 2 ) 10 ================================================================= Total params: 30 Trainable params: 20 Non-trainable params: 10 _________________________________________________________________ ( 2 , 5 ) ( 2 , 3 , 2 ) [[[ -3.25903225e+00 5 .96046448e-08 ] [ 1 .05201025e-07 -5.80141695e-08 ] [ 3 .23036957e-08 -3.17865378e-08 ]] [[ -3.25903225e+00 5 .96046448e-08 ] [ 1 .05201025e-07 -5.80141695e-08 ] [ 3 .23036957e-08 -3.17865378e-08 ]]] 在该测试里，我们的模型从配置到参数，都完完全全是从文件 my_model.h5 中读取的。注意我们馈入 customObjects 给 load_model ，使Keras能发现我们自己定义的层。同时， customObjects 还需要添加 tf.math.cos 函数，这是因为该激活函数同样不在Keras内置的几种基本的激活函数之列。 我们用完全相同的输入来测试模型的输出，得到的结果和我们上一个测试完全一致，说明对该模型（包括我们自定义的两层）的保存是成功的。 观察两个测试的输出值，我们会发现，对三维的输出，在确定后两维下标 a, b 的情况下 [:, a , b ] 的输出都是一样的。这是因为，第一维反映的是向量组中不同向量的测试结果，而我们馈入模型的向量组是两个值均为1的长度为5的向量。由于这两个向量完全相同，其对应的输出也完全相同。 数据生成 ¶ 我们仍然使用自动生成的数据。我们重新继承了自 第一节：线性分类 里定义的数据集生成类，新定义的数据集生成器 class TestDataFFTSet ( TestDataSet ): dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class TestDataFFTSet ( TestDataSet ): ''' A generator of the data set for testing the non-linear regression model. y = cos(x w^T + 1 p^T) a ''' def __init__ ( self , scale_x , len_x , omega , phi , a ): ''' Initialize the data generator. scale_x: the scale of input vector. len_x: the length of input vector. omega (w) [1 x N]: the inner linear transformation. phi (p) [1 x N]: the inner bias. a [N x 1]: the outer linear transormation. ''' self . s_x = 2 * scale_x self . omega = omega self . phi = phi self . a = a self . len_x = len_x self . config ( train = True , batch = 100 , noise = 0.0 ) def mapfunc ( self , x ): xu = np . expand_dims ( x , - 1 ) y1 = np . tensordot ( xu , self . omega , ( 2 , 0 )) y1 = np . cos ( y1 + np . tensordot ( np . ones_like ( xu ), self . phi , ( 2 , 0 ))) y2 = np . squeeze ( np . tensordot ( y1 , self . a , ( 2 , 0 )), axis =- 1 ) y2 = np . fft . rfft ( y2 ) y_r = np . expand_dims ( np . real ( y2 ), - 1 ) y_i = np . expand_dims ( np . imag ( y2 ), - 1 ) y = np . concatenate ([ y_r , y_i ], axis =- 1 ) return y def next_train ( self ): ''' Get the next train batch: (x, y) ''' x = self . next_test () y = self . mapfunc ( x ) return x , y 我们新定义的这个数据生成器，与以往的一个不同在于，其定义了 mapfunc 方法；而产生训练数据的原理，是用 mapfunc 将产生的测试数据（只有输入 \\mathbf{x} \\mathbf{x} ）映射到输出 \\mathbf{Y} \\mathbf{Y} 。这个数据集可以通过迭代不断产生随机数据，也可以通过 mapfunc 来将任意给定的向量 \\mathbf{x} \\mathbf{x} 转换成 (6) (6) 定义的频域输出 \\hat{\\mathbf{Y}} \\hat{\\mathbf{Y}} 。 特别地，这里计算矩阵的时候，不使用 np.matmul 而是 np.tensordot ，和我们为第一层定义的时候使用 tf.tensordot 的原因相同。该函数支持对两个高维度的张量，取其中的两个维度分别计算矩阵乘法。 接下来测试数据集的输出效果 dparser.py 1 2 3 4 5 6 7 8 9 10 11 def test_dataset (): omega = 3 * np . random . random ([ 1 , 12 ]) phi = 2 * np . random . random ([ 1 , 12 ]) a = np . random . normal ( 0 , 1 , [ 12 , 1 ]) dataSet = TestDataFFTSet ( 1 , 10 , omega , phi , a ) dIter = iter ( dataSet ) for i in range ( 10 ): x , y = next ( dIter ) print ( y . shape , np . abs ( y [ 0 , ... , 0 ] + 1j * y [ 0 , ... , 1 ])) test_dataset () Output ( 100 , 6 , 2 ) [ 14 .29735734 9 .75982541 4 .73928941 6 .83158726 5 .16604991 0 .66256222 ] ( 100 , 6 , 2 ) [ 9 .32749042 5 .68157606 4 .86524786 11 .17296633 6 .37133611 7 .68757874 ] ( 100 , 6 , 2 ) [ 3 .97163879 9 .35646167 1 .43682653 4 .49838507 7 .39721013 4 .34096144 ] ( 100 , 6 , 2 ) [ 6 .56389599 5 .59993345 8 .19466732 0 .72835593 5 .01080391 9 .0552016 ] ( 100 , 6 , 2 ) [ 9 .71065508 3 .08608948 8 .72857359 9 .47081321 4 .87269945 7 .02108589 ] ( 100 , 6 , 2 ) [ 6 .05645098 6 .05961698 2 .98397442 8 .83888829 2 .91282992 5 .07843238 ] ( 100 , 6 , 2 ) [ 5 .11452286 1 .34310476 4 .15953687 3 .43588933 1 .7484992 0 .21387424 ] ( 100 , 6 , 2 ) [ 0 .63167972 8 .34622626 6 .21582338 5 .01146157 1 .50978382 1 .18373357 ] ( 100 , 6 , 2 ) [ 5 .88872479 6 .18109798 6 .97300166 4 .48064652 8 .13842369 6 .01989667 ] ( 100 , 6 , 2 ) [ 1 .28678976 3 .08831315 5 .3226707 0 .86784854 7 .83722167 0 .98692777 ] 我们产生的数据长度为10，参数的长度为12，我们在测试代码中，显示每次生成batch中，第一个样本的频谱强度。测试结果显示，频谱强度分布较为合理，且FFT后的数据长度为6=10/2+1，符合我们的预期。 定义类模型 ¶ 与 上一节 相似，我们在本节使用的仍然是回归模型，因此，在主程序部分的代码改动不大。我们定义新的类 class NonLinRegHandle ( ext . AdvNetworkBase ): ，其中核心部分（构造方法）的代码如下： class NonLinRegHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def construct ( self ): ''' Construct a linear model and set the optimizer as Adam ''' # Construction input = tf . keras . Input ( shape = ( INPUT_SHAPE ,), dtype = tf . float32 ) upAff = ext . UpDimAffine ( PARAMS_SHAPE , use_bias = True , kernel_initializer = tf . keras . initializers . RandomUniform ( minval = 0.0 , maxval = 3.0 ), bias_initializer = tf . keras . initializers . RandomUniform ( minval = 0.0 , maxval = 2.0 ), kernel_constraint = tf . keras . constraints . NonNeg (), bias_constraint = tf . keras . constraints . NonNeg (), activation = tf . math . cos , name = 'up_dim_affine' )( input ) dnAff = ext . FFTAffine ( name = 'fft_affine' )( upAff ) self . model = tf . keras . Model ( inputs = input , outputs = dnAff ) # Set optimizer self . model . compile ( optimizer = self . optimizer ( self . optimizerName , self . lr ), loss = tf . keras . losses . mean_squared_error , metrics = [ self . relation ] ) self . model . summary () 在这个模型中，除了输入层以外，其余的两层都分别是我们自定义的层。第一个层内有参数 \\boldsymbol{\\omega},~\\boldsymbol{\\phi} \\boldsymbol{\\omega},~\\boldsymbol{\\phi} ，我们对这两个参数均加上了必须为正数的严格限制条件，同时对 \\boldsymbol{\\omega} \\boldsymbol{\\omega} ，我们使用均匀分布 U(0.0,~3.0) U(0.0,~3.0) 对其初始化，对 \\boldsymbol{\\phi} \\boldsymbol{\\phi} ，我们使用均匀分布 U(0.0,~2.0) U(0.0,~2.0) 对其初始化。在该层的最后，使用 \\cos(\\cdot) \\cos(\\cdot) 函数作为激活函数。 第二层内有参数 \\mathbf{a} \\mathbf{a} ，我们直接使用默认的初始化器来对其初始化。 实际测试的过程中，我们发现上一节定义的 相关系数 仍有缺陷。具体体现在，当两个被对比的向量中任何一个向量的某一维度的样本分布在方差为0时，分母 \\sigma_1^{(i)} \\sigma_2^{(i)} = 0 \\sigma_1^{(i)} \\sigma_2^{(i)} = 0 （其中 i i 表示向量的某一维度），从而导致该系数无法计算出结果。故而，我们考虑对其修正，在计算各维度相关系数的平均值时，排除掉那些无法计算相关系数的维度，改进后的代码如下： class NonLinRegHandle 1 2 3 4 5 6 7 8 9 10 @staticmethod def relation ( y_true , y_pred ): m_y_true = tf . keras . backend . mean ( y_true , axis = 0 ) m_y_pred = tf . keras . backend . mean ( y_pred , axis = 0 ) s_y_true = tf . keras . backend . sqrt ( tf . keras . backend . mean ( tf . keras . backend . square ( y_true ), axis = 0 ) - tf . keras . backend . square ( m_y_true )) s_y_pred = tf . keras . backend . sqrt ( tf . keras . backend . mean ( tf . keras . backend . square ( y_pred ), axis = 0 ) - tf . keras . backend . square ( m_y_pred )) s_denom = s_y_true * s_y_pred s_numer = tf . keras . backend . mean ( y_true * y_pred , axis = 0 ) - m_y_true * m_y_pred s_index = tf . keras . backend . greater ( s_denom , 0 ) return tf . keras . backend . mean ( tf . boolean_mask ( s_numer , s_index ) / tf . boolean_mask ( s_denom , s_index )) 调试 ¶ 在调试阶段，我们采用随机生成的参数作为真值。其中， \\boldsymbol{\\omega} \\in U(0.0,~3.0) \\boldsymbol{\\omega} \\in U(0.0,~3.0) , \\boldsymbol{\\phi} \\in U(0.0,~2.0) \\boldsymbol{\\phi} \\in U(0.0,~2.0) ， \\mathbf{a} \\in N(0.0,~1.0) \\mathbf{a} \\in N(0.0,~1.0) 。然后，我们生成大量的 (\\mathbf{x},~\\mathbf{Y}) (\\mathbf{x},~\\mathbf{Y}) ，其中 \\mathbf{x} \\in U(-3.0,~3.0) \\mathbf{x} \\in U(-3.0,~3.0) 。注意在这个问题里，模型的输入输出向量是等长的，参数的长度不影响输出向量的长度。我们将参数的长度固定为10个元素，并定义如下函数 class NonLinRegHandle 1 2 3 4 5 6 7 8 9 10 def groupSort ( * params ): sortind = np . argsort ( params [ 0 ]) . flatten () res = [] for p in params : if p . shape [ 0 ] > p . shape [ 1 ]: p = p [ sortind , :] else : p = p [:, sortind ] res . append ( p ) return res 该函数用于对一组相同长度的向量进行排序，这些向量不拘于行向量或列向量。排序的标准是第一个参数向量从小到大的顺序。定义该函数是为了修整我们的输出结果。在上文理论部分，我们已经说明，对于一组解，交换任意两个维度的值，不影响模型的效果。因此我们通过对预测值和真值分别进行排序，来评估两组解之间的差异程度。 class NonLinRegHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # Initialization omega = 3 * np . random . random ([ 1 , PARAMS_SHAPE ]) phi = 2 * np . random . random ([ 1 , PARAMS_SHAPE ]) a = np . random . normal ( 0 , 1 , [ PARAMS_SHAPE , 1 ]) dataSet = dp . TestDataFFTSet ( 3 , args . xLength , omega , phi , a ) # Generate a group of testing samples. if args . seed is not None : setSeed ( args . seed + 1000 ) dataSet . config ( batch = args . testBatchNum ) x , y = next ( dataSet ) # Set the data set for training. dataSet . config ( batch = args . trainBatchNum ) # Construct the model and train it. h = NonLinRegHandle ( xLength = args . xLength , learningRate = args . learningRate , epoch = args . epoch , steppe = args . steppe , optimizerName = args . optimizer ) h . construct () print ( 'Begin to train:' ) print ( '---------------' ) record = h . train ( iter ( dataSet )) # Generate a group of testing samples: dataSet . config ( batch = args . testBatchNum ) x2 = np . reshape ( np . linspace ( - 3 , 3 , args . xLength ), [ 1 , args . xLength ]) y2 = dataSet . mapfunc ( x2 ) x = np . concatenate ([ x , x2 ], axis = 0 ) y = np . concatenate ([ y , y2 ], axis = 0 ) # Check the testing results print ( 'Begin to test:' ) print ( '---------------' ) yp , loss_p , corr_p = h . test ( x , y ) # Check the regressed values w , p = h . model . get_layer ( name = 'up_dim_affine' ) . get_weights () b = h . model . get_layer ( name = 'fft_affine' ) . get_weights ()[ 0 ] # Resort data w , b , p = groupSort ( w , b , p ) # The solution omega , phi , a = groupSort ( omega , phi , a ) # The ground truth # Save if args . outputData is not None : np . savez_compressed ( args . outputData , epoch = record . epoch , loss = record . history [ 'loss' ], corr = record . history [ 'relation' ], test_x = x , test_y = y , pred_y = yp , pred_loss = loss_p , pred_corr = corr_p , W = w , p = p , b = b , omega = omega , phi = phi , a = a ) Output _________________________________________________________________ Layer ( type ) Output Shape Param # ================================================================= input_1 ( InputLayer ) ( None, 100 ) 0 _________________________________________________________________ up_dim_affine ( UpDimAffine ) ( None, 100 , 10 ) 20 _________________________________________________________________ fft_affine ( FFTAffine ) ( None, 51 , 2 ) 10 ================================================================= Total params: 30 Trainable params: 30 Non-trainable params: 0 _________________________________________________________________ Begin to train: --------------- Epoch 1 /20 500 /500 [==============================] - 3s 5ms/step - loss: 6 .8965 - relation: 0 .9842 Epoch 2 /20 500 /500 [==============================] - 2s 4ms/step - loss: 0 .0027 - relation: 1 .0000 Epoch 3 /20 500 /500 [==============================] - 2s 4ms/step - loss: 0 .0018 - relation: 1 .0000 Epoch 4 /20 500 /500 [==============================] - 2s 4ms/step - loss: 0 .0016 - relation: 1 .0000 Epoch 5 /20 500 /500 [==============================] - 2s 4ms/step - loss: 0 .0014 - relation: 1 .0000 Epoch 6 /20 500 /500 [==============================] - 2s 4ms/step - loss: 0 .0012 - relation: 1 .0000 Epoch 7 /20 500 /500 [==============================] - 2s 4ms/step - loss: 0 .0010 - relation: 1 .0000 Epoch 8 /20 500 /500 [==============================] - 2s 4ms/step - loss: 9 .0556e-04 - relation: 1 .0000 Epoch 9 /20 500 /500 [==============================] - 2s 4ms/step - loss: 7 .9935e-04 - relation: 1 .0000 Epoch 10 /20 500 /500 [==============================] - 2s 4ms/step - loss: 7 .0898e-04 - relation: 1 .0000 Epoch 11 /20 500 /500 [==============================] - 2s 4ms/step - loss: 6 .3418e-04 - relation: 1 .0000 Epoch 12 /20 500 /500 [==============================] - 2s 4ms/step - loss: 5 .6936e-04 - relation: 1 .0000 Epoch 13 /20 500 /500 [==============================] - 2s 4ms/step - loss: 5 .1473e-04 - relation: 1 .0000 Epoch 14 /20 500 /500 [==============================] - 2s 4ms/step - loss: 4 .6677e-04 - relation: 1 .0000 Epoch 15 /20 500 /500 [==============================] - 2s 4ms/step - loss: 4 .2542e-04 - relation: 1 .0000 Epoch 16 /20 500 /500 [==============================] - 2s 4ms/step - loss: 3 .8777e-04 - relation: 1 .0000 Epoch 17 /20 500 /500 [==============================] - 2s 4ms/step - loss: 3 .5603e-04 - relation: 1 .0000 Epoch 18 /20 500 /500 [==============================] - 2s 4ms/step - loss: 3 .2665e-04 - relation: 1 .0000 Epoch 19 /20 500 /500 [==============================] - 2s 4ms/step - loss: 3 .0134e-04 - relation: 1 .0000 Epoch 20 /20 500 /500 [==============================] - 2s 4ms/step - loss: 2 .7940e-04 - relation: 1 .0000 Begin to test: --------------- 11 /11 [==============================] - 0s 6ms/sample - loss: 2 .4728e-04 - relation: 1 .0000 Evaluated loss ( losses.MeanSquaredError ) = 0 .0002472764754202217 Evaluated metric ( Pearson ' s correlation ) = 0 .9999991 在测试阶段，我们除了生成10组随机数据以外，还生成了一组从 [-3, 3] [-3, 3] 之间均匀增长的数据。这组数据与我们之前使用的随机数据分布不同，通过检测该数据的输出结果，我们可以验证我们拟合的这个参数模型是否具有一定的鲁棒性。 输入向量 \\mathbf{x} \\mathbf{x} 的长度不但影响输出 \\mathbf{Y} \\mathbf{Y} 的长度，也影响FFT的精度。因此，我们通过使用不同的向量长度分别进行测试，并对测试结果进行评估。调用测试的代码如下： python lin-reg.py -sd 1 -do test/ { length } -xl { length } 通过指派不同的向量长度 {length} ，将输出保存到不同的文件里，以绘制它们的对比效果图。首先，我们观察训练过程的记录情况 损失函数 (MSE) 测度函数 (相关系数) 可以看见，收敛的速度非常快。并且随着数据向量长度的增加，损失函数收敛到的值也增加。这是由于傅里叶变换的影响。我们使用的傅里叶变换是还没有标准化后的数据，因此，随着输入向量的增长，傅里叶变换的精度也提高，导致低频部分的数值明显变大，从而导致损失函数的收敛值增加。而相关系数显示，预测输出和真实值之间的线性相关性迅速趋近于1，印证该训练过程非常快。 注意 需要重申的是，我们计算相关系数是基于不同样本的统计情况来确定的。因此为了估计出准确的的相关系数，我们的batch需要有足够多的样本。显然，1个样本的batch是无法用来计算相关系数的。这里我们的batch含有32个样本。 接下来运行测试集检查结果。我们对预测的频谱和真实频谱之间求均方根误差(RMSE)，结果如下： 均方根误差 (RMSE) 由 (4) (4) 知，我们的模型本质上其实是一个对向量个元素独立运算的函数，亦即元素级的函数。因此， (4) (4) 可以被改写成 \\begin{align} y(x) = \\sum_{i=1}^N a_i \\cos ( \\omega_i x + \\varphi_i ). \\end{align} 如果我们输入一组向量，值在 [-3, 3] [-3, 3] 之内从小到大均匀增长，那么对应的取傅里叶变换的输出向量，可以看成是响应 x \\in (-3,~3) x \\in (-3,~3) 的频谱。我们在上述测试过程中，虽然使用了不同的向量长度 L L ，但生成的最后一个测试向量 \\mathbf{x} \\mathbf{x} 均是在 x \\in (-3,~3) x \\in (-3,~3) 均匀采样得到的。因此，不同的向量长度的测试结果，对应的是同以频谱不同精度下的计算结果。我们将训练好的模型输出的频谱和真实数据生成的频谱对比，得到以下结果： 频谱响应 幅值 相位 L=10 L=100 L=500 L=1000 可见，我们的回归到的模型输出的频谱和真值的模型完全一致。 问题 如果一个模型在频域上对一个信号的回归是精确的，是否在原域上（即时域）的回归也是精确的？ 正是如此。因为，考虑FFT的逆变换iFFT，作为一个线性变换，iFFT毫无疑问满足Lipschitz连续条件。这意味着，如果一个信号的在频域上的回归结果满足 \\lVert \\hat{\\mathbf{Y}} - \\mathbf{Y} \\rVert < \\varepsilon \\lVert \\hat{\\mathbf{Y}} - \\mathbf{Y} \\rVert < \\varepsilon ，则必有在时域上满足 \\lVert \\hat{\\mathbf{y}} - \\mathbf{y} \\rVert < C\\varepsilon \\lVert \\hat{\\mathbf{y}} - \\mathbf{y} \\rVert < C\\varepsilon ，其中 C C 是一个有限的常量。 另一种思考方法是，iFFT和FFT互为逆变换，这说明这两者之间构成一一映射。如果两组信号的FFT相同，那么其对应的一一映射，iFFT，又或者说是原信号，是势必相同的。这意味着，如果一个模型能够在时域上回归到某组数据，那么其频域上也必然能回归到相同数据的频域表达，反之亦然。 最后，我们来观察三个参数向量的回归情况，比对不同测试回归到的参数向量和真值之间的差别，结果如下 \\boldsymbol{\\omega} \\boldsymbol{\\omega} \\boldsymbol{\\phi} \\boldsymbol{\\phi} \\mathbf{a} \\mathbf{a} 鉴于模型的解具有高度的不确定性，我们发现我们回归到的结果受到初始化值的影响非常严重。尽管我们的回归模型确实拟合出了原函数的特性，但回归到的参数却和真值有明显的区别。 本节虽然使用了一个高度不确定的、却又简单的非线性函数作为例子，但我们所希望传达的，主要有以下两个要点： 一个可以写成解析式的线性或非线性函数，可以轻易地被实现成Tensorflow-Keras模式下的可微模型。这种函数包括但不限于普通的 数学函数 （例如指数函数、三角函数、贝塞尔函数等）， 快速傅里叶变换 ， 离散余弦变换 ， 常规的线性代数操作 （例如行列式、特征值）， SVD分解 ，等等。这些函数全部都已经被Tensorflow实现出来，可以通过内置API任意组合。更重要的是，在本节中，我们没有定义任何求取导数、梯度的方法，因为上述的每一个Tensorflow内置API，都已经内置了解析级别的梯度的计算方法。因此，对于一些简单的非线性模型，用户可以完全不用关心反向传播的过程，而是合心定意在编写正向传播上。从某种程度上，这大大降低了求解非线性问题的难度。 本节重点揭示的，是如何优雅地完成一个自定义层。截至笔者写到目前为止(03/17/2019)，未见网络上有登载类似的、规范的教程。如果用户能习惯按照本节的方式，扩展Tensorflow-Keras API，会带来两大好处： 一些复杂的模块，例如Residual block，Inception block等，可以以封装好的形式利用起来，使得主程序的代码简洁干净； 使用和Keras源码一致的语言风格，确保我们编写的所有自定义API，都可以被Keras原生的存取工具（包括 to_json , save 等）正确地保存下来。 在后续的内容里，我们还会涉及自定义网络层的情况，但是我们就不会特别说明完整的定义流程。在本教程推进的过程中，我们会不断定义各种需要用到的网络层，从而不断丰富扩展模块 extension.py 的内容。到本教程结束的时候，我们期望能够建立一个对用户友好的、功能完善而又与Tensorflow-Keras源代码风格一致的扩展模块出来。这一模块将可以用来构建任何形式的Tensorflow工程。","text_tokens":["列","傅里叶","as","用户","虽然","5603e","是因为","*","期望","0010","py","最","参量","而是","numpy","意在","05961698","统计","反之亦然","方案","phi","instance","实值","非常","48","误差","接口","sqrt","where","(","奇数","调试","父类","sortind","svd","%","不拘","参数表","from","布尔","=","直属","自行","&","convert","方式","lr","交换","从小到大","xu","两码","positive","模型","应用","0014","activity","32749042","参与","34622626","equation","故而","customobjects","掉","raise","优雅","lambda","流程","两个","本例","转换成","32","argsort","过","强制","ie","地被","initializers","带来","相同","来","前提","传播","left","有关","26","逻辑","本分","分布","根据","十分","范性","模拟","必选","时则","组合","利用","be","去掉","和","丰富","get","同一个","随机","类型","在上文","diter","首先","custom","end","写","又","或列","刻意","56389599","batch","预测值","masking","周期性","没有","[","变换","接下","88872479","尽管","秩","100","某组","cos","变","regression","普通","便于","内置","生成器","but","第二","43682653","xlength","先","索引","98692777","发现","快速","商","t","objects","规范","要点","7","output","8","还","float32","do","允许","model","重构","inputspec","精度","standard","跟","层内","初始","差异","由类","关于","建立","要求","处","毫无","generic","两者之间","product","代码","降低","在","传入","成","令","then","66256222","`","相似","就是","折半","参数均","serialize","24","disable","分母","揭示","正常","新版","余弦","perform","自己","exp","样本分布","检测","不但","方便","l","废旧","取","级别","right","]","vector","dict","\"","追索","compile","独立","决定","之前","积","解","无疑","relation","pop","yp","reg","取消","07","确保","设定","元素","显式","to","详细","pred","none","且","完全","im","ifft","达到","matmul","next","各层","predict","testbatchnum","但","backend","86524786","utils","0018","固定","两层","显示","18373357","结果显示","译成","第","给定","call","仿射","集","14","前面","def","__","linspace","看作","测试代码","却","concatenate","应该","那","mathbb","length","为","限定","信息","等候","'","need","1","like","足够","ext","眼里","outputs","阅读","保存","限制","04","transformation","出","该","only","initial","3","都","第二层","其内","71065508","组","将","72835593","本身","看成","¶","fft","使用","不能","42","dnaff","space","意味","self","另","器来","检查和","don","能"," ","3.0","tf","重","should","子层","偶数","无法","第一","看见","改进","特性","tensorflow","例如","有人","73928941","h","对应","知道","~","10","params","联合","之间","一章","加上","技术","里","would","reconstructed","功能完善","3.17865378","8777e","31","83888829","expand","dim","造成","可见","its","required","spec","级","see","明显","theta","大大","修正","是","各","调用","设若","支持","x","中文","标准化","可分","为重","压缩","5ms","已知","int","层","common","43","测试","import","转换","运行","连接","相关性","下标","以下","给","after","希望","inputs","于","加入","epoch","args","一种",".","报错","k","with","知","nd","均","0012","著名","或","load","缺陷","排除","三个","称为","第一层","与","其中","corr","串","initialize","应当","iter","目前","摘要","regularizer","constraints","callable","假设","其余","glorot","broadcasting","从而","响应","created","lvert","严格","参见","正是如此","逆变","表达","的","}","name","6ms","randomuniform","提取","08","wise","已","截至","重新","compressed","matrix","abs","加在","况且","精密","关心","pearson","实际","saw","不断","关系","具有","learningrate","generate","方法","biase","41","and","分类","可分解","某些","37","偏向","真值","后续","编译","variable","02108589","排序","会","extension","note","whether","boldsymbol","方根","一组","updimaffine","training",".__","版本","构成","输入输出","87269945","01080391","context","受","mul","本质","block","自定","27","向量","36","标准","迅速","表述","，","cdot","成功","步骤","uses","二层","原域","源代码","check","_________________________________________________________________","求解","选取","单","0.0","拟合","印证","构造方法","简单","换成","97163879","默认","它们","这组","直接","h5","testdatafftset","对比","训练","+","11","采样","util","常规","绘制","各种","某种","21","熟悉","复杂","采纳","名称","construction","正式","起来","初始化","参数值","多余","u","成器","dp","noise","原因","05645098","2542e","一个","0898e","那些","nonneg","达式","并且","个","83722167","限于","construct","3418e","鉴于","“","29","arg","residual","element","rank","units","主程序","实值化","losses","advnetworkbase","outer","transormation","index","_","离散","继承","两组","模式","两大","用来","效果图","除了","r","复数","以及","多个","从小","step","ndim","跑通","当","化","唯一","i","97300166","每次","底层","某","dataset","容易","特征值","实部","指数函数","而且","行向量","层能","基于","p","eager","8965","友好","提示","采用","connected","所","广泛","append","均匀","一系列","则","new","lipschitz","mat","fftaffine","修整","计算方法","outputdata","全部","68757874","地","隐式","并未","某个","封装","重申","leftrightarrow","其","align","情况","讨论","其二","互为","可以","重要","使得","0134e","收敛","范例","器","指派","：","data","30","疑问","prior","aligned","亲切","eagerly","行列式","自由","语言","0000","up","一","upaff","正是","tensor","else","13","任意","keras","executing","nn","；","03","正数","一点","实际上","完整","问题","加诸","构建","现成","操作","export","时","计算结果","数表","d","c","好处","如何","其实","squeeze","15953687","for","super","inception","回忆","中均","默认值","imag","层中","再压缩","min","参数设置","39721013","flatten","提及","值均","以往","不断丰富","17296633","编写","并","或者","参照","其一","1473e","什么","不会","/","做到","数学","sequential","全","作","数据","models","体现","”","原生","这个","预期","虚部","已经","uniform","作用","反馈","互交","馈入","2.0","梯度","合心定","）","恢复","43588933","频域","合理","results","基本","no","列数","non","原则上","6677e","建议","arrays","pylint","最后","指数","…","e","同样","或是","500","确定性","学习","也","valueerror","线性","设该","可调","framework","贝塞尔","维来","因为","激活","r1","7484992","在于","false","f","01989667","迭代","29735734","每","np","github","吗","x2","写作","高","直","20","后","如下","circ","完成","长","size","passed","主要","线性组合","可微","非常低","属于","s","savez","相符","（","add","正确","record","干净","而言","9935e","时序","math","metrics","大致","差别","结构","想","功底","25","2d","9","设置","包括","私有","提供","gen","来自","分成","文件","y1","反向","测试数据","2665e","登载","shapes","手","需要","统一","这","sample","38","kernel","？","yaml","mask","为止","5","the","solution","选","^","shape","concat","98397442","这种","分别","1000","35646167","哪些","operation","是从","既然","2","tuple","0s","或者说","by","编译成","换言之","含有","颇为重要","导数","亦","确定","误解","argument","0016","可能","first","难度","实在","开始","my","除非","直觉","string","功能","具体操作","下","、","即","才","时域","regularizers","doc","4","介绍","squared","两码事","提倡","..","各自","张量","kwargs","found","dimensionality","同时","即使","n","表达式","预测","一些","通道","2s","json","输出","mean","工程","23036957e","有","替换","以","6","sum","a","weight","规范性","generator","mse","第二个","习惯","w",")","参看","回归","loss","加在一起","01146157","it","低","即时","param","initialization","0552016","一节","regular","j","对","涉及","了解","测度","样本","19466732","具体","dims","化器","评估","构造","把","bug","毫无疑问","不是","配置文件","这里","过该","手动","检查","时间","回调","两","ground","增加","lin","综上","will","91282992","必有","cdots","引入","inputlayer","同以","添加","通过","<","一定","一段","mathrm","相互","不变","前为","variables","读取","程度","当然","即可","must","必须","staticmethod","var","不","新","段","varbias","of","反之","复合","正则","48064652","zeros","anything","严重","test","实现","config","49","行动","your","这些","均匀分布","不确定性","系列","由于","4728e","证明","仿射变换","等","use","轻易","范化","densely","定性","这样","注意","解决方案","-","len","34096144","维和","显然","取前","mapfunc","11452286","7940e","ones","sigma","教程","影响","分解","greater","缺失","you","无","re","信号","符合","求取","一系","47081321","字符串","由","类似","上","才能","rmse","35","指出","最底层","频率","满足","替换成","等效","混合","塞尔","同一","它","条件","非线性","supports","log","之","如果","顺序","17","通顺","class","tensordot","未","快","方程","自定义","好","50978382","setseed","signal","完善","本层","到","\\","整数","线性代数","几片","大大降低","nonlinreghandle","15","然后","72857359","难道","运算","9999991","dtype","虚部值","或缺","34310476","engine","形","函数","boolean","高度","只要","数值","结果","大量","网络层","等等","05201025e","linear","lstm","高效","内容","不用","example","34","让","regressed","写出","sd","rvert","有些","阅览","行列","三角函数","例子","比","等价","相比","自带","63167972","完全一致","执行","adam","inner","核心","83158726","系数","受到","evaluated","6936e",":","维度","07843238","18","varepsilon","解析","0002472764754202217","第一节","0027","官方","input","推断出","提到","颇为","显得","least","但是","true","将子层","形状","只","指定","节","思考","网络","dot","感觉","摘自","api","整个","一般来说","准确","频谱","numer","axis","接受","实际行动","内有","innermost","低频","那么","39","49838507","square","specify","结果表明","完完全全",">","要","一行","值","16","定义","pi","一步","stackoverflow","亦然","res","有限","常量","处理","地令","推荐","y2","表示","更","其他","活","分解成","error","得到","begin","1j","if","到列","多","原","反映","b","带","建模","前者","找回","type","dparser","当初","展开","object","18109798","保护","不可","合心","观察","两轴","过程","layer","read","meansquarederror","中层","自","build","提高","save","下来","写成","属性","is","set","许多","00","3.25903225","神经","必然","两维","解均","只有","旧版","varphi","被","实部值","长度","have","代数","笔者","如","flattened","来说","3226707","summary","limits","相信","上述","域","不同","21387424","使","value","说明","区别","相位","本节","applicable","defined","从","连续","增长","产生","这一","86784854","或类","仍然","估计","一致","如此","44","出该","并用","activation","实例","触发","不可或缺","入门","繁琐","事实","axes","层来","samples","一层","history","last","线性变换","23","mathcal","类","简洁","59993345","改动","再","为什么","正向","二个","做法","存在","0556e","real","逆变换","situation","输入","就","16604991","程序","任何","为了","最终","return","@","时候","对于","seed","96046448e","optimizers","之内","一般","存取","用","back","implements","in","items","python","xl","结束","变得","既","timeless","46","22","groupsort","细节","损失","记录","启发","平均值","denom","resort","三维","list","integer","考虑","original","correlation","arguments","weights","值为","升维成","生效","效果","字符","layers","just","传达","按照","未见","28","75982541","2019","配置","|","规范化","推进","导致","than","结论","68157606","给出","ndims","扩展","下面","compute","trainable","range","类才","用到","y","能力","推断","风格","测试阶段","1.0","第一个","原理",",","能够","点","接下来","40","truth","一起","两者","中","读者","事实上","工具","因此","覆盖","random","...","看出","init","相关系数","原函数","均值","行内","还会","reshape","生成","print","anymore","steppe","精确","一一","m","指","作为","9842","解决","eta","部分","33","完全相同","强度","变量","鲁棒性","testing","仍","表明","计算","维","周期","方差","dense","三角","特别","理论","rfft","秩均","45","势必","范数","原则","0.01","该层","进行","组是","testdataset","大","几种","now","某种程度","修改","conv","可行","#","4ms","模块","total","take","数据分布","量","initializer","体操","network","08831315","而","包含","tensorshape","28678976","所有","获取","平均","全为","trainbatchnum","47","了","设法","values","特征","0","normal","工作","式","has","maxval","array","乘法","字典","相关","人","层里","dimension","列式","自动","真实","于是","function","是否","神经网","用法","我们","好后","activations","ops","选择","阶段","每个","取实部","13842369","这是","确实","37133611","。","若","以外","optimizername","not","rnn","affine","51","幅值","19","bias","无关","出来","相互交换","3s","近似","矩阵","参数","metric","趋近","意味着","形式","5.80141695","随着","21582338","scale","用于","applied","at","目前为止","神经网络","group","most","mathbf","映射","定义新","文档","改写","像","然而","层叠","optimizer","一样","base","omega","重点","hat","本","说","源码","08608948","验证","组中","表","12","速度","你","built","{","较为","train","constraint","可","minval","求均"],"title":"非线性回归","title_tokens":["非线性","回归","线性"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_1","text":"摘要 本节将讨论如何将一个解析的非线性的回归问题，表述成使用非线性函数激活的线性回归问题。特别地，我们将通过自己定义“激活层”来引入我们定义的解析的非线性函数。","text_tokens":["成","特别","层","来","讨论","定义","函数","线性","引入","“","本节","回归","将","。","如何","激活","通过","解析","使用","我们","表述","的","问题","，","非线性","自己","一个"," ","地","”","摘要"],"title":"非线性回归","title_tokens":["非线性","回归","线性"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_2","text":"","text_tokens":[],"title":"理论","title_tokens":["理论"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_3","text":"回忆我们的多输出方程 \\mathbf{y} = \\mathcal{F}(x) \\mathbf{y} = \\mathcal{F}(x) ，其中 \\mathcal{F} \\mathcal{F} 可以是非线性函数，那么我们可以考虑使用一个带可调参数的模型 \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}) \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}) 来模拟它，其中 \\boldsymbol{\\Theta} \\boldsymbol{\\Theta} 是可调的参数。于是，该问题可以被表述为 \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\boldsymbol{\\Theta}} &\\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}_k) \\right),\\\\ \\mathrm{s.t.}~&\\mathbf{y}_k = \\mathcal{F}(\\mathbf{x}_k). \\end{aligned} \\end{equation} 其中， (\\mathbf{x}_k,~\\mathbf{y}_k) \\in \\mathbb{D} (\\mathbf{x}_k,~\\mathbf{y}_k) \\in \\mathbb{D} 来自由非线性函数 \\mathcal{F} \\mathcal{F} 产生的数据集。","text_tokens":["(","^","l","s","参数","来","k","y","该","left","函数","right","线性","n",")","d","可以","方程","=","mathcal","可调","arg","&","。","mathbb","end","为","theta",",","被","{","模型","于是","考虑","模拟","aligned","是","使用","\\","输出","boldsymbol","mathbf","产生","我们","_","1","回忆","的","f","，","问题","表述","mathrm","x","}","begin","~","t","那么","它","非线性","sum","自由","数据","其中","equation","一个","min","limits","多"," ","集","in",".","带"],"title":"一般回归问题","title_tokens":["一般","回归","问题"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_4","text":"对于一个解析的非线性函数，我们假设任何这样的函数都可以分解成多个复合函数 \\mathbf{f}_i \\mathbf{f}_i ，其中每个复合函数都只包含一个仿射变换 \\mathbf{h}_j = \\mathbf{W}_j \\cdot + \\mathbf{b}_j \\mathbf{h}_j = \\mathbf{W}_j \\cdot + \\mathbf{b}_j 和一个对在各元素操作的非线性激活函数 \\Lambda_j \\Lambda_j 。因此，复合函数可以写作 \\mathbf{f}_i = \\Lambda_j \\circ \\mathbf{h}_j \\mathbf{f}_i = \\Lambda_j \\circ \\mathbf{h}_j 。于是，整个非线性的函数可以表述为： \\begin{equation} \\begin{aligned} \\mathcal{F} = \\Lambda_M \\circ \\mathbf{h}_M \\circ \\Lambda_{M-1} \\circ \\mathbf{h}_{M-1} \\circ \\cdots \\Lambda_1 \\circ \\mathbf{h}_1. \\end{aligned} \\end{equation} 例如，对函数 \\begin{align} \\mathcal{F}(\\mathbf{x}) = \\exp( \\mathbf{A} \\log ( | \\mathbf{B} \\mathbf{x} + \\mathbf{c} | ) ). \\end{align} 可以分解为： \\mathbf{h}_1 (\\mathbf{x}) = \\mathbf{B} \\mathbf{x} + \\mathbf{c} \\mathbf{h}_1 (\\mathbf{x}) = \\mathbf{B} \\mathbf{x} + \\mathbf{c} ； \\Lambda_1 (\\mathbf{h}_1) = \\log ( | \\mathbf{h}_1 | ) \\Lambda_1 (\\mathbf{h}_1) = \\log ( | \\mathbf{h}_1 | ) ； \\mathbf{h}_2 (\\Lambda_1) = \\mathbf{A} \\Lambda_1 \\mathbf{h}_2 (\\Lambda_1) = \\mathbf{A} \\Lambda_1 ； \\Lambda_2 (\\mathbf{h}_2) = \\exp ( \\mathbf{h}_2 ) \\Lambda_2 (\\mathbf{h}_2) = \\exp ( \\mathbf{h}_2 ) 。 实际上，当然还存在更复杂的情况，例如，一个非线性函数 \\mathbf{f}_j \\mathbf{f}_j 是两个非线性函数的和、积、商，或是某函数导数的范数等……但原则上，这些函数都可以写作上述（多个）可分解复合函数的（联合）变换。本质上，函数中的任何参数，都可以看作是在参与一个仿射变换。因此，任何函数只要能写出解析式，理论上就能分解为（多个）上述的可分解函数的形式。 相信有一点功底的读者都可以看出， (2) (2) 其实就是一个神经网络的表达式。换言之，只要知道一个函数的解析式，我们就可以用一个或多个神经网络来为其建模。虽然我们可能不知道这个函数里具体的参数值，但通过对网络训练，我们可以让网络的参数回归到函数的参数上。","text_tokens":["（","假设","cdots","、","mathcal","虽然","end","例如","通过","于是","存在","更","\\","到","神经网","h","表达","我们","功底","分解成","的","积","}","begin","知道","中","变换","读者","联合","因此","当然","就","看出","元素","b","建模","任何","(","这个","里","训练","+","其","每个","align","情况","对于","）","不","函数","可以","表达式","只要","=","m","数值","。","复合","实际","复杂","但","：","参数值","aligned","是","各","商","原则上","参与","x","有","用","让","写出","equation","一个","可分","a","仿射","这些","…","还","或是","达式","可分解","参数","理论","lambda","仿射变换","等","看作","范数","形式","两个","原则","线性","w","神经",")","回归","这样","为","；","激活","-","一点","解析","神经网络","实际上","mathbf","2","_","1","f","j","对","相信",".","分解","上述","换言之","具体","在","操作","来","只","多个","就是","都","|","写作","c","网络","包含","导数","本质","或","其实","整个","circ","上","{","i","cdot","，","表述","可能","非线性","和","其中","exp","log","某","能"," ","式"],"title":"非线性解析函数的分解","title_tokens":["非线性","解析","的","分解","函数","线性"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_5","text":"考虑一组三角函数的线性组合，使得列向量 \\mathbf{x} \\in \\mathbb{R}^T \\mathbf{x} \\in \\mathbb{R}^T 映射到列向量 \\mathbf{y} \\in \\mathbb{R}^T \\mathbf{y} \\in \\mathbb{R}^T ，其中 \\mathbf{x} \\mathbf{x} ， \\mathbf{y} \\mathbf{y} 均为时间域上的变量，则： \\begin{align} \\mathbf{y} = \\sum_{i=1}^N a_i \\cos ( \\omega_i \\mathbf{x} + \\varphi_i ). \\end{align} 如果我们将其写成矩阵的形式，应当有 \\begin{align} \\mathbf{y} = \\cos ( \\mathbf{x} \\boldsymbol{\\omega}^T + \\mathbf{1} \\boldsymbol{\\varphi}^T ) \\mathbf{a}. \\end{align} 设若我们有大量的样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，但我们不知道参数 \\boldsymbol{\\omega},~\\boldsymbol{\\varphi},~\\mathbf{a} \\boldsymbol{\\omega},~\\boldsymbol{\\varphi},~\\mathbf{a} 。若我们想在频率域上拟合出该模型的参数，则根据 (1) (1) ，该问题可以写作： \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\boldsymbol{\\omega},~\\boldsymbol{\\varphi},~\\mathbf{a}} &\\sum_{k=1}^K \\lVert \\mathrm{Re}\\{\\mathbf{Y}_k - \\hat{\\mathbf{Y}}_k \\} \\rVert_2^2 + \\lVert \\mathrm{Im}\\{\\mathbf{Y}_k - \\hat{\\mathbf{Y}}_k \\} \\rVert^2_2,\\\\ \\mathrm{s.t.}~&\\mathbf{Y}_k = \\mathrm{FFT}(\\mathbf{y}_k), \\\\ &\\hat{\\mathbf{Y}}_k = \\mathrm{FFT}(\\cos ( \\mathbf{x}_k \\boldsymbol{\\omega}^T + \\mathbf{1} \\boldsymbol{\\varphi}^T ) \\mathbf{a}). \\end{aligned} \\end{equation} 其中，FFT指 快速傅里叶变换 ，虽然FFT是一个线性变换，但显然，该问题是一个非线性问题，这是由于预测值的表达式 \\hat{\\mathbf{y}} \\hat{\\mathbf{y}} 的表达式 (5) (5) 是非线性的。 由于表达式 (5) (5) 是一个显式函数，相比上一节求取低秩近似的仿射变换，我们可以知道，即使该问题即使存在多个不同的 \\mathbf{x}_k \\mathbf{x}_k 对应同一个 \\mathbf{y}_k \\mathbf{y}_k ，也不影响我们对问题的求解（即训练得到的参数能和真实参数产生相同的输出）。然而，从这里的参数的定义可以看出，我们在这个问题中使用的参数是非常低秩的（所有的参数秩均为1），这将导致这个问题的解具有高度的不确定性，许多不同的参数组 \\boldsymbol{\\omega},~\\boldsymbol{\\varphi},~\\mathbf{a} \\boldsymbol{\\omega},~\\boldsymbol{\\varphi},~\\mathbf{a} 均能达到相同的效果。例如，我们已知三个参数向量是长度相同的，若我们选取三个向量各自的第 i i 个元素，和其对应的第 j j 个元素相互交换（例如 \\omega_i \\leftrightarrow \\omega_j \\omega_i \\leftrightarrow \\omega_j ），则根据 (4) (4) ，这两个不同的解均能产生相同的效果。另一个例子是，由于余弦函数具有周期性，对 \\boldsymbol{\\varphi} \\boldsymbol{\\varphi} 的任意元素 \\varphi_i \\varphi_i ，即使令 \\varphi_i = \\varphi_i + 2 \\pi \\varphi_i = \\varphi_i + 2 \\pi ，仍不影响拟合的效果。因此，通过 (6) (6) 求解的向量 \\boldsymbol{\\varphi} \\boldsymbol{\\varphi} 也具有不确定性。 根据我们前面提到的对非线性解析函数的分解方法，该问题的模型可以分解为： \\mathbf{h}_1 (\\mathbf{x})= \\mathbf{x} \\boldsymbol{\\omega}^T + \\mathbf{1} \\boldsymbol{\\varphi}^T \\mathbf{h}_1 (\\mathbf{x})= \\mathbf{x} \\boldsymbol{\\omega}^T + \\mathbf{1} \\boldsymbol{\\varphi}^T ； \\Lambda_1 (\\mathbf{h}_1) = \\cos ( \\mathbf{h}_1 ) \\Lambda_1 (\\mathbf{h}_1) = \\cos ( \\mathbf{h}_1 ) ； \\mathbf{h}_2 (\\Lambda_1) = \\Lambda_1 \\mathbf{a} \\mathbf{h}_2 (\\Lambda_1) = \\Lambda_1 \\mathbf{a} ； \\hat{\\mathbf{Y}} = \\Lambda_2 (\\mathbf{h}_2) = \\mathrm{FFT} ( \\mathbf{h}_2 ) \\hat{\\mathbf{Y}} = \\Lambda_2 (\\mathbf{h}_2) = \\mathrm{FFT} ( \\mathbf{h}_2 ) 。","text_tokens":["s","（","求解","y","列","傅里叶","选取","定义","即","线性变换","pi","拟合","lvert","虽然","end","真实","例如",",","通过","存在","\\","想","4","表达","h","我们","mathrm","的","得到","相互","解","则","预测值","}","begin","知道","~","对应","周期性","非常","中","变换","因此","到列","秩","看出","元素","各自","显式","(","这个","cos","训练","+","leftrightarrow","其","互交","align","im","不","）","函数","即使","n","可以","表达式","高度","达到","=","指","预测","使得","&","大量","。","若","这是","交换","但","这","：","模型","具有","aligned","是","变量","输出","设若","快速","x","t","有","6","仍","sum","equation","一个","第","a","rvert","5","相互交换","仿射","周期","in","不确定性","前面","方法","已知","^","三角","达式","三角函数","写成","矩阵","参数","由于","近似","例子","lambda","相比","许多","个","仿射变换","确定性","秩均","也","形式","两个","线性",")","任意","arg","解均","mathbb","varphi","定性","为","；","低","-","考虑","解析","长度","boldsymbol","mathbf","2","_","一组","映射","1","问题","一节","显然","j","效果","提到","然而","对","limits","r","omega","样本",".","域","hat","不同","影响","分解","在","相同","令","re","k","该","均","多个","求取","写作","组","将","这里","确定","从","导致","根据","向量","{","上","i","所有","fft","组合","使用","产生","频率","，","时间","三个","同一","余弦","非线性","另","和","其中","min","线性组合","能"," ","如果","出该","非常低","应当","同一个"],"title":"本节问题","title_tokens":["节","问题","本"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_6","text":"我们已经知道，该问题可以建立成一个两层的模型，两层的变换函数和激活函数分别为 (\\mathbf{h}_1,~\\Lambda_1) (\\mathbf{h}_1,~\\Lambda_1) , (\\mathbf{h}_2,~\\Lambda_2) (\\mathbf{h}_2,~\\Lambda_2) 。然而，实现该模型仍然存在技术问题。即，神经网络中，并未定义 \\mathbf{h}_1,~\\mathbf{h}_2,~\\Lambda_2 \\mathbf{h}_1,~\\mathbf{h}_2,~\\Lambda_2 的层API，因此，我们必须自己来实现这些功能。 这一节讨论的内容更偏向于技术实现，而且对新入门的读者而言具有一定的难度。但本节讨论的技术，即自定义网络层，实在是非常广泛地应用在Keras API的用户中。例如，著名的Residual network和Inception network，在Tensorflow-Keras API中均未提供现成的API，需要读者自行设法构造。 熟悉旧版Tensorflow的用户，可能会发现，在实现自定义API的过程上，旧版API使用起来更容易上手；然而，Keras式的API也有其好处，那就是强制用户必须按照规范、统一的标准处理API的定义和接口，使得用户更容易建立规范的编写习惯。","text_tokens":["入门","编写","功能","而且","定义","未","用户","即","、","而言","自定义","tensorflow","处理","例如",",","广泛","存在","更","\\","一定","神经网","h","我们","的","知道","}","~","非常","中","变换","读者","因此","接口","地","提供","(","技术","并未","已经","必须","其","讨论","函数","新","可以","自行","使得","。","手","网络层","熟悉","但","需要","过程","这","统一","起来","模型","具有","是","应用","内容","实现","发现","两层","有","规范","一个","这些","层","lambda","习惯","也","神经",")","偏向","那","分别","keras","residual","旧版","为","；","激活","-","会","神经网络","强制","mathbf","2","_","1","问题","一节","建立","于","然而","对","现成","在","成","来","构造","该","按照","就是","network","本节","网络","好处","著名","api","自定","上","{","使用","标准","inception","，","中均","可能","仍然","难度","实在","和","自己","设法"," ","容易","式"],"title":"解非线性回归问题","title_tokens":["非线性","回归","问题","解","线性"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_7","text":"自定义Keras层的方法可以参照： 编写你自己的Keras层 - Keras中文文档 编写好的层是一个类API，可以同时被顺序模型或类模型调用。","text_tokens":["层","编写","定义","参照","同时","可以","自定义","好","类","keras","。","自定","api","你","被","-","：","模型","是","调用","的","，","或类","文档","中文","自己","一个"," ","方法","顺序"],"title":"自定义网络层","title_tokens":["自定义","网络","网络层","自定","定义"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_8","text":"让我们观察Tensorflow-keras模型对最简单的层，全连接层 Dense 的定义（我们之前也分别在顺序模型和类模型中使用过该API）。下面的内容摘自 Tensorflow源码 ： import 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from tensorflow.python.eager import context from tensorflow.python.framework import common_shapes from tensorflow.python.framework import ops from tensorflow.python.framework import tensor_shape from tensorflow.python.keras import activations from tensorflow.python.keras import backend as K from tensorflow.python.keras import constraints from tensorflow.python.keras import initializers from tensorflow.python.keras import regularizers from tensorflow.python.keras.engine.base_layer import Layer from tensorflow.python.keras.engine.input_spec import InputSpec from tensorflow.python.keras.utils import conv_utils from tensorflow.python.keras.utils import generic_utils from tensorflow.python.keras.utils import tf_utils from tensorflow.python.ops import array_ops from tensorflow.python.ops import gen_math_ops from tensorflow.python.ops import math_ops from tensorflow.python.ops import nn from tensorflow.python.ops import nn_ops from tensorflow.python.ops import standard_ops from tensorflow.python.util.tf_export import tf_export class Dense 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @tf_export ( 'keras.layers.Dense' ) class Dense ( Layer ): def __init__ ( self , units , activation = None , use_bias = True , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) def build ( self , input_shape ) def call ( self , inputs ) def compute_output_shape ( self , input_shape ) def get_config ( self ) doc string 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class Dense ( Layer ): \"\"\"Just your regular densely-connected NN layer. `Dense` implements the operation: `output = activation(dot(input, kernel) + bias)` where `activation` is the element-wise activation function passed as the `activation` argument, `kernel` is a weights matrix created by the layer, and `bias` is a bias vector created by the layer (only applicable if `use_bias` is `True`). Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with `kernel`. Example: # as first layer in a sequential model: model = Sequential() model.add(Dense(32, input_shape=(16,))) # now the model will take as input arrays of shape (*, 16) # and output arrays of shape (*, 32) # after the first layer, you don't need to specify # the size of the input anymore: model.add(Dense(32)) Arguments: units: Positive integer, dimensionality of the output space. activation: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: `a(x) = x`). use_bias: Boolean, whether the layer uses a bias vector. kernel_initializer: Initializer for the `kernel` weights matrix. bias_initializer: Initializer for the bias vector. kernel_regularizer: Regularizer function applied to the `kernel` weights matrix. bias_regularizer: Regularizer function applied to the bias vector. activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\").. kernel_constraint: Constraint function applied to the `kernel` weights matrix. bias_constraint: Constraint function applied to the bias vector. Input shape: nD tensor with shape: `(batch_size, ..., input_dim)`. The most common situation would be a 2D input with shape `(batch_size, input_dim)`. Output shape: nD tensor with shape: `(batch_size, ..., units)`. For instance, for a 2D input with shape `(batch_size, input_dim)`, the output would have shape `(batch_size, units)`. \"\"\" Dense.__init__ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Dense ( Layer ): def __init__ ( self , units , activation = None , use_bias = True , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ): if 'input_shape' not in kwargs and 'input_dim' in kwargs : kwargs [ 'input_shape' ] = ( kwargs . pop ( 'input_dim' ),) super ( Dense , self ) . __init__ ( activity_regularizer = regularizers . get ( activity_regularizer ), ** kwargs ) self . units = int ( units ) self . activation = activations . get ( activation ) self . use_bias = use_bias self . kernel_initializer = initializers . get ( kernel_initializer ) self . bias_initializer = initializers . get ( bias_initializer ) self . kernel_regularizer = regularizers . get ( kernel_regularizer ) self . bias_regularizer = regularizers . get ( bias_regularizer ) self . kernel_constraint = constraints . get ( kernel_constraint ) self . bias_constraint = constraints . get ( bias_constraint ) self . supports_masking = True self . input_spec = InputSpec ( min_ndim = 2 ) Dense.build 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Dense ( Layer ): def build ( self , input_shape ): input_shape = tensor_shape . TensorShape ( input_shape ) if tensor_shape . dimension_value ( input_shape [ - 1 ]) is None : raise ValueError ( 'The last dimension of the inputs to `Dense` ' 'should be defined. Found `None`.' ) last_dim = tensor_shape . dimension_value ( input_shape [ - 1 ]) self . input_spec = InputSpec ( min_ndim = 2 , axes = { - 1 : last_dim }) self . kernel = self . add_weight ( 'kernel' , shape = [ last_dim , self . units ], initializer = self . kernel_initializer , regularizer = self . kernel_regularizer , constraint = self . kernel_constraint , dtype = self . dtype , trainable = True ) if self . use_bias : self . bias = self . add_weight ( 'bias' , shape = [ self . units ,], initializer = self . bias_initializer , regularizer = self . bias_regularizer , constraint = self . bias_constraint , dtype = self . dtype , trainable = True ) else : self . bias = None self . built = True Dense.call 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Dense ( Layer ): def call ( self , inputs ): inputs = ops . convert_to_tensor ( inputs ) rank = common_shapes . rank ( inputs ) if rank > 2 : # Broadcasting is required for the inputs. outputs = standard_ops . tensordot ( inputs , self . kernel , [[ rank - 1 ], [ 0 ]]) # Reshape the output back to the original ndim of the input. if not context . executing_eagerly (): shape = inputs . get_shape () . as_list () output_shape = shape [: - 1 ] + [ self . units ] outputs . set_shape ( output_shape ) else : outputs = gen_math_ops . mat_mul ( inputs , self . kernel ) if self . use_bias : outputs = nn . bias_add ( outputs , self . bias ) if self . activation is not None : return self . activation ( outputs ) # pylint: disable=not-callable return outputs Dense.compute_output_shape 1 2 3 4 5 6 7 8 9 class Dense ( Layer ): def compute_output_shape ( self , input_shape ): input_shape = tensor_shape . TensorShape ( input_shape ) input_shape = input_shape . with_rank_at_least ( 2 ) if tensor_shape . dimension_value ( input_shape [ - 1 ]) is None : raise ValueError ( 'The innermost dimension of input_shape must be defined, but saw: %s ' % input_shape ) return input_shape [: - 1 ] . concatenate ( self . units ) Dense.get_config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Dense ( Layer ): def get_config ( self ): config = { 'units' : self . units , 'activation' : activations . serialize ( self . activation ), 'use_bias' : self . use_bias , 'kernel_initializer' : initializers . serialize ( self . kernel_initializer ), 'bias_initializer' : initializers . serialize ( self . bias_initializer ), 'kernel_regularizer' : regularizers . serialize ( self . kernel_regularizer ), 'bias_regularizer' : regularizers . serialize ( self . bias_regularizer ), 'activity_regularizer' : regularizers . serialize ( self . activity_regularizer ), 'kernel_constraint' : constraints . serialize ( self . kernel_constraint ), 'bias_constraint' : constraints . serialize ( self . bias_constraint ) } base_config = super ( Dense , self ) . get_config () return dict ( list ( base_config . items ()) + list ( config . items ())) 为了便于读者阅读，我们将它按照重定义的方法分成了几片不同的代码。下面我们来分别观察不同代码里实现的内容。","text_tokens":["regularizer","s","（","constraints","callable",">","glorot","16","as","定义","broadcasting","add","created","math","*","最","的","}","instance","25","if","2d","9","gen","where","wise","(","分成","%","matrix","from","shapes","=","convert","观察","saw","positive","模型","38","layer","kernel","activity","5","the","build","方法","41","and","raise","is","set","shape","37","分别","operation","32","have","note","whether","2","ie","flattened","initializers",".__","by","不同","来","value","applicable","defined","context","26","mul","argument","27","36","，","first","uses","be","和","44","get","activation","string","axes","last","23","类","regularizers","简单","doc","4","batch","masking","situation","[","..","kwargs","为了","+","return","found","便于","dimensionality","11","@","but","util","21","t","back","6","implements","a","7","output","weight","in","8","items","python","22",")","29","model","element","rank","inputspec","units","it","list","standard","integer","original","arguments","_","weights","regular","generic","对","layers","product","代码","在","just","then","`","按照","28","ndim","过该","than","serialize","24","disable","下面","compute","trainable","will","eager","]","vector","dict","\"","connected",",","40","之前","mat","pop","中","读者","...","init","to","must","none","reshape","anymore","of","backend","33","zeros","：","utils","anything","30","prior","eagerly","实现","config","call","your","dense","14","def","__","tensor","else","use","concatenate","13","keras","executing","densely","nn","-","'","now","need","1","conv","outputs","greater","阅读","you","#","take","only","initial","initializer","export","3","将","tensorshape","for","super","使用","35","42","space","self","它","了","min","supports","don"," ","0","顺序","has","tf","重","should","array","17","class","tensordot","dimension","tensorflow","function","sequential","我们","全","几片","10","15","activations","ops","dtype","would","里","uniform","engine","31","）","boolean","dim","。","its","required","spec","linear","no","not","内容","example","x","19","34","让","arrays","bias","pylint","int","层","common","43","也","valueerror","import","连接","framework","after","applied","at",":","18","most","input","inputs","least","true","base",".","源码","k","with","nd","12","20","dot","摘自","api","built","{","size","constraint","innermost","passed","39","specify"],"title":"学习一个完全规范化的风格","title_tokens":["规范化","规范","风格","一个","范化","完全","学习","的"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_9","text":"首先，在 __init__ 方法中，定义了用来初始化该层的所有可选参数。从这一段代码，可以观察到以下结论： units 是 Dense 的输出维度，是唯一一个必选参量，并用 int() 强制转换的方式确保输入的是整数。 除了布尔类型的输入，其他所有的输入都使用 tensorflow.python.keras 下的对应方法保护起来。例如， kernel_regularizer 的实现通过 keras.regularizers 方法初始化。这是为了确保用户使用该接口时，既可以使用字符串指定正则化器，也可以通过一个现成的正则化器实例来指定。 supports_masking 和 input_spec 这两个量由类本身决定，不受用户初始化参数的影响。 input_spec 用来限定输入网络的张量必须具有哪些属性，参见官方文档对 tf.keras.layers.InputSpec 的说明。 supports_masking 用来表示该输入是否支持 Masking 层，参见 Masking - Keras中文文档 对该层的介绍。它主要用来取消时序模型（一般是RNN/LSTM）中缺失的时间点数据对网络结果的影响。一般来说，一个与时序无关的（或者称为时不变(timeless)的）模型，直接设该值为True即可。 特别地， activity_regularizer 通过覆盖输入参量的默认值来实现。这是因为在 Dense 的父类 Layer 中，已经定义过 activity_regularizer 。其他的参量不能通过这种方式实现，是因为它们都跟新加入的参数有关。","text_tokens":["regularizer","tf","（","或者","值","首先","定义","下","用户","tensorflow","时序","是因为","regularizers","例如","参见","/","通过","表示","参量","点","到","整数","决定","是否","一段","其他","介绍","默认","的","它们","对应","masking","不变","直接","中","数据","取消","覆盖","输入","即可","接口","地","确保","init","张量","(","为了","父类","已经","必须","不","）","新","可以","保护","布尔","结果","这是","。","方式","spec","观察","正则","这","：","初始化","起来","具有","模型","是","rnn","一般","lstm","输出","实现","kernel","layer","activity","支持","中文","无关","一个","选","dense","方法","__","int","层","参数","属性","python","特别","既","timeless","也","转换","两个",")","设该","这种","以下","keras","哪些","该层","因为","inputspec","units","为","-","维度","限定","跟","过","强制","初始","_","由类","官方","文档","来说","input","用来","加入","除了","对","true","字符","layers","代码",".","影响","缺失","现成","在","化器","来","指定","量","该","都","时","说明","有关","字符串","网络","受","从","结论","本身","所有","唯一","必选","一般来说","使用","不能","默认值","，","可","时间","称为","它","与","和","了","主要","supports"," ","并用","实例","类型"],"title":"初始化方法","title_tokens":["初始","初始化","方法"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_10","text":"接下来，让我们观察 build 方法。在该方法中，我们实现了网络中各参数的构造过程。 首先，通过 input_shape 来得到输入张量的形状。在本例中，特别检查了 input_shape 的规范性，确保输入数据的最后一个维度值为已知。因为， Dense 的API允许用户只通过输入来自上一层的张量，来推断全连接层的输入维度。特别地，如果我们的层有多个输入， input_shape 会是一个 list 类型。 接下来，通过 self.add_weight 或 self.add_variable 来添加参数。该方法的用法参见官方文档中的 Layers.add_weight 。它接受包括初始化器 initializer ，正则化器 regularizer 等一系列参数，这些参数都要求必须是具有回调属性的实例。这一条件我们已经在 __init__ 方法中满足了。 最后，设定 self.built 为 True 。事实上，Keras推荐我们使用类似 super ( Dense , self ) . build ( input_shape ) 的方式来完成这一设置，这种做法和显式地设定 self.built 等价。","text_tokens":["regularizer","事实","一层","首先","用户","add","推断","添加","参见",",","推荐","通过","做法","接下来","用法","我们","全","一系列","的","得到","中","数据","事实上","接下","设置","输入","包括","地","确保","init","设定","张量","显式","(","来自","已经","必须","。","方式","观察","器","正则","过程","初始化","具有","是","各","实现","有","让","规范","一个","最后","build","weight","这些","dense","规范性","方法","已知","下来","系列","__","层","参数","特别","属性","等价","shape","等","允许",")","本例","连接","这种","keras","因为","为","list","variable","维度","会","初始","_","值为","官方","文档","要求","input","true","layers",".","在","形状","来","化器","只","构造","该","多个","initializer","都","一系","网络","或","类似","api","上","范性","built","super","完成","使用","满足","检查","这一","，","接受","回调","self","它","条件","和","了"," ","如果","实例","类型"],"title":"构造方法","title_tokens":["构造","构造方法","方法"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_11","text":"接下来观察 call 方法。该方法接受的是该层实际输入的张量，同时也输出一个张量，该输出张量即使该层的输出结果。可以说，在该方法中，我们才正式开始实现层的实际功能。 在该范例中，该方法的实现是通过Tensorflow最底层的标准API。这些API对用户来说是封装起来的，一般情况下用户不需要使用它们。实际上，该方法实现的就是 \\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b} \\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b} 的过程。这种代码风格显得颇为繁琐，但是它提供了精密的参数检查和高效的API操作。我们不会讨论这段代码的具体实现细节，因为它既然是被封装的API，我们一般来说就不需要调用它们实现功能（除非我们需要修改使用同样被封装的底层API所编写的模块）。等价地，我们分别介绍Keras API和Tensorflow API如何做到相同的效果。 首先是Keras API版本的等效代码，它修改自 Keras中文文档 ： Dense.call 1 2 3 4 5 6 7 def call ( self , inputs ): res = K . dot ( inputs , self . kernel ) if self . use_bias : res = res + self . bias if self . activation is not None : res = self . activation ( res ) return res 接下来是Tensorflow API版本的等效代码， Dense.call 1 2 3 4 5 6 7 8 9 def call ( self , inputs ): inputs = tf . convert_to_tensor ( inputs ) rank = tf . rank ( inputs ) res = tf . tensordot ( inputs , self . kernel , [[ rank - 1 ], [ 0 ]]) if self . use_bias : res = tf . nn . bias_add ( inputs , self . bias ) if self . activation is not None : res = self . activation ( res ) return res 与Keras API的实现相比，Tensorflow API主要区别是使用 tf.tensordot 时需要指定执行矩阵计算的两轴。实际上，使用Tensorflow API对已经较为熟悉Tensorflow旧版API的用户是十分亲切的，它使我们找回了当初自己编写中层API的感觉。","text_tokens":["tf","繁琐","（","y","功能","编写","tensordot","首先","下","用户","add","风格","才","]","tensorflow","res","所",",","不会","做到","通过","接下来","\\","4","我们","介绍","的","它们","}","if","中","[","接下","9","输入","就","地","提供","b","张量","to","(","找回","封装","已经","+","return","none","当初","情况","讨论","不","）","同时","即使","可以","段","=","精密","结果","。","convert","实际","范例","熟悉","观察","两轴","需要","过程","正式","这","起来","：","not","是","亲切","一般","高效","输出","调用","实现","kernel","x","中文","6","bias","中层","自","一个","计算","call","5","7","这些","dense","8","方法","def","下来","同样","矩阵","层","参数","等价","is","tensor","相比","use","也","执行","w","细节",")","这种","分别","keras","rank","该层","因为","旧版","nn","被","-",":","实际上","既然","mathbf","2","_","1","修改","文档","来说","效果","inputs","颇为","显得","但是","对","代码",".","说","具体","模块","在","版本","相同","操作","使","k","指定","该","就是","3","时","区别","dot","感觉","如何","api","十分","{","较为","一般来说","使用","最底层","标准","检查","，","等效","接受","底层","它","self","与","开始","和","了","主要","自己","检查和"," ","除非","0","activation"],"title":"回调方法","title_tokens":["回调","方法"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_12","text":"接下来观察 compute_output_shape 方法。我们知道Tensorflow-Keras支持对每一层的输入输出作形状推断，而形状推断的具体实现就在这一步。 问题 为什么我们需要定义这个方法？难道我们不可以直接通过对输出张量计算 K.shape(output) 或 tf.shape(output) 来确定输出形状吗？ 这是由于，对Tensorflow-Keras而言，推断网络各层的形状和推断网络各层的张量是两码事。定义该方法能够确保我们在不调用任何一个 call 方法的前提下，推断出整个网络各层的输入、输出形状。 这里实现这一方法的过程仍然是调用Tensorflow的最底层API。事实上，Keras API对这一方法的输出并没有严格的要求，它可以是一个 tf.Shape ，可以是一个 list / tuple ，还可以是将两者元素混合在一起的 list 。我们不考虑对输入形状进行这些检查，那么，一个简单的，Keras风格的改写是 Dense.compute_output_shape 1 2 def compute_output_shape ( self , input_shape ): return ( * input_shape [:, - 1 ], self . units ) 或者我们可以更规范一点，使用Tensorflow API来确保该方法的输入、输出和具体操作都是对 tf.Shape 进行的 Dense.compute_output_shape 1 2 3 4 class Dense ( Layer ): def compute_output_shape ( self , input_shape ): input_shape = tensor_shape . TensorShape ( input_shape ) return input_shape [: - 1 ] . concatenate ( self . units )","text_tokens":["tf","事实","并","class","或者","具体操作","一层","定义","下","、","推断","一步","风格","而言","]","tensorflow","严格","*","什么","为什么",",","/","能够","通过","简单","接下来","更","4","我们","的","知道","一起","两码事","没有","直接","作","两者","[","事实上","接下","输入","就","难道","确保","元素","张量","这个","(","任何","return","不","可以","=","这是","。","各层","观察","需要","过程","这","两码","是","输出","调用","实现","？","layer","支持","规范","一个","计算","call","output","这些","dense","一","方法","def","下来","还","由于","tensor","shape",")","concatenate","keras","进行","units","list","-",":","一点","考虑","2","_","tuple","1","问题","要求","input","改写","推断出","对",".","具体","在","形状","来","前提","每","k","操作","输入输出","该","吗","体操","3","都","而","网络","将","tensorshape","或","确定","这里","api","整个","使用","最底层","检查","这一","，","混合","仍然","底层","它","那么","self","和"," ","compute"],"title":"输出形状方法","title_tokens":["形状","方法","输出"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_13","text":"最后，在 get_config 中，我们将我们自定义的所有参数实例，通过 serialize 方法加入到该层的参数设置字典中。实现这一步是颇为重要的（但是在 Keras中文文档 中并未提及），它允许我们将我们自己定义的网络层编译成一个包含设置信息的字典，并允许我们通过该字典重构出具有相同参数设置的层来，参见： 关于Keras网络层 - Keras中文文档 例如，对一个 Dense 层，通过该方法重构的步骤是 1 2 3 layer = Dense ( 32 ) config = layer . get_config () reconstructed_layer = Dense . from_config ( config ) 综上，我们可以从官方代码如何定义 Dense 层学习到我们自己应该如何定义类似的层。事实上，我们建议读者在自定义任何层之前，先选择一个与我们要自定义的层形式相似的层，阅览官方代码，了解定义一个这样的层大致的步骤，然后再开始实际行动。","text_tokens":["提及","综上","（","事实","字典","并","要","定义","一步","自定义","再","参见","例如","大致","通过","到","之前","我们","的","中","事实上","读者","然后","设置","(","任何","并未","选择","reconstructed","）","from","可以","=","重要","先","。","网络层","实际","这","：","具有","是","实现","layer","config","建议","中文","行动","译成","一个","为重","最后","阅览","dense","方法","层","参数","允许","学习","形式",")","应该","keras","重构","这样","编译","-","32","信息","2","_","1","官方","关于","文档","颇为","加入","但是","对","了解","代码",".","编译成","在","相同","来","颇为重要","出","该","相似","3","将","网络","包含","如何","类似","自定","从","serialize","所有","，","步骤","实际行动","它","与","开始","自己","get"," ","参数设置","实例"],"title":"设置提取方法","title_tokens":["设置","提取","方法"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_14","text":"接下来，我们考虑来自己构造一个层API。该层的表达式为： \\begin{align} \\mathbf{y} = \\eta ( \\mathbf{x} \\boldsymbol{\\omega}^T + \\mathbf{1} \\boldsymbol{\\varphi}^T ), \\end{align} 这里 \\mathbf{x} \\mathbf{x} 为输入的列向量（但是注意在代码中它是行向量）， \\mathbf{1} \\mathbf{1} 是一个与 \\mathbf{x} \\mathbf{x} 形状相同的，值全为1的向量； \\boldsymbol{\\omega},~\\boldsymbol{\\varphi} \\boldsymbol{\\omega},~\\boldsymbol{\\varphi} 为可训练的参数，而 \\eta(\\cdot) \\eta(\\cdot) 是一个应用在元素级的激活函数。我们考虑实现以下功能： 该层输入一个形状为 [ N , L ] 的向量组，输出一个形状为 [ N , L , M ] 的矩阵组，其中 M 是参数 \\boldsymbol{\\omega} \\boldsymbol{\\omega} 的长度。因此，该层不需要获取输入向量的形状，但需要定义输出矩阵的列数 M 。换言之，该层的作用是将向量升维成低秩矩阵； 参数 \\boldsymbol{\\omega},~\\boldsymbol{\\varphi} \\boldsymbol{\\omega},~\\boldsymbol{\\varphi} 都可以指定初始化器、正则化器和限制条件，就像 Dense 层一样； 可以选择是否使用 \\boldsymbol{\\varphi} \\boldsymbol{\\varphi} ，就像在 Dense 层我们可以选择是否使用 biase 一样； 激活函数 \\eta(\\cdot) \\eta(\\cdot) 可以是一个任意的激活函数，并且允许我们为它添加正则化器。 综上，我们定义该层为 class UpDimAffine ( Layer ): UpDimAffine.__init__ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def __init__ ( self , units , activation = None , use_bias = True , kernel_initializer = 'glorot_uniform' , bias_initializer = 'zeros' , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ): if 'input_shape' not in kwargs and 'input_dim' in kwargs : kwargs [ 'input_shape' ] = ( kwargs . pop ( 'input_dim' ),) super ( UpDimAffine , self ) . __init__ ( activity_regularizer = regularizers . get ( activity_regularizer ), ** kwargs ) self . units = int ( units ) self . activation = activations . get ( activation ) self . use_bias = use_bias self . kernel_initializer = initializers . get ( kernel_initializer ) self . bias_initializer = initializers . get ( bias_initializer ) self . kernel_regularizer = regularizers . get ( kernel_regularizer ) self . bias_regularizer = regularizers . get ( bias_regularizer ) self . kernel_constraint = constraints . get ( kernel_constraint ) self . bias_constraint = constraints . get ( bias_constraint ) self . supports_masking = True self . input_spec = InputSpec ( min_ndim = 2 ) UpDimAffine.build 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def build ( self , input_shape ): input_shape = tf . TensorShape ( input_shape ) self . kernel = self . add_weight ( 'kernel' , shape = [ 1 , self . units ], initializer = self . kernel_initializer , regularizer = self . kernel_regularizer , constraint = self . kernel_constraint , dtype = self . dtype , trainable = True ) if self . use_bias : self . bias = self . add_weight ( 'bias' , shape = [ 1 , self . units ], initializer = self . bias_initializer , regularizer = self . bias_regularizer , constraint = self . bias_constraint , dtype = self . dtype , trainable = True ) else : self . bias = None super ( UpDimAffine , self ) . build ( input_shape ) UpDimAffine.call 1 2 3 4 5 6 7 8 9 10 def call ( self , inputs ): inputs = tf . expand_dims ( tf . convert_to_tensor ( inputs ), - 1 ) rank = inputs . get_shape () . ndims res = tf . tensordot ( inputs , self . kernel , [[ rank - 1 ], [ 0 ]]) if self . use_bias : varbias = tf . tensordot ( tf . ones_like ( inputs ), self . bias , [[ rank - 1 ], [ 0 ]]) res = tf . add ( res , varbias ) if self . activation is not None : res = self . activation ( res ) return res UpDimAffine.compute_output_shape 1 2 3 def compute_output_shape ( self , input_shape ): input_shape = tensor_shape . TensorShape ( input_shape ) return input_shape . concatenate ( self . units ) UpDimAffine.get_config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def get_config ( self ): config = { 'units' : self . units , 'activation' : activations . serialize ( self . activation ), 'use_bias' : self . use_bias , 'kernel_initializer' : initializers . serialize ( self . kernel_initializer ), 'bias_initializer' : initializers . serialize ( self . bias_initializer ), 'kernel_regularizer' : regularizers . serialize ( self . kernel_regularizer ), 'bias_regularizer' : regularizers . serialize ( self . bias_regularizer ), 'activity_regularizer' : regularizers . serialize ( self . activity_regularizer ), 'kernel_constraint' : constraints . serialize ( self . kernel_constraint ), 'bias_constraint' : constraints . serialize ( self . bias_constraint ) } base_config = super ( UpDimAffine , self ) . get_config () return dict ( list ( base_config . items ()) + list ( config . items ())) 这组定义完全启发自 Dense 的定义，因此改动其实不多，与 Dense 相比，主要的区别是 call 方法的实现。特别地，与 Dense 相同的是，我们使用 tf.tensordot 来指定对两个高维度的张量，取前一个张量的最后一维和第二个张量的第一维来进行矩阵乘法。","text_tokens":["regularizer","l","tf","综上","（","17","constraints","乘法","进行","y","列","功能","class","tensordot","行向量","值","16","glorot","定义","第一","、","add","]","添加","dict","改动","*","res","end","regularizers",",","二个","接下来","\\","是否","4","表达","我们","的","这组","masking","}","begin","~","pop","if","10","中","[","接下","15","因此","9","输入","秩","就","多","activations","地","init","元素","张量","kwargs","to","(","dtype","选择","训练","作用","uniform","+","return","none","11","align","完全","）","不","函数","第二","n","可以","表达式","expand","varbias","=","m","dim","。","convert","eta","级","spec","器","21","但","正则","需要","zeros","：","初始化","not","是","应用","列数","输出","实现","layer","kernel","activity","config","x","t","19","6","bias","自","一个","5","7","call","output","最后","build","weight","dense","in","一","8","14","def","items","方法","下来","^","达式","biase","矩阵","__","层","参数","并且","and","int","is","tensor","特别","相比","第二个","else","shape","允许","use","22","两个",")","concatenate","13","任意","以下","启发","维来","rank","该层","varphi","为","units","inputspec","注意","；","激活","低",":","-","list","维度","18","考虑","长度","boldsymbol","mathbf","初始","2","_","1","'","like","升维成","维和","input","updimaffine","inputs","像","取前","但是","initializers","ones","true","对","一样","base","omega",".__","代码",".","换言之","限制","在","相同","dims","形状","来","化器","指定","构造","initializer","3","都","区别","组","12","而","将","ndim","高","20","tensorshape","这里","其实","api","serialize","向量","{","获取","super","使用","全为","ndims","cdot","，","constraint","可","它","与","条件","self","和","其中","自己","min","主要","supports","get"," ","trainable","compute","0","activation"],"title":"自定义第一层","title_tokens":["自定义","第一","一层","自定","定义","第一层"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_15","text":"第二层的表达式为： \\begin{equation} \\begin{aligned} \\mathbf{y}_1 = \\mathrm{Re}\\{ \\mathrm{FFT}( \\mathbf{x} \\mathbf{a} ) \\}, \\\\ \\mathbf{y}_2 = \\mathrm{Im}\\{ \\mathrm{FFT}( \\mathbf{x} \\mathbf{a} ) \\}, \\end{aligned} \\end{equation} 这里 \\mathbf{x} \\mathbf{x} 为输入的矩阵， \\mathrm{FFT}(\\cdot) \\mathrm{FFT}(\\cdot) 是快速傅里叶变换。我们使用 \\mathbf{y}_1 \\mathbf{y}_1 和 \\mathbf{y}_2 \\mathbf{y}_2 来表示输出是具有两个通道的向量。我们考虑实现以下功能： 该层输入一个形状为 [ N , L , M ] 的向量组，输出一个形状为 [ N , l , 2 ] 的矩阵组，其中 L 在 l 为奇数时，取 (L+1)/2 ；在 L 为偶数时，取 L/2+1 ； 2 是两个通道，分别表示傅里叶变换的实部值和虚部值，同时，从输出可以看出傅里叶变换将使信号长度折半； 参数只有 \\mathbf{a} \\mathbf{a} ，它可以指定初始化器、正则化器和限制条件，就像 Dense 层一样； 我们不使用两输出的形式，而是使用单输出、两通道的形式来定义该层，是为了方便处理后续的步骤（计算损失函数）。 这里我们介绍一种新的定义层的方法，即“使用层来定义层”。该层定义为 class FFTAffine ( Layer ): FFTAffine.__init__ 1 2 3 4 5 6 7 8 9 10 11 def __init__ ( self , kernel_initializer = 'glorot_uniform' , kernel_regularizer = None , kernel_constraint = None , ** kwargs ): if 'input_shape' not in kwargs and 'input_dim' in kwargs : kwargs [ 'input_shape' ] = ( kwargs . pop ( 'input_dim' ),) super ( FFTAffine , self ) . __init__ ( ** kwargs ) self . kernel_initializer = initializers . get ( kernel_initializer ) self . kernel_regularizer = regularizers . get ( kernel_regularizer ) self . kernel_constraint = constraints . get ( kernel_constraint ) self . supports_masking = True self . input_spec = InputSpec ( min_ndim = 3 ) FFTAffine.build 1 2 3 4 5 6 7 8 def build ( self , input_shape ): input_shape = tf . TensorShape ( input_shape ) self . layer_Dense = tf . keras . layers . Dense ( 1 , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = self . kernel_regularizer , kernel_constraint = self . kernel_constraint ) self . layer_Dense . build ( input_shape ) super ( FFTAffine , self ) . build ( input_shape ) FFTAffine.call 1 2 3 4 5 6 7 def call ( self , inputs ): res = tf . squeeze ( self . layer_Dense ( inputs ), - 1 ) res = tf . signal . rfft ( res ) res_r = tf . expand_dims ( tf . real ( res ), - 1 ) res_i = tf . expand_dims ( tf . imag ( res ), - 1 ) res = tf . concat ([ res_r , res_i ], - 1 ) return res FFTAffine.compute_output_shape 1 2 3 def compute_output_shape ( self , input_shape ): input_shape = tensor_shape . TensorShape ( input_shape ) return input_shape [: - 1 ] . concatenate ( 2 ) FFTAffine.get_config 1 2 3 4 5 6 7 8 def get_config ( self ): config = { 'kernel_initializer' : initializers . serialize ( self . kernel_initializer ), 'kernel_regularizer' : regularizers . serialize ( self . kernel_regularizer ), 'kernel_constraint' : constraints . serialize ( self . kernel_constraint ) } base_config = super ( FFTAffine , self ) . get_config () return dict ( list ( base_config . items ()) + list ( config . items ())) 首先，我们需要观察的是 __init__ 和 get_config 两个方法。由于该层只有一个参数 \\mathbf{a} \\mathbf{a} ，我们因此只需要为它指定初始化器、正则化器和限制条件即可。这一步和之前处理第一层的情况相似。 接下来，观察 build ，与定义第一层情况不同的是，在这里我们不是使用基本API (例如 add_weights )，而是来自 tf.keras.layers 的层API， Dense 。我们将初始化时构建的三个实例馈入 Dense 的参数中。并且，在这一阶段，我们不调用 Dense 处理张量，而是直接获取 Dense 的实例。 提示 注意我们在这里手动调用了 self.layer_Dense.build(input_shape) 。 在一些行内的人眼里，这一步是不可或缺的，因为只有调用了 build 方法，我们定义的 Dense 类才会实例化其内的参数。但事实不完全如此，即使我们去掉这一行，即不调用 Dense 的 build 方法，效果也完全一样。这是因为 Dense 的父类 Layer （当然也是我们继承的父类）具有检查 self.built 是否为 True 的能力，并在调用某些方法的时候，如果发现 self.built 为 False ，则自动调用 build 。这属于 build 方法的隐式调用。 虽然如此，我们仍然提倡用户一定要手动定义 build 。其一是因为，这种显式的定义在逻辑上是通顺、符合人的直觉的；其二是因为，我们不能完全确保自定义层里的所有子层的 build 方法一定会在任何情况下都能隐式触发。况且，这种做法是完全可行的，活用 compute_output_shape 或 tf.shape 等方法，我们可以做到手动触发一个有多个子层的自定义层中的所有子层的 build 方法。 提示 有些行内的人指出，在使用 build 方法时，应当显式地将子层的可训练、不可训练参数都反馈给自定义层的参数表（参见 StackOverflow的讨论串 ），具体的操作如下： self . _trainable_weights = self . layer_Dense . trainable_weights self . _non_trainable_variables = self . layer_Dense . non_trainable_variables 然而必须指出的是，这种做法是不正确的。因为观察 源代码 可以发现， trainable_weights 和 non_trainable_variables 都是封装好的属性方法。私有变量 _trainable_weights 和 _non_trainable_variables 与前者不同的是，这两个私有变量包含的是 直属于本层的可训练、不可训练变量 ；但前者的实现分别是 本层和本层的所有子层的所有可训练、不可训练变量 。因此，将子层的所有变量加诸自定义层的直属变量里，是多余、且容易造成误解的做法。 最后，观察 call 方法，在该方法里，我们首先将维度为 [ N , L , M ] 的参数通过无bias的全连接层映射到 [ N , L , 1 ] ，再压缩最后一维度，得到维度为 [ N , L ] 的向量，通过Tensorflow自带的实值FFT变换函数 tf.signal.rfft ，得到复数域的输出 [ N , l ] ，对该输出分别取实部和虚部，再将两实值化的结果以通道的形式并在一起，最终我们就得到两通道的输出 [ N , l , 2 ] 。其中，第一个通道是傅里叶变换的实部，第二个通道是傅里叶变换的虚部。 信息 实际上，Tensorflow的官方教程给出了一种自定义层的范例，参看 Custom layers 。在这一范例中，使用 tf.keras.Model 定义一个有多个子层的模型，且该模型的使用方法和 Layer 一样。从某种程度上，这种方法比我们使用的方法更简洁。然而，需要指出的是， Model类继承 - Keras中文文档 也提到了这种做法，但使用 Model 类继承会导致网络具体实现的细节变得不可追索，具体而言就是形如 Model.to_json 、 Model.to_yaml 、 Model.get_config 和 Model.save 等方法变得不可用。 我们的这种做法则不存在这个问题，因为我们在每个自定义层里都定义了 get_config 方法，从而使得我们可以像使用内置的层API一样来使用它们。","text_tokens":["regularizer","（","constraints","傅里叶","一行","要","glorot","定义","add","用户","正确","从而","一步","而言","虽然","stackoverflow","是因为","*","res","处理","参见","表示","而是","更","表达","活","的","得到","}","begin","实值","if","9","私有","(","奇数","来自","父类","前者","参数表","直属","=","不可","况且","实际","观察","需要","这","模型","具有","layer","kernel","yaml","equation","5","build","save","方法","下来","and","属性","shape","某些","concat","两个","这种","分别","只有","后续","会","实部值","长度","2","如","initializers",".__","域","不同","来","使","逻辑","误解","从","自定","向量","cdot","，","步骤","仍然","二层","如此","去掉","和","get","实例","直觉","触发","源代码","不可或缺","事实","功能","层来","一层","首先","custom","下","单","、","即","类","简洁","再","end","regularizers","二个","做法","存在","4","介绍","它们","real","masking","直接","提倡","[","变换","接下","输入","就","张量","kwargs","任何","为了","最终","训练","+","return","内置","11","时候","第二","同时","即使","n","表达式","某种","一些","通道","初始化","多余","json","输出","发现","快速","有","用","以","6","一个","a","7","output","in","8","items","达式","并且","第二个","变得","细节",")","“","参看","model","损失","inputspec","list","实值化","考虑","初始","_","weights","继承","效果","对","r","layers","代码","复数","具体","在","dims","化器","相似","多个","就是","不是","ndim","这里","折半","导致","serialize","化","手动","i","给出","检查","两","容易","compute","trainable","方便","l","类才","取","实部","y","能力","]","第一个","dict","提示","追索",",","通过","接下来","一定","之前","mathrm","则","variables","fftaffine","一起","pop","程度","中","因此","即可","当然","隐式","地","确保","看出","init","显式","to","封装","必须","none","行内","且","完全","im","情况","讨论","其二","不","新","可以","m","使得","范例","器","但","正则","：","aligned","变量","实现","config","计算","call","dense","一","def","__","由于","rfft","tensor","等","use","concatenate","keras","该层","为","；","注意","-","信息","实际上","'","1","问题","某种程度","加诸","教程","构建","眼里","可行","限制","无","re","操作","信号","该","initializer","符合","3","第二层","其内","时","都","数表","组","将","包含","tensorshape","squeeze","上","所有","获取","fft","super","使用","不能","指出","imag","它","条件","self","层中","再压缩","了","min","supports","能"," ","如果","tf","子层","偶数","通顺","class","人","并","第一","层里","其一","自定义","好","tensorflow","自动","signal","例如","/","做到","本层","\\","是否","到","我们","全","10","”","这个","里","虚部","uniform","虚部值","或缺","反馈","馈入","阶段","每个","）","形","函数","取实部","expand","dim","造成","结果","。","spec","基本","not","是","non","调用","x","中文","bias","有些","最后","压缩","矩阵","层","参数","比","自带","也","形式","连接","以下","因为","给",":","维度","mathbf","false","映射","官方","文档","input","inputs","提到","像","然而","true","一样","base","一种",".","将子层","形状","指定","只","网络","直","或","如下","api","built","{","constraint","可","三个","第一层","与","其中","串","属于","应当"],"title":"自定义第二层","title_tokens":["二层","自定义","自定","定义","第二层","第二"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_16","text":"注意，在两个自定义层都定义好后，需要在两层定义的最后，加上如下代码： customObjects = { 'FFTAffine' : FFTAffine , 'UpDimAffine' : UpDimAffine } 该字典提供了一个索引表，将字符串形式的层名称映射到具体实现的类Object上。我们在任何涉及读取层的设置，例如 from_json 、 load_model 等方法中，都需要传入该索引表，确保Keras知道如何从配置文件里恢复出我们自定义的层。 如下代码提供了一个简单的两个自定义层叠加在一起的测试 test_layers 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np def test_layers (): # Set model and see the summary of the model model = tf . keras . models . Sequential ([ UpDimAffine ( 10 , use_bias = True , activation = tf . math . cos , input_shape = ( 5 ,)), FFTAffine ( trainable = False ) ]) model . compile ( optimizer = optimizers . Adam ( 0.01 ), loss = tf . keras . losses . mean_squared_error , metrics = [ tf . keras . metrics . mean_squared_error ] ) model . summary () model . save ( 'my_model.h5' ) # perform the test var_input = np . ones ([ 2 , 5 ]) var_output = model . predict ( var_input ) print ( var_input . shape , var_output . shape ) print ( var_output ) test_layers () Output _________________________________________________________________ Layer ( type ) Output Shape Param # ================================================================= up_dim_affine ( UpDimAffine ) ( None, 5 , 10 ) 20 _________________________________________________________________ fft_affine ( FFTAffine ) ( None, 3 , 2 ) 10 ================================================================= Total params: 30 Trainable params: 20 Non-trainable params: 10 _________________________________________________________________ ( 2 , 5 ) ( 2 , 3 , 2 ) [[[ -3.25903225e+00 5 .96046448e-08 ] [ 1 .05201025e-07 -5.80141695e-08 ] [ 3 .23036957e-08 -3.17865378e-08 ]] [[ -3.25903225e+00 5 .96046448e-08 ] [ 1 .05201025e-07 -5.80141695e-08 ] [ 3 .23036957e-08 -3.17865378e-08 ]]] Bug 注意，Tensorflow目前的版本(r1.13)仍然有不完善之处。在上述测试中，如果我们把 tf.keras.losses.mean_squared_error 替换成 tf.keras.MeanSquaredError ，虽然该测试能正常跑通，但接下来读取已保存的网络时则会报错。这是由于目前版本的Tensorflow使用了部分废旧的API来定义读取配置的函数，在Github上的某个 讨论串 ，有人已经给出了解决方案，但仍然需要等候被新版Tensorflow采纳才能生效。 该测试首先通过顺序模型，引入了我们自定义的两个层，然后通过 summary 显示模型的详细结构，并通过 save 保存整个网络的模型配置以及具体的参数值。接下来使用一个值全为1的，形状为 [ 2 , 5 ] 的输入来测试该模型，并记录测试结果，与我们的预期完全相符。 同时，我们在设置两层的时候，刻意地令第二层的参数不可训练，实际显示的结果表明，该设置是成功的。第二个函数的10个参量确实在模型的记录里显示为不可训练的。 信息 关于如何保存网络，我们会在下一章详细展开。 接下来，为了证明我们的自定义层能完全正常地工作，我们进行读取测试， test_read 1 2 3 4 5 6 7 8 9 10 11 import numpy as np def test_read (): customObjects [ 'cos' ] = tf . math . cos new_model = tf . keras . models . load_model ( 'my_model.h5' , custom_objects = customObjects ) new_model . summary () var_input = np . ones ([ 2 , 5 ]) var_output = new_model . predict ( var_input ) print ( var_input . shape , var_output . shape ) print ( var_output ) test_read () Output _________________________________________________________________ Layer ( type ) Output Shape Param # ================================================================= up_dim_affine ( UpDimAffine ) ( None, 5 , 10 ) 20 _________________________________________________________________ fft_affine ( FFTAffine ) ( None, 3 , 2 ) 10 ================================================================= Total params: 30 Trainable params: 20 Non-trainable params: 10 _________________________________________________________________ ( 2 , 5 ) ( 2 , 3 , 2 ) [[[ -3.25903225e+00 5 .96046448e-08 ] [ 1 .05201025e-07 -5.80141695e-08 ] [ 3 .23036957e-08 -3.17865378e-08 ]] [[ -3.25903225e+00 5 .96046448e-08 ] [ 1 .05201025e-07 -5.80141695e-08 ] [ 3 .23036957e-08 -3.17865378e-08 ]]] 在该测试里，我们的模型从配置到参数，都完完全全是从文件 my_model.h5 中读取的。注意我们馈入 customObjects 给 load_model ，使Keras能发现我们自己定义的层。同时， customObjects 还需要添加 tf.math.cos 函数，这是因为该激活函数同样不在Keras内置的几种基本的激活函数之列。 我们用完全相同的输入来测试模型的输出，得到的结果和我们上一个测试完全一致，说明对该模型（包括我们自定义的两层）的保存是成功的。 观察两个测试的输出值，我们会发现，对三维的输出，在确定后两维下标 a, b 的情况下 [:, a , b ] 的输出都是一样的。这是因为，第一维反映的是向量组中不同向量的测试结果，而我们馈入模型的向量组是两个值均为1的长度为5的向量。由于这两个向量完全相同，其对应的输出也完全相同。","text_tokens":["相符","（","结果表明","完完全全","列","值","16","as","定义","虽然","是因为","math","metrics","地令","参量","numpy","结构","的","error","得到","方案","}","设置","9","包括","提供","反映","b","08","(","已","type","文件","展开","object","from","加在","=","不可","实际","观察","需要","这","模型","layer","read","meansquarederror","5","customobjects","the","save","方法","下来","and","set","shape","00","两个","3.25903225","两维","被","会","是从","长度","2","updimaffine","summary","上述","不同","版本","相同","来","使","说明","确定","自定","从","向量","时则","，","成功","仍然","一致","二层","和","my","activation","_________________________________________________________________","首先","custom","下","、","类","二个","刻意","简单","换成","4","squared","h5","[","接下","输入","任何","cos","为了","训练","+","内置","11","时候","同时","第二","96046448e","索引","optimizers","21","采纳","名称","参数值","json","输出","mean","发现","23036957e","有","替换","objects","用","6","一个","a","7","output","8","还","第二个","个",")","model","loss","记录","加在一起","三维","losses","param","_","关于","生效","处","对","涉及","字符","layers","代码","以及","具体","在","传入","把","bug","配置","配置文件","跑通","给出","正常","新版","perform","自己","trainable","废旧","层能","引入","]","添加",",","compile","通过","接下来","new","一起","fftaffine","读取","中","地","07","确保","详细","某个","none","其","完全","情况","var","讨论","print","不","of","predict","解决","但","部分","：","完全相同","30","test","实现","两层","显示","表明","up","维","14","def","由于","证明","等","use","0.01","13","keras","为","进行","组是","注意","解决方案","-","信息","等候","几种","'","1","ones","保存","#","total","出","该","3","都","第二层","字符串","将","而","如何","上","才能","fft","使用","全为","替换成","了","之","能"," ","如果","工作","顺序","tf","值均","17","字典","并","第一","自定义","tensorflow","例如","有人","完善","到","sequential","我们","对应","知道","10","params","好后","models","15","然后","一章","加上","预期","里","已经","3.17865378","馈入","）","函数","恢复","数值","dim","结果","这是","确实","。","see","05201025e","基本","是","non","affine","19","bias","最后","e","同样","层","参数","完全一致","测试","形式","5.80141695","import","也","adam","下标","因为","给","r1","激活",":","18","false","映射","input","层叠","optimizer","true","一样",".","报错","形状","np","github","组中","表","12","网络","20","后","如下","api","整个","{","load","与","串","目前"],"title":"检测效果","title_tokens":["效果","检测"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_17","text":"我们仍然使用自动生成的数据。我们重新继承了自 第一节：线性分类 里定义的数据集生成类，新定义的数据集生成器 class TestDataFFTSet ( TestDataSet ): dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class TestDataFFTSet ( TestDataSet ): ''' A generator of the data set for testing the non-linear regression model. y = cos(x w^T + 1 p^T) a ''' def __init__ ( self , scale_x , len_x , omega , phi , a ): ''' Initialize the data generator. scale_x: the scale of input vector. len_x: the length of input vector. omega (w) [1 x N]: the inner linear transformation. phi (p) [1 x N]: the inner bias. a [N x 1]: the outer linear transormation. ''' self . s_x = 2 * scale_x self . omega = omega self . phi = phi self . a = a self . len_x = len_x self . config ( train = True , batch = 100 , noise = 0.0 ) def mapfunc ( self , x ): xu = np . expand_dims ( x , - 1 ) y1 = np . tensordot ( xu , self . omega , ( 2 , 0 )) y1 = np . cos ( y1 + np . tensordot ( np . ones_like ( xu ), self . phi , ( 2 , 0 ))) y2 = np . squeeze ( np . tensordot ( y1 , self . a , ( 2 , 0 )), axis =- 1 ) y2 = np . fft . rfft ( y2 ) y_r = np . expand_dims ( np . real ( y2 ), - 1 ) y_i = np . expand_dims ( np . imag ( y2 ), - 1 ) y = np . concatenate ([ y_r , y_i ], axis =- 1 ) return y def next_train ( self ): ''' Get the next train batch: (x, y) ''' x = self . next_test () y = self . mapfunc ( x ) return x , y 我们新定义的这个数据生成器，与以往的一个不同在于，其定义了 mapfunc 方法；而产生训练数据的原理，是用 mapfunc 将产生的测试数据（只有输入 \\mathbf{x} \\mathbf{x} ）映射到输出 \\mathbf{Y} \\mathbf{Y} 。这个数据集可以通过迭代不断产生随机数据，也可以通过 mapfunc 来将任意给定的向量 \\mathbf{x} \\mathbf{x} 转换成 (6) (6) 定义的频域输出 \\hat{\\mathbf{Y}} \\hat{\\mathbf{Y}} 。 特别地，这里计算矩阵的时候，不使用 np.matmul 而是 np.tensordot ，和我们为第一层定义的时候使用 tf.tensordot 的原因相同。该函数支持对两个高维度的张量，取其中的两个维度分别计算矩阵乘法。 接下来测试数据集的输出效果 dparser.py 1 2 3 4 5 6 7 8 9 10 11 def test_dataset (): omega = 3 * np . random . random ([ 1 , 12 ]) phi = 2 * np . random . random ([ 1 , 12 ]) a = np . random . normal ( 0 , 1 , [ 12 , 1 ]) dataSet = TestDataFFTSet ( 1 , 10 , omega , phi , a ) dIter = iter ( dataSet ) for i in range ( 10 ): x , y = next ( dIter ) print ( y . shape , np . abs ( y [ 0 , ... , 0 ] + 1j * y [ 0 , ... , 1 ])) test_dataset () Output ( 100 , 6 , 2 ) [ 14 .29735734 9 .75982541 4 .73928941 6 .83158726 5 .16604991 0 .66256222 ] ( 100 , 6 , 2 ) [ 9 .32749042 5 .68157606 4 .86524786 11 .17296633 6 .37133611 7 .68757874 ] ( 100 , 6 , 2 ) [ 3 .97163879 9 .35646167 1 .43682653 4 .49838507 7 .39721013 4 .34096144 ] ( 100 , 6 , 2 ) [ 6 .56389599 5 .59993345 8 .19466732 0 .72835593 5 .01080391 9 .0552016 ] ( 100 , 6 , 2 ) [ 9 .71065508 3 .08608948 8 .72857359 9 .47081321 4 .87269945 7 .02108589 ] ( 100 , 6 , 2 ) [ 6 .05645098 6 .05961698 2 .98397442 8 .83888829 2 .91282992 5 .07843238 ] ( 100 , 6 , 2 ) [ 5 .11452286 1 .34310476 4 .15953687 3 .43588933 1 .7484992 0 .21387424 ] ( 100 , 6 , 2 ) [ 0 .63167972 8 .34622626 6 .21582338 5 .01146157 1 .50978382 1 .18373357 ] ( 100 , 6 , 2 ) [ 5 .88872479 6 .18109798 6 .97300166 4 .48064652 8 .13842369 6 .01989667 ] ( 100 , 6 , 2 ) [ 1 .28678976 3 .08831315 5 .3226707 0 .86784854 7 .83722167 0 .98692777 ] 我们产生的数据长度为10，参数的长度为12，我们在测试代码中，显示每次生成batch中，第一个样本的频谱强度。测试结果显示，频谱强度分布较为合理，且FFT后的数据长度为6=10/2+1，符合我们的预期。","text_tokens":["s","（","16","定义","*","py","y2","而是","05961698","的","phi","}","1j","25","9","(","重新","dparser","y1","测试数据","abs","18109798","=","不断","xu","38","32749042","34622626","自","5","the","方法","下来","^","分类","set","shape","两个","37","98397442","分别","35646167","只有","转换成","02108589","32","长度","2","3226707","21387424","不同","相同","来","87269945","01080391","26","27","分布","向量","36","产生","86784854","，","仍然","和","get","随机","一层","diter","0.0","23","类","59993345","换成","97163879","4","56389599","batch","real","testdatafftset","[","接下","88872479","输入","16604991","100","张量","cos","训练","+","regression","return","生成器","11","时候","n","43682653","21","98692777","成器","输出","noise","原因","05645098","t","用","6","一个","a","7","output","in","generator","8","83722167","22","w",")","29","model","01146157","outer","transormation","_","0552016","继承","一节","效果","对","r","代码","样本","19466732","在","dims","66256222","28","75982541","这里","24","68157606","i","97300166","每次","dataset","range","取","91282992","y","p","]","vector","第一个","原理",",","通过","接下来","中","68757874","random","地","...","init","其","且","生成","不","print","新","可以","of","matmul","next","33","48064652","86524786","：","data","30","强度","test","config","18373357","显示","结果显示","testing","给定","计算","集","14","def","__","rfft","特别","测试代码","concatenate","13","任意","length","为","；","-","testdataset","len","34096144","'","1","like","mapfunc","11452286","ones","transformation","该","符合","3","71065508","47081321","08831315","而","将","72835593","28678976","squeeze","15953687","for","fft","使用","35","imag","self","了"," ","0","39721013","normal","以往","tf","17296633","17","乘法","class","tensordot","第一","50978382","自动","/","73928941","到","\\","我们","10","数据","15","72857359","这个","预期","里","34310476","31","）","函数","83888829","13842369","expand","43588933","频域","结果","合理","37133611","。","linear","是","non","支持","x","19","34","bias","矩阵","参数","63167972","也","测试","转换","线性","inner","21582338","scale","83158726",":","维度","07843238","18","7484992","第一节","mathbf","在于","映射","input","01989667","true","omega",".","hat","迭代","29735734","08608948","np","12","高","20","后","{","较为","频谱","train","axis","第一层","与","其中","39","49838507","initialize","iter"],"title":"数据生成","title_tokens":["生成","数据"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_18","text":"与 上一节 相似，我们在本节使用的仍然是回归模型，因此，在主程序部分的代码改动不大。我们定义新的类 class NonLinRegHandle ( ext . AdvNetworkBase ): ，其中核心部分（构造方法）的代码如下： class NonLinRegHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def construct ( self ): ''' Construct a linear model and set the optimizer as Adam ''' # Construction input = tf . keras . Input ( shape = ( INPUT_SHAPE ,), dtype = tf . float32 ) upAff = ext . UpDimAffine ( PARAMS_SHAPE , use_bias = True , kernel_initializer = tf . keras . initializers . RandomUniform ( minval = 0.0 , maxval = 3.0 ), bias_initializer = tf . keras . initializers . RandomUniform ( minval = 0.0 , maxval = 2.0 ), kernel_constraint = tf . keras . constraints . NonNeg (), bias_constraint = tf . keras . constraints . NonNeg (), activation = tf . math . cos , name = 'up_dim_affine' )( input ) dnAff = ext . FFTAffine ( name = 'fft_affine' )( upAff ) self . model = tf . keras . Model ( inputs = input , outputs = dnAff ) # Set optimizer self . model . compile ( optimizer = self . optimizer ( self . optimizerName , self . lr ), loss = tf . keras . losses . mean_squared_error , metrics = [ self . relation ] ) self . model . summary () 在这个模型中，除了输入层以外，其余的两层都分别是我们自定义的层。第一个层内有参数 \\boldsymbol{\\omega},~\\boldsymbol{\\phi} \\boldsymbol{\\omega},~\\boldsymbol{\\phi} ，我们对这两个参数均加上了必须为正数的严格限制条件，同时对 \\boldsymbol{\\omega} \\boldsymbol{\\omega} ，我们使用均匀分布 U(0.0,~3.0) U(0.0,~3.0) 对其初始化，对 \\boldsymbol{\\phi} \\boldsymbol{\\phi} ，我们使用均匀分布 U(0.0,~2.0) U(0.0,~2.0) 对其初始化。在该层的最后，使用 \\cos(\\cdot) \\cos(\\cdot) 函数作为激活函数。 第二层内有参数 \\mathbf{a} \\mathbf{a} ，我们直接使用默认的初始化器来对其初始化。 实际测试的过程中，我们发现上一节定义的 相关系数 仍有缺陷。具体体现在，当两个被对比的向量中任何一个向量的某一维度的样本分布在方差为0时，分母 \\sigma_1^{(i)} \\sigma_2^{(i)} = 0 \\sigma_1^{(i)} \\sigma_2^{(i)} = 0 （其中 i i 表示向量的某一维度），从而导致该系数无法计算出结果。故而，我们考虑对其修正，在计算各维度相关系数的平均值时，排除掉那些无法计算相关系数的维度，改进后的代码如下： class NonLinRegHandle 1 2 3 4 5 6 7 8 9 10 @staticmethod def relation ( y_true , y_pred ): m_y_true = tf . keras . backend . mean ( y_true , axis = 0 ) m_y_pred = tf . keras . backend . mean ( y_pred , axis = 0 ) s_y_true = tf . keras . backend . sqrt ( tf . keras . backend . mean ( tf . keras . backend . square ( y_true ), axis = 0 ) - tf . keras . backend . square ( m_y_true )) s_y_pred = tf . keras . backend . sqrt ( tf . keras . backend . mean ( tf . keras . backend . square ( y_pred ), axis = 0 ) - tf . keras . backend . square ( m_y_pred )) s_denom = s_y_true * s_y_pred s_numer = tf . keras . backend . mean ( y_true * y_pred , axis = 0 ) - m_y_true * m_y_pred s_index = tf . keras . backend . greater ( s_denom , 0 ) return tf . keras . backend . mean ( tf . boolean_mask ( s_numer , s_index ) / tf . boolean_mask ( s_denom , s_index ))","text_tokens":["s","tf","maxval","（","17","constraints","y","class","相关","无法","其余","第一","16","as","定义","0.0","从而","改进","23","]","自定义","第一个","类","改动","严格","构造方法","math","metrics","*",",","compile","/","表示","\\","默认","4","我们","均匀","的","error","squared","phi","}","fftaffine","~","relation","直接","10","nonlinreghandle","params","[","中","体现","对比","15","因此","name","9","输入","加上","randomuniform","sqrt","程序","(","这个","cos","任何","dtype","相关系数","pred","必须","return","均值","其","11","@","2.0","staticmethod","不","）","同时","函数","第二","boolean","=","dim","作为","结果","m","。","以外","21","实际","lr","部分","optimizername","backend","construction","这","：","初始化","模型","linear","过程","关系","修正","是","u","affine","各","kernel","mean","发现","两层","有","19","6","bias","仍","mask","一个","a","5","7","the","up","最后","方差","计算","故而","掉","那些","均匀分布","upaff","一","14","8","def","方法","nonneg","^","and","float32","层","参数","set","shape","use","22","construct","测试","两个",")","adam","回归","13","model","loss","核心","分别","keras","平均值","系数","denom","为","主程序","激活","被","-",":","正数","维度","18","考虑","advnetworkbase","losses","层内","大","boldsymbol","2","'","初始","_","1","mathbf","index","定义新","一节","input","updimaffine","inputs","optimizer","summary","initializers","除了","true","ext","对","sigma","outputs","omega","代码",".","样本","greater","#","限制","具体","在","出","构造","相似","该","initializer","3","都","第二层","时","本节","12","20","后","如下","参数均","当","自定","导致","本分","分布","上","24","{","向量","i","fft","使用","分母","平均","缺陷","排除","numer","axis","cdot","，","constraint","dnaff","仍然","内有","二层","self","与","条件","器来","其中","了","样本分布","minval","某"," ","square","0","activation","3.0"],"title":"定义类模型","title_tokens":["定义","模型","类"]},{"location":"book-1-x/chapter-1/nonlinear-regression/#_19","text":"在调试阶段，我们采用随机生成的参数作为真值。其中， \\boldsymbol{\\omega} \\in U(0.0,~3.0) \\boldsymbol{\\omega} \\in U(0.0,~3.0) , \\boldsymbol{\\phi} \\in U(0.0,~2.0) \\boldsymbol{\\phi} \\in U(0.0,~2.0) ， \\mathbf{a} \\in N(0.0,~1.0) \\mathbf{a} \\in N(0.0,~1.0) 。然后，我们生成大量的 (\\mathbf{x},~\\mathbf{Y}) (\\mathbf{x},~\\mathbf{Y}) ，其中 \\mathbf{x} \\in U(-3.0,~3.0) \\mathbf{x} \\in U(-3.0,~3.0) 。注意在这个问题里，模型的输入输出向量是等长的，参数的长度不影响输出向量的长度。我们将参数的长度固定为10个元素，并定义如下函数 class NonLinRegHandle 1 2 3 4 5 6 7 8 9 10 def groupSort ( * params ): sortind = np . argsort ( params [ 0 ]) . flatten () res = [] for p in params : if p . shape [ 0 ] > p . shape [ 1 ]: p = p [ sortind , :] else : p = p [:, sortind ] res . append ( p ) return res 该函数用于对一组相同长度的向量进行排序，这些向量不拘于行向量或列向量。排序的标准是第一个参数向量从小到大的顺序。定义该函数是为了修整我们的输出结果。在上文理论部分，我们已经说明，对于一组解，交换任意两个维度的值，不影响模型的效果。因此我们通过对预测值和真值分别进行排序，来评估两组解之间的差异程度。 class NonLinRegHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # Initialization omega = 3 * np . random . random ([ 1 , PARAMS_SHAPE ]) phi = 2 * np . random . random ([ 1 , PARAMS_SHAPE ]) a = np . random . normal ( 0 , 1 , [ PARAMS_SHAPE , 1 ]) dataSet = dp . TestDataFFTSet ( 3 , args . xLength , omega , phi , a ) # Generate a group of testing samples. if args . seed is not None : setSeed ( args . seed + 1000 ) dataSet . config ( batch = args . testBatchNum ) x , y = next ( dataSet ) # Set the data set for training. dataSet . config ( batch = args . trainBatchNum ) # Construct the model and train it. h = NonLinRegHandle ( xLength = args . xLength , learningRate = args . learningRate , epoch = args . epoch , steppe = args . steppe , optimizerName = args . optimizer ) h . construct () print ( 'Begin to train:' ) print ( '---------------' ) record = h . train ( iter ( dataSet )) # Generate a group of testing samples: dataSet . config ( batch = args . testBatchNum ) x2 = np . reshape ( np . linspace ( - 3 , 3 , args . xLength ), [ 1 , args . xLength ]) y2 = dataSet . mapfunc ( x2 ) x = np . concatenate ([ x , x2 ], axis = 0 ) y = np . concatenate ([ y , y2 ], axis = 0 ) # Check the testing results print ( 'Begin to test:' ) print ( '---------------' ) yp , loss_p , corr_p = h . test ( x , y ) # Check the regressed values w , p = h . model . get_layer ( name = 'up_dim_affine' ) . get_weights () b = h . model . get_layer ( name = 'fft_affine' ) . get_weights ()[ 0 ] # Resort data w , b , p = groupSort ( w , b , p ) # The solution omega , phi , a = groupSort ( omega , phi , a ) # The ground truth # Save if args . outputData is not None : np . savez_compressed ( args . outputData , epoch = record . epoch , loss = record . history [ 'loss' ], corr = record . history [ 'relation' ], test_x = x , test_y = y , pred_y = yp , pred_loss = loss_p , pred_corr = corr_p , W = w , p = p , b = b , omega = omega , phi = phi , a = a ) Output _________________________________________________________________ Layer ( type ) Output Shape Param # ================================================================= input_1 ( InputLayer ) ( None, 100 ) 0 _________________________________________________________________ up_dim_affine ( UpDimAffine ) ( None, 100 , 10 ) 20 _________________________________________________________________ fft_affine ( FFTAffine ) ( None, 51 , 2 ) 10 ================================================================= Total params: 30 Trainable params: 30 Non-trainable params: 0 _________________________________________________________________ Begin to train: --------------- Epoch 1 /20 500 /500 [==============================] - 3s 5ms/step - loss: 6 .8965 - relation: 0 .9842 Epoch 2 /20 500 /500 [==============================] - 2s 4ms/step - loss: 0 .0027 - relation: 1 .0000 Epoch 3 /20 500 /500 [==============================] - 2s 4ms/step - loss: 0 .0018 - relation: 1 .0000 Epoch 4 /20 500 /500 [==============================] - 2s 4ms/step - loss: 0 .0016 - relation: 1 .0000 Epoch 5 /20 500 /500 [==============================] - 2s 4ms/step - loss: 0 .0014 - relation: 1 .0000 Epoch 6 /20 500 /500 [==============================] - 2s 4ms/step - loss: 0 .0012 - relation: 1 .0000 Epoch 7 /20 500 /500 [==============================] - 2s 4ms/step - loss: 0 .0010 - relation: 1 .0000 Epoch 8 /20 500 /500 [==============================] - 2s 4ms/step - loss: 9 .0556e-04 - relation: 1 .0000 Epoch 9 /20 500 /500 [==============================] - 2s 4ms/step - loss: 7 .9935e-04 - relation: 1 .0000 Epoch 10 /20 500 /500 [==============================] - 2s 4ms/step - loss: 7 .0898e-04 - relation: 1 .0000 Epoch 11 /20 500 /500 [==============================] - 2s 4ms/step - loss: 6 .3418e-04 - relation: 1 .0000 Epoch 12 /20 500 /500 [==============================] - 2s 4ms/step - loss: 5 .6936e-04 - relation: 1 .0000 Epoch 13 /20 500 /500 [==============================] - 2s 4ms/step - loss: 5 .1473e-04 - relation: 1 .0000 Epoch 14 /20 500 /500 [==============================] - 2s 4ms/step - loss: 4 .6677e-04 - relation: 1 .0000 Epoch 15 /20 500 /500 [==============================] - 2s 4ms/step - loss: 4 .2542e-04 - relation: 1 .0000 Epoch 16 /20 500 /500 [==============================] - 2s 4ms/step - loss: 3 .8777e-04 - relation: 1 .0000 Epoch 17 /20 500 /500 [==============================] - 2s 4ms/step - loss: 3 .5603e-04 - relation: 1 .0000 Epoch 18 /20 500 /500 [==============================] - 2s 4ms/step - loss: 3 .2665e-04 - relation: 1 .0000 Epoch 19 /20 500 /500 [==============================] - 2s 4ms/step - loss: 3 .0134e-04 - relation: 1 .0000 Epoch 20 /20 500 /500 [==============================] - 2s 4ms/step - loss: 2 .7940e-04 - relation: 1 .0000 Begin to test: --------------- 11 /11 [==============================] - 0s 6ms/sample - loss: 2 .4728e-04 - relation: 1 .0000 Evaluated loss ( losses.MeanSquaredError ) = 0 .0002472764754202217 Evaluated metric ( Pearson ' s correlation ) = 0 .9999991 在测试阶段，我们除了生成10组随机数据以外，还生成了一组从 [-3, 3] [-3, 3] 之间均匀增长的数据。这组数据与我们之前使用的随机数据分布不同，通过检测该数据的输出结果，我们可以验证我们拟合的这个参数模型是否具有一定的鲁棒性。 输入向量 \\mathbf{x} \\mathbf{x} 的长度不但影响输出 \\mathbf{Y} \\mathbf{Y} 的长度，也影响FFT的精度。因此，我们通过使用不同的向量长度分别进行测试，并对测试结果进行评估。调用测试的代码如下： python lin-reg.py -sd 1 -do test/ { length } -xl { length } 通过指派不同的向量长度 {length} ，将输出保存到不同的文件里，以绘制它们的对比效果图。首先，我们观察训练过程的记录情况 损失函数 (MSE) 测度函数 (相关系数) 可以看见，收敛的速度非常快。并且随着数据向量长度的增加，损失函数收敛到的值也增加。这是由于傅里叶变换的影响。我们使用的傅里叶变换是还没有标准化后的数据，因此，随着输入向量的增长，傅里叶变换的精度也提高，导致低频部分的数值明显变大，从而导致损失函数的收敛值增加。而相关系数显示，预测输出和真实值之间的线性相关性迅速趋近于1，印证该训练过程非常快。 注意 需要重申的是，我们计算相关系数是基于不同样本的统计情况来确定的。因此为了估计出准确的的相关系数，我们的batch需要有足够多的样本。显然，1个样本的batch是无法用来计算相关系数的。这里我们的batch含有32个样本。 接下来运行测试集检查结果。我们对预测的频谱和真实频谱之间求均方根误差(RMSE)，结果如下： 均方根误差 (RMSE) 由 (4) (4) 知，我们的模型本质上其实是一个对向量个元素独立运算的函数，亦即元素级的函数。因此， (4) (4) 可以被改写成 \\begin{align} y(x) = \\sum_{i=1}^N a_i \\cos ( \\omega_i x + \\varphi_i ). \\end{align} 如果我们输入一组向量，值在 [-3, 3] [-3, 3] 之内从小到大均匀增长，那么对应的取傅里叶变换的输出向量，可以看成是响应 x \\in (-3,~3) x \\in (-3,~3) 的频谱。我们在上述测试过程中，虽然使用了不同的向量长度 L L ，但生成的最后一个测试向量 \\mathbf{x} \\mathbf{x} 均是在 x \\in (-3,~3) x \\in (-3,~3) 均匀采样得到的。因此，不同的向量长度的测试结果，对应的是同以频谱不同精度下的计算结果。我们将训练好的模型输出的频谱和真实数据生成的频谱对比，得到以下结果： 频谱响应 幅值 相位 L=10 L=100 L=500 L=1000 可见，我们的回归到的模型输出的频谱和真值的模型完全一致。 问题 如果一个模型在频域上对一个信号的回归是精确的，是否在原域上（即时域）的回归也是精确的？ 正是如此。因为，考虑FFT的逆变换iFFT，作为一个线性变换，iFFT毫无疑问满足Lipschitz连续条件。这意味着，如果一个信号的在频域上的回归结果满足 \\lVert \\hat{\\mathbf{Y}} - \\mathbf{Y} \\rVert < \\varepsilon \\lVert \\hat{\\mathbf{Y}} - \\mathbf{Y} \\rVert < \\varepsilon ，则必有在时域上满足 \\lVert \\hat{\\mathbf{y}} - \\mathbf{y} \\rVert < C\\varepsilon \\lVert \\hat{\\mathbf{y}} - \\mathbf{y} \\rVert < C\\varepsilon ，其中 C C 是一个有限的常量。 另一种思考方法是，iFFT和FFT互为逆变换，这说明这两者之间构成一一映射。如果两组信号的FFT相同，那么其对应的一一映射，iFFT，又或者说是原信号，是势必相同的。这意味着，如果一个模型能够在时域上回归到某组数据，那么其频域上也必然能回归到相同数据的频域表达，反之亦然。 最后，我们来观察三个参数向量的回归情况，比对不同测试回归到的参数向量和真值之间的差别，结果如下 \\boldsymbol{\\omega} \\boldsymbol{\\omega} \\boldsymbol{\\phi} \\boldsymbol{\\phi} \\mathbf{a} \\mathbf{a} 鉴于模型的解具有高度的不确定性，我们发现我们回归到的结果受到初始化值的影响非常严重。尽管我们的回归模型确实拟合出了原函数的特性，但回归到的参数却和真值有明显的区别。 本节虽然使用了一个高度不确定的、却又简单的非线性函数作为例子，但我们所希望传达的，主要有以下两个要点： 一个可以写成解析式的线性或非线性函数，可以轻易地被实现成Tensorflow-Keras模式下的可微模型。这种函数包括但不限于普通的 数学函数 （例如指数函数、三角函数、贝塞尔函数等）， 快速傅里叶变换 ， 离散余弦变换 ， 常规的线性代数操作 （例如行列式、特征值）， SVD分解 ，等等。这些函数全部都已经被Tensorflow实现出来，可以通过内置API任意组合。更重要的是，在本节中，我们没有定义任何求取导数、梯度的方法，因为上述的每一个Tensorflow内置API，都已经内置了解析级别的梯度的计算方法。因此，对于一些简单的非线性模型，用户可以完全不用关心反向传播的过程，而是合心定意在编写正向传播上。从某种程度上，这大大降低了求解非线性问题的难度。 本节重点揭示的，是如何优雅地完成一个自定义层。截至笔者写到目前为止(03/17/2019)，未见网络上有登载类似的、规范的教程。如果用户能习惯按照本节的方式，扩展Tensorflow-Keras API，会带来两大好处： 一些复杂的模块，例如Residual block，Inception block等，可以以封装好的形式利用起来，使得主程序的代码简洁干净； 使用和Keras源码一致的语言风格，确保我们编写的所有自定义API，都可以被Keras原生的存取工具（包括 to_json , save 等）正确地保存下来。 在后续的内容里，我们还会涉及自定义网络层的情况，但是我们就不会特别说明完整的定义流程。在本教程推进的过程中，我们会不断定义各种需要用到的网络层，从而不断丰富扩展模块 extension.py 的内容。到本教程结束的时候，我们期望能够建立一个对用户友好的、功能完善而又与Tensorflow-Keras源代码风格一致的扩展模块出来。这一模块将可以用来构建任何形式的Tensorflow工程。","text_tokens":["s","savez","（","傅里叶",">","值","16","定义","从而","用户","record","响应","干净","正确","9935e","虽然","lvert","5603e","亦然","*","res","有限","常量","期望","差别","0010","py","y2","而是","意在","更","正是如此","逆变","表达","统计","的","得到","反之亦然","phi","}","begin","48","if","25","非常","误差","name","9","多","原","包括","6ms","b","(","截至","调试","sortind","type","svd","文件","反向","不拘","2665e","compressed","登载","=","关心","pearson","合心","方式","交换","观察","不断","从小到大","需要","过程","这","关系","模型","具有","learningrate","sample","38","generate","layer","？","0014","meansquarederror","为止","5","the","solution","提高","save","方法","下来","41","and","^","写成","优雅","is","set","shape","流程","两个","37","必然","真值","这种","分别","1000","varphi","后续","被","排序","32","argsort","会","extension","长度","代数","boldsymbol","2","方根","笔者","一组","0s","地被","updimaffine","training","带来","或者说","上述","域","不同","含有","相同","来","构成","传播","输入输出","说明","区别","相位","本节","26","本质","导数","亦","确定","block","从","自定","27","分布","连续","向量","0016","36","组合","利用","增长","标准","迅速","这一","，","估计","难度","一致","如此","和","原域","丰富","44","get","在上文","随机","源代码","check","_________________________________________________________________","求解","功能","samples","history","首先","下","0.0","、","即","线性变换","23","拟合","印证","简洁","end","时域","又","写","正向","或列","简单","0556e","4","它们","batch","这组","预测值","逆变换","没有","testdatafftset","对比","[","变换","接下","尽管","输入","就","100","某组","程序","任何","cos","为了","变","训练","+","return","普通","内置","11","采样","时候","对于","常规","seed","n","绘制","xlength","各种","某种","预测","一些","21","复杂","初始化","起来","2s","之内","u","json","dp","输出","发现","工程","快速","2542e","有","存取","以","6","要点","sum","规范","一个","a","7","output","0898e","in","8","还","mse","并且","python","xl","结束","do","个","习惯","46","22","groupsort","construct","3418e","限于","鉴于","w",")","回归","29","model","loss","记录","损失","residual","it","resort","精度","主程序","即时","考虑","losses","param","initialization","correlation","初始","离散","_","差异","weights","两组","模式","建立","两大","用来","效果","毫无","效果图","除了","对","涉及","测度","两者之间","代码","样本","降低","在","成","评估","传达","毫无疑问","按照","未见","28","从小","step","2019","这里","推进","导致","24","i","揭示","检查","扩展","余弦","ground","增加","dataset","trainable","检测","不但","lin","特征值","l","用到","取","指数函数","y","必有","行向量","级别","基于","p","inputlayer","测试阶段","同以","风格","1.0","]","第一个","8965","友好","采用","所",",","能够","通过","接下来","40","append","truth","一定","独立","<","之前","均匀","解","无疑","则","前为","lipschitz","fftaffine","relation","修整","程度","yp","outputdata","reg","中","两者","全部","计算方法","工具","因此","random","地","确保","元素","to","pred","相关系数","封装","原函数","重申","none","其","align","情况","完全","ifft","还会","生成","reshape","不","print","互为","可以","steppe","精确","一一","作为","of","反之","重要","0134e","next","使得","9842","收敛","testbatchnum","部分","33","指派","但","data","：","30","疑问","严重","test","0018","固定","实现","鲁棒性","行列式","config","显示","49","testing","语言","0000","up","计算","这些","集","14","不确定性","def","正是","三角","由于","理论","特别","linspace","4728e","else","等","45","势必","却","轻易","concatenate","13","任意","keras","定性","length","为","进行","注意","03","；","-","完整","大","'","1","问题","某种程度","显然","mapfunc","7940e","足够","教程","构建","影响","分解","保存","#","4ms","模块","total","操作","04","数据分布","出","信号","该","3","都","求取","计算结果","组","c","将","而","好处","如何","由","其实","类似","上","所有","看成","for","fft","rmse","使用","35","42","满足","inception","塞尔","trainbatchnum","意味","条件","47","另","非线性","了","values","能"," ","如果","特征","0","normal","3.0","顺序","式","flatten","不断丰富","17","编写","并","class","相关","无法","或者","第一","看见","快","特性","自定义","1473e","好","tensorflow","列式","setseed","真实","例如","不会","/","数学","完善","到","\\","是否","h","我们","线性代数","对应","~","10","nonlinreghandle","params","数据","大大降低","15","然后","之间","运算","原生","这个","9999991","里","功能完善","已经","阶段","2.0","梯度","8777e","31","）","合心定","函数","高度","数值","dim","频域","结果","这是","可见","大量","。","确实","以外","级","等等","网络层","明显","optimizername","results","大大","not","是","non","affine","调用","内容","不用","51","幅值","6677e","x","19","34","regressed","出来","标准化","sd","3s","最后","rvert","指数","行列","5ms","例子","三角函数","比","参数","层","metric","500","趋近","43","意味着","确定性","也","测试","完全一致","形式","线性","运行","随着","相关性","贝塞尔","以下","用于","因为","系数","受到","evaluated","6936e","目前为止",":","维度","18","varepsilon","解析","0002472764754202217","group","mathbf","0027","希望","映射","input","改写","于","optimizer","但是","epoch","重点","omega","args","一种",".","hat","本","源码","np","每","验证","知","x2","均","0012","节","思考","12","网络","20","后","或","如下","速度","api","{","完成","长","准确","频谱","train","axis","三个","低频","那么","与","主要","其中","corr","39","可微","求均","iter","目前"],"title":"调试","title_tokens":["调试"]}]}