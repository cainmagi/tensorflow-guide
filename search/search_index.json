{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"扉页 ¶ 摘要 Tensorflow总纲，写给初学者们。本页面将简述Tensorflow的基本原理，结构设计以及版本更新。读者亦可在本页找到本教程将提供给读者对Tensorflow怎样的认识。 注意 由于技术限制，现在中文搜索功能无法完善，请注意当搜索关键词的时候自行分词，例如： 开放源代码软件库 无法搜到任何内容。但可以搜索 开放 源代码 软件 库 即可确保索引到上述内容。 Tensorflow总说 ¶ Tensorflow官网 TensorFlow™ 是一个开放源代码软件库，用于进行高性能数值计算。借助其灵活的架构，用户可以轻松地将计算工作部署到多种平台（CPU、GPU、TPU）和设备（桌面设备、服务器集群、移动设备、边缘设备等）。TensorFlow™ 最初是由 Google Brain 团队（隶属于 Google 的 AI 部门）中的研究人员和工程师开发的，可为机器学习和深度学习提供强力支持，并且其灵活的数值计算核心广泛应用于许多其他科学领域。 Tensorflow治学 ¶ 写在所有内容之前，读者不得不看以下几个页面，本教程所述内容大略来自于对这些资料的研读。 Tensorflow官网(中/英文) : https://www.tensorflow.org/ Keras中文文档 : https://keras-zh.readthedocs.io/ Tensorflow 2.0前瞻(英文) : https://medium.com/tensorflow/effective-tensorflow-2-0-best-practices-and-whats-changed-a0ca48767aff Tensorflow 2.0前瞻(中文) : https://zhuanlan.zhihu.com/p/50049041 提示 特别值得注意的是，现在官方文档至少在教程部分， 已经支持中英双语 。相比官方文档，我们的教程更侧重于以搭建工程为导向的设计。然而现在官方文档的翔实可读程度，确实不可错过。另一方面，Keras的 官方中文文档 似乎已不再更新，上面提供的原官方中文文档译者现在正在更新、维护的版本。 大略来说，学习Tensorflow主要应当依赖于官方文档的介绍。敝人自r1.4版开始入门Tensorflow，即是通过学习官方文档的教程来快速上手。相比四处蒐集资料，官方文档能提供一个完整、系统、完全贴合时下最新API的例子，帮助用户建立一个与Tensorflow各个功能合宜的使用习惯。若是通过在Github上检索他人的project，固然是一个很好的上手办法，但是往往就会遗漏某些重要的功能而不学。例如，Tensorflow自带的网络存取和Tensorboard API，在时兴的一些project中，经常会因方便之故，以numpy的IO来代替。笔者以为，如此培养起来的使用习惯，可谓走入偏门。当然，从这一点来说，本教程也亦复如是，即使笔者写下这些内容的时候，参照的乃是时下最新的API，时间一久，难免会过时。倘若笔者懒惰一些，不再时时更新本教程，那么本教程也就入不足取之流了。 虽然如此，教程也有并不合宜的地方。最大的问题莫过于Tensorflow本身，它有着日新月异的变化，此时能圆转如意地使用的API，到了彼时也许就成了废案。这就导致Tensorflow的官方教程也不断更新。r1.4时，教程还主要集中在如何使用“底层”API上，到了r1.9.0，就已经变成基本围绕着 tf.keras 设计的思路了。最新消息显示，即将上线的Tensorflow 2.0，将会彻底抛弃过去的“中层”API以下的全部方法， tf.layers , tf.contrib 都将被移除，倘若长期墨守成规，局限在入门时候的一套技术上，迟早会被官方库如此迅速的更新所弃。这也是无可奈何之事。毕竟Tensorflow仍然处于不断改进之中。倘若希望追求一个更加稳定、长期可用的库，keras或许是一个不错的选择。 故而，本教程将基本基于Tensorflow现在的版本(r1.12)展开介绍。本教程既可以看作一个入门教程，也可以看作是本人重新自学新版Tensorflow规范化API的一个手记，故而命名本教程为“手札”。读者不妨跟着笔者的思路，有如与笔者共学一般读下来本教程。 Tensorflow原理 ¶ 一个标准的Tensorflow工作流可以表示成这样： graph TD st(开始) --> Sess[启动Session] Sess --> ConNet[构建网络] subgraph 构造流程 ioNet>读取网络参数] --> ConNet end subgraph 执行流程 ioDat>导入数据] --> Run ConNet --> Run[执行网络] Run --> ioNetS>保存网络参数] Run --> ioRes>导出结果] end ioRes --> SessCl[关闭Session] SessCl --> ed(结束) classDef styStart fill:#FAE6A9,stroke:#BA9132; classDef styIO fill:#cde498,stroke:#13540c; class st,ed styStart class ioNet,ioNetS,ioDat,ioRes styIO 与一般的计算库不同，Tensorflow的执行流程大体可以分为两步： 构造流程 : 在这一步，Tensorflow根据用户代码构造一个 数据流图(dataflow graph) 。所谓数据流图，指的是由一系列 张量(Tensor) 构成的符号运算集合。就如同一张流程图一样，在这一阶段，尽管用户定义了每一步的运算（从简单的加减乘除到复杂的网络单元），但是没有任何运算被执行。就像一个程序员撰写代码一样，Tensorflow在这一步，将用户的代码转换成它的“机器语言”，但是网络还没有进入被使用的阶段。 执行流程 : 在这一步，Tensorflow将计算用户指定的某个 Tensor 的输出结果。要得到一个Tensor的输出，则必须得计算它一系列的依赖变量。例如，我们已知 y = x_1 + x_2 y = x_1 + x_2 , x_2 = z_1 \\times z_2 x_2 = z_1 \\times z_2 。那么，如果Tensorflow要得到 y y 的结果，它就必须先计算 x_2 x_2 。这个过程被完全地封装起来，从用户看来，我们只需要调用 y y 的输出即可，不需要关心Tensorflow是怎样按照流图完成计算的。 因此，典型的Tensorflow式的语言风格也可以这样划分： 构造流程 : 用户定义整个网络的符号运算，指定网络各个节点的属性、输入和输出。这些代码往往被写成一个函数（例如 def construct (): ） 执行流程 : 创建一个Session，在Session内调用构造函数，然后输入数据，得到并保存输出结果。必要情况下，还需要导入导出网络参数。 这里提到 会话(Session) 。Session如同一个Tensorflow虚拟机，在一个Session打开的时候，设备的计算资源（GPU, CPU, 带宽等）才被加载。Session又如同一个工人，用户撰写的网络构造代码如同车间，输入的数据如同商品，“工人（Session）”利用预定义好的“车间（流图）”将可以快速、批量地生产这些“商品（数据）”。这样地设计保证了一些外围的代码，例如指定网络参数的名称、函数的属性等操作，不需要重复进行，从而确保处理数据的时候，总是执行必要的代码，加快运算速度。 另一方面，构造-执行的结构还封装了许多提高运算效率的特性，例如多线程。尽管用户定义网络构造的时候，所写的代码是按照逻辑顺序的、线性的。但实际运行的时候，Tensorflow可以自行发现网络的哪些部分可以被同时运行，从而利用多核系统的计算资源。这些过程也是被完全封装起来的，用户并不需要花费心思去专门进行这些调整。 须知 在Tensorflow已经转变为Keras导向的现在，Session的调用被Keras API封装了起来，从用户的角度来看，现在已经不再需要手动调用Session。 Tensorflow API架构 ¶ 下图显示了当前Tensorflow-API的组织形式 Tensorflow在多个平台上均有部署，包括Python, C++, Java, Javascript, Go等，未来还可能支持更多的语言。然而，不同平台上，API的使用方式和代码风格是大不相同的。例如，Python的语言风格目前已经朝向Keras转变，而Javascript仍倾向于使用中层API；C++和Java偏向于使用底层API编写程序。本教程只针对时兴Python的API编写，除非涉及到特定的专题，不会讨论其他语言上Tensorflow的用法。 一般而言，Tensorflow将API面向划分为三个层次： 高层API (High level) : 包括Estimators和Keras； 中层API (Mid level) : 包括layers, datasets, loss和metrics等具有功能性的函数，例如网络层的定义，Loss Function，对结果的测量函数等； 底层API (Low level) : 包括具体的加减乘除、具有解析式的数学函数、卷积、对Tensor属性的测量等。 从r1.4入门的用户，所接受的训练往往是从底层API开始，使用自己的代码风格构建对应中层API的函数，然后再用自己定义好的中层API构建网络。在这种代码风格的驱使下，用户除了需要定义各个网络层具体的表达式，还需要设计输入输出的接口，用来调用 优化器(optimizer) 的接口等操作。 另一种使用方式，是从中层API开始，直接使用预定义好的网络层构建网络，这样的代码风格会节省一部分时间，相当于几乎不再需要接触底层API，但是在输入输出、网络训练和测试等更高层的模块设计上，和底层API用户的代码风格相似。 Estimators是从r1.4就已经存在的API了，它可以被看成是已经集成在Tensorflow里的完整的网络。因此，Estimators往往适合分发、需求不高的应用，但是并不适合使用Tensorflow的研究者。尽管至今为止，Tensorflow仍然对Estimators倍加推崇，但本教程将完全不涉及这方面的内容。 另一个高层API是 Eager Execution ，从r1.9起，Tensorflow就加入了这种新的使用模式，并且在Tensorflow 2.0中，它仍然将会被保留。Eager摈弃了上述的构造-执行流程，任何对Tensor的定义会被立时计算、并可得到结果。对于需要进行小规模的调试、检查API功能的用户而言更加方便。但是，据一些用户的反响，现在Eager还没有做到和之前API的完全兼容，本教程将基本不涉及Eager的用法。 因此，Keras API将是本教程的重中之重。使用本教程的用户，可以对照目前的官方文档，跟进本教程的思路，自己逐步实现各个project的设计。使用Keras风格的设计，代码量会明显少于前面提到的各种风格（Estimators除外）。事实上， tf.keras 和底层API之间具有一定的兼容，这样的编写风格能让我们在满足自己设计的派生功能的基础上，尽可能使用规范化、预定义的API单元，从而减少代码出错的可能性。通过对本教程TF 1.x版的学习，对未来2.0版的上手也会带来莫大帮助。 金宇琛( @cainmagi )，2019年3月2日 Yuchen Jin( @cainmagi ), Mar. 2, 2019 教程导读 ¶ 接下来，本教程将会涉及 从线性问题入门 : 如何使用Tensorflow完成一个简单的线性分类设计，我们将从Hello World开始，逐步过渡到一个具有核函数的非线性问题project。 Hello world: 第一个Tensorflow程序。 线性分类: 一个简单的二分类问题。 线性回归: 一个同样简单的，线性拟合问题。 非线性回归: 拟合一个简单的，可以表达出解析式的非线性函数。 非线性分类: 使用线性分类器对非线性分布的数据进行分类。 训练与测试分立 : 从这一章开始，并试图解决几个更加复杂的实际问题。这里涉及到的project相对更大、更完整，训练往往需要一定时间才能完成。因此，在本章，所有项目的训练、测试环节都会分开，我们将从这里开始，使用TensorBoard跟踪我们的训练情况，并介绍如何存取神经网络。 Super Resolution: 使用神经网络进行图像像素插值。 Sparse Coding: 使用线性的词典学习(dictionary learning)来进行图像像素插值。 Generitive Model: 使用时兴的生成模型(Generitive Model)来完成图像风格的转换。 原生的数据管理 : 从这一章开始，我们将纳入Tensorflow自带的数据管理API。实际上，很多用户并不习惯使用这些API，往往倾向于自己完成数据的导入导出。虽然这并非一个复杂的工作，但据Tensorflow的文档，原生的数据管理内部实现了多线程，本身更适合用于提高数据IO的效率。读者可以酌情选择是否需要阅读本章。 使用MINIST数据集: 使用原生的数据管理来IO一个现成的MINIST数据集，这一节改自 官方文档教程 。 使用RNN处理文章: 通过LSTM模型来进行文本分类，并使用原生的数据管理来IO一个现成的IMDB数据集，这一节改自 官方文档教程 。 使用RBM生成音频: 通过限制玻尔兹曼机(RBM)来编写一个简单的音乐生成器，这一节改自 llSourcell/Music_Generator_Demo 。 其他的高级技巧 : 本章将介绍一些特殊的、利用底层API技巧，使得用户能更灵活地定制Tensorflow-keras，从而实现一些仅依靠原生库不能实现的功能。","text_tokens":["需求","服务","基于","数值","让","对本","金宇琛","一张","fill","推崇","多核","cainmagi","这些","model","稳定","+","一个","集群","方式","编写","高级","2.0","跟进","均","事实","阶段","resolution","英文","optimizer","学习","前面","资料","移动","来自","项目","义好","读者","zhuanlan",":","迅速","希望","编写程序","自己","线程","跟着","dataflow","所有","应当","语言","仍","词典","这方面","ed","；","文章","组织","，","设计","时","无可","彼时","不高","另","不足","摈弃","keras","显示","表达","可用","像","纳入","侧重于","翔实","风格","流程图","运算","广泛应用","三个","图像","系列","跟踪","最初","花费","服务器","开发","下来","不同","选择","rnn","几个","工作","走入","集合","zhihu","新版","日","四处","estimators","zh","团队","源代码","-","难免","地方","sparse","读","确保","属于","桌面","月","预定","亦复如是","对于","x","只","过于","式","但据","使得","sesscl","然后","之中","节省","网络","不会","得到","结束","datasets","文本","一步","以为","stroke","明显","莫大","将会","level","先","成规","一样","eager","更","培养","除外","已知","上","又","12","测试","机器","session","或许","加减乘除","这里","此时","角度","function",".","接口","许多","分类","相当于","和","更新","他人","据","模块","成器","/","必要","。","不得不","侧重","打开","检索","从这一点","构建","创建","第一个","难免会","一些","https","相同","2019","两步","构设","贴合","要","效率","解决","=","不能","新","起","网络单元","变化","按照","参照","强力","人","关心","商品","形式","办法","隶属","具体","导入","这种","由","不妨","教程","方法","tensor","不可","接触","逻辑","多种","摘要","借助","所谓","函数","所弃","coding","java","原","管理","定制","复杂","乃是","表示","and","库","其","可谓","新月","建立","一套","版","是否","不合宜","会","现成","逐步","输入输出","范化","变量","基本原理","神经网","r1","技术","subgraph","完整","一系","时间","数据管理","ionet","除非","z","才能","尽可","没有","哪些","玻尔兹曼","适合","以及","快速","日新月异","广泛","功能性","完善","不断改进","总纲","重要","“","围绕","加快","过去","扉页","jin","目前","各个","导出","执行","接受","集成","还","重中之重","事实上","莫过于","学","车间","metrics","介绍","并不需要","提供","时兴","可能","赖于","这样","结果","1","处理","最新","一方面","构造","generitive","思路","某些","废案","领域","3","总说","最新消息","至少","划分","sess","习惯","部分","由于","内部","generator","例子","）","量会",",","给","并","少于","org","通过","重于","必须","world","功能","倍加","试图","不断","llsourcell","请","而","构造函数","封装","一般而言","被","music","毕竟","cpu","整个","学者","出","来说","关闭","一系列","守成","核心","tpu","io","不得","应用","速度","]","才","那么","研读","对照","批量","兹曼","写成","本页","当于","相比","专门","api","可能性","iodat","简单","即可","前瞻","9.0","数据","像素","brain","单元","用户","写给","里"," ","execution","特殊","撰写","内容","从而","墨守","如果","更加","内","如同","为止","带来","拟合","13540c","一定","mid","迟早","达式","既","机","结构设计","虚拟机","除了","用于","生成器","下","数据流","神经","看成","完成","super","高层","节点","局限","styio","是","手动","等","启动","用法","一","经常","对","同时","几乎","手札","本原","原理","尽管","朝向","这","加载","写","都","c++","似乎","莫过","tensorboard","能圆转","tf","好","connet","分类器","入","并且","无法","end","原生","这方","初学者","p","因此","器","ai","一种","奈何","网络层","调试","灵活","分为","关键词","涉及","所写","提示","存在","high","以","一章","所述","不","ba9132","模式","音频",";","英双语","我们","对应","神经网络","各种","久","虽然","未来","中文搜索","资源","技巧","大体","分布","某个","完全","一部分","生成","固然","根据","规模","反响","深度","当","心思","effective","文档","依赖于","不合","展开","懒惰","_","插值","仅","不足取","研究","成","大不相同","分开","关键","隶属于","彻底","加减","提高","派生","但本","当前","起来","顺序","所","用来","已经","即使","现在","导向","变成","得","\\","重复","loss","无可奈何","保存","a0ca48767aff","javascript","敝","保留","搭建","第一","主要","图","使用","多","很多","会因","一方","代替","于","手","治学","移除","特性","倾向","官网","参数","利用","平台","中层","事","相对","入门教程","fae6a9","之前","满足","一点","50049041","多线程","[","run","上述","如意","集中","指","在","特别","y","最大","帮助","readthedocs","lstm","生产","故而","转换","设备","changed","每","google","环节","实际","至今","medium","去","尽可能","定义","（","自行","层次","双语","一部","维护","程序员","阅读","进行","带宽","一节","输入","消息","二","导致","减少","初学","class","™","程度","到","迟早会",">","小规模","看作","保证","当然","立时","rbm","有着","过时","一般","过渡","了","值得","流图","本身","很","minist","找到","地","接下来","注意","笔者","yuchen","mar","0","之故",")","底层","改","音乐","性能","部署","译者","td","classdef","另一方","能","来","玻尔","自","往往","将","支持","再用","其他","驱使","回归","go","www","本章","iores","graph","重新","规范化","存取","流程","测量","墨守成规","whats","tensorflow","发现","虚拟","长期","的","确实","开放","倘若","换成","工程师","抛弃","特定","而言","搜","2","也许","cde498","架构","入门","依赖","¶","讨论","高性","gpu","搜索","手记","边缘","属性","数学","是从","分立","但","看","加入","核","年","开始","版本","、","即","：","com","命名","限制","然而","乘除","调整","可以","任何","若","中文","接下","软件","运行","面向","相似","高性能","不再","进入","索引","#","写下","专题","numpy","自带","它","之","训练","方便","务器","输出","集","project","已","不断更新","本人","如此","如何","改进","工程","非线性","仍然","研究者","practices","张量","外围","分发","需要","值得注意","hello","酌情","人员","兼容","导读","实际上","def","自学","转变","共","操作","@","着","low","认识","线性","上面","(","名称","过程","代码","4","与","ionets","例如","基础","基本","调用","情况","不学","包括","模型","则","时时","偏门","中","官方","偏向","以下","dictionary","须知","来看","检查","分词","蒐集","之间","即将","符号","但是","结构","有","转换成","这个","具有","正在","可","也","上线","imdb","从","stystart","部门","为","如是","全部","不错","计算资源","相当","多线","看来","下图","计算","构成","直接","layers","优化","learning","时候","就","追求","多个","们","并非","依靠","处于","规范","大略","轻松","解析","另一方面","”","contrib","python","会话","系统","9","做到","github","st","科学","错过","best","遗漏","construct","义","出错","典型","同样","本","针对","之流","读取","demo","合宜","工人","机器语言","times","问题","提到","更大","简述","怎样","流","卷积","实现","程序","表达式","足取","墨守成","方面","页面","有如","总是","亦可","标准","指定"],"title":"扉页","title_tokens":["扉页"]},{"location":"#_1","text":"摘要 Tensorflow总纲，写给初学者们。本页面将简述Tensorflow的基本原理，结构设计以及版本更新。读者亦可在本页找到本教程将提供给读者对Tensorflow怎样的认识。 注意 由于技术限制，现在中文搜索功能无法完善，请注意当搜索关键词的时候自行分词，例如： 开放源代码软件库 无法搜到任何内容。但可以搜索 开放 源代码 软件 库 即可确保索引到上述内容。","text_tokens":["，","中文搜索","设计","们","代码","即可","索引","例如","tensorflow","基本","给","软件","对","写给","的"," ","本原","原理","初学","内容","开放","功能","到","当","请","教程","分词","搜","基本原理","提供","上述","无法","技术","结构","初学者","找到","结构设计","摘要","本","学者","在","注意","中文","搜索","关键","自行","但","关键词","读者","简述","版本","怎样","现在","以及","更新","：","源代码","将","。","认识","限制","库","页面","本页","由于","完善","任何","可以","总纲","确保","时候","构设","亦可"],"title":"扉页","title_tokens":["扉页"]},{"location":"#tensorflow","text":"Tensorflow官网 TensorFlow™ 是一个开放源代码软件库，用于进行高性能数值计算。借助其灵活的架构，用户可以轻松地将计算工作部署到多种平台（CPU、GPU、TPU）和设备（桌面设备、服务器集群、移动设备、边缘设备等）。TensorFlow™ 最初是由 Google Brain 团队（隶属于 Google 的 AI 部门）中的研究人员和工程师开发的，可为机器学习和深度学习提供强力支持，并且其灵活的数值计算核心广泛应用于许多其他科学领域。","text_tokens":["等","服务","tensorflow","数值","的","开放","工程师","一个","集群","并且","架构","高性","gpu","ai","学习","边缘","灵活","移动","、","广泛","可以","软件","，","高性能","务器","深度","工程","提供","广泛应用","研究","人员","最初","开发","服务器","隶属于","工作","领域","团队","源代码","属于","桌面","代码","）","于","中","官网","平台","cpu","机器","可","核心","tpu","应用","许多","部门","设备","为","和","计算","google","。","（","brain","进行","强力","用户"," ","隶属","轻松","™","由","到","科学","地","多种","借助","用于","性能","部署","将","支持","库","其","其他","是"],"title":"Tensorflow总说","title_tokens":["总说","tensorflow"]},{"location":"#tensorflow_1","text":"写在所有内容之前，读者不得不看以下几个页面，本教程所述内容大略来自于对这些资料的研读。 Tensorflow官网(中/英文) : https://www.tensorflow.org/ Keras中文文档 : https://keras-zh.readthedocs.io/ Tensorflow 2.0前瞻(英文) : https://medium.com/tensorflow/effective-tensorflow-2-0-best-practices-and-whats-changed-a0ca48767aff Tensorflow 2.0前瞻(中文) : https://zhuanlan.zhihu.com/p/50049041 提示 特别值得注意的是，现在官方文档至少在教程部分， 已经支持中英双语 。相比官方文档，我们的教程更侧重于以搭建工程为导向的设计。然而现在官方文档的翔实可读程度，确实不可错过。另一方面，Keras的 官方中文文档 似乎已不再更新，上面提供的原官方中文文档译者现在正在更新、维护的版本。 大略来说，学习Tensorflow主要应当依赖于官方文档的介绍。敝人自r1.4版开始入门Tensorflow，即是通过学习官方文档的教程来快速上手。相比四处蒐集资料，官方文档能提供一个完整、系统、完全贴合时下最新API的例子，帮助用户建立一个与Tensorflow各个功能合宜的使用习惯。若是通过在Github上检索他人的project，固然是一个很好的上手办法，但是往往就会遗漏某些重要的功能而不学。例如，Tensorflow自带的网络存取和Tensorboard API，在时兴的一些project中，经常会因方便之故，以numpy的IO来代替。笔者以为，如此培养起来的使用习惯，可谓走入偏门。当然，从这一点来说，本教程也亦复如是，即使笔者写下这些内容的时候，参照的乃是时下最新的API，时间一久，难免会过时。倘若笔者懒惰一些，不再时时更新本教程，那么本教程也就入不足取之流了。 虽然如此，教程也有并不合宜的地方。最大的问题莫过于Tensorflow本身，它有着日新月异的变化，此时能圆转如意地使用的API，到了彼时也许就成了废案。这就导致Tensorflow的官方教程也不断更新。r1.4时，教程还主要集中在如何使用“底层”API上，到了r1.9.0，就已经变成基本围绕着 tf.keras 设计的思路了。最新消息显示，即将上线的Tensorflow 2.0，将会彻底抛弃过去的“中层”API以下的全部方法， tf.layers , tf.contrib 都将被移除，倘若长期墨守成规，局限在入门时候的一套技术上，迟早会被官方库如此迅速的更新所弃。这也是无可奈何之事。毕竟Tensorflow仍然处于不断改进之中。倘若希望追求一个更加稳定、长期可用的库，keras或许是一个不错的选择。 故而，本教程将基本基于Tensorflow现在的版本(r1.12)展开介绍。本教程既可以看作一个入门教程，也可以看作是本人重新自学新版Tensorflow规范化API的一个手记，故而命名本教程为“手札”。读者不妨跟着笔者的思路，有如与笔者共学一般读下来本教程。","text_tokens":["基于","这些","稳定","一个","2.0","英文","学习","资料","来自","读者","zhuanlan",":","迅速","希望","跟着","所有","应当","，","设计","时","无可","彼时","不足","keras","显示","可用","侧重于","翔实","下来","选择","几个","走入","zhihu","新版","四处","zh","-","难免","地方","读","亦复如是","过于","之中","网络","以为","将会","成规","更","培养","上","12","或许","此时",".","和","更新","他人","/","。","不得不","侧重","检索","从这一点","难免会","https","一些","贴合","变化","参照","人","办法","不妨","教程","方法","不可","所弃","原","乃是","and","库","可谓","新月","建立","一套","版","不合宜","会","范化","r1","技术","完整","时间","快速","日新月异","不断改进","重要","“","围绕","过去","各个","还","莫过于","学","介绍","提供","时兴","赖于","最新","一方面","思路","某些","废案","最新消息","至少","习惯","部分","例子",",","并","org","通过","重于","功能","不断","而","被","毕竟","来说","守成","io","不得","那么","研读","相比","api","前瞻","9.0","用户"," ","内容","墨守","更加","迟早","既","下","局限","是","一","经常","对","手札","这","写","都","似乎","莫过","tensorboard","能圆转","tf","好","入","p","奈何","提示","以","所述","英双语","我们","久","虽然","完全","固然","effective","文档","依赖于","不合","展开","懒惰","不足取","成","彻底","起来","已经","即使","现在","导向","变成","无可奈何","a0ca48767aff","敝","搭建","主要","使用","会因","一方","代替","于","手","移除","官网","中层","事","入门教程","之前","一点","50049041","如意","集中","在","特别","最大","帮助","readthedocs","故而","changed","medium","双语","维护","消息","导致","程度","到","迟早会","看作","当然","有着","过时","一般","了","值得","本身","很","地","注意","笔者","0","之故",")","底层","译者","另一方","能","来","自","往往","将","支持","www","重新","规范化","存取","墨守成规","whats","tensorflow","长期","的","确实","倘若","抛弃","2","也许","入门","依赖","手记","看","开始","版本","、","即","com","命名","然而","可以","若","中文","不再","写下","numpy","自带","它","之","方便","project","已","不断更新","本人","如此","如何","改进","工程","仍然","practices","值得注意","自学","共","着","上面","(","4","与","例如","基本","不学","时时","偏门","中","官方","以下","蒐集","即将","但是","有","正在","可","也","上线","为","如是","全部","不错","layers","就","时候","追求","处于","规范","大略","另一方面","”","contrib","系统","github","错过","best","遗漏","本","之流","合宜","问题","足取","墨守成","方面","页面","有如"],"title":"Tensorflow治学","title_tokens":["治学","tensorflow"]},{"location":"#tensorflow_2","text":"一个标准的Tensorflow工作流可以表示成这样： graph TD st(开始) --> Sess[启动Session] Sess --> ConNet[构建网络] subgraph 构造流程 ioNet>读取网络参数] --> ConNet end subgraph 执行流程 ioDat>导入数据] --> Run ConNet --> Run[执行网络] Run --> ioNetS>保存网络参数] Run --> ioRes>导出结果] end ioRes --> SessCl[关闭Session] SessCl --> ed(结束) classDef styStart fill:#FAE6A9,stroke:#BA9132; classDef styIO fill:#cde498,stroke:#13540c; class st,ed styStart class ioNet,ioNetS,ioDat,ioRes styIO 与一般的计算库不同，Tensorflow的执行流程大体可以分为两步： 构造流程 : 在这一步，Tensorflow根据用户代码构造一个 数据流图(dataflow graph) 。所谓数据流图，指的是由一系列 张量(Tensor) 构成的符号运算集合。就如同一张流程图一样，在这一阶段，尽管用户定义了每一步的运算（从简单的加减乘除到复杂的网络单元），但是没有任何运算被执行。就像一个程序员撰写代码一样，Tensorflow在这一步，将用户的代码转换成它的“机器语言”，但是网络还没有进入被使用的阶段。 执行流程 : 在这一步，Tensorflow将计算用户指定的某个 Tensor 的输出结果。要得到一个Tensor的输出，则必须得计算它一系列的依赖变量。例如，我们已知 y = x_1 + x_2 y = x_1 + x_2 , x_2 = z_1 \\times z_2 x_2 = z_1 \\times z_2 。那么，如果Tensorflow要得到 y y 的结果，它就必须先计算 x_2 x_2 。这个过程被完全地封装起来，从用户看来，我们只需要调用 y y 的输出即可，不需要关心Tensorflow是怎样按照流图完成计算的。 因此，典型的Tensorflow式的语言风格也可以这样划分： 构造流程 : 用户定义整个网络的符号运算，指定网络各个节点的属性、输入和输出。这些代码往往被写成一个函数（例如 def construct (): ） 执行流程 : 创建一个Session，在Session内调用构造函数，然后输入数据，得到并保存输出结果。必要情况下，还需要导入导出网络参数。 这里提到 会话(Session) 。Session如同一个Tensorflow虚拟机，在一个Session打开的时候，设备的计算资源（GPU, CPU, 带宽等）才被加载。Session又如同一个工人，用户撰写的网络构造代码如同车间，输入的数据如同商品，“工人（Session）”利用预定义好的“车间（流图）”将可以快速、批量地生产这些“商品（数据）”。这样地设计保证了一些外围的代码，例如指定网络参数的名称、函数的属性等操作，不需要重复进行，从而确保处理数据的时候，总是执行必要的代码，加快运算速度。 另一方面，构造-执行的结构还封装了许多提高运算效率的特性，例如多线程。尽管用户定义网络构造的时候，所写的代码是按照逻辑顺序的、线性的。但实际运行的时候，Tensorflow可以自行发现网络的哪些部分可以被同时运行，从而利用多核系统的计算资源。这些过程也是被完全封装起来的，用户并不需要花费心思去专门进行这些调整。 须知 在Tensorflow已经转变为Keras导向的现在，Session的调用被Keras API封装了起来，从用户的角度来看，现在已经不再需要手动调用Session。","text_tokens":["graph","流程","等","启动","tensorflow","发现","虚拟","一","同时","的","一张","fill","尽管","这","加载","多核","换成","这些","+","一个","2","变量","connet","cde498","阶段","subgraph","end","一系","依赖","因此","gpu","ionet","z","属性","分为","但","手动","没有","义好","哪些","所写","开始",":","线程","、","dataflow","不","：","快速","ba9132","语言","乘除",";","我们","调整","可以","任何","ed","标准","“","运行","加快","，","资源","设计","不再","大体","进入","#","某个","完全","它","根据","导出","各个","输出","执行","还","keras","心思","车间","像","并不需要","_","风格","张量","外围","流程图","这样","运算","需要","结果","系列","1","处理","花费","成","不同","加减","提高","一方面","构造","工作","集合","起来","顺序","def","已经","现在","转变","操作","得","\\","重复","导向","-","划分","sess","保存","部分","确保","线性","预定","(","名称","图","过程","x","代码","与","）","ionets","使用",",","例如","只","调用","式","一方","sesscl","然后","情况","并","网络","得到","则","特性","结束","必须","一步","stroke","参数","利用","须知","来看","构造函数","先","一样","api","封装","被","fae6a9","符号","cpu","整个","多线程","[","run","但是","已知","指","又","机器","session","在","加减乘除","y","结构","转换成","这个","这里","关闭","角度","一系列","也","许多","速度","]","才","从","stystart","转换","那么","设备","生产","和","计算资源","批量","多线","看来","每","为","计算","必要","。","构成","打开","写成","构建","创建","实际","去","定义","（","一些","专门","iodat","就","两步","简单","要","即可","时候","效率","=","自行","数据","网络单元","程序员","按照","带宽","单元","进行","输入","用户","关心","商品"," ","撰写","从而","class","如果","导入","内","由","到","”",">","如同","会话","保证","另一方面","tensor","系统","13540c","st","一般","了","流图","逻辑","construct","典型","地","虚拟机","读取","所谓","函数","工人","下","机器语言","times","提到",")","数据流","怎样","流","td","程序","classdef","另一方","复杂","往往","将","表示","库","完成","方面","节点","styio","总是","是","iores","指定"],"title":"Tensorflow原理","title_tokens":["原理","tensorflow"]},{"location":"#tensorflow-api","text":"下图显示了当前Tensorflow-API的组织形式 Tensorflow在多个平台上均有部署，包括Python, C++, Java, Javascript, Go等，未来还可能支持更多的语言。然而，不同平台上，API的使用方式和代码风格是大不相同的。例如，Python的语言风格目前已经朝向Keras转变，而Javascript仍倾向于使用中层API；C++和Java偏向于使用底层API编写程序。本教程只针对时兴Python的API编写，除非涉及到特定的专题，不会讨论其他语言上Tensorflow的用法。 一般而言，Tensorflow将API面向划分为三个层次： 高层API (High level) : 包括Estimators和Keras； 中层API (Mid level) : 包括layers, datasets, loss和metrics等具有功能性的函数，例如网络层的定义，Loss Function，对结果的测量函数等； 底层API (Low level) : 包括具体的加减乘除、具有解析式的数学函数、卷积、对Tensor属性的测量等。 从r1.4入门的用户，所接受的训练往往是从底层API开始，使用自己的代码风格构建对应中层API的函数，然后再用自己定义好的中层API构建网络。在这种代码风格的驱使下，用户除了需要定义各个网络层具体的表达式，还需要设计输入输出的接口，用来调用 优化器(optimizer) 的接口等操作。 另一种使用方式，是从中层API开始，直接使用预定义好的网络层构建网络，这样的代码风格会节省一部分时间，相当于几乎不再需要接触底层API，但是在输入输出、网络训练和测试等更高层的模块设计上，和底层API用户的代码风格相似。 Estimators是从r1.4就已经存在的API了，它可以被看成是已经集成在Tensorflow里的完整的网络。因此，Estimators往往适合分发、需求不高的应用，但是并不适合使用Tensorflow的研究者。尽管至今为止，Tensorflow仍然对Estimators倍加推崇，但本教程将完全不涉及这方面的内容。 另一个高层API是 Eager Execution ，从r1.9起，Tensorflow就加入了这种新的使用模式，并且在Tensorflow 2.0中，它仍然将会被保留。Eager摈弃了上述的构造-执行流程，任何对Tensor的定义会被立时计算、并可得到结果。对于需要进行小规模的调试、检查API功能的用户而言更加方便。但是，据一些用户的反响，现在Eager还没有做到和之前API的完全兼容，本教程将基本不涉及Eager的用法。 因此，Keras API将是本教程的重中之重。使用本教程的用户，可以对照目前的官方文档，跟进本教程的思路，自己逐步实现各个project的设计。使用Keras风格的设计，代码量会明显少于前面提到的各种风格（Estimators除外）。事实上， tf.keras 和底层API之间具有一定的兼容，这样的编写风格能让我们在满足自己设计的派生功能的基础上，尽可能使用规范化、预定义的API单元，从而减少代码出错的可能性。通过对本教程TF 1.x版的学习，对未来2.0版的上手也会带来莫大帮助。 金宇琛( @cainmagi )，2019年3月2日 Yuchen Jin( @cainmagi ), Mar. 2, 2019","text_tokens":["需求","让","对本","金宇琛","推崇","cainmagi","方式","一个","编写","2.0","跟进","均","事实","optimizer","前面","学习","义好",":","自己","编写程序","语言","仍","这方面","；","组织","，","设计","不高","另","摈弃","显示","keras","表达","风格","三个","不同","日","estimators","-","月","预定","对于","x","只","式","然后","节省","不会","网络","得到","datasets","明显","莫大","将会","level","eager","更","除外","上","测试","加减乘除","function",".","接口","相当于","和","据","模块","。","构建","相同","一些","2019","新","起","形式","具体","这种","教程","tensor","接触","函数","java","版","会","逐步","输入输出","范化","r1","完整","时间","除非","尽可","没有","适合","功能性","jin","目前","各个","接受","集成","执行","还","重中之重","事实上","metrics","时兴","可能","这样","结果","1","构造","思路","3","划分","部分","）","量会",",","并","少于","通过","功能","倍加","而","一般而言","被","应用","对照","当于","可能性","api","单元","用户","里"," ","execution","内容","从而","更加","为止","带来","一定","mid","达式","除了","下","看成","高层","是","等","用法","对","几乎","朝向","尽管","c++","tf","好","并且","这方","因此","器","一种","调试","网络层","涉及","存在","high","不","模式","我们","对应","各种","未来","一部分","完全","规模","反响","文档","研究","大不相同","加减","派生","但本","当前","所","用来","已经","现在","loss","javascript","保留","多","使用","于","手","倾向","平台","中层","之前","满足","上述","在","帮助","至今","尽可能","定义","（","层次","一部","进行","输入","减少","到","小规模","立时","一般","了","yuchen","mar",")","底层","部署","能","往往","将","支持","再用","其他","驱使","go","流程","测量","规范化","tensorflow","的","而言","特定","2","入门","讨论","属性","数学","是从","加入","年","开始","、","：","然而","乘除","可以","任何","面向","相似","不再","专题","它","训练","方便","输出","project","研究者","仍然","分发","需要","兼容","转变","操作","@","low","(","代码","4","例如","基础","调用","基本","包括","偏向","中","官方","检查","之间","但是","有","具有","可","也","从","为","相当","下图","计算","直接","layers","优化","就","多个","规范","解析","python","9","做到","义","出错","本","针对","提到","卷积","实现","程序","表达式","方面"],"title":"Tensorflow API架构","title_tokens":[" ","架构","tensorflow","api"]},{"location":"#_2","text":"接下来，本教程将会涉及 从线性问题入门 : 如何使用Tensorflow完成一个简单的线性分类设计，我们将从Hello World开始，逐步过渡到一个具有核函数的非线性问题project。 Hello world: 第一个Tensorflow程序。 线性分类: 一个简单的二分类问题。 线性回归: 一个同样简单的，线性拟合问题。 非线性回归: 拟合一个简单的，可以表达出解析式的非线性函数。 非线性分类: 使用线性分类器对非线性分布的数据进行分类。 训练与测试分立 : 从这一章开始，并试图解决几个更加复杂的实际问题。这里涉及到的project相对更大、更完整，训练往往需要一定时间才能完成。因此，在本章，所有项目的训练、测试环节都会分开，我们将从这里开始，使用TensorBoard跟踪我们的训练情况，并介绍如何存取神经网络。 Super Resolution: 使用神经网络进行图像像素插值。 Sparse Coding: 使用线性的词典学习(dictionary learning)来进行图像像素插值。 Generitive Model: 使用时兴的生成模型(Generitive Model)来完成图像风格的转换。 原生的数据管理 : 从这一章开始，我们将纳入Tensorflow自带的数据管理API。实际上，很多用户并不习惯使用这些API，往往倾向于自己完成数据的导入导出。虽然这并非一个复杂的工作，但据Tensorflow的文档，原生的数据管理内部实现了多线程，本身更适合用于提高数据IO的效率。读者可以酌情选择是否需要阅读本章。 使用MINIST数据集: 使用原生的数据管理来IO一个现成的MINIST数据集，这一节改自 官方文档教程 。 使用RNN处理文章: 通过LSTM模型来进行文本分类，并使用原生的数据管理来IO一个现成的IMDB数据集，这一节改自 官方文档教程 。 使用RBM生成音频: 通过限制玻尔兹曼机(RBM)来编写一个简单的音乐生成器，这一节改自 llSourcell/Music_Generator_Demo 。 其他的高级技巧 : 本章将介绍一些特殊的、利用底层API技巧，使得用户能更灵活地定制Tensorflow-keras，从而实现一些仅依靠原生库不能实现的功能。","text_tokens":["存取","tensorflow","对","是否","的","会","这","这些","model","现成","逐步","一个","都","编写","高级","tensorboard","分类器","神经网","resolution","入门","完整","原生","时间","因此","数据管理","才能","分立","学习","灵活","核","涉及","项目","玻尔兹曼","读者","适合",":","开始","自己","线程","、","一章","所有","不","音频","限制","词典","我们","接下","可以","神经网络","虽然","文章","，","技巧","设计","分布","生成","自带","训练","导出","集","project","表达","keras","如何","文档","非线性","介绍","纳入","插值","时兴","风格","_","仅","hello","需要","酌情","图像","跟踪","处理","rnn","下来","分开","提高","选择","几个","实际上","generitive","工作","-","习惯","sparse","内部","线性","第一","(","generator","使用","与","很多","式","但据","使得","情况","并","于","通过","网络","模型","倾向","官方","world","dictionary","文本","功能","llsourcell","试图","利用","将会","相对","music","更","多线程","测试","在","出","具有","这里","lstm","io","imdb","从","转换","分类","多线","兹曼","成器","环节","第一个","。","实际","/","learning","api","一些","简单","不能","解决","效率","数据","像素","并非","阅读","依靠","进行","一节","用户","二"," ","特殊","从而","更加","解析","导入","到","rbm","拟合","教程","一定","过渡","了","本身","minist","机","同样","地","本","接下来","函数","demo","用于","生成器","coding","更大","问题",")","底层","管理","改","音乐","实现","程序","神经","定制","能","来","玻尔","复杂","往往","将","自","完成","super","其他","库","回归","本章"],"title":"教程导读","title_tokens":["教程","导读"]},{"location":"licenses/","text":"协议 (Licenses) ¶ 本站协议 (中文版) ¶ MIT 开源许可协议 版权所有 © 2019, 金宇琛 (cainmagi) 特此向任何得到本软件副本或相关文档的人授权：被授权人有权使用、复制、修改、 合并、出版、发布、散布、再授权和/或贩售软件及软件的副本，及授予被供应人 同等权利，只需服从以下义务： 在软件和软件的所有副本中都必须包含以上版权声明和本许可声明。 该软件是\"按原样\"提供的，没有任何形式的明示或暗示，包括但不限于为特定目的和 不侵权的适销性和适用性的保证担保。在任何情况下，作者或版权持有人，都无权要求 任何索赔，或有关损害赔偿的其他责任。无论在本软件的使用上或其他买卖交易中， 是否涉及合同，侵权或其他行为。 License of this website (English version) ¶ MIT License Copyright © 2019 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. 本站相关项目的协议 ¶ 下面介绍的诸多协议，原则上并无必要列在本条目中（例如MIT License）。列在下侧，主要是为了向诸位支持本文档的开发者致以真诚的谢意。亦将荣耀与在下的感谢致以高天之上的上帝，愿你保守这份文档，成为众人的帮助。 License of Material ¶ MIT License Copyright © 2016 - 2019 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of MkDocs ¶ BSD 2-Clause \"Simplified\" License Copyright © 2014, Tom Christie. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. License of Jieba3K ¶ The MIT License (MIT) Copyright © 2013 Sun Junyi Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of Simple Lightbox ¶ The MIT License (MIT) Copyright © 2018 Damir Brekalo Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of MathJax ¶ Apache License 2.0 See the full license here: MathJax license License of mermaid ¶ The MIT License (MIT) Copyright © 2014 - 2018 Knut Sveidqvist Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["金宇琛","data","cainmagi","implied","junyi","limited","2.0","按","goods","项目","\"",":","该软件","所有","本站","contributors","，","文版","without","dealings","permission","infringement","开发","even","reproduce","with","advised","website","lightbox","reserved","-","distribute","subject","as","再","permitted","为了","只","得到","purpose","fitness","出版","上","列","众人","无论","无权",".","merchantability","so","jieba3k","damage","和","你","开发者","/","必要","。","开源","all","2019","claim","event","any","theory","副本","人","charge","形式","is","included","hereby","赔偿","需","shall","retain","真诚","in","english","sun","and","或","bsd","disclaimer","下面","from","是否","买卖","rights","tort","2014","of","包含","files","use","没有","person","that","action","whether","donath","损害","redistribution","发布","授权人","亦","jin","whom","mkdocs","mermaid","相关","适用","授予","particular","介绍","提供","materials","诸位","版权","full","持有","条目","must","无","non","授权","）",",","声明","permit","并","必须","担保","substitute","被","持有人","possibility","above","这份","©","适用性","warranty","合同","special","limitation","binary","martin","sveidqvist","谢意","有人","行为","权利"," ","code","holder","散布","中文版","modification","及","distribution","2016","forms","do","下","性","profits","notice","以上","form","the","是","成为","都","clause","substantial","negligence","要求","connection","涉及","direct","copies","保守","不","义务",";","mathjax","荣耀","software","原则","kind","文档","version","sublicense","interruption","not","obtaining","damir","sell","原样","上帝","原则上","however","license","明示","indirect","loss","apache","such","conditions","许可","向","主要","on","复制","使用","责任","no","感谢","exemplary","here","see","适销","an","simplified","在","2013","帮助","warranties","services","2018","（","愿","material","服从","arising","merge","索赔","consequential","保证","contract","damages","free","liable","下侧","yuchen","be","met",")","copy","有权","incidental","之上","作者","将","支持","其他","致以","provided","simple","following","christie","限于","的","特定","2","暗示","are","特此","portions","¶","但","、","otherwise","：","disclaimed","a","任何","中文","软件","同等","authors","caused","way","合并","高天","修改","granted","mit","associated","列在","贩售","供应","copyright","express","目的","to","brekalo","(","deal","knut","persons","与","例如","holders","情况","furnished","包括","but","中","以下","licenses","交易","other","procurement","publish","this","协议","为","诸多","out","版权所有","损害赔偿","liability","有关","including","noninfringement","modify","list","source","by","本","tom","business","侵权","restriction","strict","for","documentation","if","or","redistributions"],"title":"协议","title_tokens":["协议"]},{"location":"licenses/#licenses","text":"","text_tokens":[],"title":"协议 (Licenses)","title_tokens":["(","协议","licenses",")"," "]},{"location":"licenses/#_1","text":"MIT 开源许可协议 版权所有 © 2019, 金宇琛 (cainmagi) 特此向任何得到本软件副本或相关文档的人授权：被授权人有权使用、复制、修改、 合并、出版、发布、散布、再授权和/或贩售软件及软件的副本，及授予被供应人 同等权利，只需服从以下义务： 在软件和软件的所有副本中都必须包含以上版权声明和本许可声明。 该软件是\"按原样\"提供的，没有任何形式的明示或暗示，包括但不限于为特定目的和 不侵权的适销性和适用性的保证担保。在任何情况下，作者或版权持有人，都无权要求 任何索赔，或有关损害赔偿的其他责任。无论在本软件的使用上或其他买卖交易中， 是否涉及合同，侵权或其他行为。","text_tokens":["限于","是否","的","金宇琛","买卖","cainmagi","都","特定","暗示","特此","按","包含","要求","但","没有","涉及","\"","、","该软件","所有","：","不","义务","任何","损害","软件","发布","，","授权人","同等","相关","文档","授予","适用","提供","合并","版权","持有","修改","原样","mit","明示","贩售","供应","授权","再","许可","向","目的","(","复制","使用","只",",","声明","责任","情况","包括","得到","中","必须","以下","担保","出版","交易","适销","被","持有人","上","协议","在","©","适用性","无论","无权","合同","为","和","/","。","开源","2019","版权所有","损害赔偿","服从","有人","索赔","行为","副本","人","权利"," ","形式","有关","保证","散布","赔偿","需","本","及","下",")","性","有权","侵权","以上","作者","其他","或","是"],"title":"本站协议 (中文版)","title_tokens":["(","协议","本站","文版",")","中文"," ","中文版"]},{"location":"licenses/#license-of-this-website-english-version","text":"MIT License Copyright © 2019 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["following","from","cainmagi","implied","rights","limited","portions","tort","substantial","of","files","use","connection","or","person","\"",":","otherwise","copies","a","action","whether","jin","whom","software","without","authors","kind","dealings","permission","particular","sublicense","not","obtaining","sell","granted","mit","with","license","associated","distribute","subject","conditions","as","copyright","express","to","(","deal","persons",",","holders","permit","furnished","no","but","purpose","fitness","other","an","publish","this","above","©","warranty","warranties",".","merchantability","so","limitation","/","out","all","2019","claim","event","arising","merge","liability","any","charge"," ","is","included","including","noninfringement","hereby","contract","modify","damages","shall","free","in","liable","do","yuchen","be",")","copy","notice","restriction","for","and","the","documentation","provided"],"title":"License of this website (English version)","title_tokens":["this","(","website","of","version","english",")"," ","license"]},{"location":"licenses/#_2","text":"下面介绍的诸多协议，原则上并无必要列在本条目中（例如MIT License）。列在下侧，主要是为了向诸位支持本文档的开发者致以真诚的谢意。亦将荣耀与在下的感谢致以高天之上的上帝，愿你保守这份文档，成为众人的帮助。","text_tokens":["，","为了","亦","）","与","荣耀","例如","并","的"," ","感谢","原则","中","主要","成为","文档","介绍","诸位","真诚","协议","列","在","本","条目","下侧","开发","高天","无","谢意","这份","上帝","众人","原则上","下","帮助","mit","license","列在","之上","保守","你","诸多","开发者","必要","将","。","支持","致以","向","（","愿","是","下面"],"title":"本站相关项目的协议","title_tokens":["协议","本站","相关","项目","的"]},{"location":"licenses/#license-of-material","text":"MIT License Copyright © 2016 - 2019 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["following","from","implied","rights","limited","portions","tort","substantial","of","files","use","connection","or","person","\"",":","otherwise","copies","a","action","donath","whether","whom","software","without","authors","kind","dealings","permission","particular","sublicense","infringement","not","obtaining","sell","granted","mit","with","non","license","associated","-","distribute","subject","conditions","as","copyright","express","to","(","deal","persons",",","holders","permit","furnished","no","but","purpose","fitness","other","an","publish","this","above","©","warranty","warranties",".","merchantability","so","limitation","/","out","martin","all","2019","claim","event","arising","merge","liability","any","charge"," ","is","included","including","hereby","contract","modify","damages","shall","free","in","liable","2016","do","be",")","copy","notice","restriction","for","and","the","documentation","provided"],"title":"License of Material","title_tokens":["material"," ","license","of"]},{"location":"licenses/#license-of-mkdocs","text":"BSD 2-Clause \"Simplified\" License Copyright © 2014, Tom Christie. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","text_tokens":["following","christie","data","implied","2","rights","limited","are","tort","clause","goods","negligence","2014","of","use","or","\"",":","direct","otherwise","that","disclaimed","a","contributors",";","whether","redistribution","software","without","caused","particular","way","materials","interruption","not","must","even","reproduce","with","however","advised","license","reserved","indirect","-","loss","such","conditions","as","permitted","copyright","express","to","(","on",",","holders","no","but","exemplary","purpose","fitness","other","substitute","procurement","simplified","this","possibility","above","©","warranties",".","merchantability","damage","special","/","services","out","binary","all","event","arising","liability","any","consequential","theory"," ","is","code","including","holder","contract","list","damages","shall","retain","source","modification","by","in","liable","distribution","tom","forms","met","be",")","business","profits","notice","incidental","strict","for","form","and","the","documentation","if","provided","bsd","disclaimer","redistributions"],"title":"License of MkDocs","title_tokens":[" ","license","of","mkdocs"]},{"location":"licenses/#license-of-jieba3k","text":"The MIT License (MIT) Copyright © 2013 Sun Junyi Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["following","from","implied","junyi","rights","limited","portions","tort","substantial","of","files","use","connection","or","person","\"",":","otherwise","copies","a","action","whether","whom","software","without","authors","kind","dealings","permission","particular","sublicense","not","obtaining","sell","granted","mit","with","license","associated","distribute","subject","conditions","as","copyright","express","to","(","deal","persons",",","holders","permit","furnished","no","but","purpose","fitness","other","an","publish","this","above","©","2013","warranty","warranties",".","merchantability","so","limitation","/","out","all","claim","event","arising","merge","liability","any","charge"," ","is","included","including","noninfringement","hereby","contract","modify","damages","shall","free","in","liable","do","be",")","copy","notice","restriction","sun","for","and","the","documentation","provided"],"title":"License of Jieba3K","title_tokens":[" ","license","jieba3k","of"]},{"location":"licenses/#license-of-simple-lightbox","text":"The MIT License (MIT) Copyright © 2018 Damir Brekalo Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["following","from","implied","rights","limited","portions","tort","substantial","of","files","use","connection","or","person","\"",":","otherwise","copies","a","action","whether","whom","software","without","authors","kind","dealings","permission","particular","sublicense","not","damir","obtaining","sell","granted","mit","with","license","associated","distribute","subject","conditions","as","copyright","express","to","brekalo","(","deal","persons",",","holders","permit","furnished","no","but","purpose","fitness","other","an","publish","this","above","©","warranty","warranties",".","merchantability","so","limitation","/","2018","out","all","claim","event","arising","merge","liability","any","charge"," ","is","included","including","noninfringement","hereby","contract","modify","damages","shall","free","in","liable","do","be",")","copy","notice","restriction","for","and","the","documentation","provided"],"title":"License of Simple Lightbox","title_tokens":["simple","of"," ","license","lightbox"]},{"location":"licenses/#license-of-mathjax","text":"Apache License 2.0 See the full license here: MathJax license","text_tokens":[":","mathjax","here","apache","see","2.0","the"," ","license","full"],"title":"License of MathJax","title_tokens":["mathjax"," ","license","of"]},{"location":"licenses/#license-of-mermaid","text":"The MIT License (MIT) Copyright © 2014 - 2018 Knut Sveidqvist Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["following","from","implied","rights","limited","portions","tort","substantial","2014","of","files","use","connection","or","person","\"",":","otherwise","copies","a","action","whether","whom","software","without","authors","kind","dealings","permission","particular","sublicense","not","obtaining","sell","granted","mit","with","license","associated","-","distribute","subject","conditions","as","copyright","express","to","(","deal","knut","persons",",","holders","permit","furnished","no","but","purpose","fitness","other","an","publish","this","above","©","warranty","warranties",".","merchantability","so","limitation","/","2018","out","sveidqvist","all","claim","event","arising","merge","liability","any","charge"," ","is","included","including","noninfringement","hereby","contract","modify","damages","shall","free","in","liable","do","be",")","copy","notice","restriction","for","and","the","documentation","provided"],"title":"License of mermaid","title_tokens":[" ","license","of","mermaid"]},{"location":"release-notes/","text":"更新记录 ¶ 大版本更新 ¶ 在此记录本文档的主要更新，读者可以在此确认经过更新后，本文档添加了哪些主要的内容、话题。 0.1 @ February 25, 2019 ¶ 正式立项，开始本文档的撰写工作。目前尚处于未完成状态，计划包括： Tensorflow 1.12 入门教程 从线性问题入门 40% 训练与测试分立 0% 原生的数据管理 0% 其他的高级技巧 0% Tensorflow 2.0 入门教程 0% Tensorflow 第三方扩展库 入门教程 0% 局部更新记录 ¶ 0.18 @ March 6, 2019 ¶ 完善“从线性问题入门”专题下的 线性分类 ，补充了一些概念便于初学者理解。 0.17 @ March 5, 2019 ¶ 完成“从线性问题入门”专题下的 线性分类 ； 修正前文的一些笔误。 0.15 @ March 4, 2019 ¶ 完成“从线性问题入门”专题下的 本章总说 和 Hello world! ； 微调图片链接， MathJax 的显示特性。 0.12 @ March 3, 2019 ¶ 补完扉页。未来可能会调整 教程导读 ； 修复 Arithmatex 对 MathJax 的引入； 引入 mermaid 库，用于绘制示意图。 0.11 @ February 25, 2019 ¶ 提交 Google Analytics 和 Google Search Console 的账户关联资料； 提交 Disqus 关联资料。 0.10 @ February 25, 2019 ¶ 正式立项，并撰写扉页的一部分。","text_tokens":["tensorflow","账户","对","关联","的","会","绘制","高级","0.12","2.0","5","入门","¶","原生","初学者","数据管理","arithmatex","分立","资料","三方","修复","哪些","读者","引入","开始","示意","版本","、","后","：","补完","话题","概念","!","调整","可以","完善","；","“","未来","，","技巧","mathjax","扉页","专题","修正","一部分","目前","训练","正式","大","显示","mermaid","文档","确认","第三方","可能","hello","导读","工作","理解","3","总说","disqus","@","部分","6","线性","主要","march","扩展","计划","与","4","图片",",","并","包括","40","便于","特性","world","入门教程","示意图","图片链接","补充","测试","链接","在","0.15","学者","未","记录","状态","局部","search","从","0.17","分类","和","更新","0.18","1.12","console","google","。","analytics","第三","一些","2019","笔误","25","数据","一部","处于","0.11"," ","初学","撰写","内容","0.1","”","教程","经过","提交","前文","了","%","0.10","本","意图","添加","用于","立项","尚","0","下","问题","february","管理","记录本","微调","完成","其他","库","本章","此"],"title":"更新记录","title_tokens":["更新","记录"]},{"location":"release-notes/#_1","text":"","text_tokens":[],"title":"更新记录","title_tokens":["更新","记录"]},{"location":"release-notes/#_2","text":"在此记录本文档的主要更新，读者可以在此确认经过更新后，本文档添加了哪些主要的内容、话题。","text_tokens":["，","的","内容","文档","确认","经过","了","在","本","添加","哪些","记录","读者","记录本","、","更新","后","。","话题","可以","此","主要"],"title":"大版本更新","title_tokens":["更新","版本","大"]},{"location":"release-notes/#01-february-25-2019","text":"正式立项，开始本文档的撰写工作。目前尚处于未完成状态，计划包括： Tensorflow 1.12 入门教程 从线性问题入门 40% 训练与测试分立 0% 原生的数据管理 0% 其他的高级技巧 0% Tensorflow 2.0 入门教程 0% Tensorflow 第三方扩展库 入门教程 0%","text_tokens":["，","技巧","数据","扩展","计划","与","处于","tensorflow","目前","包括","的","40"," ","训练","撰写","正式","文档","教程","高级","入门教程","2.0","第三方","入门","%","测试","原生","数据管理","本","分立","三方","立项","未","尚","0","工作","问题","状态","管理","开始","从","：","1.12","。","完成","其他","库","第三","线性"],"title":"0.1 @ February 25, 2019","title_tokens":["25","@","0.1","february",","," ","2019"]},{"location":"release-notes/#_3","text":"","text_tokens":[],"title":"局部更新记录","title_tokens":["局部","更新","记录"]},{"location":"release-notes/#018-march-6-2019","text":"完善“从线性问题入门”专题下的 线性分类 ，补充了一些概念便于初学者理解。","text_tokens":["，","专题","的"," ","线性","初学","便于","”","了","入门","补充","初学者","学者","下","问题","从","理解","分类","。","概念","完善","一些","“"],"title":"0.18 @ March 6, 2019","title_tokens":["march","@","0.18",",","6"," ","2019"]},{"location":"release-notes/#017-march-5-2019","text":"完成“从线性问题入门”专题下的 线性分类 ； 修正前文的一些笔误。","text_tokens":["线性","分类","；","”","专题","修正","。","完成"," ","下","前文","问题","的","入门","“","笔误","从","一些"],"title":"0.17 @ March 5, 2019","title_tokens":["march","@",",","5"," ","2019","0.17"]},{"location":"release-notes/#015-march-4-2019","text":"完成“从线性问题入门”专题下的 本章总说 和 Hello world! ； 微调图片链接， MathJax 的显示特性。","text_tokens":["，","图片","mathjax","专题","的"," ","特性","显示","world","”","入门","图片链接","hello","链接","下","本章","问题","从","总说","和","。","微调","完成","!","“","；","线性"],"title":"0.15 @ March 4, 2019","title_tokens":["march","@","0.15","4",","," ","2019"]},{"location":"release-notes/#012-march-3-2019","text":"补完扉页。未来可能会调整 教程导读 ； 修复 Arithmatex 对 MathJax 的引入； 引入 mermaid 库，用于绘制示意图。","text_tokens":["，","mathjax","扉页","对","的"," ","会","mermaid","绘制","教程","示意图","可能","arithmatex","意图","用于","导读","修复","引入","示意","补完","。","库","调整","；","未来"],"title":"0.12 @ March 3, 2019","title_tokens":["march","@","0.12",","," ","2019","3"]},{"location":"release-notes/#011-february-25-2019","text":"提交 Google Analytics 和 Google Search Console 的账户关联资料； 提交 Disqus 关联资料。","text_tokens":["；","disqus","和","console","提交","google","资料","。","analytics","账户","关联","的"," ","search"],"title":"0.11 @ February 25, 2019","title_tokens":["25","@","0.11","february",","," ","2019"]},{"location":"release-notes/#010-february-25-2019","text":"正式立项，并撰写扉页的一部分。","text_tokens":["，","一部","扉页","一部分","。","立项","部分","并","的","撰写","正式"],"title":"0.10 @ February 25, 2019","title_tokens":["25","@","february",","," ","2019","0.10"]},{"location":"book-1-x/chapter-1/","text":"从线性问题入门 ¶ 摘要 本章将从线性问题入手，讨论最简单的分类与回归问题。我们将随机生成指定分布的数据，通过理论值，检查实验效果。这些简单的、解析的问题，可以提供读者一个直观的感受，并帮助读者快速上手Tensorflow的基本概念与运用。本章中，我们不会涉及任何数据存取、跟踪测度方面的概念，也不会将训练和测试分开，旨在使读者能集中心思到代码规范和Tensorflow的基本功能上。 漫谈线性问题 ¶ 在机器学习领域，线性问题既简单也不简单。目前神经网络主要是指多层、非凸的网络结构，常常用来解复杂的、难以推导的问题。但这并不意味着线性问题容易解决。相反，在很多情况下，线性问题是解不唯一的(undetermined)，解不稳定的(ill-posed/ill-conditioned)，条件的(constrained)。同时，为了得到一个快速收敛的、高质量的解，即使对线性问题，人们也在不断提出、改进解法。例如ISTA, AMP, PGD, LISTA, vAMP等算法，都用来解线性问题。在此不作详细展开。 我们已经知道，一个线性函数具有可加性，和一次齐次性，亦即 \\begin{align} f(x_1 + x_2) &= f(x_1) + f(x_2), \\\\ f(\\alpha x) &= \\alpha f(x). \\end{align} 因此，求解一个线性问题，我们需要将问题纯粹以 线性函数 进行描述。例如， \\begin{align} \\mathbf{y} \\sim \\mathbf{A}\\mathbf{x}. \\end{align} 具体而言， \\mathbf{x} \\mathbf{x} 是我们的已知数据， \\mathbf{y} \\mathbf{y} 是我们的未知量，我们需要找到一个合适的 \\mathbf{A} \\mathbf{A} 来确保 \\mathbf{x} \\mathbf{x} 能拟合到 \\mathbf{y} \\mathbf{y} 。如果此处 \\mathbf{y} \\in \\{0,~1\\}^p \\mathbf{y} \\in \\{0,~1\\}^p 是一个代表p类-分类的向量，那么这就是一个线性分类问题；相反，如果此处 \\mathbf{y} \\in \\mathbb{R}^p \\mathbf{y} \\in \\mathbb{R}^p 在连续p维空间取值，那么这就是一个线性回归问题。 线性问题与凸问题 ¶ 请注意，虽然我们在此处提到“ 线性问题 (Linear problem) ”，但我们指的并非“ 线性规划 (Linear programming) ”。虽然严格意义上，线性规划才是真正的线性问题，但我们在此处尚不讨论线性规划相关的内容，而是着眼于机器学习应用最普遍的两个领域， 分类 和 回归 上。实际上，这两种问题虽然求解的是线性函数，但本质上是凸问题。 例如，如果我们要求解回归问题，通常可以表述成 \\begin{equation} \\begin{aligned} \\arg \\min_{\\mathbf{A}}~& \\sum_{i=1}^N \\mathcal{L}(\\mathbf{A},~\\mathbf{x}_i,~\\mathbf{y}_i), \\\\ \\mathcal{L}(\\mathbf{A},~\\mathbf{x},~\\mathbf{y}) &= \\lVert \\mathbf{y} - \\mathbf{A}\\mathbf{x} \\rVert^2_2. \\end{aligned} \\end{equation} 虽然我们求解的模型 \\mathbf{A} \\mathbf{A} 是线性的，但我们优化的函数对象 \\mathcal{L} \\mathcal{L} 是一个 凸函数 (convex function) 。在此，我们可以将这个标量函数(同时也是凸函数)表述为 \\begin{align} \\forall~\\alpha,~\\beta,~\\mathbf{x}_1,~\\mathbf{x}_2,~\\mathcal{L}(\\alpha\\mathbf{x}_1 + \\beta\\mathbf{x}_2) \\leqslant \\alpha\\mathcal{L}(\\mathbf{x}_1) + \\beta\\mathcal{L}(\\mathbf{x}_2). \\end{align} 我们将这个问题表述为 最小二乘问题(Least-square problem) 。正是由于 \\mathbf{A} \\mathbf{A} 是线性的， \\mathcal{L} \\mathcal{L} 才能被确保为一个凸函数，进而，我们才能确保上述问题能得到精确的全局最小值解。 知悉Tensorflow ¶ 在本章接下来的内容里，我们将探讨Tensorflow如何求解一些简单的问题。对于初次上手的读者而言，在安装Tensorflow，亲自开始写一些project之前，笔者推荐你到这个游乐场“抢鲜体验”一番， Tensorflow Playground 在这个在线页面里，用户不需要有任何编程知识，可以通过直观的操作界面，建立一个简单的 多层感知机网络(有时也叫做Artificial Neural Network, ANN) ，并且实时观测网络的性能和测度。它提供了几个简单的二维数据集，供用户体验不同的数据集下，各种分类、回归问题的合宜解法。如果读者已经对神经网络的基本原理有所了解，相信能通过这个小小的实验场达到心有灵犀。我们在本章所做的project和demo，大体不跳出这个试验场的范畴，只是会略微复杂一点而已。 本章要点 ¶ 下图展示了通过本章学习，能了解到的概念： graph LR st(Hello world!) --> linclas(线性分类) linclas --> linreg(线性回归) linreg --> nonlinreg(非线性回归) nonlinreg --> ed(非线性分类) lp[感知机] --> linclas sigma[Logsitc回归] --> linclas ce[交叉熵] --> linclas opt[优化器] --> linreg argpar[项目选项] --> linreg para[参数回归] --> nonlinreg kernel[核函数] --> ed classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,linclas,linreg,nonlinreg,ed styStart Hello world : 首先，在本节，读者将了解Tensorflow的安装方法，并编写通过第一个简单的Tensorflow程序。 线性分类 : 本节通过解一个简单的二分类问题，我们将引入单层感知机、Logistic回归和交叉熵的概念，并且实验结果进行可视化。 线性回归 : 本节通过解一个与上一节难度相仿的，简单的回归问题，另读者能对比不同优化器的性能、特点，并介绍如何在项目中进行arugument parsing(引入项目的可选项)。 非线性回归 : 本节将修改上一节回归问题的激活函数，将线性的回归问题推广到解析函数参数回归的范畴。 非线性分类 : 本节将通过上一节的参数回归，引入核函数的概念，将线性分类问题推广到非线性空间里。","text_tokens":["i","取值","基本功","意义","难度","对比","fill","这些","稳定","维空间","+","一个","描述","编写","意味","连续","单层","学习","项目","读者",":","!","激活","类","相仿","ed","进而","；","forall","，","而是","编程","另","}","函数参数","试验","logistic","旨在","随机","跟踪","漫谈","n","不同","下来","游乐场","neural","代表","几个","最小","-","基本概念","确保","提出","为了","对于","略微","x","知识","不会","网络","得到","实时","stroke","equation","解","已知","上","容易","测试","机器","function","r","leqslant",".","&","sim","分类","和","你","选项","/","第一个","。","详细","一些","测度","解决","质量","交叉","=","不作","凸函数","l","效果","具体","方法","常常","规划","in","摘要","函数","相反","在线","artificial","lp","para","复杂","空间","建立","入手","要点","网络结构","least","人们","一番","会","纯粹","~","基本原理","神经网","vamp","本质","才能","最小值","引入","高质量","快速","“","向量","亦","ce","目前","square","align","相信","此处","展示","凸","本节","相关","介绍","只是","提供","使","alpha","1","结果","实验场","范畴","探讨","领域","由于","lista","求解","解不",",","收敛","并","通过","world","功能","不断","请","lr","被","运用","知机","表述","logsitc","beta","lvert","应用","才","]","那么","可加性","可选","通常","nonlinreg","rvert","简单","数据","感受","用户","ista","里"," ","内容","如果","对象","两个","拟合","linreg","标量","problem","既","ill","多层","条件","下","sigma","性","神经","未知","可加","基本功能","理论值","是","首先","等","对","同时","本原","原理","这","写","都","并且","end","因此","p","器","要求","涉及","可视","达到","唯一","以","直观","不","ba9132","linear",";","我们","神经网络","各种","知道","一次","虽然","network","大体","分布","生成","pgd","心思","难以","展开","^","_","安装","实验","成","分开","opt","aligned","用来","所","已经","即使","\\","初次","begin","抢鲜","第一","主要","很多","就是","推荐","playground","操作界面","可选项","精确","意味着","amp","参数","知悉","fae6a9","之前","一点","[","指","上述","集中","在","y","帮助","游乐","未知量","试验场","理论","全局","实际","合适","叫做","非凸","着眼","小小的","进行","一节","而已","二","class","到","conditioned","convex",">","了","上手","找到","小小","着眼于","接下来","注意","正是","笔者","0",")","性能","classdef","能","来","将","最","回归","constrained","本章","此","graph","存取","arugument","tensorflow","感知","灵犀","的","做","而言","2","mathcal","心有灵犀","入门","mathbf","¶","讨论","但","核","开始","二维","、","高质","即","：","a","概念","可以","任何","接下","有所","#","它","ann","了解","训练","二乘","集","project","体验","有时","如何","改进","非线性","mathbb","需要","hello","{","sum","修改","实际上","操作","节","线性","argpar","linclas","kernel","(","代码","与","例如","基本","情况","模型","中","检查","undetermined","parsing","供","真正","结构","有","严格","具有","这个","感知机","推导","两种","也","posed","从","stystart","为","亲自","下图","min","优化","arg","集下","界面","可视化","并非","推广","规范","熵","f","解析","”","普遍","st","跳出","算法","programming","本","demo","线性规划","合宜","尚","提到","问题","特点","程序","观测","齐次","方面","页面","解法","指定"],"title":"本章总说","title_tokens":["总说","本章"]},{"location":"book-1-x/chapter-1/#_1","text":"摘要 本章将从线性问题入手，讨论最简单的分类与回归问题。我们将随机生成指定分布的数据，通过理论值，检查实验效果。这些简单的、解析的问题，可以提供读者一个直观的感受，并帮助读者快速上手Tensorflow的基本概念与运用。本章中，我们不会涉及任何数据存取、跟踪测度方面的概念，也不会将训练和测试分开，旨在使读者能集中心思到代码规范和Tensorflow的基本功能上。","text_tokens":["，","入手","数据","代码","与","分布","存取","基本功","感受","生成","tensorflow","基本","效果","并","训练","的","通过"," ","不会","规范","中","这些","解析","功能","检查","到","心思","一个","提供","使","实验","运用","旨在","随机","讨论","上手","跟踪","测试","集中","摘要","上","分开","帮助","本章","涉及","问题","也","读者","基本功能","从","分类","、","直观","和","测度","快速","理论","能","将","。","最","概念","基本概念","我们","方面","可以","任何","回归","简单","线性","理论值","指定"],"title":"从线性问题入门","title_tokens":["问题","入门","线性","从"]},{"location":"book-1-x/chapter-1/#_2","text":"在机器学习领域，线性问题既简单也不简单。目前神经网络主要是指多层、非凸的网络结构，常常用来解复杂的、难以推导的问题。但这并不意味着线性问题容易解决。相反，在很多情况下，线性问题是解不唯一的(undetermined)，解不稳定的(ill-posed/ill-conditioned)，条件的(constrained)。同时，为了得到一个快速收敛的、高质量的解，即使对线性问题，人们也在不断提出、改进解法。例如ISTA, AMP, PGD, LISTA, vAMP等算法，都用来解线性问题。在此不作详细展开。 我们已经知道，一个线性函数具有可加性，和一次齐次性，亦即 \\begin{align} f(x_1 + x_2) &= f(x_1) + f(x_2), \\\\ f(\\alpha x) &= \\alpha f(x). \\end{align} 因此，求解一个线性问题，我们需要将问题纯粹以 线性函数 进行描述。例如， \\begin{align} \\mathbf{y} \\sim \\mathbf{A}\\mathbf{x}. \\end{align} 具体而言， \\mathbf{x} \\mathbf{x} 是我们的已知数据， \\mathbf{y} \\mathbf{y} 是我们的未知量，我们需要找到一个合适的 \\mathbf{A} \\mathbf{A} 来确保 \\mathbf{x} \\mathbf{x} 能拟合到 \\mathbf{y} \\mathbf{y} 。如果此处 \\mathbf{y} \\in \\{0,~1\\}^p \\mathbf{y} \\in \\{0,~1\\}^p 是一个代表p类-分类的向量，那么这就是一个线性分类问题；相反，如果此处 \\mathbf{y} \\in \\mathbb{R}^p \\mathbf{y} \\in \\mathbb{R}^p 在连续p维空间取值，那么这就是一个线性回归问题。","text_tokens":["等","取值","网络结构","对","同时","的","人们","这","纯粹","~","稳定","维空间","都","一个","+","描述","2","意味","而言","连续","神经网","end","mathbf","因此","vamp","p","学习","但","唯一","高质量","、","高质","以","即","不","快速","a","我们","类","神经网络","知道","一次","；","向量","，","亦","目前","align","此处","}","pgd","难以","改进","展开","^","mathbb","_","alpha","需要","1","{","代表","用来","已经","即使","领域","\\","-","begin","确保","lista","线性","求解","提出","主要","(","为了","x","解不","很多","就是",",","例如","收敛","情况","并","意味着","网络","得到","amp","undetermined","不断","解","已知","容易","指","结构","机器","在","y","具有","推导","r","也",".","&","未知量","posed","sim","那么","分类","可加性","和","/","。","详细","合适","简单","解决","质量","非凸","=","数据","不作","进行","ista"," ","具体","f","如果","到","conditioned","拟合","算法","既","常常","ill","找到","多层","in","函数","相反","条件","下","0","问题",")","性","神经","未知","能","来","复杂","将","齐次","可加","解法","回归","空间","constrained","此","是"],"title":"漫谈线性问题","title_tokens":["线性","问题","漫谈"]},{"location":"book-1-x/chapter-1/#_3","text":"请注意，虽然我们在此处提到“ 线性问题 (Linear problem) ”，但我们指的并非“ 线性规划 (Linear programming) ”。虽然严格意义上，线性规划才是真正的线性问题，但我们在此处尚不讨论线性规划相关的内容，而是着眼于机器学习应用最普遍的两个领域， 分类 和 回归 上。实际上，这两种问题虽然求解的是线性函数，但本质上是凸问题。 例如，如果我们要求解回归问题，通常可以表述成 \\begin{equation} \\begin{aligned} \\arg \\min_{\\mathbf{A}}~& \\sum_{i=1}^N \\mathcal{L}(\\mathbf{A},~\\mathbf{x}_i,~\\mathbf{y}_i), \\\\ \\mathcal{L}(\\mathbf{A},~\\mathbf{x},~\\mathbf{y}) &= \\lVert \\mathbf{y} - \\mathbf{A}\\mathbf{x} \\rVert^2_2. \\end{aligned} \\end{equation} 虽然我们求解的模型 \\mathbf{A} \\mathbf{A} 是线性的，但我们优化的函数对象 \\mathcal{L} \\mathcal{L} 是一个 凸函数 (convex function) 。在此，我们可以将这个标量函数(同时也是凸函数)表述为 \\begin{align} \\forall~\\alpha,~\\beta,~\\mathbf{x}_1,~\\mathbf{x}_2,~\\mathcal{L}(\\alpha\\mathbf{x}_1 + \\beta\\mathbf{x}_2) \\leqslant \\alpha\\mathcal{L}(\\mathbf{x}_1) + \\beta\\mathcal{L}(\\mathbf{x}_2). \\end{align} 我们将这个问题表述为 最小二乘问题(Least-square problem) 。正是由于 \\mathbf{A} \\mathbf{A} 是线性的， \\mathcal{L} \\mathcal{L} 才能被确保为一个凸函数，进而，我们才能确保上述问题能得到精确的全局最小值解。","text_tokens":["i","意义","同时","的","least","这","~","+","一个","2","mathcal","end","mathbf","讨论","本质","学习","要求","才能","但","最小值","不","linear","a","我们","可以","进而","“","虽然","forall","，","而是","二乘","square","align","此处","}","凸","相关","^","_","alpha","n","1","{","sum","成","实际上","aligned","领域","\\","最小","-","begin","由于","确保","线性","求解","(","x",",","例如","精确","模型","得到","请","equation","被","解","上","真正","表述","指","上述","机器","在","严格","y","这个","function","beta","两种","也","leqslant","lvert","应用","&","才",".","分类","为","和","全局","通常","。","实际","min","rvert","优化","arg","着眼","=","并非","凸函数","l"," ","内容","如果","对象","”","两个","convex","普遍","标量","problem","programming","规划","着眼于","注意","函数","线性规划","正是","尚","提到","问题",")","能","将","最","回归","此","是"],"title":"线性问题与凸问题","title_tokens":["与","线性","问题","凸"]},{"location":"book-1-x/chapter-1/#tensorflow","text":"在本章接下来的内容里，我们将探讨Tensorflow如何求解一些简单的问题。对于初次上手的读者而言，在安装Tensorflow，亲自开始写一些project之前，笔者推荐你到这个游乐场“抢鲜体验”一番， Tensorflow Playground 在这个在线页面里，用户不需要有任何编程知识，可以通过直观的操作界面，建立一个简单的 多层感知机网络(有时也叫做Artificial Neural Network, ANN) ，并且实时观测网络的性能和测度。它提供了几个简单的二维数据集，供用户体验不同的数据集下，各种分类、回归问题的合宜解法。如果读者已经对神经网络的基本原理有所了解，相信能通过这个小小的实验场达到心有灵犀。我们在本章所做的project和demo，大体不跳出这个试验场的范畴，只是会略微复杂一点而已。","text_tokens":["tensorflow","感知","灵犀","对","的","本原","原理","一番","做","会","写","而言","一个","基本原理","神经网","并且","心有灵犀","达到","读者","开始","二维","、","直观","不","我们","接下","任何","可以","各种","神经网络","“","，","有所","network","大体","编程","它","ann","了解","集","project","相信","体验","有时","如何","试验","只是","提供","安装","实验","需要","实验场","不同","下来","游乐场","范畴","neural","几个","所","已经","探讨","操作","初次","求解","抢鲜","(","对于","略微","知识","推荐",",","playground","操作界面","基本","通过","网络","实时","之前","一点","知机","供","有","在","这个","感知机","游乐","也","亲自","分类","试验场","和","你","。","叫做","一些","测度","简单","集下","界面","数据","小小的","而已","用户","里"," ","内容","如果","到","”","跳出","了","上手","小小","多层","接下来","demo","在线","笔者","合宜","问题",")","性能","artificial","神经","能","复杂","将","观测","页面","解法","回归","本章","建立"],"title":"知悉Tensorflow","title_tokens":["知悉","tensorflow"]},{"location":"book-1-x/chapter-1/#_4","text":"下图展示了通过本章学习，能了解到的概念： graph LR st(Hello world!) --> linclas(线性分类) linclas --> linreg(线性回归) linreg --> nonlinreg(非线性回归) nonlinreg --> ed(非线性分类) lp[感知机] --> linclas sigma[Logsitc回归] --> linclas ce[交叉熵] --> linclas opt[优化器] --> linreg argpar[项目选项] --> linreg para[参数回归] --> nonlinreg kernel[核函数] --> ed classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,linclas,linreg,nonlinreg,ed styStart Hello world : 首先，在本节，读者将了解Tensorflow的安装方法，并编写通过第一个简单的Tensorflow程序。 线性分类 : 本节通过解一个简单的二分类问题，我们将引入单层感知机、Logistic回归和交叉熵的概念，并且实验结果进行可视化。 线性回归 : 本节通过解一个与上一节难度相仿的，简单的回归问题，另读者能对比不同优化器的性能、特点，并介绍如何在项目中进行arugument parsing(引入项目的可选项)。 非线性回归 : 本节将修改上一节回归问题的激活函数，将线性的回归问题推广到解析函数参数回归的范畴。 非线性分类 : 本节将通过上一节的参数回归，引入核函数的概念，将线性分类问题推广到非线性空间里。","text_tokens":["graph","首先","arugument","tensorflow","感知","难度","对比","的","fill","一个","编写","并且","单层","器","学习","核","项目","可视","读者","引入",":","、","：","ba9132","概念","!",";","我们","相仿","激活","ed","，","#","另","ce","了解","展示","本节","如何","函数参数","非线性","介绍","安装","实验","logistic","hello","结果","不同","范畴","修改","opt","节","-","线性","argpar","第一","linclas","kernel","(","与",",","可选项","并","通过","中","world","stroke","参数","parsing","lr","fae6a9","知机","解","[","上","在","logsitc","感知机","]","stystart","分类","和","选项","下图","可选","nonlinreg","第一个","。","优化","简单","交叉","可视化","进行","一节","推广","二","熵","里"," ","class","解析","到",">","方法","st","linreg","了","本","函数","问题",")","sigma","特点","性能","para","lp","程序","classdef","能","将","回归","空间","本章"],"title":"本章要点","title_tokens":["本章","要点"]},{"location":"book-1-x/chapter-1/hello-world/","text":"Hello world! ¶ 摘要 本节包含一个简易的安装指导，主要提供给Windows上的GPU用户。之后，用户可以按照本节指导编写第一个测试程序。 安装Tensorflow ¶ 本节针对Windows使用GPU的用户，提供一个安装最新版Tensorflow的方法。大致的步骤符合以下两个教程： 通过pip安装Tensorflow Tensorflow的GPU支持 但是，需要指出的是，官方教程里有些部分已经不符合当前用户的实际情况，笔者经过摸索，总结以下的安装过程。 总体而言，安装Tensorflow需要用户确保以下条件 NVIDIA驱动已经达到411.31及以上 CUDA 10.0 (10.1不确定能否正常使用) CUDNN已经达到7.3.1及以上 CUPTI版本与CUDA相配（不需要专门安装，在安装CUDA的同时已经自动集成） 除此之外，涉及多GPU支持的库(NCCL和TensorRT)，这里不考虑。因此，我们主要需要确保前三条正常安装。 提示 Tensorflow官方提供的安装包是基于CUDA 9.0，CUDNN >= 7.2的基础上编译的。然而现在NVIDIA已经不再提供CUDA 9.0了。Windows用户可以安装的版本包括过时的8.1和最新的10.1。在这种情况下，官方还提供一个令用户自行从源码编译Tensorflow库的方案，但是对于Windows用户而言过于繁琐，容易出错，这里不建议按照官方教程。 更新NVIDIA驱动 ¶ 首先，我们可以开始更新我们的NVIDIA驱动。任何载有NVIDIA-GPU显卡的电脑，都应当在系统里已经预装好了 GeForce Experience 。点击如下图标开启GeForce Experience（如果是低版本的GeForce Experience，图标可能会不同）， 如左图，在主界面上，可以观察到，当驱动版本不够高时，会自动出现更新提示，依照提示更新即可。建议在更新驱动时，关闭其他所有程序，以免造成干扰。 更新NVIDIA driver 检查更新结果 如果更新完成，如右图，可以在设置界面检查到当前的版本已经达到最新。在安装过程中，由于驱动更新，屏幕可能会闪烁数次。在看到更新提示完成后建议重启。 提示 如果GeForce Experience提示无法更新驱动，可以考虑将机器恢复出厂设置，可能可以解决这一问题。 安装CUDA ¶ 驱动更新并重启后，需要安装最新版的Visual Studio(VS)，这是因为CUDA库本身对VS具有一定的支持，如果先安装CUDA，则这部分支持无法安装到位。这对我们使用Tensorflow并不造成妨害，但是对CUDA的用户并不是一个推荐的做法，所以无论如何，建议在第一步，安装VS，下面提供VS社区版（免费）的链接： Visual Studio官网 接下来，可以开始安装CUDA。进入CUDA安装包的下载链接 下载最新版CUDA CUDA官方指南 提示 须知，并非最新版就是最适合的版本。实际能使用的CUDA版本取决于后续步骤中预编译包的支持版本。就笔者写到这里时，最新的CUDA 10.1已然不匹配最新版预编译包支持的CUDA 10.0，由于版本差距很小，不确定是否会由此引发问题，但读者可以自行尝试。 现在的安装包已经支持在线安装包和离线安装包两种模式，读者可以根据自己的实际情况选择对应的版本。注意，安装CUDA的时候，CUDA可能要求我们翻新驱动版本，但通过GeForce Experience安装的驱动一般应已达到最新，新于CUDA内部提供的驱动。因此，我们需要选择定制安装模式，根据我们需要对比版本号，将驱动更新的选项取消，如下图所示 安装CUDA后，建议重启。 危险 特别需要注意的是，如果你的机器无法通过GeForce Experience更新驱动，那么在这里选择安装驱动也是枉然。并且，这种强制更新驱动的行为很可能导致系统崩溃。因此特别建议在安装CUDA之前一定要解决驱动更新的问题，哪怕是通过恢复出厂设置。 安装CUDNN ¶ 安装CUDA的同时，已经自带安装上了CUPTI。但是CUDNN并没有包含在CUDA中，因此，我们需要到以下链接下载CUDNN并查看安装说明 下载最新版CUDNN CUDNN官方指南 不同的是，CUDNN没有安装包，是以压缩包的形式下载到本地。解压后，其中应当包含如下目录及文件 . └─ cuda/ ├─ bin/ # Binary library │ └─ cudnn64_7.dll ├─ include/ # C++ Include file │ └─ cudnn.h ├─ lib/ # C++ Lib file │ └─ x64/ │ └─ cudnn.lib └─ NVIDIA_SLA_cuDNN_Support.txt # Readme file 假设我们CUDA的安装目录在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，将上述解压的 bin , include 和 lib 三个文件夹，直接拷贝覆盖到该安装目录下，即可完成CUDNN的安装。 进行完上述步骤后，我们还需要确保几个环境变量正确设置： 存在 CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，是我们的安装目录。理论上在CUDA安装后，该目录已经自动设置好。 环境变量 PATH 中，存在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin , C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/extras/CUPTI/libx64 两个路径，由于CUDNN已经配置在CUDA安装目录下，我们不需要像官方文档那样，添加第三个CUDNN的安装路径。 安装Anaconda ¶ Windows用户建议使用Anaconda管理python环境。作为一个开源的项目，Anaconda发行版已经集成了我们所需要的大多数python包，其中有些包是我们自己难以安装上的，例如支持python3的PIL。我们选用python3版的Anaconda，下载路径如下： Anaconda 最新版下载链接 注意选择x64版的安装包。安装结束后，我们已经有python3.7的环境了。鉴于有些情况下我们可能需要使用更早的python版本，接下来我们可以安装python3.6的虚环境。 Anaconda可以安装在用户或系统目录下。注意，如果安装在系统目录下，如果不在虚环境下，安装任何包都需要管理员模式。不过这不影响本教程的示例，因为我们将安装虚环境。直接打开 Anaconda Prompt ，并键入以下命令： conda create -n py36 python = 3 .6 anaconda 按照引导流程安装，经过一段时间后，我们将得到python3.6版的Anaconda。以后我们可以直接从开始菜单 Anaconda Prompt (py36) 进入该虚环境，也可以在基环境中键入 activate py36 转换到虚环境中。 提示 如果用户愿意，还可以通过 conda install -c conda-forge opencv 安装 opencv 。opencv是一个强大的图像处理库，这里python版的opencv比C++版更容易上手。一般该命令安装的是conda库中最新版的opencv（截至笔者写到这里时是opencv3.4.4）。然而，这样安装的并非是支持GPU的版本。尽管conda还提供一个GPU的opencv版本，可以通过 conda install -c patricksnape opencv-gpu 安装 opencv2 ，但它的版本还停留在2.4.9，和opencv3的用法殊有不同，请读者注意这一点。 安装预编译好的Tensorflow ¶ 我们可以查看如下项目： fo40225/tensorflow-windows-wheel 该项目的维护者在不断根据Tensorflow的更新，预编译出适合不同版本的Tensorflow安装包。截至笔者写到这里为止，最新支持到r1.12版。我们选择对应的GPU版Tensorflow，并在虚环境下执行以下命令： pip install https://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.12.0/py36/GPU/cuda100cudnn73sse2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl 等待一段时间后，安装将会结束。 Hello world! 测试 ¶ 撰写如下代码，保存到 hello-world.py 文件， Python 1 2 3 4 5 6 7 8 9 if __name__ == '__main__' : import tensorflow as tf test_str = tf . constant ( 'Hello, world!' ) test_cal = tf . reduce_sum ( tf . random_normal ([ 1000 , 1000 ])) with tf . Session () as sess : print ( 'Current Tensorflow version is:' , tf . __version__ ) print ( 'Test string: ' , sess . run ( test_str )) print ( 'Test calculation:' , sess . run ( test_cal )) Output Current Tensorflow version is: 1 .12.0 Test string: b 'Hello, world!' Test calculation: -1948.6578 在保存目录下，执行 python hello-world.py 第一次运行可能需要等待较长的初始化时间（硬件配置）。如果能正常运行，我们将看到如下结果： 其中，显示结果之前的记录信息反映了GPU已经可以正常工作。而“Test calculation”显示的结果是 10^6 10^6 个服从 \\mathcal{N}(0,1) \\mathcal{N}(0,1) 分布的随机数之和。根据相互独立的随机正态分布互不相关的特性，我们可以推算出，该结果服从 \\mathcal{N}(0,10^3) \\mathcal{N}(0,10^3) 的正态分布。","text_tokens":["压缩包","基于","指出","到位","第一次","件夹","对比","除此之外","强大","硬件","库中","一个","编写","不是","5","三条","8.1","差距","能否","大多数","geforce","项目","读者","自己",":","比","所有","由此","应当","master","!","编译","，","├","时","较长","constant","这部分","显示","}","已然","像","包","之外","win","三个","n","图像","随机","不同","下来","选择","─","几个","工作","with","高时","新版","重启","-","name","低版","as","sla","总体","示例","确保","py","对于","过于","新于","正确","'","电脑","得到","导致系统","cp36m","结束","一步","那样","pil","将会","键入","如下","先","更","翻新","基","早","上","容易","测试","12","机器","这部","session","一段时间","这里","无论",".","其中","解压","和","更新","选项","你","/","第一个","。","打开","菜单","opencv","第三","开源","https","解决","要","=","按照","发行","形式","is","这种","崩溃","v10","殊有","教程","方法","random","摘要","反映","机数","后续","添加","在线","管理","路径","低版本","定制","本地","7.2","库","或","10","下面","readme","初始化","版","是否","建议","第一步","print","会","显卡","vs","选用","变量","visual","r1","x64","步骤","时间","包含","files","算出","没有","离线","适合","总体而言","cuda","“","computing","引导","lib","符合","做法","install","c","集成","执行","还","1000","这一","extras","相关","program","提供","危险","可能","这样","1","结果","处理","driver","枉然","str","patricksnape","最新","相配","尝试","tensorrt","截至","3","图标","sess","右图","部分","6","由于","内部","造成","一段","）","main","b",",","给","应已","并","pip","通过","libx64","world","不断","bin","请","发行版","以后","而","载有","随机数","假设","blob","cal","h","以免","出","library","关闭","停留","]","那么","观察","py36","管理员","摸索","初始","binary","专门","即可","妨害","9.0","等待","行为","自动","用户","里","│"," ","撰写","如果","文件","两个","正常","为止","段时间","__","一定","cupti","及","虚","条件","下","4.4","影响","推算出","fo40225","以上","amd64","7","完成","维护者","出现","reduce","大多","是","└","首先","用法","对","同时","独立","尽管","nccl","这","写","都","c++","tf","cudnn64","好","并且","无法","取决","版本号","因此","import","要求","所以","环境变量","涉及","达到","提示","存在","以","不","后","模式","总结","互不","我们","对应","愿意","一次","如","分布","根据","cuda100cudnn73sse2","看到","2.4","当","文档","难以","version","output","相互","windows","^","_","安装","toolkit","简易","411.31","图像处理","所示","指南","推算","normal","当前","所","已经","繁琐","现在","current","无论如何","\\","string","保存","出厂","哪怕","12.0","正态分布","第一","主要","图","屏幕","calculation","使用","多","点击","推荐","就是","方案","环境","手","鉴于","特性","闪烁","官网","最新版","之前","一点","是因为","1948.6578","大致","[","run","作为","链接","上述","在","特别","转换","txt","理论","1.12","实际","7.3","覆盖","（","自行","下载","opencv3","服从","维护","进行","test","whl","导致","到",">","过时","dll","确定","一般","中预","了","不过","本身","很","令","接下来","很小","注意","笔者","0",")","cudnn","开启","能","将","配置","支持","最","其他","考虑","流程","opencv2","tensorflow","的","该","预","社区","而言","2","说明","mathcal","wheel","预装","¶","依照","gpu","取消","10.0","但","数次","开始","版本","除此","开始菜单","：","com","然而","可以","源码","任何","接下","运行","目录","指导","不再","进入","#","自带","它","之","系统目录","包都","之后","如何","文件夹","prompt","前","查看","anaconda","主","多数","hello","需要","8","sum","{","拷贝","强制","support","干扰","file","节","experience","activate","目的","(","过程","代码","与","4","不够","例如","基础","有些","情况","包括","则","官方","中","以下","检查","须知","但是","10.1","有","驱动","命令","具有","测试程序","两种","也","记录","从","安装包","免费","完","该项","包是","第三个","直接","python3","就","时候","恢复","界面","设置","并非","信息","create","conda","python","”","9","系统","压缩","经过","github","引发","nvidia","include","出错","path","cp36","左图","本","针对","forge","studio","取决于","问题","环境变","程序","匹配","因为","if","个"],"title":"Hello world!","title_tokens":["!"," ","hello","world"]},{"location":"book-1-x/chapter-1/hello-world/#hello-world","text":"摘要 本节包含一个简易的安装指导，主要提供给Windows上的GPU用户。之后，用户可以按照本节指导编写第一个测试程序。","text_tokens":["，","指导","给","按照","用户","的"," ","之后","一个","编写","windows","提供","安装","简易","上","测试","gpu","本","摘要","包含","测试程序","节","程序","第一个","。","可以","第一","主要"],"title":"Hello world!","title_tokens":["!"," ","hello","world"]},{"location":"book-1-x/chapter-1/hello-world/#tensorflow","text":"本节针对Windows使用GPU的用户，提供一个安装最新版Tensorflow的方法。大致的步骤符合以下两个教程： 通过pip安装Tensorflow Tensorflow的GPU支持 但是，需要指出的是，官方教程里有些部分已经不符合当前用户的实际情况，笔者经过摸索，总结以下的安装过程。 总体而言，安装Tensorflow需要用户确保以下条件 NVIDIA驱动已经达到411.31及以上 CUDA 10.0 (10.1不确定能否正常使用) CUDNN已经达到7.3.1及以上 CUPTI版本与CUDA相配（不需要专门安装，在安装CUDA的同时已经自动集成） 除此之外，涉及多GPU支持的库(NCCL和TensorRT)，这里不考虑。因此，我们主要需要确保前三条正常安装。 提示 Tensorflow官方提供的安装包是基于CUDA 9.0，CUDNN >= 7.2的基础上编译的。然而现在NVIDIA已经不再提供CUDA 9.0了。Windows用户可以安装的版本包括过时的8.1和最新的10.1。在这种情况下，官方还提供一个令用户自行从源码编译Tensorflow库的方案，但是对于Windows用户而言过于繁琐，容易出错，这里不建议按照官方教程。","text_tokens":["考虑","基于","tensorflow","指出","同时","建议","除此之外","的","nccl","而言","一个","三条","8.1","步骤","因此","gpu","能否","10.0","涉及","达到","提示","版本","除此","总体而言","不","：","cuda","总结","然而","我们","编译","可以","源码","，","不再","符合","集成","还","windows","前","提供","安装","之外","需要","411.31","1","最新","相配","当前","tensorrt","已经","新版","现在","繁琐","节","部分","总体","确保","主要","(","过程","对于","使用","与","）","多","过于","基础","有些","方案","情况","pip","通过","包括","官方","以下","最新版","大致","但是","上","容易","10.1","驱动","在","这里",".","从","安装包","和","。","实际","摸索","7.3","（","专门","自行","9.0","=","自动","按照","用户","里"," ","这种","两个","正常",">","教程","方法","经过","过时","nvidia","确定","cupti","了","出错","令","针对","本","及","条件","笔者","下",")","cudnn","以上","7.2","支持","库","是"],"title":"安装Tensorflow","title_tokens":["安装","tensorflow"]},{"location":"book-1-x/chapter-1/hello-world/#nvidia","text":"首先，我们可以开始更新我们的NVIDIA驱动。任何载有NVIDIA-GPU显卡的电脑，都应当在系统里已经预装好了 GeForce Experience 。点击如下图标开启GeForce Experience（如果是低版本的GeForce Experience，图标可能会不同）， 如左图，在主界面上，可以观察到，当驱动版本不够高时，会自动出现更新提示，依照提示更新即可。建议在更新驱动时，关闭其他所有程序，以免造成干扰。 更新NVIDIA driver 检查更新结果 如果更新完成，如右图，可以在设置界面检查到当前的版本已经达到最新。在安装过程中，由于驱动更新，屏幕可能会闪烁数次。在看到更新提示完成后建议重启。 提示 如果GeForce Experience提示无法更新驱动，可以考虑将机器恢复出厂设置，可能可以解决这一问题。","text_tokens":["考虑","首先","建议","的","会","显卡","都","好","无法","预装","依照","gpu","geforce","达到","数次","提示","开始","版本","所有","后","应当","我们","可以","任何","，","时","如","这一","看到","当","主","安装","可能","结果","driver","不同","最新","当前","高时","干扰","已经","图标","重启","-","experience","右图","低版","出厂","由于","造成","过程","屏幕","）","点击","不够","电脑","中","闪烁","检查","如下","载有","上","机器","以免","驱动","在","关闭","更新","观察","。","（","恢复","即可","解决","界面","设置","自动","里"," ","如果","到","系统","nvidia","了","左图","问题","低版本","程序","开启","将","其他","完成","出现","是"],"title":"更新NVIDIA驱动","title_tokens":["更新","nvidia","驱动"]},{"location":"book-1-x/chapter-1/hello-world/#cuda","text":"驱动更新并重启后，需要安装最新版的Visual Studio(VS)，这是因为CUDA库本身对VS具有一定的支持，如果先安装CUDA，则这部分支持无法安装到位。这对我们使用Tensorflow并不造成妨害，但是对CUDA的用户并不是一个推荐的做法，所以无论如何，建议在第一步，安装VS，下面提供VS社区版（免费）的链接： Visual Studio官网 接下来，可以开始安装CUDA。进入CUDA安装包的下载链接 下载最新版CUDA CUDA官方指南 提示 须知，并非最新版就是最适合的版本。实际能使用的CUDA版本取决于后续步骤中预编译包的支持版本。就笔者写到这里时，最新的CUDA 10.1已然不匹配最新版预编译包支持的CUDA 10.0，由于版本差距很小，不确定是否会由此引发问题，但读者可以自行尝试。 现在的安装包已经支持在线安装包和离线安装包两种模式，读者可以根据自己的实际情况选择对应的版本。注意，安装CUDA的时候，CUDA可能要求我们翻新驱动版本，但通过GeForce Experience安装的驱动一般应已达到最新，新于CUDA内部提供的驱动。因此，我们需要选择定制安装模式，根据我们需要对比版本号，将驱动更新的选项取消，如下图所示 安装CUDA后，建议重启。 危险 特别需要注意的是，如果你的机器无法通过GeForce Experience更新驱动，那么在这里选择安装驱动也是枉然。并且，这种强制更新驱动的行为很可能导致系统崩溃。因此特别建议在安装CUDA之前一定要解决驱动更新的问题，哪怕是通过恢复出厂设置。","text_tokens":["tensorflow","到位","对","建议","第一步","版","的","是否","对比","会","这","预","写","vs","社区","一个","visual","不是","并且","无法","取决","版本号","步骤","因此","取消","差距","10.0","要求","所以","但","geforce","离线","达到","读者","提示","开始","适合","版本","自己","不","后","：","cuda","由此","模式","我们","编译","接下","可以","对应","，","时","进入","做法","根据","这部分","已然","如何","包","提供","安装","危险","可能","需要","枉然","下来","所示","选择","最新","指南","尝试","强制","已经","新版","现在","无论如何","重启","experience","出厂","哪怕","部分","由于","内部","造成","第一","(","图","使用","）","推荐","就是","新于","应已","情况","并","通过","则","导致系统","官方","官网","一步","须知","最新版","如下","先","之前","是因为","翻新","但是","链接","10.1","这部","驱动","在","特别","机器","具有","这里","无论","两种","也","安装包","免费","那么","和","更新","选项","你","。","实际","（","就","自行","时候","妨害","要","解决","恢复","下载","设置","行为","并非","用户","导致"," ","如果","这种","崩溃","到","系统","一定","引发","确定","一般","中预","本身","很","接下来","很小","注意","后续","在线","笔者","studio","取决于","问题",")","匹配","定制","能","将","支持","因为","库","最","是","下面"],"title":"安装CUDA","title_tokens":["安装","cuda"]},{"location":"book-1-x/chapter-1/hello-world/#cudnn","text":"安装CUDA的同时，已经自带安装上了CUPTI。但是CUDNN并没有包含在CUDA中，因此，我们需要到以下链接下载CUDNN并查看安装说明 下载最新版CUDNN CUDNN官方指南 不同的是，CUDNN没有安装包，是以压缩包的形式下载到本地。解压后，其中应当包含如下目录及文件 . └─ cuda/ ├─ bin/ # Binary library │ └─ cudnn64_7.dll ├─ include/ # C++ Include file │ └─ cudnn.h ├─ lib/ # C++ Lib file │ └─ x64/ │ └─ cudnn.lib └─ NVIDIA_SLA_cuDNN_Support.txt # Readme file 假设我们CUDA的安装目录在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，将上述解压的 bin , include 和 lib 三个文件夹，直接拷贝覆盖到该安装目录下，即可完成CUDNN的安装。 进行完上述步骤后，我们还需要确保几个环境变量正确设置： 存在 CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，是我们的安装目录。理论上在CUDA安装后，该目录已经自动设置好。 环境变量 PATH 中，存在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin , C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/extras/CUPTI/libx64 两个路径，由于CUDNN已经配置在CUDA安装目录下，我们不需要像官方文档那样，添加第三个CUDNN的安装路径。","text_tokens":["压缩包","readme","同时","件夹","的","该","c++","说明","变量","cudnn64","好","x64","步骤","因此","gpu","包含","files","环境变量","没有",":","存在","以","不","后","：","cuda","应当","我们","目录","computing","lib","，","├","#","自带","c","还","extras","文件夹","文档","program","像","查看","_","安装","toolkit","需要","三个","不同","拷贝","最新","指南","─","几个","support","已经","新版","file","sla","由于","确保",",","正确","并","环境","libx64","中","官方","以下","那样","bin","最新版","如下","假设","但是","上","h","链接","上述","在","library",".","其中","安装包","完","解压","和","txt","理论","/","第三个","。","直接","覆盖","binary","第三","即可","=","下载","设置","自动","进行","│"," ","形式","文件","到","v10","两个","压缩","dll","cupti","nvidia","了","include","path","及","添加","是","0","下","环境变","路径","cudnn","本地","7","将","配置","完成","└"],"title":"安装CUDNN","title_tokens":["cudnn","安装"]},{"location":"book-1-x/chapter-1/hello-world/#anaconda","text":"Windows用户建议使用Anaconda管理python环境。作为一个开源的项目，Anaconda发行版已经集成了我们所需要的大多数python包，其中有些包是我们自己难以安装上的，例如支持python3的PIL。我们选用python3版的Anaconda，下载路径如下： Anaconda 最新版下载链接 注意选择x64版的安装包。安装结束后，我们已经有python3.7的环境了。鉴于有些情况下我们可能需要使用更早的python版本，接下来我们可以安装python3.6的虚环境。 Anaconda可以安装在用户或系统目录下。注意，如果安装在系统目录下，如果不在虚环境下，安装任何包都需要管理员模式。不过这不影响本教程的示例，因为我们将安装虚环境。直接打开 Anaconda Prompt ，并键入以下命令： conda create -n py36 python = 3 .6 anaconda 按照引导流程安装，经过一段时间后，我们将得到python3.6版的Anaconda。以后我们可以直接从开始菜单 Anaconda Prompt (py36) 进入该虚环境，也可以在基环境中键入 activate py36 转换到虚环境中。 提示 如果用户愿意，还可以通过 conda install -c conda-forge opencv 安装 opencv 。opencv是一个强大的图像处理库，这里python版的opencv比C++版更容易上手。一般该命令安装的是conda库中最新版的opencv（截至笔者写到这里时是opencv3.4.4）。然而，这样安装的并非是支持GPU的版本。尽管conda还提供一个GPU的opencv版本，可以通过 conda install -c patricksnape opencv-gpu 安装 opencv2 ，但它的版本还停留在2.4.9，和opencv3的用法殊有不同，请读者注意这一点。","text_tokens":["流程","opencv2","用法","版","建议","强大","的","该","尽管","这","写","库中","选用","c++","一个","x64","时间","gpu","大多数","但","项目","读者","提示","自己","开始","版本","比","不","：","后","开始菜单","模式","然而","我们","接下","可以","任何","愿意","目录","引导","，","时","进入","install","它","c","集成","还","系统目录","包都","2.4","难以","prompt","windows","anaconda","包","多数","安装","提供","可能","需要","这样","n","图像","图像处理","处理","不同","下来","选择","patricksnape","最新","所","已经","截至","新版","3","-","activate","示例","6","一段","(","使用","）","例如","有些","情况","并","环境","通过","手","得到","鉴于","结束","中","以下","pil","请","发行版","如下","最新版","键入","以后","一点","更","基","早","上","作为","链接","容易","有","在","命令","一段时间","这里","停留","也",".","其中","从","安装包","转换","和","包是","py36","管理员","。","直接","打开","菜单","python3","opencv","开源","（","=","下载","opencv3","并非","按照","发行","用户"," ","create","conda","如果","python","到","9","殊有","系统","教程","经过","段时间","一般","了","不过","接下来","本","注意","forge","虚","笔者","下","4.4",")","管理","影响","路径","7","将","支持","因为","库","或","大多","是"],"title":"安装Anaconda","title_tokens":["anaconda","安装"]},{"location":"book-1-x/chapter-1/hello-world/#tensorflow_1","text":"我们可以查看如下项目： fo40225/tensorflow-windows-wheel 该项目的维护者在不断根据Tensorflow的更新，预编译出适合不同版本的Tensorflow安装包。截至笔者写到这里为止，最新支持到r1.12版。我们选择对应的GPU版Tensorflow，并在虚环境下执行以下命令： pip install https://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.12.0/py36/GPU/cuda100cudnn73sse2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl 等待一段时间后，安装将会结束。","text_tokens":["tensorflow","版","的","预","写","r1","wheel","时间","gpu","项目","适合",":","版本","：","后","com","master","我们","编译","可以","对应","，","install","根据","执行","cuda100cudnn73sse2","windows","查看","_","安装","win","不同","选择","最新","截至","-","一段","目的","并","环境","pip","cp36m","结束","以下","不断","将会","如下","blob","12","在","出","命令","这里","一段时间",".","安装包","更新","该项","1.12","py36","/","。","https","等待","维护","whl"," ","到","为止","段时间","github","cp36","虚","笔者","下","0","fo40225","amd64","支持","维护者"],"title":"安装预编译好的Tensorflow","title_tokens":["tensorflow","好","编译","安装","的","预"]},{"location":"book-1-x/chapter-1/hello-world/#hello-world_1","text":"撰写如下代码，保存到 hello-world.py 文件， Python 1 2 3 4 5 6 7 8 9 if __name__ == '__main__' : import tensorflow as tf test_str = tf . constant ( 'Hello, world!' ) test_cal = tf . reduce_sum ( tf . random_normal ([ 1000 , 1000 ])) with tf . Session () as sess : print ( 'Current Tensorflow version is:' , tf . __version__ ) print ( 'Test string: ' , sess . run ( test_str )) print ( 'Test calculation:' , sess . run ( test_cal )) Output Current Tensorflow version is: 1 .12.0 Test string: b 'Hello, world!' Test calculation: -1948.6578 在保存目录下，执行 python hello-world.py 第一次运行可能需要等待较长的初始化时间（硬件配置）。如果能正常运行，我们将看到如下结果： 其中，显示结果之前的记录信息反映了GPU已经可以正常工作。而“Test calculation”显示的结果是 10^6 10^6 个服从 \\mathcal{N}(0,1) \\mathcal{N}(0,1) 分布的随机数之和。根据相互独立的随机正态分布互不相关的特性，我们可以推算出，该结果服从 \\mathcal{N}(0,10^3) \\mathcal{N}(0,10^3) 的正态分布。","text_tokens":["tensorflow","初始化","第一次","的","该","print","独立","硬件","2","tf","mathcal","5","时间","import","gpu","算出",":","：","!","互不","我们","可以","运行","一次","“","目录","，","较长","分布","constant","之","根据","执行","1000","显示","看到","}","version","output","相互","相关","^","_","可能","hello","需要","n","1","结果","随机","8","sum","{","str","推算","normal","工作","with","已经","3","current","\\","-","string","name","sess","保存","as","12.0","正态分布","6","第一","py","(","calculation","代码","4","）","main","b",",","'","特性","world","而","如下","之前","随机数","1948.6578","cal","[","run","session","在","记录",".","]","其中","和","。","初始","（","=","等待","服从","信息","test"," ","is","撰写","如果","文件","到","python","9","__","正常","”","了","random","反映","机数","下","0",")","推算出","能","7","配置","将","10","if","reduce","个","是"],"title":"Hello world! 测试","title_tokens":["测试","world","!"," ","hello"]},{"location":"book-1-x/chapter-1/linear-classification/","text":"线性分类 ¶ 摘要 本节介绍如何使用顺序模型(sequential model)来编写一个线性分类器，使用sigmoid函数激活，并验证其效果。 理论 ¶ 问题描述 ¶ 考虑我们有如下的二分类数据集 (\\mathbf{x},~y_i) \\in \\mathbb{D}_i (\\mathbf{x},~y_i) \\in \\mathbb{D}_i ，并且有一个未知的常数向量 \\mathbf{a}_i \\mathbf{a}_i 和未知的常数标量 c_i c_i ，使得： \\begin{equation} y_i = \\left\\{ \\begin{aligned} 0, && \\mathbf{a}_i^T \\mathbf{x} + c_i \\leqslant 0, \\\\ 1, && \\mathbf{a}_i^T \\mathbf{x} + c_i > 0. \\end{aligned} \\right. \\end{equation} 其中， \\mathbf{a} \\mathbf{a} 可以看成是某超平面的（未标准化的）法向量，那么 \\mathbf{a}^T \\mathbf{x} + c = 0 \\mathbf{a}^T \\mathbf{x} + c = 0 是该超平面的截距式定义，亦即该平面与 x_i x_i 轴的交点可以显式表述为 x_i^{(0)} = - \\frac{c}{a_i} x_i^{(0)} = - \\frac{c}{a_i} 。由此可知，式 (1) (1) 显式定义了一个点在超平面的哪一侧。特别地，若 \\mathbf{x} \\mathbf{x} 是一个二维向量，则该超平面退化为一维平面；若 \\mathbf{x} \\mathbf{x} 是一个标量，则该超平面退化为一条直线。 若我们定义 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} ，有 \\begin{equation} \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, ~~ \\mathbf{A} = \\begin{bmatrix} \\mathbf{a}^T_1 \\\\ \\mathbf{a}^T_2 \\\\ \\vdots \\\\ \\mathbf{a}^T_n \\end{bmatrix}, ~~ \\mathbf{c} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}. \\end{equation} 则我们可以认为 \\begin{align} \\mathbf{y} = \\left\\{ \\begin{bmatrix}\\hat{y}_1 > 0 & \\hat{y}_2 > 0 & \\cdots & \\hat{y}_n > 0\\end{bmatrix}^T, ~ \\left| ~ \\hat{\\mathbf{y}} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon} \\right. \\right\\}, \\end{align} 其中 \\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon} 是一个定义噪声的向量。 我们可以把向量 \\mathbf{y} \\mathbf{y} 的元素看成是互不相关的多个超平面对向量 \\mathbf{x} \\mathbf{x} 各自独立的分类结果。即 y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} 。由于每个超平面构成一个二分类，如果把每个二分类看作是向量是否属于这个类的测度，那么 \\mathbf{y} \\mathbf{y} 可以被看作是一个多分类的结果，尽管向量 \\mathbf{x} \\mathbf{x} 可能被同时分入多个类中。 假设我们的数据集 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} 符合 (3) (3) 定义的数据分布特征。我们的基本要求是，在我们不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 的情况下，使用大量 (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} 样本训练一个线性分类器，使得当我们给定任意一个新样本 \\mathbf{x} \\mathbf{x} 的时候，分类器能推断出其对应的 \\mathbf{y} \\mathbf{y} 来（亦即是否属于该分类）。 在这个问题里，我们虽然不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，但我们知道由 (3) (3) 确定的线性关系，因此，我们可以随机生成一组 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ，构建线性模型： \\begin{align} \\tilde{\\mathbf{y}} = \\sigma ( \\mathbf{W} \\mathbf{x} + \\mathbf{b} ). \\end{align} 其中，可微函数 \\sigma \\sigma 是一个将实数空间 \\mathbb{R}^n \\mathbb{R}^n 映射到有限范围的实数空间 [0,~1]^n [0,~1]^n 内的函数。特别地， \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 。因此，可以将 \\sigma \\sigma 看作是二分类布尔函数的插值函数。理论上，只要我们找到 \\mathbf{W}=\\mathbf{A} \\mathbf{W}=\\mathbf{A} ， \\mathbf{b}=\\mathbf{c} \\mathbf{b}=\\mathbf{c} ，则该线性分类器可以直接拟合出原分布来。 感知机 ¶ 我们将 (4) (4) 定义的线性模型称为 单层感知机 (Single-layer perceptron) 模型。它包含一个权重矩阵 \\mathbf{W} \\mathbf{W} 和一个偏置矩阵 \\mathbf{b} \\mathbf{b} 。事实上，可以将 (4) (4) 改写成如下形式 \\begin{align} \\tilde{\\mathbf{y}} = \\sigma \\left( \\begin{bmatrix} \\mathbf{W} & \\mathbf{b} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ 1 \\end{bmatrix} \\right). \\end{align} 可见偏置本身可以看成是输入向量多了一个常数元素的等价模型。 感知机是最早的神经网络形式，它非常孱弱，只能解线性问题，但却为神经网络后来的发展开了先河。在单层感知机里，我们视输入向量 \\mathbf{x} \\mathbf{x} 的每个元素为一个“神经元”，矩阵 \\mathbf{W} \\mathbf{W} 和偏置 \\mathbf{b} \\mathbf{b} 将我们的输入映射到输出层 \\mathbf{y} \\mathbf{y} ，输出层的每个元素也视为一个神经元。在这个过程中， W_{ij} W_{ij} 作为 i i 行 j j 列的元素，代表了连接两个神经元的权重。我们用红线代表正值，蓝线代表负值，感知机可以被图示为 线性感知机的输出也是输入的线性组合，但我们可以添加激活函数，即 \\sigma(\\cdot) \\sigma(\\cdot) 将其映射到非线性空间。这要求我们添加的激活函数是一个非线性函数。 事实上，将单层感知机层叠，前一层的输出作为后一层的输入，就构建出早期的神经网络。这种网络每一层都是全连接的（两个神经元之间总是有权重，尽管值可能为0），每一层都有激活函数。理论上，任意一个两层堆叠的感知机，只要神经元数目足够多，就可以拟合出任意一个非线性函数。然而，实际测试中，这一理论的效果并不尽如人意，因此又有陆续地改进，才有了后来的深度学习。饮水思源，鉴往知来，我们也将从这个简简单单的单层模型开始，走上学习“深度学习”之旅。 Sigmoid函数 ¶ 在上述介绍中，我们没有解决的两个问题是， 如何定义插值函数 \\sigma \\sigma ？ 如何找到合适的 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ？ 我们首先讨论第一个问题。一般地，多分类问题中，如果各个分类彼此并非相斥，且不一定要将结果分入任一类的话，我们可以用 Sigmoid 函数来定义 \\sigma \\sigma ，亦即 \\begin{align} \\sigma(\\mathbf{x}) = \\frac{1}{ 1 + e^{-\\mathbf{x}}}. \\end{align} 它同时满足 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 ，且是一个单调函数。以下代码向我们展示了这种函数的特性： Python 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import matplotlib.pyplot as plt def test_sigmoid (): x = np . linspace ( - 5 , 5 , 100 ) y = 1 / ( 1 + np . exp ( - x ) ) plt . plot ( x , y ) plt . xlabel ( 'x' ), plt . ylabel ( 'y' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () if __name__ == '__main__' : test_sigmoid () Output 使用sigmoid函数的一大好处是，它的导数求解非常简单，很适合用来做神经网络这样一个复杂模型的激活函数。注意虽然 \\sigma(\\mathbf{x}) \\sigma(\\mathbf{x}) 和 \\mathbf{x} \\mathbf{x} 都是向量，这意味着导数是Jacobian矩阵，但由于 \\sigma \\sigma 是一个对 \\mathbf{x} \\mathbf{x} 各元素独立的解析函数，这个Jacobian矩阵实际上是一个对角矩阵，对角线上第j个元素的值为 \\begin{align} \\left. \\frac{ \\partial \\sigma(x) }{ \\partial x } \\right|_{x=x_j} = \\left. - e^{-x} \\left( - \\frac{1}{\\left( 1+e^{-x} \\right)^2} \\right) \\right|_{x=x_j} = - \\sigma(x_j) \\sigma(1 - x_j). \\end{align} 可见，该函数的导数和计算函数本身的复杂度相若，可以做到快速求导。 求解问题 ¶ 接下来，我们需要解决第二个问题，亦即找到 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} 。这一问题通常可以写成反问题的形式： \\begin{align} \\arg \\min_\\limits{\\mathbf{W},~\\mathbf{b}} \\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}^{(k)},~ \\sigma ( \\mathbf{W} \\mathbf{x}^{(k)} + \\mathbf{b} ) \\right). \\end{align} 最简单的情况下，我们可以把 损失函数(loss function) 定义为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = \\lVert \\mathbf{y} - \\tilde{\\mathbf{y}} \\rVert_2^2. \\end{align} 我们称 (8) (8) 为 逻辑斯蒂回归(logistic regression) 。有趣的是，虽然这个术语叫“回归”，但它解的其实是个分类问题。但是，既然这是一个分类问题，我们可以不使用这个损失函数，而是从概率论的角度看待这个问题。由此，我们引出一个新的损失函数：“交叉熵”。 交叉熵 ¶ 我们视sigmoid函数输出的值为一个概率，表示分类器对预测结果的确信程度，记 \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} ，则 \\begin{equation} \\begin{aligned} \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}), \\\\ \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) &= 1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}). \\end{aligned} \\end{equation} 注意这里的概率向量的含义是，其中第i个元素表明第i个超平面分类结果的确信程度。 然而，这个概率只是分类器对分类结果的确信程度，却并非是分类准确度的概率，实际上，分类准确度的概率，应当表述为 \\begin{equation} \\begin{aligned} \\mathbf{p}(\\mathbf{y}|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta})^{1-\\mathbf{y}}\\\\ &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\left(1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}) \\right)^{1-\\mathbf{y}}. \\end{aligned} \\end{equation} 注意 这里 \\mathbf{x}^{\\mathbf{y}} \\mathbf{x}^{\\mathbf{y}} 表示的是对每个元素一一求取指数，即函数第i个元素的返回值应当为 {x_i}^{y_i} {x_i}^{y_i} 。 我们使用真实值 \\mathbf{y} \\mathbf{y} 作为指数给概率向量加权。当 \\mathbf{y}=1 \\mathbf{y}=1 时，以预测值为1的可信度作为概率；反之则以预测值为0的可信度作为概率。这就是最大似然估计方法。至此，我们可以写出似然估计函数 \\begin{align} L(\\boldsymbol{\\Theta}) = \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}). \\end{align} 对似然估计函数取对数，则有 \\begin{equation} \\begin{aligned} l(\\boldsymbol{\\Theta}) &= \\sum_{k=1}^N \\log \\left( \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}) \\right) \\\\ &= \\sum_{k=1}^N \\mathbf{y}^{(k)} \\cdot \\log\\left(\\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right) + \\left(1 - \\mathbf{y}^{(k)} \\right) \\cdot \\log\\left(1 - \\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right). \\end{aligned} \\end{equation} 我们最终的目的是要最大化似然函数，亦即 \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) ，这等价于最小化 -l(\\boldsymbol{\\Theta}) -l(\\boldsymbol{\\Theta}) 。对比 (8) (8) 和 (9) (9) ，于是我们可以定义交叉熵为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = \\mathrm{mean}\\left[ \\mathbf{y} \\cdot \\log\\left( \\tilde{\\mathbf{y}} \\right) + \\left(1 - \\mathbf{y} \\right) \\cdot \\log\\left(1 - \\tilde{\\mathbf{y}} \\right) \\right]. \\end{align} 注意这里我们使用 \\mathrm{mean}\\left[ \\cdot \\right] \\mathrm{mean}\\left[ \\cdot \\right] 表示求取一个向量所有元素的平均值。实际上，Tensorflow允许我们定义损失函数的输出为一个和输出向量维度相同的向量，Tensorflow自带的交叉熵也是这样定义的。实际应用时，Tensorflow会自动在向量维度上求均值，并压缩成上述 (14) (14) 的形式。 若我们记 \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) ，代入sigmoid函数，为了确保该损失函数的稳定性，我们可以将 (14) (14) 整理为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{x}} \\right) = \\mathrm{mean}\\left[ \\max(\\tilde{\\mathbf{x}}, \\mathbf{0}) - \\tilde{\\mathbf{x}} \\cdot \\mathbf{y} + \\log\\left(1 + e^{-|\\tilde{\\mathbf{x}}|} \\right) \\right]. \\end{align} 提示 这里交叉熵整理的推导过程参见 Tensorflow-API官方文档 。 实际情况下，我们使用 (15) (15) 来求取sigmoid函数激活下的交叉熵。 解线性多分类问题 ¶ 代码规范 ¶ 建立一个具有较强可读性的Tensorflow工程需要我们活用python的模块化设计。我们通常推荐以下的结构 . ├─ data/ # where we store our data │ └─ ... ├─ tools.py # codes for post-processing and analyzing records. ├─ extension.py # codes for extending the tensorflow model. ├─ dparser.py # data parser └─ main.py # main module where we define our tensorflow model. 除了保存数据的文件夹，我们应当有三个子模块。其中 tool : 用来处理、分析生成的数据，通常与Tensorflow无关； extension : 用来扩展tensorflow，例如在这里自定义网络层和操作符； dparser : 数据处理器，用来读取并预处理送入网络的数据； main : 主模块，只定义跟Tensorflow模型有关的内容，需要引用 extension 和 dparser 。 视情况可以灵活调整结构，但建议将定义Tensorflow模型的代码单独放在主模块里，和其他外围代码分离。 撰写各个模块时，建议使用类封装各组功能相同的函数。具有良好使用习惯的coder应当注意给各个面向用户的类、函数撰写（哪怕简短的）说明文字，在一些较长的函数、方法的定义中，适当注释各部分的功能，以便读者能正确理解代码意义。 另外，在对象命名上，python有如下必须遵守或不成文的规定，和C/C++用户熟悉的蛇形命名法不同，它大致包括 类与函数多用驼峰命名法，变量可以采用驼峰或蛇形命名法。 驼峰命名法指的是用大小写区分每个单词块，例如 alphaBetaFunction () ； 蛇形命名法指的是用下划线区分每个单词块，例如 alpha_beta_function = 10 ； 宏变量使用全字大写+蛇形命名法 函数/方法，还有模块均是首字母小写，但类的首字母大写。 用单下划线 _ 表示临时存储器，或省略参数，例如一个函数 func () 有两个返回值时，可以用 _ , b = func () 表示我们只需要第二个返回值；单下划线还可以与星号连用省略多个返回值； 以单下划线开头的方法，表示模块级的私有方法，在模块以外使用 import 导入类时，不会导入这些方法，例如 def _alphaBeta ( self ): ； 以单下划线结尾的对象，用来和python的关键字区分，例如 func ( x , class_ ) ; 以双下划线开头的方法，如果不以双下划线结尾，则表示类级的私有方法，只有类内部的方法能调用这些方法，在类外部、包括继承的子类里都原则上不能调用（但其实也有办法调用），例如 def _alphaBeta ( self ): ； 以双下划线同时开头和结尾的方法，一般是用来 重写 (override) 特殊功能，例如 def __getattribute__ (): 将重写获得类属性的方法。 Tensorflow的数据概念 ¶ 在Tensorflow中，我们把变量都称为“ 张量 (Tensor) ”。这是因为我们有零维的标量，一维的向量，二维的矩阵，更高维的我们都称为张量。作为一个更大的概念，张量当然也可以用来包括标量、向量和矩阵了。在Tensorflow中，有的张量是 可以训练 (trainable) 的，有的则不是。比如一个张量的形状（指各维大小），当然可以是一个 < tf . Tensor 'Shape:0' shape = ( 1 ,) dtype = int32 > 类型的张量，但它不是变量，当然就不可训练。我们也可以人为控制某些张量可以训练或不可以训练，但本节、乃至本章所介绍的凡是我们接触到的张量，都是可以训练的。 特别地，对于神经网络而言，在网络内计算（或者说流动、传播）的一个n维数据，通常按照以下形式组织： tensor [ batch , dim1 , dim2 , ... , dimn , channel ] 其中，第一个维度 batch 一定存在，它表示的是单个batch中的某一个样本。如果一个batch只有一个样本，那么 batch 只能取0。 从 dim1 到 dimn 指的是实际的n维数据的各个维度； channel 指的是数据的通道，例如，一个二维RGB图像，每种颜色代表一个通道，因此有三个通道。 channel 通常用在卷积网络里，我们经常需要在深度卷积网络里不断增大通道数的同时，缩小数据尺寸。 在某些特殊情况下， channel 维度可以不存在，例如我们使用的是全连接层而不是卷积网络， tf.keras.layer.Flatten 可以用来将一个有通道的张量压缩成一个没有通道的一维向量（但是注意 batch 维度仍然存在，不会被压缩）。 因此，我们知道一个n维的数据，在神经网络中通常被描述为一个n+2维的矩阵，而一个一维向量，在卷积网络里是三维的： vector [ batch , length , channel ] 但是在全连接网络里，是二维的： vector [ batch , channel ] 在本节，乃至本章里，我们还不讨论卷积网络，因此我们都是使用二维张量（一维向量组）作为我们的数据。 数据生成 ¶ 在本项目里，我们不需要扩展Tensorflow。但是，我们需要以随机生成数据代替数据集。因此，首先，通过以下代码定义数据生成器 dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class TestDataSet : ''' A generator of the data set for testing the linear model. ''' def __init__ ( self , scale_x , A , c ): ''' Initialize the data generator. scale_x: the scale of input vector. A, c: the linear transformation. ''' self . s_x = 2 * scale_x self . A = A self . c = c self . len_x = A . shape [ 0 ] self . config () def config ( self , train = True , batch = 100 , noise = 0.1 ): ''' Configuration train: a flag for controlling the iterator mode. batch: the number of samples in a batch noise: std. of the error added to the y. ''' self . train = bool ( train ) self . batch = batch self . noise = noise def next_train ( self ): ''' Get the next train batch: (x, y) ''' x = self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) y = np . matmul ( x , self . A ) + self . c if self . noise > 1e-3 : y = y + np . random . normal ( 0 , self . noise , size = y . shape ) y = np . greater ( y , 0.0 ) . astype ( np . float32 ) return x , y def next_test ( self ): ''' Get the next test batch x. ''' return self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) def __iter__ ( self ): while True : samp = self . __next__ () yield samp def __next__ ( self ): if self . train : return self . next_train () else : return self . next_test () 该生成器输入一组 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，以及相关配置，之后就可以通过 迭代器 (iterator) 或 方法 (method) 随机生成数据。这种数据集写法我们在后面还会用到， model . fit 允许我们不是馈入样本（或样本批次），而是馈入一个 生成器(generator) 。因此我们重写了 __iter__ 方法，并使其通过 yield 返回一个生成器。这样我们定义的数据集类就可以被Keras的训练函数 model . fit 使用。接下来，调用如下测试代码： dparser.py 1 2 3 4 5 6 7 8 9 10 def test_dataset (): A = np . random . normal ( 0 , 10 , [ 10 , 6 ]) c = np . random . uniform ( 1 , 3 , [ 1 , 6 ]) dataSet = TestDataSet ( 10 , A , c ) dIter = iter ( dataSet ) for i in range ( 10 ): x , y = next ( dIter ) print ( np . sum ( y , axis = 0 ) / 100 ) test_dataset () Output [ 0.47 0.57 0.58 0.56 0.5 0.38 ] [ 0.6 0.61 0.47 0.48 0.38 0.52 ] [ 0.5 0.61 0.49 0.42 0.45 0.53 ] [ 0.59 0.52 0.44 0.44 0.49 0.51 ] [ 0.54 0.59 0.48 0.5 0.51 0.47 ] [ 0.49 0.57 0.56 0.49 0.53 0.4 ] [ 0.5 0.61 0.51 0.54 0.51 0.52 ] [ 0.5 0.51 0.61 0.5 0.44 0.5 ] [ 0.44 0.46 0.53 0.45 0.56 0.52 ] [ 0.52 0.46 0.51 0.52 0.49 0.44 ] 我们随机生成了 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的数据，每组数据100个，并且测试了10组。输出结果是各组测试中， \\mathbf{y} \\mathbf{y} 在对应维度上分类为1的概率估计。结果基本都在0.5左右，说明我们的这种数据生成模式产生的点能均匀分布在各个超平面两侧，适合进行后续测试。 定义线性顺序模型 ¶ 顺序(sequential) 模型是一个单输入单输出模型，网络结构较为简单，也不存在跨层短接（残差连接）。在大多数情况下，已经上手的Tensorflow用户不使用这个模型，故而作为我们入门的第一个project，我们姑且用之，但我们将不再使用顺序模型来实现后续的project。一个顺序模型大致可以描述为下图的模式： graph LR st(输<br/>入) --> l1[层<br/>1] l1 --> l2[层<br/>2] l2 --> l3[层<br/>3] l3 --> ldots[层<br/>...] ldots --> ed(输<br/>出) classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,ed styStart 由于我们完成的是一个线性分类器，故而我们使用单层的序列模型即可。 接下来，我们来定义一个类， class LinClsHandle : 。定义一个类的时候，我们通常需要定义的内容包括 在初始化方法 __init__ 里定义传入网络的固定参数，例如学习速率，存取路径等； 在方法 construct 里定义网络的构造和使用的优化器； 在方法 train 里定义训练网络的过程，主要需要调用 model . fit 。如果我们在数据集的定义非常完善，则这一环节不需要花费太多的功夫； 在方法 test 里定义测试网络的过程，主要需要调用 model . evaluate 。如果有必要，可以通过 model . predict 返回测试结果。 初始化方法 ¶ 首先，定义初始化方法： lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 def __init__ ( self , learning_rate = 0.01 , epoch = 10 , steppe = 30 ): ''' Initialization and pass fixed parameters. learning_rate: the learning rate for optimizer. epoch: training epochs. steppe: steps per epoch ''' self . lr = learning_rate self . epoch = epoch self . steppe = steppe 由于目前我们的project还非常简单，这里只需要有学习速率( learning_rate )，轮次数( epoch )和每轮迭代次数( steppe )即可。 构造方法 ¶ 接下来定义网络构造 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def construct ( self ): ''' Construct a linear model and set the optimizer as Adam ''' # Construction self . model = tf . keras . Sequential () self . dense1 = tf . keras . layers . Dense ( LABEL_SHAPE , use_bias = True , input_shape = ( INPUT_SHAPE ,), kernel_initializer = tf . keras . initializers . RandomNormal ( 0.0 , stddev = 10.0 ), bias_initializer = tf . keras . initializers . Constant ( 2 ), activation = None ) self . model . add ( self . dense1 ) # Set optimizer self . model . compile ( optimizer = tf . train . AdamOptimizer ( self . lr ), loss = self . loss , metrics = [ self . accuracy ] ) @staticmethod def loss ( y_true , y_pred ): return tf . nn . sigmoid_cross_entropy_with_logits ( labels = y_true , logits = y_pred ) @staticmethod def accuracy ( y_true , y_pred ): return tf . keras . backend . mean ( tf . keras . backend . equal ( y_true , tf . keras . backend . round ( tf . keras . backend . sigmoid ( y_pred )))) 须知 这里 LABEL_SHAPE 和 INPUT_SHAPE 为两个宏变量，分别为输出和输入的向量维度。 我们使用 Dense 定义全连接层，它的用法请参照 这里 。由于我们已经知道 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 可能的取值范围，这里我们重定义了 \\mathbf{W} \\mathbf{W} 和 \\mathbf{b} \\mathbf{b} 的初始化方式。 信息: Dense API tf . keras . layers . Dense ( shape , ** kwargs ) 指全连接层，其输入一组已知形状的向量，输出一组形状为 shape 的向量。可用的API如下： shape : 正整数，输出空间维度。 activation : 激活函数。 若不指定，则不使用激活函数 (即，线性激活: a(\\mathbf{y}) = \\mathbf{y} a(\\mathbf{y}) = \\mathbf{y} )。该函数可以定义为任何元素级操作的Tensorflow函数。 use_bias : 布尔值，该层是否使用偏置向量。 True 则网络定义为 \\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} \\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} ， False 则定义为 \\mathbf{y} = \\mathbf{W}\\mathbf{x} \\mathbf{y} = \\mathbf{W}\\mathbf{x} 。 kernel_initializer : kernel 权值矩阵的初始化器，自定义的初始化器需要使用Keras后端API编写。 bias_initializer : 偏置向量的初始化器，同上。 kernel_regularizer : 运用到 kernel 权值矩阵的正则化函数，自定义的正则化函数需要使用Keras后端API编写。 bias_regularizer : 运用到偏置向的的正则化函数，同上。 activity_regularizer : 运用到层的输出的正则化函数，同上。 kernel_constraint : 运用到 kernel 权值矩阵的约束函数，只能使用Keras备选的几种方案，不能自定义。 bias_constraint : 运用到偏置向量的约束函数，同上。 信息: model.compile API model . compile ( optimizer , ** kwargs ) 在这里指的是顺序模型的编译函数，其可用的API如下： optimizer : 优化器，可以使用Tensorflow内置的优化器。 loss : 损失函数，也是目标函数。顺序模型只有一个输出，因此只能传入一个损失函数。可以使用形式为 func ( y_true , y_pred ) 的Tensorflow函数。 metrics : 测度函数，一般是一组函数，如果是一个函数则定义为 [ func ] 即可。自定义的测度函数目前还需要使用Keras后端API编写。 loss_weights : 损失的权重，顺序模型只有一个损失函数，因此只有一个权重，但要使用一维列表 [ value ] 定义。可以使用张量来控制可变权重。 sample_weight_mode : 按时间步采样权重，默认不提供。相比上面的损失权重，该选项会随着迭代次数使用不同的权重，因此输入的是二维列表。 weighted_metrics : 测度的权重，和损失权重类似，用来加给不同的测度函数。由于我们可以使用不只一个测度函数，这里的权重是个一维列表。 target_tensors : 默认情况下，Keras 将为模型的目标创建一个占位符，在训练过程中将使用目标数据。相反，如果你想使用自己的目标张量（反过来说，Keras在训练期间不会载入这些目标张量的外部 Numpy数据），您可以通过 target_tensors 参数指定它们。对于单输出的顺序模型，它应该是单个张量。 ** kwargs : 其他参量，会传递给 tf . Session . run 。 另外，注意我们这里构造网络的时候有如下技巧： 我们定义的网络输出是 \\mathbf{W}\\mathbf{x} + \\mathbf{b} \\mathbf{W}\\mathbf{x} + \\mathbf{b} ，而非 \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) 。这是因为我们需要通过还未被激活的输出用来计算sigmoid交叉熵，亦即式 (15) (15) ； 我们通过静态方法，直接调用Tensorflow自带的 sigmoid交叉熵 函数来作为Keras模型的损失函数 self..loss ； 我们通过静态方法，调用Keras的后端API，自己定义了预测准确度的测度函数 self.accuracy ； 我们将网络层的关键字 self.dense1 保留在了实例中，这是为了确保接下来我们能通过实例抽取该层的参数。 之所以煞费周折地进行这些处理，盖因为Keras的内建API里目前还没有提供对互不相斥的多分类的支持。例如，无论是 tf.keras.metrics.categorical_accuracy 还是 tf.keras.metrics.categorical_crossentropy ，都要求分类的真实值为one-hot类型的向量组，因而它们只适合用在softmax分类器上。为了解决这一问题，我们自己实现了sigmoid分类器。 训练和测试方法 ¶ 最后定义的式训练和测试方法。由于我们目前的project还比较简单，关于这两部分都直接调用现有的API即可。使用的API在之前已经说明。 model.fit 在没有额外设置的情况下，默认会返回一个 History回调器 ； model.evaluate 返回的是测试样本给出的损失函数和准确值测度。 model.predict 返回的是测试样本给出的网络输出。详情请参照 顺序模型API 。 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def train ( self , dataSet ): ''' Use a data set to train the network. ''' return self . model . fit ( dataSet , epochs = self . epoch , steps_per_epoch = self . steppe ) def test ( self , data , labels ): ''' Use (data, label) pairs to test the results. ''' loss , accu = self . model . evaluate ( data , labels ) print ( 'Evaluated loss =' , loss ) print ( 'Evaluated accuracy =' , accu ) return self . model . predict ( data ) 信息: model.fit API model . fit ( dataSet , ** kwargs ) 是训练函数，注意这个函数虽然支持输入一组 x,~y x,~y 用来代替 dataSet ，我们还是建议在任何情况下都用dataSet馈送数据，以免内存中数据集占用过多。 dataSet : 数据集，其本身应当是一个 tf.data.Dataset 类型的类，或者是一个能不断迭代产生新数据的生成器。数据的 batch 大小由 dataSet 本身决定。 epochs : 整数，终止训练时经历的世代(轮次)数，通常一个epoch表示遍历整个数据集一回。 verbose : 0, 1或2。日志显示模式。 0=安静模式, 1=进度条, 2=每轮一行。默认是1。 callbacks : 回调器，它是 tf.keras.callbacks 模块下的类，用来在训练中进行记录保存和数据检查点更新。默认是 tf.keras.callbacks.History 。 validation_split : 在 0 和 1 之间浮动。用作验证集的训练数据的比例。模型将分出一部分不会被训练的验证数据，并将在每一轮结束时评估这些验证数据的误差和任何其他模型指标。验证数据是混洗之前 x 和 y 数据的最后一部分样本中。 validation_data : 元组 (x_val，y_val) 或元组 (x_val，y_val，val_sample_weights) ，用来评估损失，以及在每轮结束时的任何模型度量指标。模型将不会在这个数据上进行训练。这个参数会覆盖 validation_split 。 shuffle : 布尔值（是否在每轮迭代之前混洗数据）。当 steps_per_epoch 非 None 时，这个参数无效。 class_weight : 可选的字典，用来映射类索引（整数）到权重（浮点）值，用于加权损失函数（仅在训练期间）。这可能有助于告诉模型 「更多关注」来自代表性不足的类的样本。 sample_weight : 用来给损失函数添加权重，作用类似 model.compile 的同一参数。 initial_epoch : 开始训练的轮次（有助于恢复之前的训练）。 steps_per_epoch : 在声明一个轮次完成并开始下一个轮次之前的总步数（样品批次）。使用TensorFlow数据张量等输入张量进行训练时，默认值 None 等于数据集中样本的数量除以 batch 的大小，如果无法确定，则为1。 validation_steps : 只有在指定了 steps_per_epoch 时才有用。停止前要验证的总步数（批次样本）。 该函数会返回 callbacks 定义的实例。 信息 model.evaluate API model . evaluate ( x , y , ** kwargs ) 是测试函数，需要传入 label 即 y 来验证性能。 x , y : Numpy 数组，分别是输入和输出的真实参照值。 batch_size : 计算的 batch 大小，该函数会将输入的数据组拆分成许多 batch 分别计算，并组合起来，这个设定值对效果不产生影响，只影响计算过程。 verbose : 0, 1。日志显示模式。0=安静模式, 1=进度条。默认是1。 sample_weight : 用来给损失函数添加权重，作用类似 model.compile 的同一参数。 steps : 整数或 None 。 声明评估结束之前的总步数（批次样本）。默认值 None 。 该函数会返回损失函数和测度（列表）。 信息 model.predict API model . predict ( x , ** kwargs ) 是预测函数，用在只知道输入 x 的场合。 x : Numpy 数组，输入值。 batch_size : 计算的 batch 大小，该函数会将输入的数据组拆分成许多 batch 分别计算，并组合起来，这个设定值对效果不产生影响，只影响计算过程。 verbose : 0, 1。日志显示模式。0=安静模式, 1=进度条。默认是1。 steps : 整数或 None 。 声明评估结束之前的总步数（批次样本）。默认值 None 。 该函数返回预测结果。 调试 ¶ 首先，训练网络。我们随机生成 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的线性变换，并且设置好数据集，给定噪声扰动为 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 。设定20个epoch，每个epoch迭代500次，每次馈入32个样本构成的batch，然后开始训练： lin-cls.py 1 2 3 4 5 6 7 8 A = np . random . normal ( 0 , 10 , [ INPUT_SHAPE , LABEL_SHAPE ]) c = np . random . uniform ( 1 , 3 , [ 1 , LABEL_SHAPE ]) dataSet = dp . TestDataSet ( 10 , A , c ) dataSet . config ( batch = 32 , noise = 0.1 ) # Construct the model and train it. h = LinClsHandle ( learning_rate = 0.01 , epoch = 20 , steppe = 500 ) h . construct () record = h . train ( iter ( dataSet )) Output Epoch 1 /20 500 /500 [==============================] - 3s 5ms/step - loss: 48 .2269 - accuracy: 0 .5458 Epoch 2 /20 500 /500 [==============================] - 1s 2ms/step - loss: 25 .5149 - accuracy: 0 .6491 Epoch 3 /20 500 /500 [==============================] - 1s 2ms/step - loss: 11 .9822 - accuracy: 0 .7607 Epoch 4 /20 500 /500 [==============================] - 1s 2ms/step - loss: 5 .6580 - accuracy: 0 .8513 Epoch 5 /20 500 /500 [==============================] - 1s 2ms/step - loss: 2 .7230 - accuracy: 0 .9106 Epoch 6 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .1082 - accuracy: 0 .9462 Epoch 7 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .3278 - accuracy: 0 .9708 Epoch 8 /20 500 /500 [==============================] - 1s 3ms/step - loss: 0 .0618 - accuracy: 0 .9878 Epoch 9 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0149 - accuracy: 0 .9963 Epoch 10 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9979 Epoch 11 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0124 - accuracy: 0 .9976 Epoch 12 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9978 Epoch 13 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9973 Epoch 14 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0120 - accuracy: 0 .9974 Epoch 15 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9970 Epoch 16 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0116 - accuracy: 0 .9971 Epoch 17 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0120 - accuracy: 0 .9967 Epoch 18 /20 500 /500 [==============================] - 1s 3ms/step - loss: 0 .0114 - accuracy: 0 .9971 Epoch 19 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0114 - accuracy: 0 .9969 Epoch 20 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0113 - accuracy: 0 .9970 接下来，从训练返回的 History 类型的回调器中抽取对loss和accuracy的记录。 lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 12 13 def showCurve ( x , y , xlabel = None , ylabel = None , log = False ): if log : plt . semilogy ( x , y ) else : plt . plot ( x , y ) if xlabel is not None : plt . xlabel ( xlabel ) if ylabel is not None : plt . ylabel ( ylabel ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () showCurve ( record . epoch , record . history [ 'loss' ], xlabel = 'epoch' , ylabel = 'Cross entropy' , log = True ) showCurve ( record . epoch , record . history [ 'accuracy' ], xlabel = 'epoch' , ylabel = 'Accuracy' ) Output 重新设定数据集的产生方式，变为每个batch含10个样本。使用这组重新随机生成的数据测试网络输出， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 12 # Generate a group of testing samples: dataSet . config ( batch = 10 ) x , y = next ( dataSet ) # Check the testing results yp = dp . sigmoid ( h . test ( x , y )) _ , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 ) ax1 . imshow ( y , interpolation = 'nearest' , aspect = 'auto' ) ax1 . set_title ( 'True class' ) ax2 . imshow ( yp , interpolation = 'nearest' , aspect = 'auto' ) ax2 . set_title ( 'Predicted class' ) plt . gcf () . set_size_inches ( 10 , 5 ), plt . show () Output 注意我们未对测量的结果阈值化，因此显示出来的测量结果和理想值略有差别，但从图可知，阈值化后则测量结果全部准确。 通过抽取 h.dense1 的参数，我们可以对比 \\mathbf{A} \\mathbf{A} 和 \\mathbf{W} \\mathbf{W} ，以及 \\mathbf{c} \\mathbf{c} 和 \\mathbf{b} \\mathbf{b} ， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 # Check the regressed values W , b = h . dense1 . get_weights () plt . imshow ( A , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'A' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . imshow ( W , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'W' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . plot ( c . T , label = 'c' ) plt . plot ( b . T , label = 'b' ) plt . legend () plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () Output 可以发现，虽然我们训练的分类器十分有效，但其权值和预期的 \\mathbf{A} \\mathbf{A} , \\mathbf{c} \\mathbf{c} 并不相同。这是由于我们训练的样本加入了噪声。这种技术常用于神经网络的训练，被认为是一种提高鲁棒性、减小过拟合、避免不稳定解的一个有效手段。可以看出真实值 \\mathbf{A} \\mathbf{A} 存在偏高值，但 \\mathbf{W} \\mathbf{W} 的数值更加均匀。","text_tokens":["两","24","adam","」","36","这些","model","编写","construction","均","似然","bool","跟","optimizer","单调","来自","项目",":","用作","数据测试","饮水","字典","编译","trainable","shape","于是","详情","世代","内存","pairs","exp","fixed","决定","callbacks","用","误差","max","name","占用","0.46","nearest","确保","反过来","属于","initialization","dp","整数","为了","正整数","常用","'","然后","哪","推断出","简简单单","解","上","角度","0.56","labels","整理","和","模块","你","6580","还有","35","。","测度","交叉","规定","办法","思源","导入","由","tensor","bmatrix","softmax","区分","9878","摘要","dtype","较为","网络结构","一行","print","~","0.0","等于","技术","evaluate","备选","全","特征","54","快速","可见","层","亦","c","事实上","这一","估计","可读性","只是","alpha","可能","alphabetafunction","这样","内置","colorbar","分析","构造","馈入","3","rate","9106","generator",",","analyzing","给","声明","良好","通过","40","gca","linspace","check","而","it","常数","6491","initializer","weights","后来","上求","0.59","应用","]","才","dparser","简单","即可","每个","无论是","遍历","val","数组","姑且","如果","对象","默认","标量","输","yield","100","大量","各","传递","7","完成","verbose","法","跨层","等","用法","22","有用","求导","added","batch","采用","传播","信度","分类器","showcurve","并且","无法","因此","import","器","调试","一种","entropy","get","称为","模式","缩成",";","神经网络","知道","network","给出","label","一部分","log","后面","首字","每轮","深度","文档","^","载入","符","数","关键","提高","0116","一轮","aligned","原则上","起来","所","短接","\\","却","extension","步数","指各维","主要","彼此","表明","推荐","group","codes","设定值","意味着","导数","input","0.44","这组","fae6a9","引出","之前","是因为","集中","调整结构","*","logits","一部","23","截距","进行","add","器中","值为","外部","target","足够","抽取","了","很","上手","接下来","16","0114","标准化",")","33","流动","配置","支持","vector","考虑","该","而言","k","9978","入门","¶","理器","现有","但","加入","左右","iter","即","线性变换","命名","之旅","define","若","任何","尽如","返回值","走上","linclshandle","推断","的话","索引","训练","集","0.38","零维","非线性","发展","1e","多数","0.57","staticmethod","{","sum","next","aspect","实际上","节","组合","2269","上面","9979","int32","费周折","一大","轴","基本","包括","则","中","官方","称","须知","之间","向用","出原","某","准确度","axis","记录","预测值","同一","全部","直接","layers","优化","learning","一组","无关","arg","额外","回调","设置","并非","级","熵","9822","0.1","9","分别","压缩","做到","子","多用","混洗","总","问题","实现","组拆","因为","0.5","总是","个","取值","残差","意义","数值","sequential","data","fill","还会","ax1","描述","意味","可信","39","人为","有限","次数","准确","0.6","j","蛇形","ed","组织","设计","每次","9969","split","显示","可用","dimn","ylabel","关于","不同","代表","our","反之","之所以","-","端","对于","只能","x","形状","非","最早","只","30","矩阵","parser","0.61","大小写","如下","equal","已知","matmul","测试","function","rgb","成器","不能","解决","要","=","实数","盖","参照","效果","单单","这种","46","大化","方法","预处理","不可","？","auto","random","compile","activity","函数","添加","子类","宏","路径","引用","复杂","表示","线性关系","建立","history","s","还是","交点","34","该层","5ms","of","包含","视为","extending","以及","映射","一个多","默认值","activation","对数","2ms","各个","比较","用单","还","化后","adamoptimizer","最小化","metrics","介绍","提供","值","1","结果","处理","channel","27","部分","6","不成文","）","main","类时","被","dense1","假设","知机","整个","controlling","lvert","9708","那么","train","通常","组","有效","相比","9967","weight","用户","关注"," ","特殊","撰写","内容","两个","分入","cls","布尔","行","除了","mapsto","下","影响","过","化","开","看成","the","堆叠","参量","└","两层","经常","对","返回","尽管","鲁棒性","小写","jacobian","tf","scale","好","end","星号","imshow","mode","pass","含义","iterator","存在","np","一个二维","我们","虽然","plt","分布","省略","生成","size","error","原则","output","_","仅","dim1","比较简单","成","不成","37","保存","第一","多","就是","方案","特性","或者说","差别","获得","满足","ax2","既然","run","作为","上述","在","以双","理论","环节","合适","输入","test","二","class","程度","true",">","legend","看作","当然","44","0.58","有趣","煞","地","1082","以单","性能","重写","51","classdef","能","来","驼峰","pred","最","single","本章","frac","反过","感知","mathbf","设定","、","diter","a","然而","可以","接下","可读","50","numpy","反","kwargs","分出","regressed","之后","steppe","工程","周折","mathbb","主","8","crossentropy","指数","操作","values","@","静态方法","浮动","控制","目的","kernel","占位","与","情况","私有","每组","下划","至此","有","未","较强","0618","下图","对角线","构成","时候","method","0120","”","python","st","几种","比例","一层","轮","偏置","epoch","度量","文字","法指","varepsilon","压缩成","标准","指定","store","件夹","对比","5458","稳定","正确理解","+","一个","事实","双","sigmoid","学习","大多数","第二","自己","sample","所有","由此","成文","早期","类","false","42","，","├","而是","e","不足","样品","keras","11","predicted","logistic","configuration","9963","三个","随机","图像","花费","下来","─","线性组合","第二个","with","维度","0121","任","最小","41","w","as","使得","视","正确","epochs","不会","扰动","概率","结束","stroke","自定","xlabel","您","关键字","正则","这里","非常","..","无论","&","其中","许多","各组","分类","更新","选项","hat","第一个","凡是","创建","13","新","按照","9974","好处","以外","重","只有","关系","结尾","三维","接触","逻辑","0.53","in","相反","目标","每种","数据分布","第","0.51","and","两侧","10","权重","uniform","record","空间","subplots","下划线","字母","初始化","显式","是否","建议","title","变量","alphabeta","神经网","5149","17","时间","可知","适合","先河","regression","正值","“","向量","符合","权值","类中","相关","使","增大","partial","astype","斯蒂","习惯","由于","内部","均匀分布","47","求解","<","扩展","b","18","initialize","改写","d","并","写法","tool","适当","必须","功能","阈值","送入","lr","请","封装","weighted","出来","指标","h","以免","放在","stddev","类似","安静","产生","可选","初始","gcf","rvert","通道","自动","孱弱","pyplot","一条","48","指全","else","鉴往知来","内","只要","更加","mean","一定","维","一类","用于","dense","神经","复杂度","即式","首先","各自","一","均匀","独立","dim2","这","都","c++","49","过多","入","高维","代表性","p","training","要求","网络层","灵活","38","以","不","func","set","对应","二个","尽如人意","过程中将","samples","dataset","分离","0.48","用到","right","顺序","用来","一回","0.42","loss","flatten","begin","向","含","图","看待","20","代替","于","&&","叫","参数","[","连接","应该","指","最大","馈送","layer","show","可微","regularizer","每","module","覆盖","「","定义","inches","平面","手段","进度","功夫","0124","后端","简短","到","以便","轮次","确定","本身","找到","迭代","0","将","回归","负值","范围","采样","处理器","43","存取","测量","backend","单词","求取","ldots","2","测试代码","lin","0149","讨论","10.0","属性","float32","开始","给定","round","最后","l1","调整","想","不再","#","自带","它","we","之","输出","相若","真实","实例","缩小","改进","categorical","前","仍然","coder","张量","需要","8513","evaluated","无效","def","return","减小","1s","写出","加","有助","线性","to","per","(","代码","4","操作符","shuffle","例如","调用","模型","19","这是","检查","结构","具有","3278","stystart","为","步","饮水思源","samp","0.54","min","颜色","太多","nn","就","且","self","accu","processing","十分","有关","解析","validation","vdots","均值","固定","26","31","更大","for","首字母","定性","if","br","i","日志","稳定性","records","但类","cdot","方式","不是","一维","5","因而","单层","按","其实","集类","读者","应当","性关系","|","激活","对角","中将","；","flag","noise","时","较长","全字","constant","7607","}","认为","accuracy","limits","经历","n","尺寸","同","fit","where","range","概率论","py","式","测试函数","连用","one","存储器","反过来说","网络","initializers","equation","场合","更","又","12","session","列","9971","r","静态","leqslant",".","把","sim","value","predict","/","必要","样本","构建","不以","相同","一些","确信","53","9462","过来","验证","浮点","l","semilogy","形式","开头","is","传入","post","0.45","bias","后续","29","9970","其","initial","或","plot","会","tools","config","boldsymbol","use","没有","噪声","活用","matplotlib","完善","略有","目前","32","熟悉","align","展示","查点","本节","0113","评估","直线","14","某些","interpolation","详情请","generate","约束","3ms","不断","mathrm","perceptron","时才","运用","表述","cross","出","来说","一侧","beta","看出","length","写成","api","len","none","数据","构造方法","比如","块","里","│","有助于","0.01","hot","文件","__","临时","拟合","theta","生成器","面向用户","或者","sigma","未知","9973","避免","大多","元素","测试方法","是","元组","15","500","同时","28","作用","超平面","parameters","可变","45","9976","并不相同","所以","自定义","21","变换","提示","transformation","它们","变为","后","ba9132","linear","互不","进度条","cdots","一一","技巧","遵守","术语","理想","当","陆续","left","前要","插值","布尔值","testdataset","not","预期","testing","0.49","单独","init","normal","已经","由此可知","理解","52","单","哪怕","数量","检查点","加权","tilde","保留","使用","随着","除以","大小","等价","大致","y","特别","点","...","故而","实际","可信度","（","大写","非常简单","平均值","告诉","偏高值","预测","一般","内建","分成","另外","注意","任意","number","次","图示","助于","速率","其他","允许","类型","0.4","graph","划线","重新","tensorflow","蓝线","发现","randomnormal","的","退化","做","定义数据","说明","7230","mathcal","人意","相斥","乃至","二维","：","注释","概念","面向","一个点","最终","停止","批次","project","t","记","如何","文件夹","外围","平均","override","过程","yp","l3","取","代入","红线","以下","模块化","期间","参见","列表","l2","但是","层叠","这个","感知机","终止","推导","0.47","也","从","ij","计算","序列","恢复","0.52","多个","25","信息","单个","类级","std","损失","constraint","规范","继承","3s","最大化","存储","steps","construct","较为简单","本","getattribute","读取","神经元","卷积","greater","step","infty","数目","tensors","while","results"],"title":"线性分类","title_tokens":["分类","线性"]},{"location":"book-1-x/chapter-1/linear-classification/#_1","text":"摘要 本节介绍如何使用顺序模型(sequential model)来编写一个线性分类器，使用sigmoid函数激活，并验证其效果。","text_tokens":["(","，","使用","验证","sequential","效果","并"," ","模型","model","如何","一个","编写","介绍","分类器","sigmoid","本","摘要","函数","顺序",")","分类","节","来","。","其","激活","线性"],"title":"线性分类","title_tokens":["分类","线性"]},{"location":"book-1-x/chapter-1/linear-classification/#_2","text":"","text_tokens":[],"title":"理论","title_tokens":["理论"]},{"location":"book-1-x/chapter-1/linear-classification/#_3","text":"考虑我们有如下的二分类数据集 (\\mathbf{x},~y_i) \\in \\mathbb{D}_i (\\mathbf{x},~y_i) \\in \\mathbb{D}_i ，并且有一个未知的常数向量 \\mathbf{a}_i \\mathbf{a}_i 和未知的常数标量 c_i c_i ，使得： \\begin{equation} y_i = \\left\\{ \\begin{aligned} 0, && \\mathbf{a}_i^T \\mathbf{x} + c_i \\leqslant 0, \\\\ 1, && \\mathbf{a}_i^T \\mathbf{x} + c_i > 0. \\end{aligned} \\right. \\end{equation} 其中， \\mathbf{a} \\mathbf{a} 可以看成是某超平面的（未标准化的）法向量，那么 \\mathbf{a}^T \\mathbf{x} + c = 0 \\mathbf{a}^T \\mathbf{x} + c = 0 是该超平面的截距式定义，亦即该平面与 x_i x_i 轴的交点可以显式表述为 x_i^{(0)} = - \\frac{c}{a_i} x_i^{(0)} = - \\frac{c}{a_i} 。由此可知，式 (1) (1) 显式定义了一个点在超平面的哪一侧。特别地，若 \\mathbf{x} \\mathbf{x} 是一个二维向量，则该超平面退化为一维平面；若 \\mathbf{x} \\mathbf{x} 是一个标量，则该超平面退化为一条直线。 若我们定义 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} ，有 \\begin{equation} \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, ~~ \\mathbf{A} = \\begin{bmatrix} \\mathbf{a}^T_1 \\\\ \\mathbf{a}^T_2 \\\\ \\vdots \\\\ \\mathbf{a}^T_n \\end{bmatrix}, ~~ \\mathbf{c} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}. \\end{equation} 则我们可以认为 \\begin{align} \\mathbf{y} = \\left\\{ \\begin{bmatrix}\\hat{y}_1 > 0 & \\hat{y}_2 > 0 & \\cdots & \\hat{y}_n > 0\\end{bmatrix}^T, ~ \\left| ~ \\hat{\\mathbf{y}} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon} \\right. \\right\\}, \\end{align} 其中 \\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon} 是一个定义噪声的向量。 我们可以把向量 \\mathbf{y} \\mathbf{y} 的元素看成是互不相关的多个超平面对向量 \\mathbf{x} \\mathbf{x} 各自独立的分类结果。即 y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} 。由于每个超平面构成一个二分类，如果把每个二分类看作是向量是否属于这个类的测度，那么 \\mathbf{y} \\mathbf{y} 可以被看作是一个多分类的结果，尽管向量 \\mathbf{x} \\mathbf{x} 可能被同时分入多个类中。 假设我们的数据集 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} 符合 (3) (3) 定义的数据分布特征。我们的基本要求是，在我们不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 的情况下，使用大量 (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} 样本训练一个线性分类器，使得当我们给定任意一个新样本 \\mathbf{x} \\mathbf{x} 的时候，分类器能推断出其对应的 \\mathbf{y} \\mathbf{y} 来（亦即是否属于该分类）。 在这个问题里，我们虽然不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，但我们知道由 (3) (3) 确定的线性关系，因此，我们可以随机生成一组 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ，构建线性模型： \\begin{align} \\tilde{\\mathbf{y}} = \\sigma ( \\mathbf{W} \\mathbf{x} + \\mathbf{b} ). \\end{align} 其中，可微函数 \\sigma \\sigma 是一个将实数空间 \\mathbb{R}^n \\mathbb{R}^n 映射到有限范围的实数空间 [0,~1]^n [0,~1]^n 内的函数。特别地， \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 。因此，可以将 \\sigma \\sigma 看作是二分类布尔函数的插值函数。理论上，只要我们找到 \\mathbf{W}=\\mathbf{A} \\mathbf{W}=\\mathbf{A} ， \\mathbf{b}=\\mathbf{c} \\mathbf{b}=\\mathbf{c} ，则该线性分类器可以直接拟合出原分布来。","text_tokens":["frac","考虑","i","交点","各自","显式","对","是否","同时","的","该","退化","独立","尽管","~","+","一个","超平面","k","2","分类器","并且","一维","end","mathbf","因此","要求","boldsymbol","有限","但","可知","二维","噪声","特征","给定","即","不","：","由此","性关系","映射","一个二维","a","|","空间","一个多","互不","我们","类","可以","若","对应","；","cdots","知道","虽然","一个点","向量","，","亦","推断","符合","分布","生成","c","训练","align","集","t","}","认为","类中","当","left","相关","^","mathbb","right","_","插值","可能","n","1","结果","随机","{","直线","aligned","由此可知","3","\\","-","w","begin","由于","属于","tilde","线性","(","x","使用","）","与","轴","式",",","b","使得","基本","d","情况","哪","模型","则","&&","如下","推断出","equation","常数","被","假设","出原","[","上","某","表述","有","在","y","特别","这个","未","一侧","r","leqslant",".","&","其中","]","把","那么","分类","为","和","可微","理论","hat","。","构成","样本","直接","构建","定义","（","一组","测度","时候","每个","=","多个","数据","实数","新","平面","截距","一条","二","里"," ","如果","内","只要","由","到",">","分入","看作","拟合","vdots","bmatrix","关系","标量","确定","了","布尔","找到","in","地","函数","大量","标准化","任意","0","下","问题",")","sigma","infty","未知","能","来","数据分布","将","看成","其","0.5","法","varepsilon","元素","线性关系","是","标准","范围"],"title":"问题描述","title_tokens":["描述","问题"]},{"location":"book-1-x/chapter-1/linear-classification/#_4","text":"我们将 (4) (4) 定义的线性模型称为 单层感知机 (Single-layer perceptron) 模型。它包含一个权重矩阵 \\mathbf{W} \\mathbf{W} 和一个偏置矩阵 \\mathbf{b} \\mathbf{b} 。事实上，可以将 (4) (4) 改写成如下形式 \\begin{align} \\tilde{\\mathbf{y}} = \\sigma \\left( \\begin{bmatrix} \\mathbf{W} & \\mathbf{b} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ 1 \\end{bmatrix} \\right). \\end{align} 可见偏置本身可以看成是输入向量多了一个常数元素的等价模型。 感知机是最早的神经网络形式，它非常孱弱，只能解线性问题，但却为神经网络后来的发展开了先河。在单层感知机里，我们视输入向量 \\mathbf{x} \\mathbf{x} 的每个元素为一个“神经元”，矩阵 \\mathbf{W} \\mathbf{W} 和偏置 \\mathbf{b} \\mathbf{b} 将我们的输入映射到输出层 \\mathbf{y} \\mathbf{y} ，输出层的每个元素也视为一个神经元。在这个过程中， W_{ij} W_{ij} 作为 i i 行 j j 列的元素，代表了连接两个神经元的权重。我们用红线代表正值，蓝线代表负值，感知机可以被图示为 线性感知机的输出也是输入的线性组合，但我们可以添加激活函数，即 \\sigma(\\cdot) \\sigma(\\cdot) 将其映射到非线性空间。这要求我们添加的激活函数是一个非线性函数。 事实上，将单层感知机层叠，前一层的输出作为后一层的输入，就构建出早期的神经网络。这种网络每一层都是全连接的（两个神经元之间总是有权重，尽管值可能为0），每一层都有激活函数。理论上，任意一个两层堆叠的感知机，只要神经元数目足够多，就可以拟合出任意一个非线性函数。然而，实际测试中，这一理论的效果并不尽如人意，因此又有陆续地改进，才有了后来的深度学习。饮水思源，鉴往知来，我们也将从这个简简单单的单层模型开始，走上学习“深度学习”之旅。","text_tokens":["i","两层","感知","蓝线","的","尽管","这","都","一个","cdot","神经网","事实","单层","end","mathbf","人意","因此","包含","要求","学习","视为","但","全","开始","称为","j","即","不","后","先河","映射","然而","空间","早期","饮水","之旅","可见","我们","激活","可以","神经网络","正值","“","尽如","向量","，","走上","层","尽如人意","它","输出","align","事实上","这一","}","深度","陆续","left","改进","非线性","right","发展","_","前","可能","值","1","{","成","用","代表","线性组合","\\","-","却","组合","w","begin","tilde","线性","(","过程","只能","x","多","4","）","b","最早","改写","矩阵","视","并","网络","模型","红线","中","等价","总是","如下","常数","被","之间","简简单单","perceptron","知机","解","连接","上","作为","测试","又","有","后来","在","列","y","层叠","出","感知机","非常","这个","也","layer",".","&","才","从","为","和","ij","理论","每","饮水思源","。","实际","构建","定义","（","就","简单","每个","=","输入","孱弱","效果","里"," ","形式","单单","鉴往知来","这种","只要","思源","”","到","两个","拟合","bmatrix","足够","了","本身","地","行","一层","神经元","函数","添加","任意","0","偏置",")","问题","sigma","数目","图示","神经","将","看成","开","其","堆叠","权重","single","元素","负值","是"],"title":"感知机","title_tokens":["感知机","知机","感知"]},{"location":"book-1-x/chapter-1/linear-classification/#sigmoid","text":"在上述介绍中，我们没有解决的两个问题是， 如何定义插值函数 \\sigma \\sigma ？ 如何找到合适的 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ？ 我们首先讨论第一个问题。一般地，多分类问题中，如果各个分类彼此并非相斥，且不一定要将结果分入任一类的话，我们可以用 Sigmoid 函数来定义 \\sigma \\sigma ，亦即 \\begin{align} \\sigma(\\mathbf{x}) = \\frac{1}{ 1 + e^{-\\mathbf{x}}}. \\end{align} 它同时满足 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 ，且是一个单调函数。以下代码向我们展示了这种函数的特性： Python 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import matplotlib.pyplot as plt def test_sigmoid (): x = np . linspace ( - 5 , 5 , 100 ) y = 1 / ( 1 + np . exp ( - x ) ) plt . plot ( x , y ) plt . xlabel ( 'x' ), plt . ylabel ( 'y' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () if __name__ == '__main__' : test_sigmoid () Output 使用sigmoid函数的一大好处是，它的导数求解非常简单，很适合用来做神经网络这样一个复杂模型的激活函数。注意虽然 \\sigma(\\mathbf{x}) \\sigma(\\mathbf{x}) 和 \\mathbf{x} \\mathbf{x} 都是向量，这意味着导数是Jacobian矩阵，但由于 \\sigma \\sigma 是一个对 \\mathbf{x} \\mathbf{x} 各元素独立的解析函数，这个Jacobian矩阵实际上是一个对角矩阵，对角线上第j个元素的值为 \\begin{align} \\left. \\frac{ \\partial \\sigma(x) }{ \\partial x } \\right|_{x=x_j} = \\left. - e^{-x} \\left( - \\frac{1}{\\left( 1+e^{-x} \\right)^2} \\right) \\right|_{x=x_j} = - \\sigma(x_j) \\sigma(1 - x_j). \\end{align} 可见，该函数的导数和计算函数本身的复杂度相若，可以做到快速求导。","text_tokens":["frac","首先","对","同时","求导","的","该","独立","做","这","~","+","一个","都","2","意味","jacobian","神经网","5","end","mathbf","sigmoid","讨论","import","但","单调","没有","相斥","适合",":","j","np","即","不","：","快速","|","matplotlib","set","我们","激活","可以","神经网络","对角","可见","虽然","plt","向量","，","亦","的话","e","numpy","它","size","各个","align","exp","}","展示","相若","如何","left","output","11","^","介绍","right","插值","_","ylabel","这样","元素","值","结果","1","partial","{","8","用","实际上","def","用来","任","3","\\","-","name","w","begin","as","6","由于","向","求解","第一","(","彼此","x","多","代码","4","b","main",",","使用","一大","矩阵","'","意味着","导数","网络","模型","特性","中","以下","linspace","xlabel","满足","上","12","上述","在","y","非常","这个",".","分类","为","show","和","计算","/","第一个","。","实际","gcf","对角线","合适","定义","inches","简单","要","解决","=","且","并非","非常简单","test","pyplot"," ","好处","如果","这种","解析","python","两个","分入","9","__","做到","一定","一般","？","了","本身","很","找到","地","一类","100","函数","注意","0","问题",")","sigma","各","infty","神经","来","复杂","将","7","第","复杂度","0.5","10","if","plot","个","是"],"title":"Sigmoid函数","title_tokens":["函数","sigmoid"]},{"location":"book-1-x/chapter-1/linear-classification/#_5","text":"接下来，我们需要解决第二个问题，亦即找到 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} 。这一问题通常可以写成反问题的形式： \\begin{align} \\arg \\min_\\limits{\\mathbf{W},~\\mathbf{b}} \\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}^{(k)},~ \\sigma ( \\mathbf{W} \\mathbf{x}^{(k)} + \\mathbf{b} ) \\right). \\end{align} 最简单的情况下，我们可以把 损失函数(loss function) 定义为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = \\lVert \\mathbf{y} - \\tilde{\\mathbf{y}} \\rVert_2^2. \\end{align} 我们称 (8) (8) 为 逻辑斯蒂回归(logistic regression) 。有趣的是，虽然这个术语叫“回归”，但它解的其实是个分类问题。但是，既然这是一个分类问题，我们可以不使用这个损失函数，而是从概率论的角度看待这个问题。由此，我们引出一个新的损失函数：“交叉熵”。","text_tokens":["的","~","+","k","一个","2","mathcal","end","其实","mathbf","但","第二","即","不","：","由此","我们","regression","接下","可以","“","虽然","，","二个","亦","而是","反","它","术语","align","这一","}","left","limits","^","right","_","logistic","需要","n","1","{","sum","8","下来","第二个","斯蒂","\\","loss","-","w","begin","概率论","tilde","(","看待","x","使用","b",",","情况","概率","这是","称","叫","引出","既然","解","但是","y","这个","function","角度",".","lvert","把","从","为","分类","通常","。","写成","min","rvert","定义","简单","解决","arg","交叉","=","新","l","损失","熵"," ","形式","”","有趣","逻辑","找到","接下来","函数","下","问题",")","sigma","最","回归","个","是"],"title":"求解问题","title_tokens":["求解","问题"]},{"location":"book-1-x/chapter-1/linear-classification/#_6","text":"我们视sigmoid函数输出的值为一个概率，表示分类器对预测结果的确信程度，记 \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} ，则 \\begin{equation} \\begin{aligned} \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}), \\\\ \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) &= 1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}). \\end{aligned} \\end{equation} 注意这里的概率向量的含义是，其中第i个元素表明第i个超平面分类结果的确信程度。 然而，这个概率只是分类器对分类结果的确信程度，却并非是分类准确度的概率，实际上，分类准确度的概率，应当表述为 \\begin{equation} \\begin{aligned} \\mathbf{p}(\\mathbf{y}|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta})^{1-\\mathbf{y}}\\\\ &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\left(1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}) \\right)^{1-\\mathbf{y}}. \\end{aligned} \\end{equation} 注意 这里 \\mathbf{x}^{\\mathbf{y}} \\mathbf{x}^{\\mathbf{y}} 表示的是对每个元素一一求取指数，即函数第i个元素的返回值应当为 {x_i}^{y_i} {x_i}^{y_i} 。 我们使用真实值 \\mathbf{y} \\mathbf{y} 作为指数给概率向量加权。当 \\mathbf{y}=1 \\mathbf{y}=1 时，以预测值为1的可信度作为概率；反之则以预测值为0的可信度作为概率。这就是最大似然估计方法。至此，我们可以写出似然估计函数 \\begin{align} L(\\boldsymbol{\\Theta}) = \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}). \\end{align} 对似然估计函数取对数，则有 \\begin{equation} \\begin{aligned} l(\\boldsymbol{\\Theta}) &= \\sum_{k=1}^N \\log \\left( \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}) \\right) \\\\ &= \\sum_{k=1}^N \\mathbf{y}^{(k)} \\cdot \\log\\left(\\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right) + \\left(1 - \\mathbf{y}^{(k)} \\right) \\cdot \\log\\left(1 - \\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right). \\end{aligned} \\end{equation} 我们最终的目的是要最大化似然函数，亦即 \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) ，这等价于最小化 -l(\\boldsymbol{\\Theta}) -l(\\boldsymbol{\\Theta}) 。对比 (8) (8) 和 (9) (9) ，于是我们可以定义交叉熵为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = \\mathrm{mean}\\left[ \\mathbf{y} \\cdot \\log\\left( \\tilde{\\mathbf{y}} \\right) + \\left(1 - \\mathbf{y} \\right) \\cdot \\log\\left(1 - \\tilde{\\mathbf{y}} \\right) \\right]. \\end{align} 注意这里我们使用 \\mathrm{mean}\\left[ \\cdot \\right] \\mathrm{mean}\\left[ \\cdot \\right] 表示求取一个向量所有元素的平均值。实际上，Tensorflow允许我们定义损失函数的输出为一个和输出向量维度相同的向量，Tensorflow自带的交叉熵也是这样定义的。实际应用时，Tensorflow会自动在向量维度上求均值，并压缩成上述 (14) (14) 的形式。 若我们记 \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) ，代入sigmoid函数，为了确保该损失函数的稳定性，我们可以将 (14) (14) 整理为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{x}} \\right) = \\mathrm{mean}\\left[ \\max(\\tilde{\\mathbf{x}}, \\mathbf{0}) - \\tilde{\\mathbf{x}} \\cdot \\mathbf{y} + \\log\\left(1 + e^{-|\\tilde{\\mathbf{x}}|} \\right) \\right]. \\end{align} 提示 这里交叉熵整理的推导过程参见 Tensorflow-API官方文档 。 实际情况下，我们使用 (15) (15) 来求取sigmoid函数激活下的交叉熵。","text_tokens":["i","15","tensorflow","稳定性","求取","对","对比","的","返回","该","会","这","~","信度","稳定","+","一个","超平面","k","cdot","可信","分类器","mathcal","似然","end","mathbf","sigmoid","p","含义","boldsymbol","准确","提示","以","即","所有","应当","|","然而","压缩成","缩成",";","激活","我们","可以","若","一一","；","返回值","向量","，","最终","时","于是","对数","亦","log","自带","e","输出","align","估计","}","记","真实","当","最小化","left","limits","文档","^","只是","right","_","平均","值","这样","n","结果","1","{","sum","8","aligned","实际上","指数","反之","维度","14","\\","最小","max","-","却","w","写出","begin","加权","确保","tilde","目的","(","为了","过程","x","使用","表明","b","就是",",","给","视","取","代入","于","并","情况","概率","则","官方","等价","mathrm","参见","equation","至此","[","表述","作为","上述","准确度","有","在","上求","y","这里","这个","最大","推导","也",".","&","应用","其中","预测值","]","整理","为","分类","和","。","实际","可信度","定义","相同","api","确信","要","arg","每个","交叉","=","平面","并非","l","自动","损失","熵","平均值"," ","形式","预测","程度","最大化","9","mean","大化","压缩","方法","theta","均值","in","函数","注意","0","下",")","sigma","来","第","将","表示","允许","定性","元素","个","是"],"title":"交叉熵","title_tokens":["交叉","熵"]},{"location":"book-1-x/chapter-1/linear-classification/#_7","text":"","text_tokens":[],"title":"解线性多分类问题","title_tokens":["分类","多","问题","线性","解"]},{"location":"book-1-x/chapter-1/linear-classification/#_8","text":"建立一个具有较强可读性的Tensorflow工程需要我们活用python的模块化设计。我们通常推荐以下的结构 . ├─ data/ # where we store our data │ └─ ... ├─ tools.py # codes for post-processing and analyzing records. ├─ extension.py # codes for extending the tensorflow model. ├─ dparser.py # data parser └─ main.py # main module where we define our tensorflow model. 除了保存数据的文件夹，我们应当有三个子模块。其中 tool : 用来处理、分析生成的数据，通常与Tensorflow无关； extension : 用来扩展tensorflow，例如在这里自定义网络层和操作符； dparser : 数据处理器，用来读取并预处理送入网络的数据； main : 主模块，只定义跟Tensorflow模型有关的内容，需要引用 extension 和 dparser 。 视情况可以灵活调整结构，但建议将定义Tensorflow模型的代码单独放在主模块里，和其他外围代码分离。 撰写各个模块时，建议使用类封装各组功能相同的函数。具有良好使用习惯的coder应当注意给各个面向用户的类、函数撰写（哪怕简短的）说明文字，在一些较长的函数、方法的定义中，适当注释各部分的功能，以便读者能正确理解代码意义。 另外，在对象命名上，python有如下必须遵守或不成文的规定，和C/C++用户熟悉的蛇形命名法不同，它大致包括 类与函数多用驼峰命名法，变量可以采用驼峰或蛇形命名法。 驼峰命名法指的是用大小写区分每个单词块，例如 alphaBetaFunction () ； 蛇形命名法指的是用下划线区分每个单词块，例如 alpha_beta_function = 10 ； 宏变量使用全字大写+蛇形命名法 函数/方法，还有模块均是首字母小写，但类的首字母大写。 用单下划线 _ 表示临时存储器，或省略参数，例如一个函数 func () 有两个返回值时，可以用 _ , b = func () 表示我们只需要第二个返回值；单下划线还可以与星号连用省略多个返回值； 以单下划线开头的方法，表示模块级的私有方法，在模块以外使用 import 导入类时，不会导入这些方法，例如 def _alphaBeta ( self ): ； 以单下划线结尾的对象，用来和python的关键字区分，例如 func ( x , class_ ) ; 以双下划线开头的方法，如果不以双下划线结尾，则表示类级的私有方法，只有类内部的方法能调用这些方法，在类外部、包括继承的子类里都原则上不能调用（但其实也有办法调用），例如 def _alphaBeta ( self ): ； 以双下划线同时开头和结尾的方法，一般是用来 重写 (override) 特殊功能，例如 def __getattribute__ (): 将重写获得类属性的方法。","text_tokens":["划线","处理器","字母","继承","store","使用","意义","tensorflow","单词","件夹","建议","同时","的","返回","data","采用","办法","records","但类","这些","tools","model","正确理解","小写","c++","一个","+","都","说明","变量","alphabeta","均","星号","双","其实","跟","理器","import","网络层","属性","灵活","但","extending","自定义","第二",",","读者",":","、","特殊","应当","成文","命名","活用","蛇形","func","注释",";","我们","调整","define","可以","类","；","面向","返回值","，","可读","设计","├","时","）","遵守","二个","较长","#","省略","生成","它","全字","we","c","分离","各个","熟悉","首字","用单","还","原则","可读性","文件夹","工程","_","主","coder","外围","alphabetafunction","alpha","需要","三个","处理","不成","不同","用","关键","─","分析","our","单独","第二个","原则上","用来","def","理解","操作","override","-","单","保存","习惯","哪怕","extension","where","部分","内部","py","不成文","(","扩展","调用","x","代码","与","操作符","main","推荐","只","analyzing","例如","codes","视","给","存储器","良好","parser","并","情况","正确","网络","包括","模型","大小","连用","私有","不会","tool","中","适当","以下","功能","模块化","必须","大小写","自定","参数","送入","如下","下划","获得","关键字","封装","类时","向用","大致","上","结构","有","在","具有","这里","调整结构","function","beta","外部","放在","也",".","较强","dparser","其中","各组","...","以双","和","模块","module","还有","/","通常","。","定义","相同","（","一些","不以","无关","不能","下划线","每个","=","多个","数据","self","processing","大写","类级","规定","块","级","用户","│","里"," ","简短","撰写","内容","有关","开头","class","如果","以外","对象","导入","文件","python","以便","临时","两个","只有","__","方法","post","子","结尾","预处理","区分","存储","一般","则","多用","getattribute","除了","读取","函数","注意","另外","b","是","子类","面向用户",")","宏","以单","各","重写","引用","能","for","将","驼峰","表示","and","其他","the","文字","法指","或","法","10","首字母","建立","└"],"title":"代码规范","title_tokens":["规范","代码"]},{"location":"book-1-x/chapter-1/linear-classification/#tensorflow","text":"在Tensorflow中，我们把变量都称为“ 张量 (Tensor) ”。这是因为我们有零维的标量，一维的向量，二维的矩阵，更高维的我们都称为张量。作为一个更大的概念，张量当然也可以用来包括标量、向量和矩阵了。在Tensorflow中，有的张量是 可以训练 (trainable) 的，有的则不是。比如一个张量的形状（指各维大小），当然可以是一个 < tf . Tensor 'Shape:0' shape = ( 1 ,) dtype = int32 > 类型的张量，但它不是变量，当然就不可训练。我们也可以人为控制某些张量可以训练或不可以训练，但本节、乃至本章所介绍的凡是我们接触到的张量，都是可以训练的。 特别地，对于神经网络而言，在网络内计算（或者说流动、传播）的一个n维数据，通常按照以下形式组织： tensor [ batch , dim1 , dim2 , ... , dimn , channel ] 其中，第一个维度 batch 一定存在，它表示的是单个batch中的某一个样本。如果一个batch只有一个样本，那么 batch 只能取0。 从 dim1 到 dimn 指的是实际的n维数据的各个维度； channel 指的是数据的通道，例如，一个二维RGB图像，每种颜色代表一个通道，因此有三个通道。 channel 通常用在卷积网络里，我们经常需要在深度卷积网络里不断增大通道数的同时，缩小数据尺寸。 在某些特殊情况下， channel 维度可以不存在，例如我们使用的是全连接层而不是卷积网络， tf.keras.layer.Flatten 可以用来将一个有通道的张量压缩成一个没有通道的一维向量（但是注意 batch 维度仍然存在，不会被压缩）。 因此，我们知道一个n维的数据，在神经网络中通常被描述为一个n+2维的矩阵，而一个一维向量，在卷积网络里是三维的： vector [ batch , length , channel ] 但是在全连接网络里，是二维的： vector [ batch , channel ] 在本节，乃至本章里，我们还不讨论卷积网络，因此我们都是使用二维张量（一维向量组）作为我们的数据。","text_tokens":["tensorflow","经常","同时","的","batch","dim2","这","传播","都","一个","而言","描述","tf","变量","+","2","神经网","不是","一维","高维","讨论","因此","人为","但","没有","全","乃至","二维",":","称为","存在","、","不","：","一个二维","缩成","概念","我们","可以","trainable","shape","神经网络","；","“","知道","组织","向量","，","层","它","训练","各个","还","keras","深度","本节","dimn","零维","缩小","介绍","仍然","张量","三个","需要","n","1","dim1","图像","增大","数","尺寸","用","channel","代表","用来","所","某些","维度","flatten","控制","指各维","第一","(","<","int32","对于","只能","形状","）","使用",",","例如","取","矩阵","大小","'","情况","包括","网络","不会","则","中","以下","不断","而","或者说","被","是因为","更","[","连接","作为","某","指","但是","有","在","特别","也","layer",".","]","其中","把","...","那么","从","为","rgb","和","计算","length","通常","。","凡是","第一个","样本","实际","颜色","组","（","就","通道","=","数据","单个","比如","按照","里"," ","形式","特殊","如果","内","只有","”","到",">","压缩","当然","tensor","一定","三维","标量","不可","了","接触","地","维","注意","dtype","0","每种","更大","或者",")","下","卷积","神经","流动","将","表示","因为","或","类型","vector","本章","压缩成","是"],"title":"Tensorflow的数据概念","title_tokens":["概念","数据","tensorflow","的"]},{"location":"book-1-x/chapter-1/linear-classification/#_9","text":"在本项目里，我们不需要扩展Tensorflow。但是，我们需要以随机生成数据代替数据集。因此，首先，通过以下代码定义数据生成器 dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class TestDataSet : ''' A generator of the data set for testing the linear model. ''' def __init__ ( self , scale_x , A , c ): ''' Initialize the data generator. scale_x: the scale of input vector. A, c: the linear transformation. ''' self . s_x = 2 * scale_x self . A = A self . c = c self . len_x = A . shape [ 0 ] self . config () def config ( self , train = True , batch = 100 , noise = 0.1 ): ''' Configuration train: a flag for controlling the iterator mode. batch: the number of samples in a batch noise: std. of the error added to the y. ''' self . train = bool ( train ) self . batch = batch self . noise = noise def next_train ( self ): ''' Get the next train batch: (x, y) ''' x = self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) y = np . matmul ( x , self . A ) + self . c if self . noise > 1e-3 : y = y + np . random . normal ( 0 , self . noise , size = y . shape ) y = np . greater ( y , 0.0 ) . astype ( np . float32 ) return x , y def next_test ( self ): ''' Get the next test batch x. ''' return self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) def __iter__ ( self ): while True : samp = self . __next__ () yield samp def __next__ ( self ): if self . train : return self . next_train () else : return self . next_test () 该生成器输入一组 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，以及相关配置，之后就可以通过 迭代器 (iterator) 或 方法 (method) 随机生成数据。这种数据集写法我们在后面还会用到， model . fit 允许我们不是馈入样本（或样本批次），而是馈入一个 生成器(generator) 。因此我们重写了 __iter__ 方法，并使其通过 yield 返回一个生成器。这样我们定义的数据集类就可以被Keras的训练函数 model . fit 使用。接下来，调用如下测试代码： dparser.py 1 2 3 4 5 6 7 8 9 10 def test_dataset (): A = np . random . normal ( 0 , 10 , [ 10 , 6 ]) c = np . random . uniform ( 1 , 3 , [ 1 , 6 ]) dataSet = TestDataSet ( 10 , A , c ) dIter = iter ( dataSet ) for i in range ( 10 ): x , y = next ( dIter ) print ( np . sum ( y , axis = 0 ) / 100 ) test_dataset () Output [ 0.47 0.57 0.58 0.56 0.5 0.38 ] [ 0.6 0.61 0.47 0.48 0.38 0.52 ] [ 0.5 0.61 0.49 0.42 0.45 0.53 ] [ 0.59 0.52 0.44 0.44 0.49 0.51 ] [ 0.54 0.59 0.48 0.5 0.51 0.47 ] [ 0.49 0.57 0.56 0.49 0.53 0.4 ] [ 0.5 0.61 0.51 0.54 0.51 0.52 ] [ 0.5 0.51 0.61 0.5 0.44 0.5 ] [ 0.44 0.46 0.53 0.45 0.56 0.52 ] [ 0.52 0.46 0.51 0.52 0.49 0.44 ] 我们随机生成了 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的数据，每组数据100个，并且测试了10组。输出结果是各组测试中， \\mathbf{y} \\mathbf{y} 在对应维度上分类为1的概率估计。结果基本都在0.5左右，说明我们的这种数据生成模式产生的点能均匀分布在各个超平面两侧，适合进行后续测试。","text_tokens":["0.4","i","24","首先","43","s","15","tensorflow","22","均匀","added","batch","返回","36","该","data","的","28","print","~","定义数据","还会","34","model","0.0","+","一个","都","超平面","2","测试代码","说明","49","scale","39","不是","5","bool","并且","config","17","mathbf","mode","45","因此","of","器","38","float32","get","21","项目","集类","左右","0.6",":","iterator","适合","transformation","iter","以","54","np","不","以及","：","模式","linear","diter","a","set","我们","对应","可以","接下","shape","42","flag","noise","，","而是","50","分布","生成","size","后面","samples","error","c","32","批次","训练","dataset","输出","集","各个","keras","0.38","估计","}","之后","0.48","相关","output","11","用到","^","mathbb","_","1e","使","configuration","需要","这样","0.57","随机","1","结果","8","next","testdataset","{","下来","sum","fit","testing","0.49","馈入","init","normal","astype","37","def","14","return","维度","3","0.42","27","\\","52","-","41","0.46","6","range","均匀分布","47","to","py","(","generator","扩展","x","代码","4","20","）","18","30",",","initialize","使用","调用","基本","代替","'","并","0.61","通过","40","写法","19","input","每组","概率","0.44","中","以下","如下","被","但是","[","matmul","12","测试","上","在","controlling","y","axis","0.47","0.56","r",".","0.59","dparser","*","]","各组","点","分类","为","uniform","产生","成器","35","train","。","samp","样本","/","0.54","组","定义","一组","13","（","53","就","len","0.52","method","25","self","数据","=","平面","23","std","进行","test","输入","里","48"," ","class","else","46","true","这种","0.1","9","__",">","方法","44","0.45","0.58","了","random","26","in","31","0.53","本","16","yield","100","函数","接下来","后续","生成器","迭代","mapsto","0","number",")","greater","33","51","29","重写","能","for","7","配置","0.51","0.5","the","while","允许","10","或","if","其","两侧","vector","个","是"],"title":"数据生成","title_tokens":["数据","生成"]},{"location":"book-1-x/chapter-1/linear-classification/#_10","text":"顺序(sequential) 模型是一个单输入单输出模型，网络结构较为简单，也不存在跨层短接（残差连接）。在大多数情况下，已经上手的Tensorflow用户不使用这个模型，故而作为我们入门的第一个project，我们姑且用之，但我们将不再使用顺序模型来实现后续的project。一个顺序模型大致可以描述为下图的模式： graph LR st(输<br/>入) --> l1[层<br/>1] l1 --> l2[层<br/>2] l2 --> l3[层<br/>3] l3 --> ldots[层<br/>...] ldots --> ed(输<br/>出) classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,ed styStart 由于我们完成的是一个线性分类器，故而我们使用单层的序列模型即可。 接下来，我们来定义一个类， class LinClsHandle : 。定义一个类的时候，我们通常需要定义的内容包括 在初始化方法 __init__ 里定义传入网络的固定参数，例如学习速率，存取路径等； 在方法 construct 里定义网络的构造和使用的优化器； 在方法 train 里定义训练网络的过程，主要需要调用 model . fit 。如果我们在数据集的定义非常完善，则这一环节不需要花费太多的功夫； 在方法 test 里定义测试网络的过程，主要需要调用 model . evaluate 。如果有必要，可以通过 model . predict 返回测试结果。","text_tokens":["graph","存取","等","残差","网络结构","tensorflow","sequential","ldots","初始化","一","的","返回","fill","这","model","描述","一个","2","分类器","入","入门","单层","器","学习","evaluate","大多数","但",":","存在","不","：","ba9132","模式","l1",";","我们","类","可以","接下","ed","完善","；","，","层","linclshandle","不再","#","之","训练","输出","集","project","多数","需要","1","结果","花费","用","下来","fit","构造","init","顺序","已经","3","短接","-","单","由于","线性","第一","主要","(","<","过程","使用","）","l3",",","例如","调用","情况","包括","通过","网络","模型","则","stroke","参数","lr","l2","fae6a9","大致","连接","[","作为","测试","结构","有","在","出","这个","非常","也",".","]","故而","...","stystart","为","分类","和","predict","下图","/","第一个","。","序列","通常","初始","train","环节","优化","太多","定义","（","简单","时候","即可","数据","功夫","输入","test","用户","里"," ","内容","class","姑且","如果","传入",">","__","方法","st","输","固定","construct","上手","较为简单","接下来","后续","下",")","路径","实现","classdef","必要","较为","来","速率","将","完成","跨层","大多","br","是"],"title":"定义线性顺序模型","title_tokens":["线性","模型","顺序","定义"]},{"location":"book-1-x/chapter-1/linear-classification/#_11","text":"首先，定义初始化方法： lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 def __init__ ( self , learning_rate = 0.01 , epoch = 10 , steppe = 30 ): ''' Initialization and pass fixed parameters. learning_rate: the learning rate for optimizer. epoch: training epochs. steppe: steps per epoch ''' self . lr = learning_rate self . epoch = epoch self . steppe = steppe 由于目前我们的project还非常简单，这里只需要有学习速率( learning_rate )，轮次数( epoch )和每轮迭代次数( steppe )即可。","text_tokens":["首先","初始化","的","2","parameters","lin","5","training","pass","optimizer","学习","次数",":","：","我们","，","linclshandle","目前","project","还","每轮","fixed","steppe","_","需要","1","8","init","def","3","-","rate","6","由于","initialization","per","py","(","4","30",",","只","'","epochs","lr","有","这里","非常",".","和","。","初始","定义","learning","简单","即可","=","self","非常简单"," ","class","0.01","9","__","方法","cls","steps","轮","迭代",")","epoch","速率","for","7","and","the","10"],"title":"初始化方法","title_tokens":["方法","初始","初始化"]},{"location":"book-1-x/chapter-1/linear-classification/#_12","text":"接下来定义网络构造 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def construct ( self ): ''' Construct a linear model and set the optimizer as Adam ''' # Construction self . model = tf . keras . Sequential () self . dense1 = tf . keras . layers . Dense ( LABEL_SHAPE , use_bias = True , input_shape = ( INPUT_SHAPE ,), kernel_initializer = tf . keras . initializers . RandomNormal ( 0.0 , stddev = 10.0 ), bias_initializer = tf . keras . initializers . Constant ( 2 ), activation = None ) self . model . add ( self . dense1 ) # Set optimizer self . model . compile ( optimizer = tf . train . AdamOptimizer ( self . lr ), loss = self . loss , metrics = [ self . accuracy ] ) @staticmethod def loss ( y_true , y_pred ): return tf . nn . sigmoid_cross_entropy_with_logits ( labels = y_true , logits = y_pred ) @staticmethod def accuracy ( y_true , y_pred ): return tf . keras . backend . mean ( tf . keras . backend . equal ( y_true , tf . keras . backend . round ( tf . keras . backend . sigmoid ( y_pred )))) 须知 这里 LABEL_SHAPE 和 INPUT_SHAPE 为两个宏变量，分别为输出和输入的向量维度。 我们使用 Dense 定义全连接层，它的用法请参照 这里 。由于我们已经知道 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 可能的取值范围，这里我们重定义了 \\mathbf{W} \\mathbf{W} 和 \\mathbf{b} \\mathbf{b} 的初始化方式。 信息: Dense API tf . keras . layers . Dense ( shape , ** kwargs ) 指全连接层，其输入一组已知形状的向量，输出一组形状为 shape 的向量。可用的API如下： shape : 正整数，输出空间维度。 activation : 激活函数。 若不指定，则不使用激活函数 (即，线性激活: a(\\mathbf{y}) = \\mathbf{y} a(\\mathbf{y}) = \\mathbf{y} )。该函数可以定义为任何元素级操作的Tensorflow函数。 use_bias : 布尔值，该层是否使用偏置向量。 True 则网络定义为 \\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} \\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} ， False 则定义为 \\mathbf{y} = \\mathbf{W}\\mathbf{x} \\mathbf{y} = \\mathbf{W}\\mathbf{x} 。 kernel_initializer : kernel 权值矩阵的初始化器，自定义的初始化器需要使用Keras后端API编写。 bias_initializer : 偏置向量的初始化器，同上。 kernel_regularizer : 运用到 kernel 权值矩阵的正则化函数，自定义的正则化函数需要使用Keras后端API编写。 bias_regularizer : 运用到偏置向的的正则化函数，同上。 activity_regularizer : 运用到层的输出的正则化函数，同上。 kernel_constraint : 运用到 kernel 权值矩阵的约束函数，只能使用Keras备选的几种方案，不能自定义。 bias_constraint : 运用到偏置向量的约束函数，同上。 信息: model.compile API model . compile ( optimizer , ** kwargs ) 在这里指的是顺序模型的编译函数，其可用的API如下： optimizer : 优化器，可以使用Tensorflow内置的优化器。 loss : 损失函数，也是目标函数。顺序模型只有一个输出，因此只能传入一个损失函数。可以使用形式为 func ( y_true , y_pred ) 的Tensorflow函数。 metrics : 测度函数，一般是一组函数，如果是一个函数则定义为 [ func ] 即可。自定义的测度函数目前还需要使用Keras后端API编写。 loss_weights : 损失的权重，顺序模型只有一个损失函数，因此只有一个权重，但要使用一维列表 [ value ] 定义。可以使用张量来控制可变权重。 sample_weight_mode : 按时间步采样权重，默认不提供。相比上面的损失权重，该选项会随着迭代次数使用不同的权重，因此输入的是二维列表。 weighted_metrics : 测度的权重，和损失权重类似，用来加给不同的测度函数。由于我们可以使用不只一个测度函数，这里的权重是个一维列表。 target_tensors : 默认情况下，Keras 将为模型的目标创建一个占位符，在训练过程中将使用目标数据。相反，如果你想使用自己的目标张量（反过来说，Keras在训练期间不会载入这些目标张量的外部 Numpy数据），您可以通过 target_tensors 参数指定它们。对于单输出的顺序模型，它应该是单个张量。 ** kwargs : 其他参量，会传递给 tf . Session . run 。 另外，注意我们这里构造网络的时候有如下技巧： 我们定义的网络输出是 \\mathbf{W}\\mathbf{x} + \\mathbf{b} \\mathbf{W}\\mathbf{x} + \\mathbf{b} ，而非 \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) 。这是因为我们需要通过还未被激活的输出用来计算sigmoid交叉熵，亦即式 (15) (15) ； 我们通过静态方法，直接调用Tensorflow自带的 sigmoid交叉熵 函数来作为Keras模型的损失函数 self..loss ； 我们通过静态方法，调用Keras的后端API，自己定义了预测准确度的测度函数 self.accuracy ； 我们将网络层的关键字 self.dense1 保留在了实例中，这是为了确保接下来我们能通过实例抽取该层的参数。 之所以煞费周折地进行这些处理，盖因为Keras的内建API里目前还没有提供对互不相斥的多分类的支持。例如，无论是 tf.keras.metrics.categorical_accuracy 还是 tf.keras.metrics.categorical_crossentropy ，都要求分类的真实值为one-hot类型的向量组，因而它们只适合用在softmax分类器上。为了解决这一问题，我们自己实现了sigmoid分类器。","text_tokens":["24","取值","adam","sequential","这些","model","+","方式","一个","编写","construction","5","一维","因而","按","sigmoid","optimizer","次数","准确",":","自己","sample","激活","编译","中将","false","shape","；","，","constant","keras","}","accuracy","可用","11","同","不同","下来","用","with","维度","之所以","-","w","as","反过来","确保","端","py","整数","为了","对于","只能","x","形状","非","只","正整数","矩阵","one","反过来说","'","网络","不会","自定","initializers","如下","您","关键字","正则","equal","已知","上","12","session","这里","..","静态","labels","无论",".","分类","value","和","选项","你","。","创建","13","测度","不能","要","解决","交叉","=","过来","盖","参照","形式","重","只有","传入","方法","softmax","bias","compile","activity","函数","相反","目标","宏","and","其","10","权重","空间","还是","初始化","是否","会","0.0","变量","17","该层","时间","use","没有","备选","全","适合","activation","向量","层","亦","权值","c","目前","还","这一","adamoptimizer","metrics","提供","可能","1","处理","内置","构造","14","3","6","由于","）","b","18",",","给","约束","通过","lr","请","而","weighted","被","initializer","运用","dense1","weights","cross","来说","stddev","]","类似","train","组","初始","相比","api","即可","无论是","none","数据","weight","里"," ","指全","如果","默认","hot","两个","mean","cls","布尔","下","sigma","dense","传递","化","7","the","参量","元素","是","即式","15","用法","22","对","这","都","tf","分类器","可变","mode","因此","器","要求","entropy","网络层","所以","21","自定义","它们","不","后","linear","func","set","我们","互不","知道","技巧","过程中将","label","载入","_","符","布尔值","关键","顺序","用来","已经","\\","loss","单","向","保留","使用","随着","20","多","方案","input","参数","是因为","[","连接","run","应该","指","作为","在","y","*","regularizer","定义","（","logits","23","进行","add","输入","后端","值为","class","外部","预测","true","到","target","抽取","一般","了","煞","内建","地","接下来","16","另外","注意","迭代",")","能","来","将","pred","支持","其他","类型","范围","反过","采样","backend","tensorflow","randomnormal","的","该","2","lin","mathbf","10.0","但","相斥","二维","即","round","：","a","接下","若","可以","任何","想","linclshandle","#","numpy","自带","它","kwargs","训练","输出","真实","实例","周折","categorical","张量","需要","staticmethod","8","{","crossentropy","def","return","操作","@","静态方法","控制","加","线性","上面","kernel","(","占位","过程","费周折","4","例如","调用","情况","19","则","模型","中","这是","须知","期间","列表","有","准确度","未","也","为","步","计算","直接","layers","优化","nn","一组","时候","25","self","信息","单个","级","constraint","损失","熵","9","分别","几种","construct","26","偏置","问题","实现","tensors","因为","个","指定"],"title":"构造方法","title_tokens":["构造","构造方法","方法"]},{"location":"book-1-x/chapter-1/linear-classification/#_13","text":"最后定义的式训练和测试方法。由于我们目前的project还比较简单，关于这两部分都直接调用现有的API即可。使用的API在之前已经说明。 model.fit 在没有额外设置的情况下，默认会返回一个 History回调器 ； model.evaluate 返回的是测试样本给出的损失函数和准确值测度。 model.predict 返回的是测试样本给出的网络输出。详情请参照 顺序模型API 。 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def train ( self , dataSet ): ''' Use a data set to train the network. ''' return self . model . fit ( dataSet , epochs = self . epoch , steps_per_epoch = self . steppe ) def test ( self , data , labels ): ''' Use (data, label) pairs to test the results. ''' loss , accu = self . model . evaluate ( data , labels ) print ( 'Evaluated loss =' , loss ) print ( 'Evaluated accuracy =' , accu ) return self . model . predict ( data ) 信息: model.fit API model . fit ( dataSet , ** kwargs ) 是训练函数，注意这个函数虽然支持输入一组 x,~y x,~y 用来代替 dataSet ，我们还是建议在任何情况下都用dataSet馈送数据，以免内存中数据集占用过多。 dataSet : 数据集，其本身应当是一个 tf.data.Dataset 类型的类，或者是一个能不断迭代产生新数据的生成器。数据的 batch 大小由 dataSet 本身决定。 epochs : 整数，终止训练时经历的世代(轮次)数，通常一个epoch表示遍历整个数据集一回。 verbose : 0, 1或2。日志显示模式。 0=安静模式, 1=进度条, 2=每轮一行。默认是1。 callbacks : 回调器，它是 tf.keras.callbacks 模块下的类，用来在训练中进行记录保存和数据检查点更新。默认是 tf.keras.callbacks.History 。 validation_split : 在 0 和 1 之间浮动。用作验证集的训练数据的比例。模型将分出一部分不会被训练的验证数据，并将在每一轮结束时评估这些验证数据的误差和任何其他模型指标。验证数据是混洗之前 x 和 y 数据的最后一部分样本中。 validation_data : 元组 (x_val，y_val) 或元组 (x_val，y_val，val_sample_weights) ，用来评估损失，以及在每轮结束时的任何模型度量指标。模型将不会在这个数据上进行训练。这个参数会覆盖 validation_split 。 shuffle : 布尔值（是否在每轮迭代之前混洗数据）。当 steps_per_epoch 非 None 时，这个参数无效。 class_weight : 可选的字典，用来映射类索引（整数）到权重（浮点）值，用于加权损失函数（仅在训练期间）。这可能有助于告诉模型 「更多关注」来自代表性不足的类的样本。 sample_weight : 用来给损失函数添加权重，作用类似 model.compile 的同一参数。 initial_epoch : 开始训练的轮次（有助于恢复之前的训练）。 steps_per_epoch : 在声明一个轮次完成并开始下一个轮次之前的总步数（样品批次）。使用TensorFlow数据张量等输入张量进行训练时，默认值 None 等于数据集中样本的数量除以 batch 的大小，如果无法确定，则为1。 validation_steps : 只有在指定了 steps_per_epoch 时才有用。停止前要验证的总步数（批次样本）。 该函数会返回 callbacks 定义的实例。 信息 model.evaluate API model . evaluate ( x , y , ** kwargs ) 是测试函数，需要传入 label 即 y 来验证性能。 x , y : Numpy 数组，分别是输入和输出的真实参照值。 batch_size : 计算的 batch 大小，该函数会将输入的数据组拆分成许多 batch 分别计算，并组合起来，这个设定值对效果不产生影响，只影响计算过程。 verbose : 0, 1。日志显示模式。0=安静模式, 1=进度条。默认是1。 sample_weight : 用来给损失函数添加权重，作用类似 model.compile 的同一参数。 steps : 整数或 None 。 声明评估结束之前的总步数（批次样本）。默认值 None 。 该函数会返回损失函数和测度（列表）。 信息 model.predict API model . predict ( x , ** kwargs ) 是预测函数，用在只知道输入 x 的场合。 x : Numpy 数组，输入值。 batch_size : 计算的 batch 大小，该函数会将输入的数据组拆分成许多 batch 分别计算，并组合起来，这个设定值对效果不产生影响，只影响计算过程。 verbose : 0, 1。日志显示模式。0=安静模式, 1=进度条。默认是1。 steps : 整数或 None 。 声明评估结束之前的总步数（批次样本）。默认值 None 。 该函数返回预测结果。","text_tokens":["两","日志","」","data","这些","model","一个","5","来自","准确",":","sample","用作","应当","字典","类","；","，","详情","时","世代","内存","不足","pairs","样品","split","显示","keras","accuracy","11","决定","callbacks","经历","关于","用","fit","代表","误差","-","占用","results","py","整数","x","非","式","测试函数","只","'","epochs","网络","不会","结束","场合","更","上","测试","12","labels",".","许多","和","更新","predict","模块","成器","。","样本","13","测度","=","新","验证","浮点","参照","效果","由","只有","传入","方法","compile","函数","添加","表示","其","initial","10","或","权重","history","还是","是否","建议","一行","print","会","~","等于","evaluate","use","没有","以及","映射","默认值","目前","比较","还","查点","可能","值","评估","1","结果","14","3","部分","6","由于","详情请","）",",","声明","给","并","不断","被","时才","整个","指标","weights","以免","类似","安静","产生","可选","train","通常","api","简单","即可","val","none","遍历","数据","weight","数组","有助于","关注"," ","如果","默认","cls","布尔","用于","生成器","下","或者","影响","7","verbose","the","完成","测试方法","是","元组","等","有用","对","返回","batch","这","作用","都","tf","过多","无法","代表性","器","不","模式","set","我们","进度条","知道","虽然","network","给出","label","一部分","生成","size","dataset","每轮","当","前要","_","仅","布尔值","数","比较简单","一轮","顺序","起来","用来","已经","一回","loss","保存","数量","检查点","步数","加权","使用","多","除以","代替","大小","设定值","参数","之前","集中","在","y","馈送","*","每","覆盖","「","定义","（","一部","进度","进行","test","输入","告诉","class","预测","到","轮次","确定","了","本身","分成","注意","迭代","0",")","性能","助于","能","来","将","支持","其他","类型","tensorflow","的","该","2","说明","lin","现有","设定","开始","即","最后","a","任何","linclshandle","索引","numpy","kwargs","它","训练","停止","批次","输出","集","project","分出","真实","实例","steppe","张量","需要","8","evaluated","无效","def","return","组合","浮动","有助","to","per","(","过程","4","shuffle","调用","情况","模型","则","中","检查","期间","列表","之间","这个","终止","记录","同一","为","计算","直接","一组","恢复","额外","回调","设置","self","accu","信息","损失","validation","9","分别","steps","比例","混洗","总","epoch","组拆","度量","指定"],"title":"训练和测试方法","title_tokens":["测试","和","方法","训练","测试方法"]},{"location":"book-1-x/chapter-1/linear-classification/#_14","text":"首先，训练网络。我们随机生成 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的线性变换，并且设置好数据集，给定噪声扰动为 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 。设定20个epoch，每个epoch迭代500次，每次馈入32个样本构成的batch，然后开始训练： lin-cls.py 1 2 3 4 5 6 7 8 A = np . random . normal ( 0 , 10 , [ INPUT_SHAPE , LABEL_SHAPE ]) c = np . random . uniform ( 1 , 3 , [ 1 , LABEL_SHAPE ]) dataSet = dp . TestDataSet ( 10 , A , c ) dataSet . config ( batch = 32 , noise = 0.1 ) # Construct the model and train it. h = LinClsHandle ( learning_rate = 0.01 , epoch = 20 , steppe = 500 ) h . construct () record = h . train ( iter ( dataSet )) Output Epoch 1 /20 500 /500 [==============================] - 3s 5ms/step - loss: 48 .2269 - accuracy: 0 .5458 Epoch 2 /20 500 /500 [==============================] - 1s 2ms/step - loss: 25 .5149 - accuracy: 0 .6491 Epoch 3 /20 500 /500 [==============================] - 1s 2ms/step - loss: 11 .9822 - accuracy: 0 .7607 Epoch 4 /20 500 /500 [==============================] - 1s 2ms/step - loss: 5 .6580 - accuracy: 0 .8513 Epoch 5 /20 500 /500 [==============================] - 1s 2ms/step - loss: 2 .7230 - accuracy: 0 .9106 Epoch 6 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .1082 - accuracy: 0 .9462 Epoch 7 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .3278 - accuracy: 0 .9708 Epoch 8 /20 500 /500 [==============================] - 1s 3ms/step - loss: 0 .0618 - accuracy: 0 .9878 Epoch 9 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0149 - accuracy: 0 .9963 Epoch 10 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9979 Epoch 11 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0124 - accuracy: 0 .9976 Epoch 12 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9978 Epoch 13 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9973 Epoch 14 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0120 - accuracy: 0 .9974 Epoch 15 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9970 Epoch 16 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0116 - accuracy: 0 .9971 Epoch 17 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0120 - accuracy: 0 .9967 Epoch 18 /20 500 /500 [==============================] - 1s 3ms/step - loss: 0 .0114 - accuracy: 0 .9971 Epoch 19 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0114 - accuracy: 0 .9969 Epoch 20 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0113 - accuracy: 0 .9970 接下来，从训练返回的 History 类型的回调器中抽取对loss和accuracy的记录。 lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 12 13 def showCurve ( x , y , xlabel = None , ylabel = None , log = False ): if log : plt . semilogy ( x , y ) else : plt . plot ( x , y ) if xlabel is not None : plt . xlabel ( xlabel ) if ylabel is not None : plt . ylabel ( ylabel ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () showCurve ( record . epoch , record . history [ 'loss' ], xlabel = 'epoch' , ylabel = 'Cross entropy' , log = True ) showCurve ( record . epoch , record . history [ 'accuracy' ], xlabel = 'epoch' , ylabel = 'Accuracy' ) Output 重新设定数据集的产生方式，变为每个batch含10个样本。使用这组重新随机生成的数据测试网络输出， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 12 # Generate a group of testing samples: dataSet . config ( batch = 10 ) x , y = next ( dataSet ) # Check the testing results yp = dp . sigmoid ( h . test ( x , y )) _ , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 ) ax1 . imshow ( y , interpolation = 'nearest' , aspect = 'auto' ) ax1 . set_title ( 'True class' ) ax2 . imshow ( yp , interpolation = 'nearest' , aspect = 'auto' ) ax2 . set_title ( 'Predicted class' ) plt . gcf () . set_size_inches ( 10 , 5 ), plt . show () Output 注意我们未对测量的结果阈值化，因此显示出来的测量结果和理想值略有差别，但从图可知，阈值化后则测量结果全部准确。 通过抽取 h.dense1 的参数，我们可以对比 \\mathbf{A} \\mathbf{A} 和 \\mathbf{W} \\mathbf{W} ，以及 \\mathbf{c} \\mathbf{c} 和 \\mathbf{b} \\mathbf{b} ， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 # Check the regressed values W , b = h . dense1 . get_weights () plt . imshow ( A , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'A' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . imshow ( W , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'W' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . plot ( c . T , label = 'c' ) plt . plot ( b . T , label = 'b' ) plt . legend () plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () Output 可以发现，虽然我们训练的分类器十分有效，但其权值和预期的 \\mathbf{A} \\mathbf{A} , \\mathbf{c} \\mathbf{c} 并不相同。这是由于我们训练的样本加入了噪声。这种技术常用于神经网络的训练，被认为是一种提高鲁棒性、减小过拟合、避免不稳定解的一个有效手段。可以看出真实值 \\mathbf{A} \\mathbf{A} 存在偏高值，但 \\mathbf{W} \\mathbf{W} 的数值更加均匀。","text_tokens":["数值","对比","5458","model","ax1","稳定","方式","一个","5","sigmoid","准确",":","数据测试","false","shape","noise","，","每次","9969","7607","显示","}","accuracy","认为","11","predicted","9963","ylabel","n","随机","下来","0121","-","w","nearest","dp","py","x","常用","然后","'","网络","扰动","xlabel","解","12","测试","9971","r",".","sim","分类","和","6580","/","。","样本","相同","13","9462","=","semilogy","is","9974","这种","auto","random","9878","9970","and","其","10","uniform","record","plot","subplots","history","title","~","神经网","5149","技术","config","17","5ms","of","boldsymbol","可知","噪声","以及","权值","略有","c","32","2ms","化后","0113","值","1","结果","colorbar","馈入","14","3","interpolation","rate","6","由于","9106","b","18",",","generate","3ms","通过","gca","阈值","check","it","被","6491","出来","dense1","h","weights","cross","看出","]","9708","产生","train","gcf","有效","9967","每个","none","数据","48"," ","else","更加","0.01","拟合","cls","mapsto","过","神经","化","9973","7","the","避免","是","首先","15","500","对","均匀","batch","返回","鲁棒性","showcurve","好","分类器","并且","imshow","因此","9976","一种","并不相同","entropy","get","变换","存在","np","变为","不","set","我们","神经网络","虽然","plt","label","log","生成","size","samples","dataset","理想","output","^","_","testdataset","not","预期","提高","testing","0116","normal","\\","loss","含","图","使用","20","group","于","input","这组","参数","差别","ax2","[","y","show","inches","手段","test","0124","器中","class","偏高值","true","legend","抽取","了","接下来","16","注意","0114","迭代","0",")","1082","次","类型","重新","测量","发现","的","2","7230","lin","mathcal","0149","9978","mathbf","但","加入","设定","开始","iter","、","给定","：","线性变换","a","接下","可以","linclshandle","#","训练","输出","集","regressed","t","真实","steppe","mathbb","{","8","next","8513","aspect","def","values","1s","减小","2269","线性","(","9979","yp","4","19","则","这是","3278","未","记录","0618","从","为","全部","构成","learning","回调","25","设置","0120","十分","9822","3s","0.1","9","construct","epoch","step","if","varepsilon","个","results"],"title":"调试","title_tokens":["调试"]}]}