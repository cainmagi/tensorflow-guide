{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"扉页 ¶ 摘要 Tensorflow总纲，写给初学者们。本页面将简述Tensorflow的基本原理，结构设计以及版本更新。读者亦可在本页找到本教程将提供给读者对Tensorflow怎样的认识。 注意 由于技术限制，现在中文搜索功能无法完善，请注意当搜索关键词的时候自行分词，例如： 开放源代码软件库 无法搜到任何内容。但可以搜索 开放 源代码 软件 库 即可确保索引到上述内容。 Tensorflow总说 ¶ Tensorflow官网 TensorFlow™ 是一个开放源代码软件库，用于进行高性能数值计算。借助其灵活的架构，用户可以轻松地将计算工作部署到多种平台（CPU、GPU、TPU）和设备（桌面设备、服务器集群、移动设备、边缘设备等）。TensorFlow™ 最初是由 Google Brain 团队（隶属于 Google 的 AI 部门）中的研究人员和工程师开发的，可为机器学习和深度学习提供强力支持，并且其灵活的数值计算核心广泛应用于许多其他科学领域。 Tensorflow治学 ¶ 写在所有内容之前，读者不得不看以下几个页面，本教程所述内容大略来自于对这些资料的研读。 Tensorflow官网(中/英文) : https://www.tensorflow.org/ Keras中文文档 : https://keras-zh.readthedocs.io/ Tensorflow 2.0前瞻(英文) : https://medium.com/tensorflow/effective-tensorflow-2-0-best-practices-and-whats-changed-a0ca48767aff Tensorflow 2.0前瞻(中文) : https://zhuanlan.zhihu.com/p/50049041 提示 特别值得注意的是，现在官方文档至少在教程部分， 已经支持中英双语 。相比官方文档，我们的教程更侧重于以搭建工程为导向的设计。然而现在官方文档的翔实可读程度，确实不可错过。另一方面，Keras的 官方中文文档 似乎已不再更新，上面提供的原官方中文文档译者现在正在更新、维护的版本。 大略来说，学习Tensorflow主要应当依赖于官方文档的介绍。敝人自r1.4版开始入门Tensorflow，即是通过学习官方文档的教程来快速上手。相比四处蒐集资料，官方文档能提供一个完整、系统、完全贴合时下最新API的例子，帮助用户建立一个与Tensorflow各个功能合宜的使用习惯。若是通过在Github上检索他人的project，固然是一个很好的上手办法，但是往往就会遗漏某些重要的功能而不学。例如，Tensorflow自带的网络存取和Tensorboard API，在时兴的一些project中，经常会因方便之故，以numpy的IO来代替。笔者以为，如此培养起来的使用习惯，可谓走入偏门。当然，从这一点来说，本教程也亦复如是，即使笔者写下这些内容的时候，参照的乃是时下最新的API，时间一久，难免会过时。倘若笔者懒惰一些，不再时时更新本教程，那么本教程也就入不足取之流了。 虽然如此，教程也有并不合宜的地方。最大的问题莫过于Tensorflow本身，它有着日新月异的变化，此时能圆转如意地使用的API，到了彼时也许就成了废案。这就导致Tensorflow的官方教程也不断更新。r1.4时，教程还主要集中在如何使用“底层”API上，到了r1.9.0，就已经变成基本围绕着 tf.keras 设计的思路了。最新消息显示，即将上线的Tensorflow 2.0，将会彻底抛弃过去的“中层”API以下的全部方法， tf.layers , tf.contrib 都将被移除，倘若长期墨守成规，局限在入门时候的一套技术上，迟早会被官方库如此迅速的更新所弃。这也是无可奈何之事。毕竟Tensorflow仍然处于不断改进之中。倘若希望追求一个更加稳定、长期可用的库，keras或许是一个不错的选择。 故而，本教程将基本基于Tensorflow现在的版本(r1.12)展开介绍。本教程既可以看作一个入门教程，也可以看作是本人重新自学新版Tensorflow规范化API的一个手记，故而命名本教程为“手札”。读者不妨跟着笔者的思路，有如与笔者共学一般读下来本教程。 Tensorflow原理 ¶ 一个标准的Tensorflow工作流可以表示成这样： graph TD st(开始) --> Sess[启动Session] Sess --> ConNet[构建网络] subgraph 构造流程 ioNet>读取网络参数] --> ConNet end subgraph 执行流程 ioDat>导入数据] --> Run ConNet --> Run[执行网络] Run --> ioNetS>保存网络参数] Run --> ioRes>导出结果] end ioRes --> SessCl[关闭Session] SessCl --> ed(结束) classDef styStart fill:#FAE6A9,stroke:#BA9132; classDef styIO fill:#cde498,stroke:#13540c; class st,ed styStart class ioNet,ioNetS,ioDat,ioRes styIO 与一般的计算库不同，Tensorflow的执行流程大体可以分为两步： 构造流程 : 在这一步，Tensorflow根据用户代码构造一个 数据流图(dataflow graph) 。所谓数据流图，指的是由一系列 张量(Tensor) 构成的符号运算集合。就如同一张流程图一样，在这一阶段，尽管用户定义了每一步的运算（从简单的加减乘除到复杂的网络单元），但是没有任何运算被执行。就像一个程序员撰写代码一样，Tensorflow在这一步，将用户的代码转换成它的“机器语言”，但是网络还没有进入被使用的阶段。 执行流程 : 在这一步，Tensorflow将计算用户指定的某个 Tensor 的输出结果。要得到一个Tensor的输出，则必须得计算它一系列的依赖变量。例如，我们已知 y = x_1 + x_2 y = x_1 + x_2 , x_2 = z_1 \\times z_2 x_2 = z_1 \\times z_2 。那么，如果Tensorflow要得到 y y 的结果，它就必须先计算 x_2 x_2 。这个过程被完全地封装起来，从用户看来，我们只需要调用 y y 的输出即可，不需要关心Tensorflow是怎样按照流图完成计算的。 因此，典型的Tensorflow式的语言风格也可以这样划分： 构造流程 : 用户定义整个网络的符号运算，指定网络各个节点的属性、输入和输出。这些代码往往被写成一个函数（例如 def construct (): ） 执行流程 : 创建一个Session，在Session内调用构造函数，然后输入数据，得到并保存输出结果。必要情况下，还需要导入导出网络参数。 这里提到 会话(Session) 。Session如同一个Tensorflow虚拟机，在一个Session打开的时候，设备的计算资源（GPU, CPU, 带宽等）才被加载。Session又如同一个工人，用户撰写的网络构造代码如同车间，输入的数据如同商品，“工人（Session）”利用预定义好的“车间（流图）”将可以快速、批量地生产这些“商品（数据）”。这样地设计保证了一些外围的代码，例如指定网络参数的名称、函数的属性等操作，不需要重复进行，从而确保处理数据的时候，总是执行必要的代码，加快运算速度。 另一方面，构造-执行的结构还封装了许多提高运算效率的特性，例如多线程。尽管用户定义网络构造的时候，所写的代码是按照逻辑顺序的、线性的。但实际运行的时候，Tensorflow可以自行发现网络的哪些部分可以被同时运行，从而利用多核系统的计算资源。这些过程也是被完全封装起来的，用户并不需要花费心思去专门进行这些调整。 须知 在Tensorflow已经转变为Keras导向的现在，Session的调用被Keras API封装了起来，从用户的角度来看，现在已经不再需要手动调用Session。 Tensorflow API架构 ¶ 下图显示了当前Tensorflow-API的组织形式 Tensorflow在多个平台上均有部署，包括Python, C++, Java, Javascript, Go等，未来还可能支持更多的语言。然而，不同平台上，API的使用方式和代码风格是大不相同的。例如，Python的语言风格目前已经朝向Keras转变，而Javascript仍倾向于使用中层API；C++和Java偏向于使用底层API编写程序。本教程只针对时兴Python的API编写，除非涉及到特定的专题，不会讨论其他语言上Tensorflow的用法。 一般而言，Tensorflow将API面向划分为三个层次： 高层API (High level) : 包括Estimators和Keras； 中层API (Mid level) : 包括layers, datasets, loss和metrics等具有功能性的函数，例如网络层的定义，Loss Function，对结果的测量函数等； 底层API (Low level) : 包括具体的加减乘除、具有解析式的数学函数、卷积、对Tensor属性的测量等。 从r1.4入门的用户，所接受的训练往往是从底层API开始，使用自己的代码风格构建对应中层API的函数，然后再用自己定义好的中层API构建网络。在这种代码风格的驱使下，用户除了需要定义各个网络层具体的表达式，还需要设计输入输出的接口，用来调用 优化器(optimizer) 的接口等操作。 另一种使用方式，是从中层API开始，直接使用预定义好的网络层构建网络，这样的代码风格会节省一部分时间，相当于几乎不再需要接触底层API，但是在输入输出、网络训练和测试等更高层的模块设计上，和底层API用户的代码风格相似。 Estimators是从r1.4就已经存在的API了，它可以被看成是已经集成在Tensorflow里的完整的网络。因此，Estimators往往适合分发、需求不高的应用，但是并不适合使用Tensorflow的研究者。尽管至今为止，Tensorflow仍然对Estimators倍加推崇，但本教程将完全不涉及这方面的内容。 另一个高层API是 Eager Execution ，从r1.9起，Tensorflow就加入了这种新的使用模式，并且在Tensorflow 2.0中，它仍然将会被保留。Eager摈弃了上述的构造-执行流程，任何对Tensor的定义会被立时计算、并可得到结果。对于需要进行小规模的调试、检查API功能的用户而言更加方便。但是，据一些用户的反响，现在Eager还没有做到和之前API的完全兼容，本教程将基本不涉及Eager的用法。 因此，Keras API将是本教程的重中之重。使用本教程的用户，可以对照目前的官方文档，跟进本教程的思路，自己逐步实现各个project的设计。使用Keras风格的设计，代码量会明显少于前面提到的各种风格（Estimators除外）。事实上， tf.keras 和底层API之间具有一定的兼容，这样的编写风格能让我们在满足自己设计的派生功能的基础上，尽可能使用规范化、预定义的API单元，从而减少代码出错的可能性。通过对本教程TF 1.x版的学习，对未来2.0版的上手也会带来莫大帮助。 金宇琛( @cainmagi )，2019年3月2日 Yuchen Jin( @cainmagi ), Mar. 2, 2019 教程导读 ¶ 接下来，本教程将会涉及 从线性问题入门 : 如何使用Tensorflow完成一个简单的线性分类设计，我们将从Hello World开始，逐步过渡到一个具有核函数的非线性问题project。 Hello world: 第一个Tensorflow程序。 线性分类: 一个简单的二分类问题。 线性回归: 一个同样简单的，线性拟合问题。 非线性回归: 拟合一个简单的，可以表达出解析式的非线性函数。 非线性分类: 使用线性分类器对非线性分布的数据进行分类。 训练与测试分立 : 从这一章开始，并试图解决几个更加复杂的实际问题。这里涉及到的project相对更大、更完整，训练往往需要一定时间才能完成。因此，在本章，所有项目的训练、测试环节都会分开，我们将从这里开始，使用TensorBoard跟踪我们的训练情况，并介绍如何存取神经网络。 Super Resolution: 使用神经网络进行图像像素插值。 Sparse Coding: 使用线性的词典学习(dictionary learning)来进行图像像素插值。 Generitive Model: 使用时兴的生成模型(Generitive Model)来完成图像风格的转换。 原生的数据管理 : 从这一章开始，我们将纳入Tensorflow自带的数据管理API。实际上，很多用户并不习惯使用这些API，往往倾向于自己完成数据的导入导出。虽然这并非一个复杂的工作，但据Tensorflow的文档，原生的数据管理内部实现了多线程，本身更适合用于提高数据IO的效率。读者可以酌情选择是否需要阅读本章。 使用MINIST数据集: 使用原生的数据管理来IO一个现成的MINIST数据集，这一节改自 官方文档教程 。 使用RNN处理文章: 通过LSTM模型来进行文本分类，并使用原生的数据管理来IO一个现成的IMDB数据集，这一节改自 官方文档教程 。 使用RBM生成音频: 通过限制玻尔兹曼机(RBM)来编写一个简单的音乐生成器，这一节改自 llSourcell/Music_Generator_Demo 。 其他的高级技巧 : 本章将介绍一些特殊的、利用底层API技巧，使得用户能更灵活地定制Tensorflow-keras，从而实现一些仅依靠原生库不能实现的功能。","text_tokens":["相对","值得注意","线性","关键","之","技巧","找到","虚拟机","二","之流","c++","了","此时","看",":","看作","机器","几个","相比","全部","手动","表达式","方面","架构","应当",">","某个","llsourcell","这方面","满足","sparse","一种","用户","自学","层次","给","model","不断","之故","9","可","逻辑","¶","到","super","导致","各个","运算","设备","上面","与","流图","有着","用法","跟进","介绍","要","足取","重复","除非","墨守成规","+","批量","学","依赖","定制","模块","jin","这样","起来","也许","他人","成","跟着","构造函数","编写","处理","即","版","应用","使得","layers","13540c","sesscl","拟合","神经网","iores","卷积","styio","构设","玻尔兹曼","参数","但是","速度","调整","程序","培养","将","也","python","问题","mar","机","不再","似乎","朝向","时间","新月","分布","读者","\\","围绕","需求","莫过于","官网","；","很","不学","如何","subgraph","抛弃","输入","更加","月","总纲","进入","不足取","加入","为","加快","如同","分发","通过","错过","很多","相当于","zhuanlan","偏门","对于","敝","兼容","本页","手","手记","（","入门教程","功能","接受","代替","倾向","桌面","软件","花费","除了","思路","专门","尽可","遗漏","rnn","隶属于","contrib","乃是","graph","多线","流程图","，","亦可","研读","指","]","stroke","人员","版本","节点","长期","上述","测量","阶段","构造","可能","一定","class","以及","不得不","分类","几乎","成规","运行","实际上","dictionary","mid","集中","基础","按照","当前","关心","阅读","自","前面","一","着","优化","z","一章","[","iodat","走入","又","简述","调用","帮助","表示","不合","音频","生成器","工程师","网络层","原","依赖于","不断更新","文本","手札","中层","并不需要","车间","例子","coding","是从","下来","可用","工人","metrics","中文搜索","换成","式","流程","必须","才","其他","cainmagi","像素","像","所述","y","而","得","日新月异","io","r1","一点","适合","义","封装","一个","至今","线程","执行","倍加","music","只",".","但","effective","@","imdb","、","还","团队","角度","故而","文档","涉及","关闭","practices","如是","即可","#","打开","而言","改","p","玻尔","单元","参照","来说","莫过","本身","构成","外围","规范化","一般而言","readthedocs","datasets","现成","rbm","best","逐步","以为","分开","最初","起","复杂","大略","英双语","平台","展开","包括","里","跟踪","依靠","许多","自带","确实","不","保存","官方","能圆转","必要","dataflow","怎样","保留","事实上","ba9132","研究","开发","如此","须知","对本","一套","三个","变化","st","神经","让","合宜","内容","zh","集成","其","同样","于","少于","针对","一部分","加减乘除","一方面","tensor","限制","已经","维护","无法","至少","从这一点","金宇琛","做到","年","入","标准","明显","无可奈何","改进","资源","仍","对照","加减","world","领域","彻底","虽然","入门","注意","因此","借助","目前","管理","久","demo","多种","既","可谓","本章","重要","事","tensorflow","构建","这方","当","仍然","tf","本","接口","提高","功能性","人","”","重中之重","形式","org","张量","导入","小规模","工作","之中","写成","转变","存取","集","建立","分类器","whats","实际","和","效率","侧重","tensorboard","去","2.0","稳定","流","测试","出","generator","自己","ionets","那么","环节","科学","方法","广泛","会因","移除","规范","td","多","高层","过渡","语言","商品","高性能","共","高级","反响","一部","zhihu","™","3","侧重于","简单","转换","connet","用来","接下来","写下","固然","迟早会","但本","不高","50049041","基于","节省","一步","无可","内部","系列","表达","摈弃","generitive","“","仅","能","时时","部门","墨守成","对应","词典","它","(","事实","stystart","亦复如是","def","如果","技术","开始","服务器","库","搭建","numpy","并","网络","编写程序","更","具体","来","项目","不合宜","名称","扉页","中文","导出","检查","www","看成","正在","时","fae6a9","com","不会","非线性","面向","的","发现","好","classdef","-","数值","索引","结果","确保","所写","乘除","新","时候","哪些","可以","12","：","construct","这些","更大","相似","前瞻","由于","莫大","来看","重于","不得","当于","研究者","规模","提到","导向","分词","/","不能","ionet","赖于","一方","google","方式","从","需要","1","ed","地方","系统","解决","不断改进","命名","轻松","一节","得到"," ","=","并非","减少","快速","页面","数据流","之间","中","核","源代码","具有","出错","难免","过时","往往","范化","上","然后","新版","看来","每","撰写","上线","由","写","对","翔实","x","function","相当","属于","性能","我们","大不相同","整个","一系","gpu","函数","hello","迟早","量会","不同","推崇","变成","不妨","调试","贴合","认识","github","输入输出","tpu","4","有","双语","初学者","_","划分","属性","实现","但据","程序员","试图","所谓","典型","组织","服务","导读","带宽","先","两步","数据管理","过去","各种","若","毕竟","自行","回归","译者","神经网络","集合","是否","execution",";","地","底层","go","现在","预定","资料","基本原理","读","读取","optimizer","过于","操作","结构","大体","虚拟","存在","更新","等","9.0","图","广泛应用","一般","集群","迅速","某些","下","已","用于","这种","结束","0","音乐","最新消息",")","changed","特别","a0ca48767aff","就","搜索","达式","们","javascript","另一方","分立","特殊","派生","支持","时兴","消息","然而","来自","才能","low","）","保证","符号","多核","尽可能","启动","经常","这个","本人","end","medium","之前","开放","and","处于","变量","cde498","深度","所弃","输出",",","如意","不错","即将","转换成","未来","加载","总说","同时","定义","提供","专题","2019","计算资源","learning","不可","特性","https","为止","部分","被","api","讨论","。","英文","数学","指定","内","都","特定","日","图像","懒惰","立时","times","摘要","心思","隶属","level","sess","所","并且","习惯","java","根据","本原","带来","以","brain","治学","estimators","下图","结构设计","则","eager","当然","lstm","蒐集","强力","生产","第一个","数据","偏向","模型","在","程度","loss","提示","原生","纳入","务器","检索","fill","尽管","学习","显示","可能性","分为","完整","多线程","总是","重新","多个","兹曼","解析","相同","一样","笔者","文章","例如","原理","彼时","有如","请","没有","据","2","一些","高性","模式","办法","所有","这里","灵活","局限","初学","high","方便","生成","再用","奈何","风格","接触","酌情","创建","除外","ai","训练","这","顺序","部署","会","代码","核心","器","resolution","学者","插值","基本","边缘","网络单元","搜","主要","完全","难免会","任何","值得","教程","使用","倘若","最新","追求","选择","即使","一系列","希望","会话","义好","驱使","过程","直接","minist","一张","另一方面","机器语言","计算","最大","session","废案","是","移动","完善","从而","完成","不足","进行","另","均","已知","设计","第一","或许","project","以下","工程","利用","关键词","守成","四处","写给","yuchen","run","情况","keras","墨守","成器","cpu","接下","将会"],"title":"扉页","title_tokens":["扉页"]},{"location":"#_1","text":"摘要 Tensorflow总纲，写给初学者们。本页面将简述Tensorflow的基本原理，结构设计以及版本更新。读者亦可在本页找到本教程将提供给读者对Tensorflow怎样的认识。 注意 由于技术限制，现在中文搜索功能无法完善，请注意当搜索关键词的时候自行分词，例如： 开放源代码软件库 无法搜到任何内容。但可以搜索 开放 源代码 软件 库 即可确保索引到上述内容。","text_tokens":["简述","，","亦可","认识","教程","注意","关键","搜索","找到","例如","任何","初学者","。","们","原理","版本","中文","请","tensorflow","但","总纲","怎样","上述","的","初学","摘要","完善","当","自行"," ","本","以及","索引","本原","页面","设计","中文搜索","现在","分词","源代码","本页","内容","确保","关键词","基本原理","写给","时候","结构设计","可以","功能","构设","：","即可","结构","库","更新","代码","限制","给","在","对","由于","学者","开放","软件","无法","基本","将","到","搜","技术","提供","读者"],"title":"扉页","title_tokens":["扉页"]},{"location":"#tensorflow","text":"Tensorflow官网 TensorFlow™ 是一个开放源代码软件库，用于进行高性能数值计算。借助其灵活的架构，用户可以轻松地将计算工作部署到多种平台（CPU、GPU、TPU）和设备（桌面设备、服务器集群、移动设备、边缘设备等）。TensorFlow™ 最初是由 Google Brain 团队（隶属于 Google 的 AI 部门）中的研究人员和工程师开发的，可为机器学习和深度学习提供强力支持，并且其灵活的数值计算核心广泛应用于许多其他科学领域。","text_tokens":["广泛","高性能","工程师","支持","机器","™","）","架构","其他","用户","开放","可","深度","到","部门","设备","服务器","提供","库","一个","。","的","、","隶属","数值","并且","团队","应用","brain","可以","强力","将","务器","学习","最初","平台","google","官网","许多","高性","灵活","轻松","研究","开发","为"," ","ai","中","源代码","其","于","（","部署","由","代码","核心","桌面","软件","属于","性能","边缘","隶属于","gpu","领域","，","tpu","借助","多种","人员","tensorflow","计算","服务","是","移动","进行","地","工程","工作","等","广泛应用","和","集群","cpu","用于","科学"],"title":"Tensorflow总说","title_tokens":["tensorflow","总说"]},{"location":"#tensorflow_1","text":"写在所有内容之前，读者不得不看以下几个页面，本教程所述内容大略来自于对这些资料的研读。 Tensorflow官网(中/英文) : https://www.tensorflow.org/ Keras中文文档 : https://keras-zh.readthedocs.io/ Tensorflow 2.0前瞻(英文) : https://medium.com/tensorflow/effective-tensorflow-2-0-best-practices-and-whats-changed-a0ca48767aff Tensorflow 2.0前瞻(中文) : https://zhuanlan.zhihu.com/p/50049041 提示 特别值得注意的是，现在官方文档至少在教程部分， 已经支持中英双语 。相比官方文档，我们的教程更侧重于以搭建工程为导向的设计。然而现在官方文档的翔实可读程度，确实不可错过。另一方面，Keras的 官方中文文档 似乎已不再更新，上面提供的原官方中文文档译者现在正在更新、维护的版本。 大略来说，学习Tensorflow主要应当依赖于官方文档的介绍。敝人自r1.4版开始入门Tensorflow，即是通过学习官方文档的教程来快速上手。相比四处蒐集资料，官方文档能提供一个完整、系统、完全贴合时下最新API的例子，帮助用户建立一个与Tensorflow各个功能合宜的使用习惯。若是通过在Github上检索他人的project，固然是一个很好的上手办法，但是往往就会遗漏某些重要的功能而不学。例如，Tensorflow自带的网络存取和Tensorboard API，在时兴的一些project中，经常会因方便之故，以numpy的IO来代替。笔者以为，如此培养起来的使用习惯，可谓走入偏门。当然，从这一点来说，本教程也亦复如是，即使笔者写下这些内容的时候，参照的乃是时下最新的API，时间一久，难免会过时。倘若笔者懒惰一些，不再时时更新本教程，那么本教程也就入不足取之流了。 虽然如此，教程也有并不合宜的地方。最大的问题莫过于Tensorflow本身，它有着日新月异的变化，此时能圆转如意地使用的API，到了彼时也许就成了废案。这就导致Tensorflow的官方教程也不断更新。r1.4时，教程还主要集中在如何使用“底层”API上，到了r1.9.0，就已经变成基本围绕着 tf.keras 设计的思路了。最新消息显示，即将上线的Tensorflow 2.0，将会彻底抛弃过去的“中层”API以下的全部方法， tf.layers , tf.contrib 都将被移除，倘若长期墨守成规，局限在入门时候的一套技术上，迟早会被官方库如此迅速的更新所弃。这也是无可奈何之事。毕竟Tensorflow仍然处于不断改进之中。倘若希望追求一个更加稳定、长期可用的库，keras或许是一个不错的选择。 故而，本教程将基本基于Tensorflow现在的版本(r1.12)展开介绍。本教程既可以看作一个入门教程，也可以看作是本人重新自学新版Tensorflow规范化API的一个手记，故而命名本教程为“手札”。读者不妨跟着笔者的思路，有如与笔者共学一般读下来本教程。","text_tokens":["值得注意","之","之流","了","此时","看",":","看作","几个","相比","全部","方面","应当","用户","自学","不断","之故","可","到","导致","各个","上面","与","有着","介绍","足取","墨守成规","学","依赖","起来","也许","他人","成","跟着","即","版","layers","但是","培养","也","将","问题","不再","似乎","时间","新月","读者","围绕","莫过于","官网","很","不学","如何","抛弃","更加","不足取","为","通过","错过","zhuanlan","偏门","敝","手","手记","入门教程","功能","代替","思路","遗漏","contrib","乃是","，","研读","版本","长期","不得不","成规","集中","自","一","着","走入","帮助","不合","原","依赖于","不断更新","手札","中层","例子","下来","可用","所述","而","日新月异","io","r1","一点","一个",".","effective","、","还","故而","文档","practices","如是","p","参照","来说","莫过","本身","规范化","readthedocs","best","以为","英双语","大略","展开","自带","确实","官方","能圆转","如此","一套","变化","合宜","内容","zh","于","一方面","已经","维护","至少","从这一点","入","无可奈何","改进","彻底","虽然","入门","注意","久","既","可谓","重要","事","tensorflow","仍然","tf","本","人","”","org","之中","存取","建立","whats","和","侧重","tensorboard","2.0","稳定","那么","方法","会因","移除","规范","共","zhihu","侧重于","写下","固然","迟早会","50049041","基于","无可","“","能","时时","墨守成","它","(","亦复如是","技术","开始","库","搭建","numpy","并","网络","更","来","不合宜","中文","www","正在","时","com","的","好","-","时候","可以","12","这些","前瞻","重于","不得","导向","/","赖于","一方","地方","系统","不断改进","命名"," ","快速","页面","中","难免","过时","往往","范化","上","新版","上线","写","对","翔实","我们","迟早","变成","不妨","贴合","github","4","有","双语","过去","若","毕竟","译者","地","底层","现在","资料","读","过于","更新","9.0","一般","迅速","某些","下","已","0","最新消息",")","changed","特别","a0ca48767aff","就","另一方","支持","时兴","消息","然而","来自","经常","本人","medium","之前","and","处于","所弃",",","如意","不错","即将","提供","不可","https","部分","被","api","。","英文","都","懒惰","习惯","以","当然","蒐集","在","程度","提示","检索","学习","显示","完整","重新","笔者","例如","彼时","有如","2","一些","办法","所有","局限","方便","奈何","这","会","基本","主要","完全","难免会","值得","追求","倘若","教程","使用","最新","选择","即使","希望","另一方面","最大","废案","是","不足","设计","或许","project","以下","工程","守成","四处","keras","墨守","将会"],"title":"Tensorflow治学","title_tokens":["tensorflow","治学"]},{"location":"#tensorflow_2","text":"一个标准的Tensorflow工作流可以表示成这样： graph TD st(开始) --> Sess[启动Session] Sess --> ConNet[构建网络] subgraph 构造流程 ioNet>读取网络参数] --> ConNet end subgraph 执行流程 ioDat>导入数据] --> Run ConNet --> Run[执行网络] Run --> ioNetS>保存网络参数] Run --> ioRes>导出结果] end ioRes --> SessCl[关闭Session] SessCl --> ed(结束) classDef styStart fill:#FAE6A9,stroke:#BA9132; classDef styIO fill:#cde498,stroke:#13540c; class st,ed styStart class ioNet,ioNetS,ioDat,ioRes styIO 与一般的计算库不同，Tensorflow的执行流程大体可以分为两步： 构造流程 : 在这一步，Tensorflow根据用户代码构造一个 数据流图(dataflow graph) 。所谓数据流图，指的是由一系列 张量(Tensor) 构成的符号运算集合。就如同一张流程图一样，在这一阶段，尽管用户定义了每一步的运算（从简单的加减乘除到复杂的网络单元），但是没有任何运算被执行。就像一个程序员撰写代码一样，Tensorflow在这一步，将用户的代码转换成它的“机器语言”，但是网络还没有进入被使用的阶段。 执行流程 : 在这一步，Tensorflow将计算用户指定的某个 Tensor 的输出结果。要得到一个Tensor的输出，则必须得计算它一系列的依赖变量。例如，我们已知 y = x_1 + x_2 y = x_1 + x_2 , x_2 = z_1 \\times z_2 x_2 = z_1 \\times z_2 。那么，如果Tensorflow要得到 y y 的结果，它就必须先计算 x_2 x_2 。这个过程被完全地封装起来，从用户看来，我们只需要调用 y y 的输出即可，不需要关心Tensorflow是怎样按照流图完成计算的。 因此，典型的Tensorflow式的语言风格也可以这样划分： 构造流程 : 用户定义整个网络的符号运算，指定网络各个节点的属性、输入和输出。这些代码往往被写成一个函数（例如 def construct (): ） 执行流程 : 创建一个Session，在Session内调用构造函数，然后输入数据，得到并保存输出结果。必要情况下，还需要导入导出网络参数。 这里提到 会话(Session) 。Session如同一个Tensorflow虚拟机，在一个Session打开的时候，设备的计算资源（GPU, CPU, 带宽等）才被加载。Session又如同一个工人，用户撰写的网络构造代码如同车间，输入的数据如同商品，“工人（Session）”利用预定义好的“车间（流图）”将可以快速、批量地生产这些“商品（数据）”。这样地设计保证了一些外围的代码，例如指定网络参数的名称、函数的属性等操作，不需要重复进行，从而确保处理数据的时候，总是执行必要的代码，加快运算速度。 另一方面，构造-执行的结构还封装了许多提高运算效率的特性，例如多线程。尽管用户定义网络构造的时候，所写的代码是按照逻辑顺序的、线性的。但实际运行的时候，Tensorflow可以自行发现网络的哪些部分可以被同时运行，从而利用多核系统的计算资源。这些过程也是被完全封装起来的，用户并不需要花费心思去专门进行这些调整。 须知 在Tensorflow已经转变为Keras导向的现在，Session的调用被Keras API封装了起来，从用户的角度来看，现在已经不再需要手动调用Session。","text_tokens":["调用",")","td","就","表示","线性","语言","商品","虚拟机","另一方","了",":","并不需要","机器","车间","手动","方面","简单","）","connet","符号","转换","换成","式","工人","保证",">","启动","流程","某个","必须","才","一步","系列","这个","用户","像","end","“","变量","y","cde498","得","输出","到","逻辑",",","各个","运算","(","转换成","它","设备","加载","stystart","与","流图","如果","def","同时","开始","定义","库","封装","一个","网络","要","并","计算资源","线程","执行","重复","特性","被","名称","部分","+","api","。","导出","批量","只","指定","内","但","依赖","fae6a9","times","的","这样","还","起来","成","classdef","、","-","构造函数","sess","心思","处理","根据","角度","13540c","结果","确保","所写","sesscl","乘除","时候","哪些","iores","关闭","styio","可以","：","则","即可","construct","#","这些","打开","生产","数据","在","参数","单元","但是","速度","调整","程序","来看","构成","将","也","外围","fill","尽管","不再","提到","分为","多线程","导向","总是","ionet","复杂","\\","一方","一样","许多","例如","从","不","subgraph","保存","没有","需要","1","输入","2","ed","必要","一些","dataflow","怎样","进入","系统","这里","ba9132","须知","为","加快","得到","风格"," ","=","如同","创建","数据流","st","快速","这","（","往往","顺序","加减乘除","然后","看来","一方面","每","tensor","撰写","由","代码","x","已经","花费","专门","我们","整个","一系","标准","网络单元","资源","函数","graph","加减","gpu","多线","完全","不同","流程图","，","任何","因此","一系列","使用","指","节点","会话","义好","]","属性","stroke","_","过程","划分","tensorflow","一张","构建","程序员","所谓","机器语言","计算","典型","另一方面","session","带宽","是","先","阶段","构造","两步","从而","完成","自行","class","进行","集合","提高",";","已知","地","”","设计","现在","预定","利用","多核","读取","运行","张量","导入","工作","转变","写成","操作","大体","虚拟","结构","run","按照","等","关心","情况","图","实际","一般","和","效率","发现","keras","cpu","去","流","一","下","z","[","iodat","ionets","那么","结束","又"],"title":"Tensorflow原理","title_tokens":["tensorflow","原理"]},{"location":"#tensorflow-api","text":"下图显示了当前Tensorflow-API的组织形式 Tensorflow在多个平台上均有部署，包括Python, C++, Java, Javascript, Go等，未来还可能支持更多的语言。然而，不同平台上，API的使用方式和代码风格是大不相同的。例如，Python的语言风格目前已经朝向Keras转变，而Javascript仍倾向于使用中层API；C++和Java偏向于使用底层API编写程序。本教程只针对时兴Python的API编写，除非涉及到特定的专题，不会讨论其他语言上Tensorflow的用法。 一般而言，Tensorflow将API面向划分为三个层次： 高层API (High level) : 包括Estimators和Keras； 中层API (Mid level) : 包括layers, datasets, loss和metrics等具有功能性的函数，例如网络层的定义，Loss Function，对结果的测量函数等； 底层API (Low level) : 包括具体的加减乘除、具有解析式的数学函数、卷积、对Tensor属性的测量等。 从r1.4入门的用户，所接受的训练往往是从底层API开始，使用自己的代码风格构建对应中层API的函数，然后再用自己定义好的中层API构建网络。在这种代码风格的驱使下，用户除了需要定义各个网络层具体的表达式，还需要设计输入输出的接口，用来调用 优化器(optimizer) 的接口等操作。 另一种使用方式，是从中层API开始，直接使用预定义好的网络层构建网络，这样的代码风格会节省一部分时间，相当于几乎不再需要接触底层API，但是在输入输出、网络训练和测试等更高层的模块设计上，和底层API用户的代码风格相似。 Estimators是从r1.4就已经存在的API了，它可以被看成是已经集成在Tensorflow里的完整的网络。因此，Estimators往往适合分发、需求不高的应用，但是并不适合使用Tensorflow的研究者。尽管至今为止，Tensorflow仍然对Estimators倍加推崇，但本教程将完全不涉及这方面的内容。 另一个高层API是 Eager Execution ，从r1.9起，Tensorflow就加入了这种新的使用模式，并且在Tensorflow 2.0中，它仍然将会被保留。Eager摈弃了上述的构造-执行流程，任何对Tensor的定义会被立时计算、并可得到结果。对于需要进行小规模的调试、检查API功能的用户而言更加方便。但是，据一些用户的反响，现在Eager还没有做到和之前API的完全兼容，本教程将基本不涉及Eager的用法。 因此，Keras API将是本教程的重中之重。使用本教程的用户，可以对照目前的官方文档，跟进本教程的思路，自己逐步实现各个project的设计。使用Keras风格的设计，代码量会明显少于前面提到的各种风格（Estimators除外）。事实上， tf.keras 和底层API之间具有一定的兼容，这样的编写风格能让我们在满足自己设计的派生功能的基础上，尽可能使用规范化、预定义的API单元，从而减少代码出错的可能性。通过对本教程TF 1.x版的学习，对未来2.0版的上手也会带来莫大帮助。 金宇琛( @cainmagi )，2019年3月2日 Yuchen Jin( @cainmagi ), Mar. 2, 2019","text_tokens":["c++","了",":","表达式","方面","这方面","满足","一种","用户","层次","9","可","到","各个","用法","跟进","除非","模块","jin","这样","编写","版","应用","layers","卷积","但是","程序","将","也","python","mar","不再","朝向","时间","需求","；","输入","更加","月","加入","为","分发","通过","相当于","对于","兼容","手","（","功能","接受","倾向","尽可","除了","思路","，","上述","测量","可能","构造","一定","几乎","mid","基础","当前","优化","调用","帮助","网络层","中层","是从","metrics","式","流程","其他","cainmagi","而","r1","适合","义","一个","至今","执行","倍加","只",".","@","还","、","涉及","文档","而言","单元","一般而言","datasets","规范化","逐步","起","包括","平台","里","不","官方","保留","事实上","研究","对本","三个","让","内容","集成","于","少于","针对","一部分","加减乘除","tensor","已经","金宇琛","做到","年","明显","仍","对照","加减","入门","因此","目前","tensorflow","构建","这方","仍然","接口","本","tf","功能性","重中之重","形式","小规模","转变","和","2.0","测试","自己","规范","多","高层","语言","反响","一部","3","用来","但本","不高","节省","表达","摈弃","能","对应","它","(","事实","开始","并","网络","编写程序","更","具体","检查","看成","不会","面向","的","好","-","结果","乘除","新","可以","：","相似","莫大","当于","研究者","规模","提到","方式","从","需要","1","得到"," ","减少","之间","中","具有","出错","往往","范化","上","然后","相当","对","x","function","我们","大不相同","函数","量会","不同","推崇","调试","输入输出","4","有","属性","实现","划分","组织","各种","execution","go","底层","现在","预定","optimizer","操作","存在","等","一般","下","这种",")","就","达式","javascript","派生","支持","时兴","然而","low","）","尽可能","之前","输出",",","未来","定义","专题","2019","为止","部分","被","api","讨论","。","数学","特定","日","立时","level","所","并且","java","带来","estimators","下图","eager","偏向","在","loss","尽管","显示","学习","可能性","完整","多个","解析","相同","例如","没有","据","2","一些","模式","high","方便","再用","风格","接触","除外","训练","部署","会","代码","器","基本","完全","任何","教程","使用","前面","义好","驱使","直接","计算","是","从而","进行","另","均","设计","project","yuchen","keras","将会"],"title":"Tensorflow API架构","title_tokens":["tensorflow","架构","api"," "]},{"location":"#_2","text":"接下来，本教程将会涉及 从线性问题入门 : 如何使用Tensorflow完成一个简单的线性分类设计，我们将从Hello World开始，逐步过渡到一个具有核函数的非线性问题project。 Hello world: 第一个Tensorflow程序。 线性分类: 一个简单的二分类问题。 线性回归: 一个同样简单的，线性拟合问题。 非线性回归: 拟合一个简单的，可以表达出解析式的非线性函数。 非线性分类: 使用线性分类器对非线性分布的数据进行分类。 训练与测试分立 : 从这一章开始，并试图解决几个更加复杂的实际问题。这里涉及到的project相对更大、更完整，训练往往需要一定时间才能完成。因此，在本章，所有项目的训练、测试环节都会分开，我们将从这里开始，使用TensorBoard跟踪我们的训练情况，并介绍如何存取神经网络。 Super Resolution: 使用神经网络进行图像像素插值。 Sparse Coding: 使用线性的词典学习(dictionary learning)来进行图像像素插值。 Generitive Model: 使用时兴的生成模型(Generitive Model)来完成图像风格的转换。 原生的数据管理 : 从这一章开始，我们将纳入Tensorflow自带的数据管理API。实际上，很多用户并不习惯使用这些API，往往倾向于自己完成数据的导入导出。虽然这并非一个复杂的工作，但据Tensorflow的文档，原生的数据管理内部实现了多线程，本身更适合用于提高数据IO的效率。读者可以酌情选择是否需要阅读本章。 使用MINIST数据集: 使用原生的数据管理来IO一个现成的MINIST数据集，这一节改自 官方文档教程 。 使用RNN处理文章: 通过LSTM模型来进行文本分类，并使用原生的数据管理来IO一个现成的IMDB数据集，这一节改自 官方文档教程 。 使用RBM生成音频: 通过限制玻尔兹曼机(RBM)来编写一个简单的音乐生成器，这一节改自 llSourcell/Music_Generator_Demo 。 其他的高级技巧 : 本章将介绍一些特殊的、利用底层API技巧，使得用户能更灵活地定制Tensorflow-keras，从而实现一些仅依靠原生库不能实现的功能。","text_tokens":["相对","音乐",")","线性","技巧","过渡","音频","生成器","高级","分立","特殊","二","了","文本","时兴",":","几个","才能","coding","下来","接下来","简单","转换","式","llsourcell","其他","sparse","内部","表达","用户","generitive","像素","model","仅","能","到","super","io","词典","(","适合","与","开始","库","介绍","并","一个","网络","learning","线程","更","来","项目","api","。","导出","music","都","图像","定制","非线性","imdb","的","、","编写","-","习惯","处理","使得","拟合","涉及","神经网","文档","可以","lstm","这些","更大","玻尔兹曼","第一个","改","数据","模型","玻尔","在","程序","将","原生","本身","问题","机","纳入","现成","rbm","学习","逐步","完整","多线程","分开","/","时间","复杂","不能","兹曼","分布","解析","跟踪","读者","依靠","文章","如何","从","自带","不","官方","需要","更加","一些","解决","这里","所有","灵活","生成","一节","风格"," ","酌情","并非","通过","核","神经","很多","训练","这","具有","同样","于","往往","功能","会","倾向","resolution","限制","对","插值","我们","rnn","函数","hello","world","多线","虽然","入门","，","因此","教程","使用","选择","管理","demo","实现","_","本章","但据","minist","tensorflow","试图","数据管理","从而","回归","完成","一定","本","神经网络","进行","提高","是否","分类","底层","设计","第一","地","project","利用","实际上","导入","dictionary","工作","存取","集","情况","分类器","阅读","实际","效率","keras","自","成器","接下","tensorboard","测试","generator","出","一章","自己","用于","环节","将会"],"title":"教程导读","title_tokens":["导读","教程"]},{"location":"licenses/","text":"协议 (Licenses) ¶ 本站协议 (中文版) ¶ MIT 开源许可协议 版权所有 © 2019, 金宇琛 (cainmagi) 特此向任何得到本软件副本或相关文档的人授权：被授权人有权使用、复制、修改、 合并、出版、发布、散布、再授权和/或贩售软件及软件的副本，及授予被供应人 同等权利，只需服从以下义务： 在软件和软件的所有副本中都必须包含以上版权声明和本许可声明。 该软件是\"按原样\"提供的，没有任何形式的明示或暗示，包括但不限于为特定目的和 不侵权的适销性和适用性的保证担保。在任何情况下，作者或版权持有人，都无权要求 任何索赔，或有关损害赔偿的其他责任。无论在本软件的使用上或其他买卖交易中， 是否涉及合同，侵权或其他行为。 License of this website (English version) ¶ MIT License Copyright © 2019 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. 本站相关项目的协议 ¶ 下面介绍的诸多协议，原则上并无必要列在本条目中（例如MIT License）。列在下侧，主要是为了向诸位支持本文档的开发者致以真诚的谢意。亦将荣耀与在下的感谢致以高天之上的上帝，愿你保守这份文档，成为众人的帮助。 License of Material ¶ MIT License Copyright © 2016 - 2019 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of MkDocs ¶ BSD 2-Clause \"Simplified\" License Copyright © 2014, Tom Christie. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. License of Jieba3K ¶ The MIT License (MIT) Copyright © 2013 Sun Junyi Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of Simple Lightbox ¶ The MIT License (MIT) Copyright © 2018 Damir Brekalo Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of MathJax ¶ Apache License 2.0 See the full license here: MathJax license License of mermaid ¶ The MIT License (MIT) Copyright © 2014 - 2018 Knut Sveidqvist Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["noninfringement","portions","or",":","sell","modification","connection","声明","向","¶","高天","贩售","下侧","与","介绍","merge","provided","caused","of","\"","jin","sveidqvist","上帝","damir","为了","subject","disclaimer","限于","theory","merchantability","谢意","将","met","version","express","however","whom","permission","by","mkdocs","为","an","do","2013","in","（","2014","荣耀","及","软件","data","2018","责任","mathjax","条目","licenses","forms","indirect","，","must","conditions","notice","以上","full","out","damage","真诚","或","christie","感谢","purpose","副本","action","particular","目的","this","all","authors","claim","帮助","junyi","with","有人","reproduce","按","mermaid","列","侵权","必须","其他","列在","cainmagi","合同","strict","授权","exemplary","is","redistribution","特此","发布","无权","只",".","但","permitted","、","on","without","文档","涉及","material","致以","software","clause","开源","kind","negligence","warranties","包括","供应","无论","see","不","for","必要","你","direct","interruption","开发","rights","the","有关","金宇琛","granted","服从","goods","website","license","limited","modify","permit","martin","incidental","contributors","jieba3k","are","本","亦","to","人","形式","not","和","2.0","适用","同等","持有人","hereby","substitute","including","该软件","if","donath","买卖","whether","arising","原样","损害","出版","even","holders","(","use","services","并","tom","项目","contract","中文","fitness","person","散布","的","保守","-","下面","disclaimed","原则上","list","原则","restriction","copyright","：","any","materials","reserved","无","procurement","/","holder","©","shall","lightbox","way","要求","开发者","诸位","warranty","furnished","得到"," ","中","advised","义务","上","明示","行为","再","charge","之上","so","复制","bsd","special","publish","权利","相关","需","是否","redistributions",";","damages","deal","liable","associated","persons","mit","下","中文版","协议","apache","no","such",")","愿","files","brekalo","成为","支持","substantial","许可","obtaining","保证","）","infringement","limitation","documentation","above","and","distribute",",","性","code","2016","form","适销","from","提供","2019","implied","被","。","索赔","特定","都","a","consequential","众人","possibility","simplified","distribution","retain","在","loss","business","授予","适用性","free","文版","有权","担保","持有","暗示","例如","here","没有","2","所有","as","版权所有","that","版权","english","交易","but","本站","other","这份","simple","following","修改","授权人","knut","诸多","sun","主要","non","任何","使用","dealings","sublicense","liability","赔偿","copies","是","be","included","包含","损害赔偿","以下","otherwise","event","source","profits","合并","作者","yuchen","情况","copy","tort","binary"],"title":"协议","title_tokens":["协议"]},{"location":"licenses/#licenses","text":"","text_tokens":[],"title":"协议 (Licenses)","title_tokens":[")","("," ","协议","licenses"]},{"location":"licenses/#_1","text":"MIT 开源许可协议 版权所有 © 2019, 金宇琛 (cainmagi) 特此向任何得到本软件副本或相关文档的人授权：被授权人有权使用、复制、修改、 合并、出版、发布、散布、再授权和/或贩售软件及软件的副本，及授予被供应人 同等权利，只需服从以下义务： 在软件和软件的所有副本中都必须包含以上版权声明和本许可声明。 该软件是\"按原样\"提供的，没有任何形式的明示或暗示，包括但不限于为特定目的和 不侵权的适销性和适用性的保证担保。在任何情况下，作者或版权持有人，都无权要求 任何索赔，或有关损害赔偿的其他责任。无论在本软件的使用上或其他买卖交易中， 是否涉及合同，侵权或其他行为。","text_tokens":[")","该软件","有人","按","许可","买卖","保证","侵权","必须","其他","声明","原样","cainmagi","向","出版","损害","合同",",","贩售","(","性","授权","适销","提供","2019","特此","被","发布","。","无权","只","索赔","\"","都","特定","但","散布","的","、","文档","涉及","限于","：","在","授予","开源","适用性","有权","担保","/","包括","©","供应","持有","暗示","无论","不","没有","要求","所有","为","版权所有","得到"," ","版权","中","交易","有关","义务","上","明示","修改","行为","再","及","软件","授权人","复制","金宇琛","服从","责任","，","任何","使用","以上","权利","相关","需","赔偿","是","或","本","是否","人","包含","损害赔偿","形式","以下","副本","合并","作者","目的","情况","和","mit","下","适用","协议","同等","持有人"],"title":"本站协议 (中文版)","title_tokens":["本站",")","("," ","中文版","中文","协议","文版"]},{"location":"licenses/#license-of-this-website-english-version","text":"MIT License Copyright © 2019 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":[")","including","portions","noninfringement","files","with","or","substantial",":","obtaining","whether","arising","sell","limitation","connection","documentation","cainmagi","above","and","holders","distribute",",","(","use","is","from","2019","implied","merge","provided","contract","fitness","of","\"",".","person","jin","a","subject","without","restriction","copyright","any","merchantability","software","kind","free","express","warranties","/","whom","©","shall","for","permission","as","warranty","furnished","an"," ","rights","the","but","do","in","other","following","charge","so","granted","license","publish","notice","conditions","limited","dealings","modify","sublicense","permit","out","liability","copies","to","be","purpose","included","otherwise","event","damages","yuchen","deal","particular","liable","not","action","this","associated","all","copy","persons","mit","tort","authors","claim","hereby","no"],"title":"License of this website (English version)","title_tokens":["license",")","("," ","website","of","english","version","this"]},{"location":"licenses/#_2","text":"下面介绍的诸多协议，原则上并无必要列在本条目中（例如MIT License）。列在下侧，主要是为了向诸位支持本文档的开发者致以真诚的谢意。亦将荣耀与在下的感谢致以高天之上的上帝，愿你保守这份文档，成为众人的帮助。","text_tokens":["license","，","帮助","愿","例如","。","成为","必要","你","支持","开发者","是","的","诸位","开发","真诚","上帝","保守","为了","本"," ","亦","下面","列","）","感谢","众人","中","原则上","文档","原则","列在","（","致以","荣耀","这份","向","在","谢意","之上","将","高天","诸多","mit","下","无","下侧","协议","与","主要","条目","介绍","并"],"title":"本站相关项目的协议","title_tokens":["相关","本站","的","项目","协议"]},{"location":"licenses/#license-of-material","text":"MIT License Copyright © 2016 - 2019 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":[")","including","portions","files","with","or","substantial","donath",":","obtaining","infringement","whether","arising","sell","limitation","connection","documentation","above","and","holders","distribute",",","(","2016","use","is","from","2019","implied","merge","provided","contract","fitness","of","\"",".","person","a","-","subject","without","restriction","copyright","any","merchantability","software","kind","free","express","warranties","/","whom","©","shall","for","permission","as","warranty","furnished","an"," ","rights","the","but","do","in","other","following","charge","so","granted","non","license","publish","notice","conditions","limited","dealings","modify","sublicense","permit","out","martin","liability","copies","to","be","purpose","included","otherwise","event","damages","deal","particular","liable","action","not","this","associated","all","copy","persons","mit","tort","authors","claim","hereby","no"],"title":"License of Material","title_tokens":["license","material","of"," "]},{"location":"licenses/#license-of-mkdocs","text":"BSD 2-Clause \"Simplified\" License Copyright © 2014, Tom Christie. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","text_tokens":["substitute","such",")","including","with","if","or","reproduce",":","whether","arising","modification","documentation","even","above","and","holders",",","(","strict","code","exemplary","use","form","is","services","redistribution","implied","tom","provided","caused","contract","fitness","of",".","\"","permitted","a","-","consequential","disclaimer","on","without","disclaimed","list","copyright","theory","possibility","simplified","any","distribution","materials","merchantability","retain","software","clause","loss","business","negligence","reserved","met","express","warranties","procurement","/","however","holder","©","shall","for","2","way","by","direct","interruption","as"," ","rights","that","the","but","advised","in","2014","other","following","data","bsd","goods","forms","special","license","indirect","must","conditions","notice","limited","out","damage","incidental","liability","contributors","christie","are","to","purpose","be","redistributions",";","source","otherwise","event","damages","profits","particular","liable","not","this","all","tort","binary","no"],"title":"License of MkDocs","title_tokens":["license","mkdocs","of"," "]},{"location":"licenses/#license-of-jieba3k","text":"The MIT License (MIT) Copyright © 2013 Sun Junyi Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":[")","including","portions","noninfringement","files","junyi","with","or","substantial",":","obtaining","whether","arising","sell","limitation","connection","documentation","above","and","holders","distribute",",","(","use","is","from","implied","merge","provided","contract","fitness","of","\"",".","person","a","subject","without","restriction","copyright","any","merchantability","software","kind","free","express","warranties","/","whom","©","shall","for","permission","as","warranty","furnished","an"," ","rights","the","but","do","2013","in","other","following","charge","so","granted","sun","license","publish","notice","conditions","limited","dealings","modify","sublicense","permit","out","liability","copies","to","be","purpose","included","otherwise","event","damages","deal","particular","liable","action","not","this","associated","all","copy","persons","mit","tort","authors","claim","hereby","no"],"title":"License of Jieba3K","title_tokens":["jieba3k","license","of"," "]},{"location":"licenses/#license-of-simple-lightbox","text":"The MIT License (MIT) Copyright © 2018 Damir Brekalo Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":[")","including","portions","noninfringement","brekalo","files","with","or","substantial",":","obtaining","whether","arising","sell","limitation","connection","documentation","above","and","holders","distribute",",","(","use","is","from","implied","merge","provided","contract","fitness","of","\"",".","person","a","damir","subject","without","restriction","copyright","any","merchantability","software","kind","free","express","warranties","/","whom","©","shall","for","permission","as","warranty","furnished","an"," ","rights","the","but","do","in","other","following","charge","2018","so","granted","license","publish","notice","conditions","limited","dealings","modify","sublicense","permit","out","liability","copies","to","be","purpose","included","otherwise","event","damages","deal","particular","liable","action","not","this","associated","all","copy","persons","mit","tort","authors","claim","hereby","no"],"title":"License of Simple Lightbox","title_tokens":["license","lightbox"," ","simple","of"]},{"location":"licenses/#license-of-mathjax","text":"Apache License 2.0 See the full license here: MathJax license","text_tokens":["license",":","2.0","see"," ","here","mathjax","full","apache","the"],"title":"License of MathJax","title_tokens":["license","of","mathjax"," "]},{"location":"licenses/#license-of-mermaid","text":"The MIT License (MIT) Copyright © 2014 - 2018 Knut Sveidqvist Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":[")","including","portions","noninfringement","files","with","or","substantial",":","obtaining","whether","arising","sell","limitation","connection","documentation","above","and","holders","distribute",",","(","use","is","from","implied","merge","provided","contract","fitness","of","\"",".","person","a","sveidqvist","-","subject","without","restriction","copyright","any","merchantability","software","kind","free","express","warranties","/","whom","©","shall","for","permission","as","warranty","furnished","an"," ","rights","the","but","do","in","2014","other","following","charge","2018","knut","so","granted","license","publish","notice","conditions","limited","dealings","modify","sublicense","permit","out","liability","copies","to","be","purpose","included","otherwise","event","damages","deal","particular","liable","action","not","this","associated","all","copy","persons","mit","tort","authors","claim","hereby","no"],"title":"License of mermaid","title_tokens":["license","of","mermaid"," "]},{"location":"release-notes/","text":"更新记录 ¶ 大版本更新 ¶ 在此记录本文档的主要更新，读者可以在此确认经过更新后，本文档添加了哪些主要的内容、话题。 0.1 @ February 25, 2019 ¶ 正式立项，开始本文档的撰写工作。目前尚处于未完成状态，计划包括： Tensorflow 1.12 入门教程 从线性问题入门 40% 训练与测试分立 0% 原生的数据管理 0% 其他的高级技巧 0% Tensorflow 2.0 入门教程 0% Tensorflow 第三方扩展库 入门教程 0% 局部更新记录 ¶ 0.17 @ March 5, 2019 ¶ 完成“从线性问题入门”专题下的 线性分类 ； 修正前文的一些笔误。 0.15 @ March 4, 2019 ¶ 完成“从线性问题入门”专题下的 本章总说 和 Hello world! ； 微调图片链接， MathJax 的显示特性。 0.12 @ March 3, 2019 ¶ 补完扉页。未来可能会调整 教程导读 ； 修复 Arithmatex 对 MathJax 的引入； 引入 mermaid 库，用于绘制示意图。 0.11 @ February 25, 2019 ¶ 提交 Google Analytics 和 Google Search Console 的账户关联资料； 提交 Disqus 关联资料。 0.10 @ February 25, 2019 ¶ 正式立项，并撰写扉页的一部分。","text_tokens":["补完","线性","技巧","高级","分立","状态","了","一部","未","3","经过","!","mermaid","其他","局部","修复","扩展","“","立项","处于","记录","¶",",","0.10","未来","总说","与","开始","链接","库","专题","2019","并","引入","特性","部分","扉页","。","图片","@","analytics","修正","的","、","40","账户","0.1","文档","哪些","0.15","微调","可以","：","february","大","数据","在","调整","原生","问题","示意","0.11","显示","话题","%","包括","读者","关联","图片链接","google","25","；","从","search","正式","一些","确认","记录本","march"," ","意图","0.17","添加","内容","训练","入门教程","一部分","会","撰写","尚","对","后","绘制","mathjax","主要","hello","world","入门","，","计划","console","教程","目前","管理","前文","4","版本","本章","tensorflow","导读","可能","数据管理","完成","本","arithmatex","分类","”","资料","disqus","第三","示意图","工作","更新","第三方","5","和","此","2.0","测试","下","1.12","笔误","提交","0.12","用于","0","三方"],"title":"更新记录","title_tokens":["记录","更新"]},{"location":"release-notes/#_1","text":"","text_tokens":[],"title":"更新记录","title_tokens":["记录","更新"]},{"location":"release-notes/#_2","text":"在此记录本文档的主要更新，读者可以在此确认经过更新后，本文档添加了哪些主要的内容、话题。","text_tokens":["，","。","了","确认","记录本","的","、","本","经过","添加","内容","文档","哪些","可以","更新","在","后","记录","此","话题","主要","读者"],"title":"大版本更新","title_tokens":["版本","更新","大"]},{"location":"release-notes/#01-february-25-2019","text":"正式立项，开始本文档的撰写工作。目前尚处于未完成状态，计划包括： Tensorflow 1.12 入门教程 从线性问题入门 40% 训练与测试分立 0% 原生的数据管理 0% 其他的高级技巧 0% Tensorflow 2.0 入门教程 0% Tensorflow 第三方扩展库 入门教程 0%","text_tokens":["入门","，","计划","教程","线性","技巧","目前","管理","从","高级","。","分立","正式","状态","tensorflow","的","未","数据管理","完成","本"," ","40","训练","其他","文档","第三","入门教程","：","工作","扩展","撰写","尚","数据","处于","立项","第三方","原生","问题","0","测试","2.0","1.12","与","%","开始","库","包括","三方"],"title":"0.1 @ February 25, 2019","title_tokens":["25",",","february"," ","2019","0.1","@"]},{"location":"release-notes/#_3","text":"","text_tokens":[],"title":"局部更新记录","title_tokens":["更新","记录","局部"]},{"location":"release-notes/#017-march-5-2019","text":"完成“从线性问题入门”专题下的 线性分类 ； 修正前文的一些笔误。","text_tokens":["入门","修正","的","线性","问题","；","前文","下","完成","从"," ","笔误","一些","。","分类","“","”","专题"],"title":"0.17 @ March 5, 2019","title_tokens":[",","march"," ","2019","0.17","5","@"]},{"location":"release-notes/#015-march-4-2019","text":"完成“从线性问题入门”专题下的 本章总说 和 Hello world! ； 微调图片链接， MathJax 的显示特性。","text_tokens":["入门","图片链接","，","线性","；","特性","从","。","本章","图片","的","完成"," ","!","”","微调","“","和","问题","下","显示","总说","mathjax","hello","world","链接","专题"],"title":"0.15 @ March 4, 2019","title_tokens":["0.15","march",",","4"," ","2019","@"]},{"location":"release-notes/#012-march-3-2019","text":"补完扉页。未来可能会调整 教程导读 ； 修复 Arithmatex 对 MathJax 的引入； 引入 mermaid 库，用于绘制示意图。","text_tokens":["补完","，","引入","教程","；","扉页","。","导读","的","可能"," ","mermaid","意图","arithmatex","示意图","修复","会","对","调整","绘制","示意","未来","mathjax","用于","库"],"title":"0.12 @ March 3, 2019","title_tokens":[",","march","3"," ","2019","0.12","@"]},{"location":"release-notes/#011-february-25-2019","text":"提交 Google Analytics 和 Google Search Console 的账户关联资料； 提交 Disqus 关联资料。","text_tokens":["关联","disqus","google","console","的","账户","；","提交"," ","。","search","和","资料","analytics"],"title":"0.11 @ February 25, 2019","title_tokens":["25",",","0.11","february"," ","2019","@"]},{"location":"release-notes/#010-february-25-2019","text":"正式立项，并撰写扉页的一部分。","text_tokens":["，","的","部分","一部分","扉页","撰写","。","正式","立项","一部","并"],"title":"0.10 @ February 25, 2019","title_tokens":["25",",","0.10","february"," ","2019","@"]},{"location":"book-1-x/chapter-1/","text":"从线性问题入门 ¶ 摘要 本章将从线性问题入手，讨论最简单的分类与回归问题。我们将随机生成指定分布的数据，通过理论值，检查实验效果。这些简单的、解析的问题，可以提供读者一个直观的感受，并帮助读者快速上手Tensorflow的基本概念与运用。本章中，我们不会涉及任何数据存取、跟踪测度方面的概念，也不会将训练和测试分开，旨在使读者能集中心思到代码规范和Tensorflow的基本功能上。 漫谈线性问题 ¶ 在机器学习领域，线性问题既简单也不简单。目前神经网络主要是指多层、非凸的网络结构，常常用来解复杂的、难以推导的问题。但这并不意味着线性问题容易解决。相反，在很多情况下，线性问题是解不唯一的(undetermined)，解不稳定的(ill-posed/ill-conditioned)，条件的(constrained)。同时，为了得到一个快速收敛的、高质量的解，即使对线性问题，人们也在不断提出、改进解法。例如ISTA, AMP, PGD, LISTA, vAMP等算法，都用来解线性问题。在此不作详细展开。 我们已经知道，一个线性函数具有可加性，和一次齐次性，亦即 \\begin{align} f(x_1 + x_2) &= f(x_1) + f(x_2), \\\\ f(\\alpha x) &= \\alpha f(x). \\end{align} 因此，求解一个线性问题，我们需要将问题纯粹以 线性函数 进行描述。例如， \\begin{align} \\mathbf{y} \\sim \\mathbf{A}\\mathbf{x}. \\end{align} 具体而言， \\mathbf{x} \\mathbf{x} 是我们的已知数据， \\mathbf{y} \\mathbf{y} 是我们的未知量，我们需要找到一个合适的 \\mathbf{A} \\mathbf{A} 来确保 \\mathbf{x} \\mathbf{x} 能拟合到 \\mathbf{y} \\mathbf{y} 。如果此处 \\mathbf{y} \\in \\{0,~1\\}^p \\mathbf{y} \\in \\{0,~1\\}^p 是一个代表p类-分类的向量，那么这就是一个线性分类问题；相反，如果此处 \\mathbf{y} \\in \\mathbb{R}^p \\mathbf{y} \\in \\mathbb{R}^p 在连续p维空间取值，那么这就是一个线性回归问题。 线性问题与凸问题 ¶ 请注意，虽然我们在此处提到“ 线性问题 (Linear problem) ”，但我们指的并非“ 线性规划 (Linear programming) ”。虽然严格意义上，线性规划才是真正的线性问题，但我们在此处尚不讨论线性规划相关的内容，而是着眼于机器学习应用最普遍的两个领域， 分类 和 回归 上。实际上，这两种问题虽然求解的是线性函数，但本质上是凸问题。 例如，如果我们要求解回归问题，通常可以表述成 \\begin{equation} \\begin{aligned} \\arg \\min_{\\mathbf{A}}~& \\sum_{i=1}^N \\mathcal{L}(\\mathbf{A},~\\mathbf{x}_i,~\\mathbf{y}_i), \\\\ \\mathcal{L}(\\mathbf{A},~\\mathbf{x},~\\mathbf{y}) &= \\lVert \\mathbf{y} - \\mathbf{A}\\mathbf{x} \\rVert^2_2. \\end{aligned} \\end{equation} 虽然我们求解的模型 \\mathbf{A} \\mathbf{A} 是线性的，但我们优化的函数对象 \\mathcal{L} \\mathcal{L} 是一个 凸函数 (convex function) 。在此，我们可以将这个标量函数(同时也是凸函数)表述为 \\begin{align} \\forall~\\alpha,~\\beta,~\\mathbf{x}_1,~\\mathbf{x}_2,~\\mathcal{L}(\\alpha\\mathbf{x}_1 + \\beta\\mathbf{x}_2) \\leqslant \\alpha\\mathcal{L}(\\mathbf{x}_1) + \\beta\\mathcal{L}(\\mathbf{x}_2). \\end{align} 我们将这个问题表述为 最小二乘问题(Least-square problem) 。正是由于 \\mathbf{A} \\mathbf{A} 是线性的， \\mathcal{L} \\mathcal{L} 才能被确保为一个凸函数，进而，我们才能确保上述问题能得到精确的全局最小值解。 知悉Tensorflow ¶ 在本章接下来的内容里，我们将探讨Tensorflow如何求解一些简单的问题。对于初次上手的读者而言，在安装Tensorflow，亲自开始写一些project之前，笔者推荐你到这个游乐场“抢鲜体验”一番， Tensorflow Playground 在这个在线页面里，用户不需要有任何编程知识，可以通过直观的操作界面，建立一个简单的 多层感知机网络(有时也叫做Artificial Neural Network, ANN) ，并且实时观测网络的性能和测度。它提供了几个简单的二维数据集，供用户体验不同的数据集下，各种分类、回归问题的合宜解法。如果读者已经对神经网络的基本原理有所了解，相信能通过这个小小的实验场达到心有灵犀。我们在本章所做的project和demo，大体不跳出这个试验场的范畴，只是会略微复杂一点而已。 本章要点 ¶ 下图展示了通过本章学习，能了解到的概念： graph LR st(Hello world!) --> linclas(线性分类) linclas --> linreg(线性回归) linreg --> nonlinreg(非线性回归) nonlinreg --> ed(非线性分类) lp[感知机] --> linclas sigma[Logsitc回归] --> linclas ce[交叉熵] --> linclas opt[优化器] --> linreg argpar[项目选项] --> linreg para[参数回归] --> nonlinreg kernel[核函数] --> ed classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,linclas,linreg,nonlinreg,ed styStart Hello world : 首先，在本节，读者将了解Tensorflow的安装方法，并编写通过第一个简单的Tensorflow程序。 线性分类 : 本节通过解一个简单的二分类问题，我们将引入单层感知机、Logistic回归和交叉熵的概念，并且实验结果进行可视化。 线性回归 : 本节通过解一个与上一节难度相仿的，简单的回归问题，另读者能对比不同优化器的性能、特点，并介绍如何在项目中进行arugument parsing(引入项目的可选项)。 非线性回归 : 本节将修改上一节回归问题的激活函数，将线性的回归问题推广到解析函数参数回归的范畴。 非线性分类 : 本节将通过上一节的参数回归，引入核函数的概念，将线性分类问题推广到非线性空间里。","text_tokens":["i","leqslant","线性","找到","二","求解","了","齐次",":","机器","几个","方面","小小","posed","知机","对比",">","r","感知","用户","不断","¶","到","代表","sim","与","可选项","介绍","引入","+","有所","理论值","成","为了","编写","即","着眼","linreg","应用","拟合","神经网","推广","ill","nonlinreg","参数","程序","将","也","问题","分布","读者","\\","最小值","；","^","如何","展示","两个","实验场","rvert","为","通过","很多","对于","in","功能","alpha","规划","logsitc","解法","合适","lista","graph","，","playground","指","普遍","]","可视","stroke","上述","kernel","class","conditioned","sigma","分类","实际上","undetermined","意义","实时","集中","多层","心有灵犀","取值","意味","标量","直观","优化","[","难度","帮助","初次","交叉","试验","使","下来","才","体验","二维","最","入手","漫谈","y","游乐场","常常","概念","本质","一点","质量","向量","相仿","para","一个","感受","知道","{","align",".","难以","但","而已","略微","、","上手","aligned","涉及","而言","#","ista","p","mathcal","基本功能","就是","equation","分开","复杂","未知量","里","跟踪","唯一","展开","convex","opt","不","begin","运用","你","&","square","ba9132","一次","神经","st","合宜","内容","全局","尚","已经","相信","有时","改进","world","领域","虽然","入门","mathbf","推荐","因此","注意","基本功","目前","demo","ce","本章","既","tensorflow","线性规划","本","亦","”","空间","理论","本节","基本概念","推导","存取","集","建立","实际","和","凸","随机","稳定","测试","范畴","维空间","操作界面","amp","那么","方法","小小的","规范","知悉","人们","网络结构","收敛","严格","高质量","简单","用来","mathbb","接下来","!","进而","只是","f","“","能","意味着","可选","函数参数","它","(","做","要点","stystart","如果","开始","抢鲜","并","网络","linear","具体","来","项目","检查","不会","非线性","fae6a9","详细","的","classdef","-","artificial","结果","确保","高质","可以","：","这些","表述","linclas","连续","由于","界面","安装","真正","容易","探讨","亲自","提到","/","sum","观测","从","需要","1","ed","要求","解决","未知","一节","得到","类"," ","=","并非","快速","页面","核","中","具有","不作","知识","上","跳出","arg","写","对","x","function","性能","我们","条件","节","激活","函数","通常","hello","对象","不同","least","着眼于","解","forall","有","_","精确","解不","相关","network","各种","回归","神经网络",";","达到","非凸","基本原理","可加性","logistic","操作","结构","大体","等","相反","算法","parsing","argpar","下","ann","programming","编程","0",")","一番","而是","lp","单层","才能","此处","lr","感知机","这个","end","测度","l","之前","首先","可加",",","凸函数","性","实验","neural","同时","提供","效果","正是","被","讨论","。","指定","都","lvert","a","摘要","心思","所","并且","本原","以","在线","constrained","集下","pgd","下图","~","problem","第一个","数据","模型","在","提出","两种","描述","fill","学习","解析","笔者","例如","原理","请","2","一些","可视化","生成","灵犀","选项","训练","这","特点","游乐","n","会","代码","器","修改","试验场","供","基本","min","最小","主要","熵","任何","即使","了解","是","beta","二乘","进行","叫做","另","已知","第一","}","project","arugument","vamp","情况","旨在","此","接下","纯粹"],"title":"本章总说","title_tokens":["总说","本章"]},{"location":"book-1-x/chapter-1/#_1","text":"摘要 本章将从线性问题入手，讨论最简单的分类与回归问题。我们将随机生成指定分布的数据，通过理论值，检查实验效果。这些简单的、解析的问题，可以提供读者一个直观的感受，并帮助读者快速上手Tensorflow的基本概念与运用。本章中，我们不会涉及任何数据存取、跟踪测度方面的概念，也不会将训练和测试分开，旨在使读者能集中心思到代码规范和Tensorflow的基本功能上。","text_tokens":["，","读者","帮助","线性","任何","规范","基本功","从","感受","讨论","。","本章","检查","指定","运用","tensorflow","不会","理论值","的","生成","、","上手","摘要","使","心思","回归","方面"," ","简单","通过","分类","快速","中","训练","基本功能","理论","涉及","可以","功能","上","最","这些","一个","基本概念","集中","跟踪","入手","数据","存取","代码","测度","能","并","基本","和","将","也","随机","旨在","问题","我们","测试","概念","到","直观","实验","与","分开","提供","效果","分布","解析"],"title":"从线性问题入门","title_tokens":["问题","入门","从","线性"]},{"location":"book-1-x/chapter-1/#_2","text":"在机器学习领域，线性问题既简单也不简单。目前神经网络主要是指多层、非凸的网络结构，常常用来解复杂的、难以推导的问题。但这并不意味着线性问题容易解决。相反，在很多情况下，线性问题是解不唯一的(undetermined)，解不稳定的(ill-posed/ill-conditioned)，条件的(constrained)。同时，为了得到一个快速收敛的、高质量的解，即使对线性问题，人们也在不断提出、改进解法。例如ISTA, AMP, PGD, LISTA, vAMP等算法，都用来解线性问题。在此不作详细展开。 我们已经知道，一个线性函数具有可加性，和一次齐次性，亦即 \\begin{align} f(x_1 + x_2) &= f(x_1) + f(x_2), \\\\ f(\\alpha x) &= \\alpha f(x). \\end{align} 因此，求解一个线性问题，我们需要将问题纯粹以 线性函数 进行描述。例如， \\begin{align} \\mathbf{y} \\sim \\mathbf{A}\\mathbf{x}. \\end{align} 具体而言， \\mathbf{x} \\mathbf{x} 是我们的已知数据， \\mathbf{y} \\mathbf{y} 是我们的未知量，我们需要找到一个合适的 \\mathbf{A} \\mathbf{A} 来确保 \\mathbf{x} \\mathbf{x} 能拟合到 \\mathbf{y} \\mathbf{y} 。如果此处 \\mathbf{y} \\in \\{0,~1\\}^p \\mathbf{y} \\in \\{0,~1\\}^p 是一个代表p类-分类的向量，那么这就是一个线性分类问题；相反，如果此处 \\mathbf{y} \\in \\mathbb{R}^p \\mathbf{y} \\in \\mathbb{R}^p 在连续p维空间取值，那么这就是一个线性回归问题。","text_tokens":[")","线性","找到","人们","网络结构","收敛","求解","齐次","机器","高质量","此处","简单","用来","mathbb","posed","r","end","f","能","y","意味着","不断","可加","到","常常",",","sim","代表","(","性","质量","向量","如果","同时","一个","网络","并","具体","来","知道","{","+","。","align",".","难以","都","但","详细","a","、","的","为了","-","即","以","确保","高质","constrained","拟合","神经网","pgd","~","ill","而言","数据","ista","p","在","连续","提出","也","将","问题","描述","就是","容易","学习","/","复杂","未知量","唯一","展开","\\","；","^","例如","不","begin","1","需要","2","&","解决","未知","一次","得到","类"," ","=","快速","神经","很多","这","具有","in","不作","alpha","对","x","已经","我们","条件","解法","改进","lista","合适","函数","主要","领域","，","因此","mathbf","指","目前","解","即使","_","既","解不","是","回归","神经网络","亦","conditioned","进行","已知","分类","}","空间","非凸","可加性","undetermined","vamp","多层","结构","推导","等","相反","情况","算法","取值","意味","和","此","稳定","维空间","下","amp","纯粹","那么","0"],"title":"漫谈线性问题","title_tokens":["漫谈","问题","线性"]},{"location":"book-1-x/chapter-1/#_3","text":"请注意，虽然我们在此处提到“ 线性问题 (Linear problem) ”，但我们指的并非“ 线性规划 (Linear programming) ”。虽然严格意义上，线性规划才是真正的线性问题，但我们在此处尚不讨论线性规划相关的内容，而是着眼于机器学习应用最普遍的两个领域， 分类 和 回归 上。实际上，这两种问题虽然求解的是线性函数，但本质上是凸问题。 例如，如果我们要求解回归问题，通常可以表述成 \\begin{equation} \\begin{aligned} \\arg \\min_{\\mathbf{A}}~& \\sum_{i=1}^N \\mathcal{L}(\\mathbf{A},~\\mathbf{x}_i,~\\mathbf{y}_i), \\\\ \\mathcal{L}(\\mathbf{A},~\\mathbf{x},~\\mathbf{y}) &= \\lVert \\mathbf{y} - \\mathbf{A}\\mathbf{x} \\rVert^2_2. \\end{aligned} \\end{equation} 虽然我们求解的模型 \\mathbf{A} \\mathbf{A} 是线性的，但我们优化的函数对象 \\mathcal{L} \\mathcal{L} 是一个 凸函数 (convex function) 。在此，我们可以将这个标量函数(同时也是凸函数)表述为 \\begin{align} \\forall~\\alpha,~\\beta,~\\mathbf{x}_1,~\\mathbf{x}_2,~\\mathcal{L}(\\alpha\\mathbf{x}_1 + \\beta\\mathbf{x}_2) \\leqslant \\alpha\\mathcal{L}(\\mathbf{x}_1) + \\beta\\mathcal{L}(\\mathbf{x}_2). \\end{align} 我们将这个问题表述为 最小二乘问题(Least-square problem) 。正是由于 \\mathbf{A} \\mathbf{A} 是线性的， \\mathcal{L} \\mathcal{L} 才能被确保为一个凸函数，进而，我们才能确保上述问题能得到精确的全局最小值解。","text_tokens":["i",")","leqslant","线性","求解","而是","机器","才能","严格","此处","才","进而","这个","最","end","“","l","y","能",",","本质","(","凸函数","如果","同时","linear","一个","正是","被","+","{","讨论","。","align",".","但","lvert","a","的","成","-","aligned","着眼","应用","确保","可以","~","problem","表述","mathcal","模型","在","由于","将","也","问题","真正","两种","学习","提到","equation","sum","\\","最小值","^","例如","convex","不","两个","请","begin","1","2","要求","&","square","rvert","为","得到"," ","=","并非","内容","这","全局","n","上","尚","arg","alpha","x","function","规划","我们","min","最小","函数","通常","领域","对象","虽然","least","，","注意","着眼于","mathbf","指","普遍","解","forall","_","精确","上述","相关","是","线性规划","beta","回归","二乘","分类","”","}","实际上","意义","实际","和","此","凸","标量","优化","programming"],"title":"线性问题与凸问题","title_tokens":["问题","凸","与","线性"]},{"location":"book-1-x/chapter-1/#tensorflow","text":"在本章接下来的内容里，我们将探讨Tensorflow如何求解一些简单的问题。对于初次上手的读者而言，在安装Tensorflow，亲自开始写一些project之前，笔者推荐你到这个游乐场“抢鲜体验”一番， Tensorflow Playground 在这个在线页面里，用户不需要有任何编程知识，可以通过直观的操作界面，建立一个简单的 多层感知机网络(有时也叫做Artificial Neural Network, ANN) ，并且实时观测网络的性能和测度。它提供了几个简单的二维数据集，供用户体验不同的数据集下，各种分类、回归问题的合宜解法。如果读者已经对神经网络的基本原理有所了解，相信能通过这个小小的实验场达到心有灵犀。我们在本章所做的project和demo，大体不跳出这个试验场的范畴，只是会略微复杂一点而已。","text_tokens":[")","小小的","初次","一番","求解","了","试验","几个","下来","接下来","简单","小小","知机","体验","二维","感知机","感知","这个","用户","只是","“","之前","测度","能","游乐场","到",",","一点","它","(","做","实验","neural","如果","抢鲜","开始","提供","一个","网络","。","而已","有所","略微","的","、","上手","所","并且","本原","artificial","在线","神经网","集下","可以","而言","数据","在","界面","将","也","安装","问题","探讨","亲自","观测","复杂","里","读者","笔者","如何","不","原理","需要","实验场","你","一些","灵犀"," ","通过","页面","神经","对于","合宜","内容","游乐","知识","会","跳出","写","对","试验场","已经","供","相信","基本","性能","我们","解法","有时","不同","，","推荐","playground","任何","demo","有","本章","tensorflow","了解","network","各种","回归","神经网络","叫做","分类","”","达到","project","基本原理","实时","操作","多层","大体","集","心有灵犀","建立","和","接下","范畴","ann","直观","操作界面","编程"],"title":"知悉Tensorflow","title_tokens":["tensorflow","知悉"]},{"location":"book-1-x/chapter-1/#_4","text":"下图展示了通过本章学习，能了解到的概念： graph LR st(Hello world!) --> linclas(线性分类) linclas --> linreg(线性回归) linreg --> nonlinreg(非线性回归) nonlinreg --> ed(非线性分类) lp[感知机] --> linclas sigma[Logsitc回归] --> linclas ce[交叉熵] --> linclas opt[优化器] --> linreg argpar[项目选项] --> linreg para[参数回归] --> nonlinreg kernel[核函数] --> ed classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,linclas,linreg,nonlinreg,ed styStart Hello world : 首先，在本节，读者将了解Tensorflow的安装方法，并编写通过第一个简单的Tensorflow程序。 线性分类 : 本节通过解一个简单的二分类问题，我们将引入单层感知机、Logistic回归和交叉熵的概念，并且实验结果进行可视化。 线性回归 : 本节通过解一个与上一节难度相仿的，简单的回归问题，另读者能对比不同优化器的性能、特点，并介绍如何在项目中进行arugument parsing(引入项目的可选项)。 非线性回归 : 本节将修改上一节回归问题的激活函数，将线性的回归问题推广到解析函数参数回归的范畴。 非线性分类 : 本节将通过上一节的参数回归，引入核函数的概念，将线性分类问题推广到非线性空间里。","text_tokens":["方法",")","线性","二","交叉","了","lp",":","单层","!","简单","知机","对比",">","lr","感知机","感知","能","首先","可选","到",",","概念","函数参数","(","实验","stystart","与","相仿","para","可选项","介绍","一个","并","引入","项目","。","fae6a9","非线性","的","、","classdef","-","编写","并且","linreg","结果","推广","下图","：","#","第一个","linclas","nonlinreg","在","参数","程序","将","安装","问题","fill","学习","里","读者","解析","opt","如何","展示","ed","可视化","ba9132","一节"," ","通过","选项","核","st","中","特点","上","器","修改","logsitc","性能","我们","节","激活","函数","graph","hello","world","不同","熵","，","ce","解","]","可视","stroke","本章","tensorflow","了解","kernel","回归","class","本","进行","sigma","另","分类",";","第一","空间","本节","arugument","logistic","parsing","和","argpar","范畴","优化","[","难度"],"title":"本章要点","title_tokens":["要点","本章"]},{"location":"book-1-x/chapter-1/hello-world/","text":"Hello world! ¶ 摘要 本节包含一个简易的安装指导，主要提供给Windows上的GPU用户。之后，用户可以按照本节指导编写第一个测试程序。 安装Tensorflow ¶ 本节针对Windows使用GPU的用户，提供一个安装最新版Tensorflow的方法。大致的步骤符合以下两个教程： 通过pip安装Tensorflow Tensorflow的GPU支持 但是，需要指出的是，官方教程里有些部分已经不符合当前用户的实际情况，笔者经过摸索，总结以下的安装过程。 总体而言，安装Tensorflow需要用户确保以下条件 NVIDIA驱动已经达到411.31及以上 CUDA 10.0 (10.1不确定能否正常使用) CUDNN已经达到7.3.1及以上 CUPTI版本与CUDA相配（不需要专门安装，在安装CUDA的同时已经自动集成） 除此之外，涉及多GPU支持的库(NCCL和TensorRT)，这里不考虑。因此，我们主要需要确保前三条正常安装。 提示 Tensorflow官方提供的安装包是基于CUDA 9.0，CUDNN >= 7.2的基础上编译的。然而现在NVIDIA已经不再提供CUDA 9.0了。Windows用户可以安装的版本包括过时的8.1和最新的10.1。在这种情况下，官方还提供一个令用户自行从源码编译Tensorflow库的方案，但是对于Windows用户而言过于繁琐，容易出错，这里不建议按照官方教程。 更新NVIDIA驱动 ¶ 首先，我们可以开始更新我们的NVIDIA驱动。任何载有NVIDIA-GPU显卡的电脑，都应当在系统里已经预装好了 GeForce Experience 。点击如下图标开启GeForce Experience（如果是低版本的GeForce Experience，图标可能会不同）， 如左图，在主界面上，可以观察到，当驱动版本不够高时，会自动出现更新提示，依照提示更新即可。建议在更新驱动时，关闭其他所有程序，以免造成干扰。 更新NVIDIA driver 检查更新结果 如果更新完成，如右图，可以在设置界面检查到当前的版本已经达到最新。在安装过程中，由于驱动更新，屏幕可能会闪烁数次。在看到更新提示完成后建议重启。 提示 如果GeForce Experience提示无法更新驱动，可以考虑将机器恢复出厂设置，可能可以解决这一问题。 安装CUDA ¶ 驱动更新并重启后，需要安装最新版的Visual Studio(VS)，这是因为CUDA库本身对VS具有一定的支持，如果先安装CUDA，则这部分支持无法安装到位。这对我们使用Tensorflow并不造成妨害，但是对CUDA的用户并不是一个推荐的做法，所以无论如何，建议在第一步，安装VS，下面提供VS社区版（免费）的链接： Visual Studio官网 接下来，可以开始安装CUDA。进入CUDA安装包的下载链接 下载最新版CUDA CUDA官方指南 提示 须知，并非最新版就是最适合的版本。实际能使用的CUDA版本取决于后续步骤中预编译包的支持版本。就笔者写到这里时，最新的CUDA 10.1已然不匹配最新版预编译包支持的CUDA 10.0，由于版本差距很小，不确定是否会由此引发问题，但读者可以自行尝试。 现在的安装包已经支持在线安装包和离线安装包两种模式，读者可以根据自己的实际情况选择对应的版本。注意，安装CUDA的时候，CUDA可能要求我们翻新驱动版本，但通过GeForce Experience安装的驱动一般应已达到最新，新于CUDA内部提供的驱动。因此，我们需要选择定制安装模式，根据我们需要对比版本号，将驱动更新的选项取消，如下图所示 安装CUDA后，建议重启。 危险 特别需要注意的是，如果你的机器无法通过GeForce Experience更新驱动，那么在这里选择安装驱动也是枉然。并且，这种强制更新驱动的行为很可能导致系统崩溃。因此特别建议在安装CUDA之前一定要解决驱动更新的问题，哪怕是通过恢复出厂设置。 安装CUDNN ¶ 安装CUDA的同时，已经自带安装上了CUPTI。但是CUDNN并没有包含在CUDA中，因此，我们需要到以下链接下载CUDNN并查看安装说明 下载最新版CUDNN CUDNN官方指南 不同的是，CUDNN没有安装包，是以压缩包的形式下载到本地。解压后，其中应当包含如下目录及文件 . └─ cuda/ ├─ bin/ # Binary library │ └─ cudnn64_7.dll ├─ include/ # C++ Include file │ └─ cudnn.h ├─ lib/ # C++ Lib file │ └─ x64/ │ └─ cudnn.lib └─ NVIDIA_SLA_cuDNN_Support.txt # Readme file 假设我们CUDA的安装目录在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，将上述解压的 bin , include 和 lib 三个文件夹，直接拷贝覆盖到该安装目录下，即可完成CUDNN的安装。 进行完上述步骤后，我们还需要确保几个环境变量正确设置： 存在 CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，是我们的安装目录。理论上在CUDA安装后，该目录已经自动设置好。 环境变量 PATH 中，存在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin , C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/extras/CUPTI/libx64 两个路径，由于CUDNN已经配置在CUDA安装目录下，我们不需要像官方文档那样，添加第三个CUDNN的安装路径。 安装Anaconda ¶ Windows用户建议使用Anaconda管理python环境。作为一个开源的项目，Anaconda发行版已经集成了我们所需要的大多数python包，其中有些包是我们自己难以安装上的，例如支持python3的PIL。我们选用python3版的Anaconda，下载路径如下： Anaconda 最新版下载链接 注意选择x64版的安装包。安装结束后，我们已经有python3.7的环境了。鉴于有些情况下我们可能需要使用更早的python版本，接下来我们可以安装python3.6的虚环境。 Anaconda可以安装在用户或系统目录下。注意，如果安装在系统目录下，如果不在虚环境下，安装任何包都需要管理员模式。不过这不影响本教程的示例，因为我们将安装虚环境。直接打开 Anaconda Prompt ，并键入以下命令： conda create -n py36 python = 3 .6 anaconda 按照引导流程安装，经过一段时间后，我们将得到python3.6版的Anaconda。以后我们可以直接从开始菜单 Anaconda Prompt (py36) 进入该虚环境，也可以在基环境中键入 activate py36 转换到虚环境中。 提示 如果用户愿意，还可以通过 conda install -c conda-forge opencv 安装 opencv 。opencv是一个强大的图像处理库，这里python版的opencv比C++版更容易上手。一般该命令安装的是conda库中最新版的opencv（截至笔者写到这里时是opencv3.4.4）。然而，这样安装的并非是支持GPU的版本。尽管conda还提供一个GPU的opencv版本，可以通过 conda install -c patricksnape opencv-gpu 安装 opencv2 ，但它的版本还停留在2.4.9，和opencv3的用法殊有不同，请读者注意这一点。 安装预编译好的Tensorflow ¶ 我们可以查看如下项目： fo40225/tensorflow-windows-wheel 该项目的维护者在不断根据Tensorflow的更新，预编译出适合不同版本的Tensorflow安装包。截至笔者写到这里为止，最新支持到r1.12版。我们选择对应的GPU版Tensorflow，并在虚环境下执行以下命令： pip install https://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.12.0/py36/GPU/cuda100cudnn73sse2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl 等待一段时间后，安装将会结束。 Hello world! 测试 ¶ 撰写如下代码，保存到 hello-world.py 文件， Python 1 2 3 4 5 6 7 8 9 if __name__ == '__main__' : import tensorflow as tf test_str = tf . constant ( 'Hello, world!' ) test_cal = tf . reduce_sum ( tf . random_normal ([ 1000 , 1000 ])) with tf . Session () as sess : print ( 'Current Tensorflow version is:' , tf . __version__ ) print ( 'Test string: ' , sess . run ( test_str )) print ( 'Test calculation:' , sess . run ( test_cal )) Output Current Tensorflow version is: 1 .12.0 Test string: b 'Hello, world!' Test calculation: -1948.6578 在保存目录下，执行 python hello-world.py 第一次运行可能需要等待较长的初始化时间（硬件配置）。如果能正常运行，我们将看到如下结果： 其中，显示结果之前的记录信息反映了GPU已经可以正常工作。而“Test calculation”显示的结果是 10^6 10^6 个服从 \\mathcal{N}(0,1) \\mathcal{N}(0,1) 分布的随机数之和。根据相互独立的随机正态分布互不相关的特性，我们可以推算出，该结果服从 \\mathcal{N}(0,10^3) \\mathcal{N}(0,10^3) 的正态分布。","text_tokens":["强制","之","干扰","print","尝试","停留","c++","了","readme","conda",":","机器","几个","应当","对比",">","x64","令","用户","正常","引发","给","信息","互不","不断","9","¶","到","导致","示例","独立","win","与","用法","第一步","要","枉然","鉴于","10.1","定制","点击","support","这样","取决","cuda","编写","除此之外","处理","版","但是","崩溃","程序","也","将","不是","件夹","问题","python","完","到位","不再","那样","version","时间","分布","读者","2.4","\\","新于","官网","^","12.0","很","如何","install","两个","进入","符合","program","current","b","通过","对于","包都","反映","添加","手","取决于","后续","（","总体","算出","及","社区","opencv3","专门","多数","数次","取消","图像处理","该项","，","extras","以上","]","版本","studio","上述","geforce","可能","或","一定","导致系统","命令","str","硬件","方案","运行","基础","目的","按照","当前","显卡","experience","patricksnape","[","菜单","有些","低版本","with","fo40225","包是","blob","下来","─","源码","指出","流程","最新版","其他","最","像","normal","而","记录","匹配","r1","一点","cal","anaconda","适合","总体而言","很小","重启","is","一个","正确","cp36","name","下载","执行","{","闪烁",".","难以","设置","但","键入","还","简易","预","恢复","路径","个","涉及","cudnn","文档","关闭","c","prompt","即可","10","而言","#","打开","这一","应已","python3","mathcal","8.1","本身","开源","opencv2","file","就是","random","里","包括","第一次","步骤","无论","自带","不","因为","保存","constant","官方","你","环境变","opencv","须知","一次","三个","py","第三个","作为","出现","include","集成","针对","1000","主","calculation","已经","维护","无法","411.31","强大","早","服从","world","8","虚","本地","配置","注意","因此","推荐","管理","driver","做法","tensorflow","确定","cudnn64","当","tf","本","依照","之外","”","形式","理论","tensorrt","已然","第三","create","工作","前","总结","所示","实际","和","随机","离线","说明","测试","出","三条","该","自己","那么","环境变量","方法","发行","多","if","翻新","3","!","经过","接下来","一段","转换","基于","7.2","引导","一步","4.4","危险","path","内部","影响","“","能","vs","正态分布","nvidia","安装包","对应","它","(","预装","选用","如果","开始","链接","库","以免","并","dll","更","项目","检查","系统目录","比","时","com","的","amd64","好","-","下面","6","由此","master","结果","确保","时候","lib","如下","可以","12","：","推算","由于","界面","造成","安装","libx64","繁琐","容易","activate","h","/","sum","假设","基","低版","不够","从","需要","1","观察","要求","系统","是因为","解决","forge","测试程序","py36","开始菜单","得到"," ","=","└","并非","库中","相配","中","之后","具有","出错","指导","载有","过时","上","新版","环境","visual","撰写","行为","写","对","我们","条件","维护者","节","差距","gpu","hello","建议","电脑","一段时间","不同","__","github","大多","4","有","发行版","_","7","windows","相关","先","自行","是否","等待","达到","现在","文件夹","其中","main","过于","指南","殊有","reduce","存在","更新","│","9.0","包","图","一般","5","下","sla","1.12","这种","结束","0",")","就","特别","中预","txt","files","如","解压","左图","支持","├","然而","）","cp36m","'","高时","相互","除此","不过","之前","变量","首先",",","1948.6578","这部","nccl","同时","提供","bin","为止","https","特性","部分","开启","。","初始","都","大多数","图像","import","摘要","所","sess","并且","压缩包","toolkit","根据","wheel","以","在线","版本号","则","哪怕","第一个","在","v10","大致","提示","驱动","library","两种","尽管","显示","屏幕","覆盖","这部分","随机数","目录","笔者","图标","例如","没有","请","看到","妨害","2","模式","截至","pil","推算出","这里","所有","as","自动","cupti","选项","无论如何","文件","pip","这","n","以后","机数","会","代码","右图","后","output","string","7.3","所以","查看","较长","主要","whl","能否","任何","教程","使用","选择","最新","10.0","直接","过程","session","test","段时间","是","完成","初始化","管理员","进行","免费","第一","包含","}","出厂","cuda100cudnn73sse2","以下","拷贝","愿意","run","情况","接下","压缩","computing","编译","binary","考虑","将会","摸索"],"title":"Hello world!","title_tokens":["!","world","hello"," "]},{"location":"book-1-x/chapter-1/hello-world/#hello-world","text":"摘要 本节包含一个简易的安装指导，主要提供给Windows上的GPU用户。之后，用户可以按照本节指导编写第一个测试程序。","text_tokens":["，","。","windows","测试程序","的","摘要","简易","编写","本"," ","第一","包含","之后","指导","可以","上","用户","第一个","按照","给","程序","安装","测试","节","gpu","主要","提供","一个"],"title":"Hello world!","title_tokens":["!","world","hello"," "]},{"location":"book-1-x/chapter-1/hello-world/#tensorflow","text":"本节针对Windows使用GPU的用户，提供一个安装最新版Tensorflow的方法。大致的步骤符合以下两个教程： 通过pip安装Tensorflow Tensorflow的GPU支持 但是，需要指出的是，官方教程里有些部分已经不符合当前用户的实际情况，笔者经过摸索，总结以下的安装过程。 总体而言，安装Tensorflow需要用户确保以下条件 NVIDIA驱动已经达到411.31及以上 CUDA 10.0 (10.1不确定能否正常使用) CUDNN已经达到7.3.1及以上 CUPTI版本与CUDA相配（不需要专门安装，在安装CUDA的同时已经自动集成） 除此之外，涉及多GPU支持的库(NCCL和TensorRT)，这里不考虑。因此，我们主要需要确保前三条正常安装。 提示 Tensorflow官方提供的安装包是基于CUDA 9.0，CUDNN >= 7.2的基础上编译的。然而现在NVIDIA已经不再提供CUDA 9.0了。Windows用户可以安装的版本包括过时的8.1和最新的10.1。在这种情况下，官方还提供一个令用户自行从源码编译Tensorflow库的方案，但是对于Windows用户而言过于繁琐，容易出错，这里不建议按照官方教程。","text_tokens":["方法",")","有些","多","支持","了","然而","经过","）","源码","指出",">","最新版","基于","7.2","令","用户","除此","正常","nvidia","安装包","(","总体而言","与","nccl","同时","提供","库","一个","部分","。","10.1",".","的","还","cuda","除此之外","确保","涉及","cudnn","可以","：","而言","在","但是","大致","提示","8.1","安装","驱动","繁琐","不再","容易","里","包括","步骤","笔者","不","从","两个","官方","需要","1","这里","符合","自动"," ","=","cupti","通过","相配","对于","pip","集成","出错","（","过时","针对","总体","上","新版","及","已经","专门","411.31","7.3","我们","条件","节","gpu","主要","建议","能否","，","因此","教程","使用","最新","以上","10.0","过程","版本","tensorflow","确定","windows","是","自行","本","之外","方案","达到","现在","以下","tensorrt","过于","基础","前","当前","总结","9.0","情况","按照","实际","和","三条","下","编译","考虑","这种","摸索"],"title":"安装Tensorflow","title_tokens":["tensorflow","安装"]},{"location":"book-1-x/chapter-1/hello-world/#nvidia","text":"首先，我们可以开始更新我们的NVIDIA驱动。任何载有NVIDIA-GPU显卡的电脑，都应当在系统里已经预装好了 GeForce Experience 。点击如下图标开启GeForce Experience（如果是低版本的GeForce Experience，图标可能会不同）， 如左图，在主界面上，可以观察到，当驱动版本不够高时，会自动出现更新提示，依照提示更新即可。建议在更新驱动时，关闭其他所有程序，以免造成干扰。 更新NVIDIA driver 检查更新结果 如果更新完成，如右图，可以在设置界面检查到当前的版本已经达到最新。在安装过程中，由于驱动更新，屏幕可能会闪烁数次。在看到更新提示完成后建议重启。 提示 如果GeForce Experience提示无法更新驱动，可以考虑将机器恢复出厂设置，可能可以解决这一问题。","text_tokens":["如","干扰","低版本","左图","了","机器","）","应当","高时","其他","首先","nvidia","到","预装","如果","重启","开始","以免","开启","。","闪烁","检查","都","设置","时","点击","的","好","-","恢复","结果","关闭","如下","可以","即可","这一","在","由于","界面","提示","程序","造成","将","驱动","安装","问题","屏幕","里","低版","不够","图标","看到","观察","系统","解决","所有","自动"," ","出现","中","载有","（","上","会","主","右图","后","已经","无法","数次","我们","gpu","建议","电脑","不同","，","任何","最新","版本","过程","driver","是","geforce","可能","当","完成","依照","出厂","达到","更新","当前","显卡","experience","考虑"],"title":"更新NVIDIA驱动","title_tokens":["驱动","更新","nvidia"]},{"location":"book-1-x/chapter-1/hello-world/#cuda","text":"驱动更新并重启后，需要安装最新版的Visual Studio(VS)，这是因为CUDA库本身对VS具有一定的支持，如果先安装CUDA，则这部分支持无法安装到位。这对我们使用Tensorflow并不造成妨害，但是对CUDA的用户并不是一个推荐的做法，所以无论如何，建议在第一步，安装VS，下面提供VS社区版（免费）的链接： Visual Studio官网 接下来，可以开始安装CUDA。进入CUDA安装包的下载链接 下载最新版CUDA CUDA官方指南 提示 须知，并非最新版就是最适合的版本。实际能使用的CUDA版本取决于后续步骤中预编译包的支持版本。就笔者写到这里时，最新的CUDA 10.1已然不匹配最新版预编译包支持的CUDA 10.0，由于版本差距很小，不确定是否会由此引发问题，但读者可以自行尝试。 现在的安装包已经支持在线安装包和离线安装包两种模式，读者可以根据自己的实际情况选择对应的版本。注意，安装CUDA的时候，CUDA可能要求我们翻新驱动版本，但通过GeForce Experience安装的驱动一般应已达到最新，新于CUDA内部提供的驱动。因此，我们需要选择定制安装模式，根据我们需要对比版本号，将驱动更新的选项取消，如下图所示 安装CUDA后，建议重启。 危险 特别需要注意的是，如果你的机器无法通过GeForce Experience更新驱动，那么在这里选择安装驱动也是枉然。并且，这种强制更新驱动的行为很可能导致系统崩溃。因此特别建议在安装CUDA之前一定要解决驱动更新的问题，哪怕是通过恢复出厂设置。","text_tokens":[")","就","特别","强制","中预","尝试","支持","翻新","机器","下来","接下来","）","对比","最新版","一步","危险","内部","用户","最","引发","之前","能","vs","匹配","到","导致","安装包","对应","(","这部","适合","很小","重启","如果","开始","链接","提供","第一步","库","一个","并","要","枉然","下载","部分","。","10.1","但","时","设置","定制","的","取决","cuda","预","恢复","并且","下面","版","根据","由此","在线","时候","版本号","可以","如下","则","：","哪怕","应已","在","由于","但是","崩溃","提示","造成","本身","将","驱动","安装","不是","问题","两种","也","到位","就是","这部分","读者","新于","步骤","无论","官网","笔者","很","如何","因为","不","官方","需要","妨害","你","模式","要求","是因为","进入","系统","解决","这里","须知"," ","并非","通过","选项","无论如何","这","具有","取决于","后续","（","新版","会","visual","行为","写","对","后","已经","社区","无法","我们","所以","取消","差距","建议","，","推荐","注意","选择","使用","因此","最新","10.0","版本","做法","tensorflow","确定","studio","是","先","可能","geforce","自行","一定","是否","免费","导致系统","第一","出厂","现在","达到","已然","指南","更新","情况","所示","包","实际","一般","和","图","experience","接下","离线","自己","编译","那么","这种"],"title":"安装CUDA","title_tokens":["cuda","安装"]},{"location":"book-1-x/chapter-1/hello-world/#cudnn","text":"安装CUDA的同时，已经自带安装上了CUPTI。但是CUDNN并没有包含在CUDA中，因此，我们需要到以下链接下载CUDNN并查看安装说明 下载最新版CUDNN CUDNN官方指南 不同的是，CUDNN没有安装包，是以压缩包的形式下载到本地。解压后，其中应当包含如下目录及文件 . └─ cuda/ ├─ bin/ # Binary library │ └─ cudnn64_7.dll ├─ include/ # C++ Include file │ └─ cudnn.h ├─ lib/ # C++ Lib file │ └─ x64/ │ └─ cudnn.lib └─ NVIDIA_SLA_cuDNN_Support.txt # Readme file 假设我们CUDA的安装目录在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，将上述解压的 bin , include 和 lib 三个文件夹，直接拷贝覆盖到该安装目录下，即可完成CUDNN的安装。 进行完上述步骤后，我们还需要确保几个环境变量正确设置： 存在 CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，是我们的安装目录。理论上在CUDA安装后，该目录已经自动设置好。 环境变量 PATH 中，存在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin , C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/extras/CUPTI/libx64 两个路径，由于CUDNN已经配置在CUDA安装目录下，我们不需要像官方文档那样，添加第三个CUDNN的安装路径。","text_tokens":["txt","files","解压","c++","├","了","readme",":","几个","─","应当","最新版","x64","path","像","变量","到","nvidia",",","安装包","同时","链接","并","正确","下载","bin","dll","。",".","设置","support","的","还","cuda","好","压缩包","toolkit","路径","以","确保","文档","cudnn","lib","如下","c","：","即可","#","在","由于","但是","v10","将","安装","件夹","library","libx64","file","完","那样","h","覆盖","/","假设","目录","步骤","自带","不","两个","官方","没有","需要","环境变","program","自动","三个"," ","=","└","cupti","第三个","中","include","文件","添加","上","新版","环境","及","后","已经","我们","查看","gpu","本地","不同","配置","，","因此","extras","最新","直接","_","7","上述","cudnn64","是","完成","进行","包含","形式","文件夹","以下","其中","理论","第三","拷贝","指南","存在","│","和","说明","下","压缩","sla","该","computing","binary","环境变量","0"],"title":"安装CUDNN","title_tokens":["cudnn","安装"]},{"location":"book-1-x/chapter-1/hello-world/#anaconda","text":"Windows用户建议使用Anaconda管理python环境。作为一个开源的项目，Anaconda发行版已经集成了我们所需要的大多数python包，其中有些包是我们自己难以安装上的，例如支持python3的PIL。我们选用python3版的Anaconda，下载路径如下： Anaconda 最新版下载链接 注意选择x64版的安装包。安装结束后，我们已经有python3.7的环境了。鉴于有些情况下我们可能需要使用更早的python版本，接下来我们可以安装python3.6的虚环境。 Anaconda可以安装在用户或系统目录下。注意，如果安装在系统目录下，如果不在虚环境下，安装任何包都需要管理员模式。不过这不影响本教程的示例，因为我们将安装虚环境。直接打开 Anaconda Prompt ，并键入以下命令： conda create -n py36 python = 3 .6 anaconda 按照引导流程安装，经过一段时间后，我们将得到python3.6版的Anaconda。以后我们可以直接从开始菜单 Anaconda Prompt (py36) 进入该虚环境，也可以在基环境中键入 activate py36 转换到虚环境中。 提示 如果用户愿意，还可以通过 conda install -c conda-forge opencv 安装 opencv 。opencv是一个强大的图像处理库，这里python版的opencv比C++版更容易上手。一般该命令安装的是conda库中最新版的opencv（截至笔者写到这里时是opencv3.4.4）。然而，这样安装的并非是支持GPU的版本。尽管conda还提供一个GPU的opencv版本，可以通过 conda install -c patricksnape opencv-gpu 安装 opencv2 ，但它的版本还停留在2.4.9，和opencv3的用法殊有不同，请读者注意这一点。","text_tokens":["发行",")","有些","停留","c++","支持","了","conda","包是","然而","下来","接下来","3","经过","一段","转换","）","流程","最新版","x64","引导","4.4","用户","不过","影响","9","到","安装包","示例","anaconda","(","它","一点","用法","选用","如果","开始","链接","库","提供","一个","并","下载","更","项目","鉴于","。",".","大多数","难以","系统目录","键入","比","图像","时","但","的","还","这样","-","所","处理","路径","版","6","如下","可以","prompt","：","c","打开","python3","在","提示","开源","将","安装","python","也","opencv2","尽管","容易","activate","时间","目录","基","2.4","读者","笔者","例如","install","不","因为","从","需要","请","模式","截至","pil","系统","进入","forge","opencv","这里","py36","开始菜单","得到"," ","=","并非","通过","库中","作为","中","包都","这","集成","手","n","（","以后","上","新版","环境","写","后","已经","opencv3","多数","强大","我们","早","gpu","建议","图像处理","一段时间","虚","不同","，","注意","任何","选择","使用","大多","最新","管理","教程","有","发行版","直接","版本","7","windows","段时间","是","可能","或","管理员","本","菜单","命令","其中","以下","create","殊有","愿意","按照","情况","包","一般","和","接下","patricksnape","下","该","自己","结束"],"title":"安装Anaconda","title_tokens":["anaconda","安装"]},{"location":"book-1-x/chapter-1/hello-world/#tensorflow_1","text":"我们可以查看如下项目： fo40225/tensorflow-windows-wheel 该项目的维护者在不断根据Tensorflow的更新，预编译出适合不同版本的Tensorflow安装包。截至笔者写到这里为止，最新支持到r1.12版。我们选择对应的GPU版Tensorflow，并在虚环境下执行以下命令： pip install https://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.12.0/py36/GPU/cuda100cudnn73sse2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl 等待一段时间后，安装将会结束。","text_tokens":["支持","fo40225",":","blob","一段","cp36m","不断","到","r1","安装包","对应","win","适合","并","cp36","执行","为止","https","项目","。",".","com","的","amd64","预","-","版","根据","wheel","master","如下","可以","12","：","在","安装","/","时间","笔者","install","截至","这里","py36"," ","pip","环境","写","后","维护","我们","维护者","查看","gpu","一段时间","该项","虚","不同","whl","，","github","选择","最新","_","版本","tensorflow","windows","段时间","命令","等待","cuda100cudnn73sse2","以下","目的","更新","出","下","1.12","编译","将会","结束","0"],"title":"安装预编译好的Tensorflow","title_tokens":["安装","的","好","预","编译","tensorflow"]},{"location":"book-1-x/chapter-1/hello-world/#hello-world_1","text":"撰写如下代码，保存到 hello-world.py 文件， Python 1 2 3 4 5 6 7 8 9 if __name__ == '__main__' : import tensorflow as tf test_str = tf . constant ( 'Hello, world!' ) test_cal = tf . reduce_sum ( tf . random_normal ([ 1000 , 1000 ])) with tf . Session () as sess : print ( 'Current Tensorflow version is:' , tf . __version__ ) print ( 'Test string: ' , sess . run ( test_str )) print ( 'Test calculation:' , sess . run ( test_cal )) Output Current Tensorflow version is: 1 .12.0 Test string: b 'Hello, world!' Test calculation: -1948.6578 在保存目录下，执行 python hello-world.py 第一次运行可能需要等待较长的初始化时间（硬件配置）。如果能正常运行，我们将看到如下结果： 其中，显示结果之前的记录信息反映了GPU已经可以正常工作。而“Test calculation”显示的结果是 10^6 10^6 个服从 \\mathcal{N}(0,1) \\mathcal{N}(0,1) 分布的随机数之和。根据相互独立的随机正态分布互不相关的特性，我们可以推算出，该结果服从 \\mathcal{N}(0,10^3) \\mathcal{N}(0,10^3) 的正态分布。","text_tokens":[")","之","print","with","if","了",":","3","!","'","）","相互","正常","“","之前","能","normal","信息","正态分布","9","而","互不","记录","到",",","cal","(","1948.6578","独立","is","如果","name","执行","特性","{","。","初始",".","import","的","-","sess","6","根据","个","结果","如下","可以","：","10","推算","mathcal","在","将","python","显示","random","version","sum","时间","随机数","目录","分布","\\","第一次","^","12.0","constant","保存","需要","1","看到","2","推算出","as","current","一次"," ","=","py","b","反映","文件","（","n","机数","1000","撰写","calculation","代码","算出","已经","output","string","我们","服从","较长","gpu","hello","8","world","配置","__","，","4","]","_","tensorflow","7","相关","session","test","是","可能","初始化","tf","str","第一","等待","硬件","”","}","其中","main","运行","工作","reduce","run","5","和","随机","下","该","[","0"],"title":"Hello world! 测试","title_tokens":["测试"," ","!","hello","world"]},{"location":"book-1-x/chapter-1/linear-classification/","text":"线性分类 ¶ 摘要 本节介绍如何使用顺序模型(sequential model)来编写一个线性分类器，使用sigmoid函数激活，并验证其效果。 理论 ¶ 问题描述 ¶ 考虑我们有如下的二分类数据集 (\\mathbf{x},~y_i) \\in \\mathbb{D}_i (\\mathbf{x},~y_i) \\in \\mathbb{D}_i ，并且有一个未知的常数向量 \\mathbf{a}_i \\mathbf{a}_i 和未知的常数标量 c_i c_i ，使得： \\begin{equation} y_i = \\left\\{ \\begin{aligned} 0, && \\mathbf{a}_i^T \\mathbf{x} + c_i \\leqslant 0, \\\\ 1, && \\mathbf{a}_i^T \\mathbf{x} + c_i > 0. \\end{aligned} \\right. \\end{equation} 其中， \\mathbf{a} \\mathbf{a} 可以看成是某超平面的（未标准化的）法向量，那么 \\mathbf{a}^T \\mathbf{x} + c = 0 \\mathbf{a}^T \\mathbf{x} + c = 0 是该超平面的截距式定义，亦即该平面与 x_i x_i 轴的交点可以显式表述为 x_i^{(0)} = - \\frac{c}{a_i} x_i^{(0)} = - \\frac{c}{a_i} 。由此可知，式 (1) (1) 显式定义了一个点在超平面的哪一侧。特别地，若 \\mathbf{x} \\mathbf{x} 是一个二维向量，则该超平面退化为一维平面；若 \\mathbf{x} \\mathbf{x} 是一个标量，则该超平面退化为一条直线。 若我们定义 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} ，有 \\begin{equation} \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, ~~ \\mathbf{A} = \\begin{bmatrix} \\mathbf{a}^T_1 \\\\ \\mathbf{a}^T_2 \\\\ \\vdots \\\\ \\mathbf{a}^T_n \\end{bmatrix}, ~~ \\mathbf{c} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}. \\end{equation} 则我们可以认为 \\begin{align} \\mathbf{y} = \\left\\{ \\begin{bmatrix}\\hat{y}_1 > 0 & \\hat{y}_2 > 0 & \\cdots & \\hat{y}_n > 0\\end{bmatrix}^T, ~ \\left| ~ \\hat{\\mathbf{y}} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon} \\right. \\right\\}, \\end{align} 其中 \\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon} 是一个定义噪声的向量。 我们可以把向量 \\mathbf{y} \\mathbf{y} 的元素看成是互不相关的多个超平面对向量 \\mathbf{x} \\mathbf{x} 各自独立的分类结果。即 y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} 。由于每个超平面构成一个二分类，如果把每个二分类看作是向量是否属于这个类的测度，那么 \\mathbf{y} \\mathbf{y} 可以被看作是一个多分类的结果，尽管向量 \\mathbf{x} \\mathbf{x} 可能被同时分入多个类中。 假设我们的数据集 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} 符合 (3) (3) 定义的数据分布特征。我们的基本要求是，在我们不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 的情况下，使用大量 (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} 样本训练一个线性分类器，使得当我们给定任意一个新样本 \\mathbf{x} \\mathbf{x} 的时候，分类器能推断出其对应的 \\mathbf{y} \\mathbf{y} 来（亦即是否属于该分类）。 在这个问题里，我们虽然不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，但我们知道由 (3) (3) 确定的线性关系，因此，我们可以随机生成一组 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ，构建线性模型： \\begin{align} \\tilde{\\mathbf{y}} = \\sigma ( \\mathbf{W} \\mathbf{x} + \\mathbf{b} ). \\end{align} 其中，可微函数 \\sigma \\sigma 是一个将实数空间 \\mathbb{R}^n \\mathbb{R}^n 映射到有限范围的实数空间 [0,~1]^n [0,~1]^n 内的函数。特别地， \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 。因此，可以将 \\sigma \\sigma 看作是二分类布尔函数的插值函数。理论上，只要我们找到 \\mathbf{W}=\\mathbf{A} \\mathbf{W}=\\mathbf{A} ， \\mathbf{b}=\\mathbf{c} \\mathbf{b}=\\mathbf{c} ，则该线性分类器可以直接拟合出原分布来。 感知机 ¶ 我们将 (4) (4) 定义的线性模型称为 单层感知机 (Single-layer perceptron) 模型。它包含一个权重矩阵 \\mathbf{W} \\mathbf{W} 和一个偏置矩阵 \\mathbf{b} \\mathbf{b} 。事实上，可以将 (4) (4) 改写成如下形式 \\begin{align} \\tilde{\\mathbf{y}} = \\sigma \\left( \\begin{bmatrix} \\mathbf{W} & \\mathbf{b} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ 1 \\end{bmatrix} \\right). \\end{align} 可见偏置本身可以看成是输入向量多了一个常数元素的等价模型。 感知机是最早的神经网络形式，它非常孱弱，只能解线性问题，但却为神经网络后来的发展开了先河。在单层感知机里，我们视输入向量 \\mathbf{x} \\mathbf{x} 的每个元素为一个“神经元”，矩阵 \\mathbf{W} \\mathbf{W} 和偏置 \\mathbf{b} \\mathbf{b} 将我们的输入映射到输出层 \\mathbf{y} \\mathbf{y} ，输出层的每个元素也视为一个神经元。在这个过程中， W_{ij} W_{ij} 作为 i i 行 j j 列的元素，代表了连接两个神经元的权重。我们用红线代表正值，蓝线代表负值，感知机可以被图示为 线性感知机的输出也是输入的线性组合，但我们可以添加激活函数，即 \\sigma(\\cdot) \\sigma(\\cdot) 将其映射到非线性空间。这要求我们添加的激活函数是一个非线性函数。 事实上，将单层感知机层叠，前一层的输出作为后一层的输入，就构建出早期的神经网络。这种网络每一层都是全连接的（两个神经元之间总是有权重，尽管值可能为0），每一层都有激活函数。理论上，任意一个两层堆叠的感知机，只要神经元数目足够多，就可以拟合出任意一个非线性函数。然而，实际测试中，这一理论的效果并不尽如人意，因此又有陆续地改进，才有了后来的深度学习。饮水思源，鉴往知来，我们也将从这个简简单单的单层模型开始，走上学习“深度学习”之旅。 Sigmoid函数 ¶ 在上述介绍中，我们没有解决的两个问题是， 如何定义插值函数 \\sigma \\sigma ？ 如何找到合适的 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ？ 我们首先讨论第一个问题。一般地，多分类问题中，如果各个分类彼此并非相斥，且不一定要将结果分入任一类的话，我们可以用 Sigmoid 函数来定义 \\sigma \\sigma ，亦即 \\begin{align} \\sigma(\\mathbf{x}) = \\frac{1}{ 1 + e^{-\\mathbf{x}}}. \\end{align} 它同时满足 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 ，且是一个单调函数。以下代码向我们展示了这种函数的特性： Python 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import matplotlib.pyplot as plt def test_sigmoid (): x = np . linspace ( - 5 , 5 , 100 ) y = 1 / ( 1 + np . exp ( - x ) ) plt . plot ( x , y ) plt . xlabel ( 'x' ), plt . ylabel ( 'y' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () if __name__ == '__main__' : test_sigmoid () Output 使用sigmoid函数的一大好处是，它的导数求解非常简单，很适合用来做神经网络这样一个复杂模型的激活函数。注意虽然 \\sigma(\\mathbf{x}) \\sigma(\\mathbf{x}) 和 \\mathbf{x} \\mathbf{x} 都是向量，这意味着导数是Jacobian矩阵，但由于 \\sigma \\sigma 是一个对 \\mathbf{x} \\mathbf{x} 各元素独立的解析函数，这个Jacobian矩阵实际上是一个对角矩阵，对角线上第j个元素的值为 \\begin{align} \\left. \\frac{ \\partial \\sigma(x) }{ \\partial x } \\right|_{x=x_j} = \\left. - e^{-x} \\left( - \\frac{1}{\\left( 1+e^{-x} \\right)^2} \\right) \\right|_{x=x_j} = - \\sigma(x_j) \\sigma(1 - x_j). \\end{align} 可见，该函数的导数和计算函数本身的复杂度相若，可以做到快速求导。 求解问题 ¶ 接下来，我们需要解决第二个问题，亦即找到 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} 。这一问题通常可以写成反问题的形式： \\begin{align} \\arg \\min_\\limits{\\mathbf{W},~\\mathbf{b}} \\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}^{(k)},~ \\sigma ( \\mathbf{W} \\mathbf{x}^{(k)} + \\mathbf{b} ) \\right). \\end{align} 最简单的情况下，我们可以把 损失函数(loss function) 定义为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = \\lVert \\mathbf{y} - \\tilde{\\mathbf{y}} \\rVert_2^2. \\end{align} 我们称 (7) (7) 为 逻辑斯蒂回归(logistic regression) 。有趣的是，虽然这个术语叫“回归”，但它解的其实是个分类问题。但是，既然这是一个分类问题，我们可以不使用这个损失函数，而是从概率论的角度看待这个问题。由此，我们引出一个新的损失函数：“交叉熵”。 交叉熵 ¶ 我们视sigmoid函数输出的值为一个概率，表示分类器对预测结果的确信程度，记 \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} ，则 \\begin{equation} \\begin{aligned} \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}), \\\\ \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) &= 1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}). \\end{aligned} \\end{equation} 注意这里的概率向量的含义是，其中第i个元素表明第i个超平面分类结果的确信程度。 然而，这个概率只是分类器对分类结果的确信程度，却并非是分类准确度的概率，实际上，分类准确度的概率，应当表述为 \\begin{equation} \\begin{aligned} \\mathbf{p}(\\mathbf{y}|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta})^{1-\\mathbf{y}}\\\\ &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\left(1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}) \\right)^{1-\\mathbf{y}}. \\end{aligned} \\end{equation} 注意 这里 \\mathbf{x}^{\\mathbf{y}} \\mathbf{x}^{\\mathbf{y}} 表示的是对每个元素一一求取指数，即函数第i个元素的返回值应当为 {x_i}^{y_i} {x_i}^{y_i} 。 我们使用真实值 \\mathbf{y} \\mathbf{y} 作为指数给概率向量加权。当 \\mathbf{y}=1 \\mathbf{y}=1 时，以预测值为1的可信度作为概率；反之则以预测值为0的可信度作为概率。这就是最大似然估计方法。至此，我们可以写出似然估计函数 \\begin{align} L(\\boldsymbol{\\Theta}) = \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}). \\end{align} 对似然估计函数取对数，则有 \\begin{equation} \\begin{aligned} l(\\boldsymbol{\\Theta}) &= \\sum_{k=1}^N \\log \\left( \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}) \\right) \\\\ &= \\sum_{k=1}^N \\mathbf{y}^{(k)} \\cdot \\log\\left(\\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right) + \\left(1 - \\mathbf{y}^{(k)} \\right) \\cdot \\log\\left(1 - \\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right). \\end{aligned} \\end{equation} 我们最终的目的是要最大化似然函数，亦即 \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) ，这等价于最小化 -l(\\boldsymbol{\\Theta}) -l(\\boldsymbol{\\Theta}) 。对比 (7) (7) 和 (8) (8) ，于是我们可以定义交叉熵为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = \\mathbf{y} \\cdot \\log\\left( \\tilde{\\mathbf{y}} \\right) + \\left(1 - \\mathbf{y} \\right) \\cdot \\log\\left(1 - \\tilde{\\mathbf{y}} \\right). \\end{align} 若我们记 \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) ，代入sigmoid函数，为了确保该损失函数的稳定性，我们可以将 (13) (13) 整理为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{x}} \\right) = \\max(\\tilde{\\mathbf{x}}, \\mathbf{0}) - \\tilde{\\mathbf{x}} \\cdot \\mathbf{y} + \\log\\left(1 + e^{-|\\tilde{\\mathbf{x}}|} \\right). \\end{align} 提示 这里交叉熵整理的推导过程参见 Tensorflow-API官方文档 。 实际情况下，我们使用 (14) (14) 来求取sigmoid函数激活下的交叉熵。 解线性多分类问题 ¶ 代码规范 ¶ 建立一个具有较强可读性的Tensorflow工程需要我们活用python的模块化设计。我们通常推荐以下的结构 . ├─ data/ # where we store our data │ └─ ... ├─ tools.py # codes for post-processing and analyzing records. ├─ extension.py # codes for extending the tensorflow model. ├─ dparser.py # data parser └─ main.py # main module where we define our tensorflow model. 除了保存数据的文件夹，我们应当有三个子模块。其中 tool : 用来处理、分析生成的数据，通常与Tensorflow无关； extension : 用来扩展tensorflow，例如在这里自定义网络层和操作符； dparser : 数据处理器，用来读取并预处理送入网络的数据； main : 主模块，只定义跟Tensorflow模型有关的内容，需要引用 extension 和 dparser 。 视情况可以灵活调整结构，但建议将定义Tensorflow模型的代码单独放在主模块里，和其他外围代码分离。 撰写各个模块时，建议使用类封装各组功能相同的函数。具有良好使用习惯的coder应当注意给各个面向用户的类、函数撰写（哪怕简短的）说明文字，在一些较长的函数、方法的定义中，适当注释各部分的功能，以便读者能正确理解代码意义。 另外，在对象命名上，python有如下必须遵守或不成文的规定，和C/C++用户熟悉的蛇形命名法不同，它大致包括 类与函数多用驼峰命名法，变量可以采用驼峰或蛇形命名法。 驼峰命名法指的是用大小写区分每个单词块，例如 alphaBetaFunction () ； 蛇形命名法指的是用下划线区分每个单词块，例如 alpha_beta_function = 10 ； 宏变量使用全字大写+蛇形命名法 函数/方法，还有模块均是首字母小写，但类的首字母大写。 用单下划线 _ 表示临时存储器，或省略参数，例如一个函数 func () 有两个返回值时，可以用 _ , b = func () 表示我们只需要第二个返回值；单下划线还可以与星号连用省略多个返回值； 以单下划线开头的方法，表示模块级的私有方法，在模块以外使用 import 导入类时，不会导入这些方法，例如 def _alphaBeta ( self ): ； 以单下划线结尾的对象，用来和python的关键字区分，例如 func ( x , class_ ) ; 以双下划线开头的方法，如果不以双下划线结尾，则表示类级的私有方法，只有类内部的方法能调用这些方法，在类外部、包括继承的子类里都原则上不能调用（但其实也有办法调用），例如 def _alphaBeta ( self ): ； 以双下划线同时开头和结尾的方法，一般是用来 重写 (override) 特殊功能，例如 def __getattribute__ (): 将重写获得类属性的方法。 数据生成 ¶ 在本项目里，我们不需要扩展Tensorflow。但是，我们需要以随机生成数据代替数据集。因此，首先，通过以下代码定义数据生成器 dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class TestDataSet : ''' A generator of the data set for testing the linear model. ''' def __init__ ( self , scale_x , A , c ): ''' Initialize the data generator. scale_x: the scale of input vector. A, c: the linear transformation. ''' self . s_x = 2 * scale_x self . A = A self . c = c self . len_x = A . shape [ 0 ] self . config () def config ( self , train = True , batch = 100 , noise = 0.1 ): ''' Configuration train: a flag for controlling the iterator mode. batch: the number of samples in a batch noise: std. of the error added to the y. ''' self . train = bool ( train ) self . batch = batch self . noise = noise def next_train ( self ): ''' Get the next train batch: (x, y) ''' x = self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) y = np . matmul ( x , self . A ) + self . c if self . noise > 1e-3 : y = y + np . random . normal ( 0 , self . noise , size = y . shape ) y = np . greater ( y , 0.0 ) . astype ( np . float32 ) return x , y def next_test ( self ): ''' Get the next test batch x. ''' return self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) def __iter__ ( self ): while True : samp = self . __next__ () yield samp def __next__ ( self ): if self . train : return self . next_train () else : return self . next_test () 该生成器输入一组 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，以及相关配置，之后就可以通过 迭代器 (iterator) 或 方法 (method) 随机生成数据。这种数据集写法我们在后面还会用到， model . fit 允许我们不是馈入样本（或样本批次），而是馈入一个 生成器(generator) 。因此我们重写了 __iter__ 方法，并使其通过 yield 返回一个生成器。这样我们定义的数据集类就可以被Keras的训练函数 model . fit 使用。接下来，调用如下测试代码： dparser.py 1 2 3 4 5 6 7 8 9 10 def test_dataset (): A = np . random . normal ( 0 , 10 , [ 10 , 6 ]) c = np . random . uniform ( 1 , 3 , [ 1 , 6 ]) dataSet = TestDataSet ( 10 , A , c ) dIter = iter ( dataSet ) for i in range ( 10 ): x , y = next ( dIter ) print ( np . sum ( y , axis = 0 ) / 100 ) test_dataset () Output [ 0.47 0.57 0.58 0.56 0.5 0.38 ] [ 0.6 0.61 0.47 0.48 0.38 0.52 ] [ 0.5 0.61 0.49 0.42 0.45 0.53 ] [ 0.59 0.52 0.44 0.44 0.49 0.51 ] [ 0.54 0.59 0.48 0.5 0.51 0.47 ] [ 0.49 0.57 0.56 0.49 0.53 0.4 ] [ 0.5 0.61 0.51 0.54 0.51 0.52 ] [ 0.5 0.51 0.61 0.5 0.44 0.5 ] [ 0.44 0.46 0.53 0.45 0.56 0.52 ] [ 0.52 0.46 0.51 0.52 0.49 0.44 ] 我们随机生成了 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的数据，每组数据100个，并且测试了10组。输出结果是各组测试中， \\mathbf{y} \\mathbf{y} 在对应维度上分类为1的概率估计。结果基本都在0.5左右，说明我们的这种数据生成模式产生的点能均匀分布在各个超平面两侧，适合进行后续测试。 定义线性顺序模型 ¶ 顺序(sequential) 模型是一个单输入单输出模型，网络结构较为简单，也不存在跨层短接（残差连接）。在大多数情况下，已经上手的Tensorflow用户不使用这个模型，故而作为我们入门的第一个project，我们姑且用之，但我们将不再使用顺序模型来实现后续的project。一个顺序模型大致可以描述为下图的模式： graph LR st(输<br/>入) --> l1[层<br/>1] l1 --> l2[层<br/>2] l2 --> l3[层<br/>3] l3 --> ldots[层<br/>...] ldots --> ed(输<br/>出) classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,ed styStart 由于我们完成的是一个线性分类器，故而我们使用单层的序列模型即可。 接下来，我们来定义一个类， class LinClsHandle : 。定义一个类的时候，我们通常需要定义的内容包括 在初始化方法 __init__ 里定义传入网络的固定参数，例如学习速率，存取路径等； 在方法 construct 里定义网络的构造和使用的优化器； 在方法 train 里定义训练网络的过程，主要需要调用 model . fit 。如果我们在数据集的定义非常完善，则这一环节不需要花费太多的功夫； 在方法 test 里定义测试网络的过程，主要需要调用 model . evaluate 。如果有必要，可以通过 model . predict 返回测试结果。 初始化方法 ¶ 首先，定义初始化方法： lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 def __init__ ( self , learning_rate = 0.01 , epoch = 10 , steppe = 30 ): ''' Initialization and pass fixed parameters. learning_rate: the learning rate for optimizer. epoch: training epochs. steppe: steps per epoch ''' self . lr = learning_rate self . epoch = epoch self . steppe = steppe 由于目前我们的project还非常简单，这里只需要有学习速率( learning_rate )，轮次数( epoch )和每轮迭代次数( steppe )即可。 构造方法 ¶ 接下来定义网络构造 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def construct ( self ): ''' Construct a linear model and set the optimizer as Adam ''' # Construction self . model = tf . keras . Sequential () self . dense1 = tf . keras . layers . Dense ( LABEL_SHAPE , use_bias = True , input_shape = ( INPUT_SHAPE ,), kernel_initializer = tf . keras . initializers . RandomNormal ( 0.0 , stddev = 10.0 ), bias_initializer = tf . keras . initializers . Constant ( 2 ), activation = None ) self . model . add ( self . dense1 ) # Set optimizer self . model . compile ( optimizer = tf . train . AdamOptimizer ( self . lr ), loss = self . loss , metrics = [ self . accuracy ] ) @staticmethod def loss ( y_true , y_pred ): return tf . nn . sigmoid_cross_entropy_with_logits ( labels = y_true , logits = y_pred ) @staticmethod def accuracy ( y_true , y_pred ): return tf . keras . backend . mean ( tf . keras . backend . equal ( y_true , tf . keras . backend . round ( tf . keras . backend . sigmoid ( y_pred )))) 须知 这里 LABEL_SHAPE 和 INPUT_SHAPE 为两个宏变量，分别为输出和输入的向量维度。 我们使用 Dense 定义全连接层，它的用法请参照 这里 。由于我们已经知道 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 可能的取值范围，这里我们重定义了 \\mathbf{W} \\mathbf{W} 和 \\mathbf{b} \\mathbf{b} 的初始化方式。 另外，注意我们这里构造网络的时候有如下技巧： 我们定义的网络输出是 \\mathbf{W}\\mathbf{x} + \\mathbf{b} \\mathbf{W}\\mathbf{x} + \\mathbf{b} ，而非 \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) 。这是因为我们需要通过还未被激活的输出用来计算sigmoid交叉熵，亦即式 (14) (14) ； 我们通过静态方法，直接调用Tensorflow自带的 sigmoid交叉熵 函数来作为Keras模型的损失函数 self..loss ； 我们通过静态方法，调用Keras的后端API，自己定义了预测准确度的测度函数 self.accuracy ； 我们将网络层的关键字 self.dense1 保留在了实例中，这是为了确保接下来我们能通过实例抽取该层的参数。 之所以煞费周折地进行这些处理，盖因为Keras的内建API里目前还没有提供对互不相斥的多分类的支持。例如，无论是 tf.keras.metrics.categorical_accuracy 还是 tf.keras.metrics.categorical_crossentropy ，都要求分类的真实值为one-hot类型的向量组，因而它们只适合用在softmax分类器上。为了解决这一问题，我们自己实现了sigmoid分类器。 训练和测试方法 ¶ 最后定义的式训练和测试方法。由于我们目前的project还比较简单，关于这两部分都直接调用现有的API即可。使用的API在之前已经说明。 model.fit 在没有额外设置的情况下，默认会返回一个 History回调器 ； model.evaluate 返回的是测试样本给出的损失函数和准确值测度。 model.predict 返回的是测试样本给出的网络输出。详情请参照 顺序模型API 。 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def train ( self , dataSet ): ''' Use a data set to train the network. ''' return self . model . fit ( dataSet , epochs = self . epoch , steps_per_epoch = self . steppe ) def test ( self , data , labels ): ''' Use (data, label) pairs to test the results. ''' loss , accu = self . model . evaluate ( data , labels ) print ( 'Evaluated loss =' , loss ) print ( 'Evaluated accuracy =' , accu ) return self . model . predict ( data ) 调试 ¶ 首先，训练网络。我们随机生成 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的线性变换，并且设置好数据集，给定噪声扰动为 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 。设定20个epoch，每个epoch迭代500次，每次馈入32个样本构成的batch，然后开始训练： lin-cls.py 1 2 3 4 5 6 7 8 A = np . random . normal ( 0 , 10 , [ INPUT_SHAPE , LABEL_SHAPE ]) c = np . random . uniform ( 1 , 3 , [ 1 , LABEL_SHAPE ]) dataSet = dp . TestDataSet ( 10 , A , c ) dataSet . config ( batch = 32 , noise = 0.1 ) # Construct the model and train it. h = LinClsHandle ( learning_rate = 0.01 , epoch = 20 , steppe = 500 ) h . construct () record = h . train ( iter ( dataSet )) Output Epoch 1 /20 500 /500 [==============================] - 3s 5ms/step - loss: 48 .2269 - accuracy: 0 .5458 Epoch 2 /20 500 /500 [==============================] - 1s 2ms/step - loss: 25 .5149 - accuracy: 0 .6491 Epoch 3 /20 500 /500 [==============================] - 1s 2ms/step - loss: 11 .9822 - accuracy: 0 .7607 Epoch 4 /20 500 /500 [==============================] - 1s 2ms/step - loss: 5 .6580 - accuracy: 0 .8513 Epoch 5 /20 500 /500 [==============================] - 1s 2ms/step - loss: 2 .7230 - accuracy: 0 .9106 Epoch 6 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .1082 - accuracy: 0 .9462 Epoch 7 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .3278 - accuracy: 0 .9708 Epoch 8 /20 500 /500 [==============================] - 1s 3ms/step - loss: 0 .0618 - accuracy: 0 .9878 Epoch 9 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0149 - accuracy: 0 .9963 Epoch 10 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9979 Epoch 11 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0124 - accuracy: 0 .9976 Epoch 12 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9978 Epoch 13 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9973 Epoch 14 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0120 - accuracy: 0 .9974 Epoch 15 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9970 Epoch 16 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0116 - accuracy: 0 .9971 Epoch 17 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0120 - accuracy: 0 .9967 Epoch 18 /20 500 /500 [==============================] - 1s 3ms/step - loss: 0 .0114 - accuracy: 0 .9971 Epoch 19 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0114 - accuracy: 0 .9969 Epoch 20 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0113 - accuracy: 0 .9970 接下来，从训练返回的 History 类型的回调器中抽取对loss和accuracy的记录。 lin-cls.py 1 2 showCurve ( record . epoch , record . history [ 'loss' ], xlabel = 'epoch' , ylabel = 'Cross entropy' , log = True ) showCurve ( record . epoch , record . history [ 'accuracy' ], xlabel = 'epoch' , ylabel = 'Accuracy' ) Output 重新设定数据集的产生方式，变为每个batch含10个样本。使用这组重新随机生成的数据测试网络输出， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 12 # Generate a group of testing samples: dataSet . config ( batch = 10 ) x , y = next ( dataSet ) # Check the testing results yp = dp . sigmoid ( h . test ( x , y )) _ , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 ) ax1 . imshow ( y , interpolation = 'nearest' , aspect = 'auto' ) ax1 . set_title ( 'True class' ) ax2 . imshow ( yp , interpolation = 'nearest' , aspect = 'auto' ) ax2 . set_title ( 'Predicted class' ) plt . gcf () . set_size_inches ( 10 , 5 ), plt . show () Output 注意我们未对测量的结果阈值化，因此显示出来的测量结果和理想值略有差别，但从图可知，阈值化后则测量结果全部准确。 通过抽取 h.dense1 的参数，我们可以对比 \\mathbf{A} \\mathbf{A} 和 \\mathbf{W} \\mathbf{W} ，以及 \\mathbf{c} \\mathbf{c} 和 \\mathbf{b} \\mathbf{b} ， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 # Check the regressed values W , b = h . dense1 . get_weights () plt . imshow ( A , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'A' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . imshow ( W , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'W' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . plot ( c . T , label = 'c' ) plt . plot ( b . T , label = 'b' ) plt . legend () plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () Output 可以发现，虽然我们训练的分类器十分有效，但其权值和预期的 \\mathbf{A} \\mathbf{A} , \\mathbf{c} \\mathbf{c} 并不相同。这是由于我们训练的样本加入了噪声。这种技术常用于神经网络的训练，被认为是一种提高鲁棒性、减小过拟合、避免不稳定解的一个有效手段。可以看出真实值 \\mathbf{A} \\mathbf{A} 存在偏高值，但 \\mathbf{W} \\mathbf{W} 的数值更加均匀。","text_tokens":["i","leqslant","线性","称为","关键","之","找到","技巧","神经元","该层","一一","大小写","print","0.38","二","概率","放在","c++","求解","了","0.58","dataset","pairs","看作",":","全部","exp","似然","53","概率论","引出","活用","速率","知机","应当","对比",">","以双","0.53","linclshandle","各","label","满足","关系","一种","r","34","感知","ldots","用户","向","backend","加权","30","model","给","互不","error","samp","类型","布尔","9","分别","逻辑","¶","到","21","代表","还有","各个","一维","sim","独立","0618","与","早期","用法","最早","一层","inches","关于","预处理","介绍","要","right","9708","+","蛇形","of","true","flag","序列","差别","性关系","模块","存储器","54","这样","predict","权值","成","为了","编写","有趣","处理","即","点","per","d","文字","使得","只能","layers","给定","拟合","分离","神经网","labels","max","post","greater","大写","较强","组","参数","但是","调整","0.5","送入","15","将","也","不是","python","问题","二个","件夹","自定义","bmatrix","反","controlling","不再","auto","epoch","0.54","left","后面","把","分布","读者","\\","evaluate","；","遵守","^","37","字母","boldsymbol","如何","很","展示","两个","testdataset","..","并不相同","跨层","输入","静态方法","用单","categorical","9462","更加","rvert","9967","加入","宏","符合","为","周折","b","熟悉","通过","默认","稳定性","添加","in","轴","fit","小写","（","后续","add","功能","组合","多用","5149","yp","colorbar","人意","processing","代替","alpha","epochs","38","data","l2","花费","除了","<","多数","变为","第二","ij","...","override","0.56","合适","tool","iter","graph","0121","比较简单","0124","extending","，","只有","50","method","只要","dp","29","]","initialize","stroke","yield","22","k","用","短接","孱弱","上述","表明","煞","check","测量","可能","构造","kernel","图示","或","迭代","既然","一定","另外","class","sigma","一大","分类","好处","以及","全","推断出","单独","showcurve","一侧","alphabetafunction","实际上","where","意义","双","目的","维度","analyzing","softmax","参见","取值","意味","标准化","our","标量","|","一","各组","优化","[","one","对角","写出","又","首字","gca","调用","求取","表示","42","集类","show","尽如人意","self","round","2ms","生成器","interpolation","网络层","with","交叉","t","各自","initializers","单单","抽取","使","5458","perceptron","下来","─","plot","显式","线性关系","列","处理器","35","式","metrics","vector","9963","才","一条","必须","开","其他","每个","二维","记","交点","9979","0.59","最","generate","级","不成文","y","normal","鲁棒性","两层","而","关键字","记录","7607","推断","样本","适合","向量","准确度","详情","封装","47","对角线","52","一个","name","正确","get","略有","number","19","构造方法","vdots","{","知道","单词","align","none","只",".","可信度","但","设置","矩阵","看出","@","shape","、","还","上手","估计","aligned","路径","40","个","角度","0.1","故而","有限","文档","train","一个二维","每组","define","c","10","0.46","即可","#","plt","这一","mathcal","p","27","大化","参照","偏高值","不以","足够","开头","构成","本身","6491","32","外围","残差","就是","ax2","xlabel","均匀","跟","random","equation","batch","0113","复杂","里","包括","无论","25","std","自带","不","for","验证","保存","官方","31","begin","rate","存储","必要","&","entropy","每次","保留","事实上","叫","相若","*","损失","iterator","ba9132","theta","须知","现有","太多","adam","三个","数据分布","cdot","py","0120","we","noise","作为","神经","st","it","the","next","内容","bool","有关","其","33","于","samples","？","主","3278","已经","由此可知","cls","外部","1e","3ms","做到","标准","入","单","改进","预测值","层","向用","pass","之前","dense","8","9970","权重","可读","sequential","返回","虽然","配置","入门","mathbf","因此","注意","推荐","第二个","因而","目前","费周折","值为","legend","后来","下划线","28","0.6","tensorflow","added","构建","确定","允许","临时","stddev","常用","复杂度","0.45","当","固定","tf","本","亦","to","mapsto","端","预期","set","较为","提高","”","正值","空间","形式","group","理论","cross","导入","写成","推导","前","l3","0.44","集","存取","分析","建立","分类器","实际","activation","和","3s","任意","随机","subplots","稳定","测试","dparser","parser","出","说明","generator","该","负值","br","training","equal","自己","常数","那么","最终","环节","简简单单","0.51","0.49","0.61","一个多","linspace","方法","astype","每轮","规范","6580","0116","法","多","子","0114","退化","网络结构","if","定义数据","正确理解","9976","私有","varepsilon","3","mathbb","简单","用来","接下来","module","1082","鉴往知来","layer","step","内部","tilde","偏置","首字母","星号","只是","steps","划线","左右","jacobian","“","getattribute","能","意味着","config","姑且","45","2269","100","46","思源","采用","对应","块","它","(","做","自定","行","化后","事实","单调","stystart","use","def","如果","出来","regressed","技术","开始","数据测试","认为","numpy","并","网络","区分","linear","initialization","最大化","详情请","17","来","手段","项目","coder","看成","时","9971","不会","非线性","fae6a9","面向","的","ax1","发现","出原","阈值","classdef","好","-","数值","49","理想","6","有效","变换","0.4","由此","48","原则上","结果","确保","新","走上","时候","至此","非常","原则","record","如下","可以","12","：","求导","可信","float32","这些","construct","表述","即式","construction","9106","却","由于","先河","无关","accu","这是","0.0","h","pred","l1","/","sum","不能","注释","假设","重","8513","values","size","于是","randomnormal","馈入","方式","从","较为简单","需要","1","比较","ed","要求","指数","是因为","解决","未知","命名","线性组合","相斥","某","sigmoid","类","7230"," ","=","称","并非","└","层叠","快速","下划","之间","staticmethod","中","分入","之后","测试代码","真实","具有","理器","一组","次数","返回值","模块化","logits","上","然后","实例","哪","每","撰写","records","无论是","由","类中","arg","化","对","x","function","alphabeta","属于","最小化","我们","成文","节","激活","0149","均匀分布","while","函数","通常","理解","建议","面向用户","init","matplotlib","500","对象","不同","__","mode","调试","gcf","数目","mean","大多","的话","第","4","解","有","regression","它们","属性","实现","_","结尾","9978","7","设定","16","相关","range","非常简单","network","51","parameters","减小","饮水","若","回归","操作符","神经网络","是否","partial","地",";","文件夹","重写","其中","映射","main","非","读取","optimizer","值","logistic","以单","规定","操作","结构","存在","│","等","看待","适当","扰动","次","确信","5","一般","constant","图","良好","十分","器中","斯蒂","initializer","下","产生","ylabel","fixed","compile","这种","s","matmul","9974","且","0","术语",")","41","特别","就","return","nearest","20","title","aspect","特殊","0.47","├","支持","而是","法指","未","整理","单层","然而","codes","任","类时","axis","反之","36","静态","含义","&&","）","'","可知","含","lr","之所以","bias","感知机","weights","扩展","这个","可微","信度","18","adamoptimizer","predicted","configuration","一类","end","尽如","元素","func","测度","堆叠","infty","l","and","变量","39","首先","因为","深度","输出","用到","np",",","连接","9878","导数","testing","直线","最后","输","9822","同时","红线","定义","省略","提供","效果","陆续","简短","24","0.42","learning","引用","特性","被","部分","api","讨论","。","大小","0.57","内","初始","都","大多数","import","饮水思源","lvert","子类","a","tools","j","store","摘要","批次","实数","预测","e","并且","习惯","回调","这组","uniform","len","以便","其实","以","limits","frac","视","全字","代入","下图","~","则","log","获得","范围","彼此","第一个","哪怕","else","数据","模型","results","在","程度","loss","scale","大致","等价","提示","43","lin","平面","imshow","描述","尽管","fill","5ms","9973","学习","nn","内建","history","显示","总是","多个","input","重新","解析","还是","特征","相同","功夫","例如","0.48","steppe","轮","没有","请","2","一些","26","模式","9969","crossentropy","线性变换","之旅","办法","额外","这里","改写","hat","生成","as","灵活","连用","截距","视为","大量","cdots","训练","这","文件","dense1","n","evaluated","顺序","调整结构","驼峰","还会","会","给出","accuracy","可见","代码","可读性","器","后","插值","取","定性","基本","output","所以","23","min","对数","最小","13","较长","0.52","主要","diter","准确","写法","熵","single","以外","避免","使用","10.0","44","直接","过程","计算","最大","test","一个点","是","传入","beta","类级","完善","完成","初始化","发展","进行","11","均","1s","第一","}","包含","设计","蓝线","project","以下","工程","不成","0.01","超平面","14","噪声","两","情况","hot","测试方法","extension","keras","过","成器","盖","接下","transformation","w","两侧","考虑","继承","pyplot","但类"],"title":"线性分类","title_tokens":["分类","线性"]},{"location":"book-1-x/chapter-1/linear-classification/#_1","text":"摘要 本节介绍如何使用顺序模型(sequential model)来编写一个线性分类器，使用sigmoid函数激活，并验证其效果。","text_tokens":[")","，","线性","使用","来","如何","验证","。","摘要","编写","sigmoid","本"," ","分类","其","顺序","模型","model","分类器","并","节","(","激活","函数","介绍","效果","sequential","一个"],"title":"线性分类","title_tokens":["分类","线性"]},{"location":"book-1-x/chapter-1/linear-classification/#_2","text":"","text_tokens":[],"title":"理论","title_tokens":["理论"]},{"location":"book-1-x/chapter-1/linear-classification/#_3","text":"考虑我们有如下的二分类数据集 (\\mathbf{x},~y_i) \\in \\mathbb{D}_i (\\mathbf{x},~y_i) \\in \\mathbb{D}_i ，并且有一个未知的常数向量 \\mathbf{a}_i \\mathbf{a}_i 和未知的常数标量 c_i c_i ，使得： \\begin{equation} y_i = \\left\\{ \\begin{aligned} 0, && \\mathbf{a}_i^T \\mathbf{x} + c_i \\leqslant 0, \\\\ 1, && \\mathbf{a}_i^T \\mathbf{x} + c_i > 0. \\end{aligned} \\right. \\end{equation} 其中， \\mathbf{a} \\mathbf{a} 可以看成是某超平面的（未标准化的）法向量，那么 \\mathbf{a}^T \\mathbf{x} + c = 0 \\mathbf{a}^T \\mathbf{x} + c = 0 是该超平面的截距式定义，亦即该平面与 x_i x_i 轴的交点可以显式表述为 x_i^{(0)} = - \\frac{c}{a_i} x_i^{(0)} = - \\frac{c}{a_i} 。由此可知，式 (1) (1) 显式定义了一个点在超平面的哪一侧。特别地，若 \\mathbf{x} \\mathbf{x} 是一个二维向量，则该超平面退化为一维平面；若 \\mathbf{x} \\mathbf{x} 是一个标量，则该超平面退化为一条直线。 若我们定义 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} ，有 \\begin{equation} \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, ~~ \\mathbf{A} = \\begin{bmatrix} \\mathbf{a}^T_1 \\\\ \\mathbf{a}^T_2 \\\\ \\vdots \\\\ \\mathbf{a}^T_n \\end{bmatrix}, ~~ \\mathbf{c} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}. \\end{equation} 则我们可以认为 \\begin{align} \\mathbf{y} = \\left\\{ \\begin{bmatrix}\\hat{y}_1 > 0 & \\hat{y}_2 > 0 & \\cdots & \\hat{y}_n > 0\\end{bmatrix}^T, ~ \\left| ~ \\hat{\\mathbf{y}} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon} \\right. \\right\\}, \\end{align} 其中 \\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon} 是一个定义噪声的向量。 我们可以把向量 \\mathbf{y} \\mathbf{y} 的元素看成是互不相关的多个超平面对向量 \\mathbf{x} \\mathbf{x} 各自独立的分类结果。即 y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} 。由于每个超平面构成一个二分类，如果把每个二分类看作是向量是否属于这个类的测度，那么 \\mathbf{y} \\mathbf{y} 可以被看作是一个多分类的结果，尽管向量 \\mathbf{x} \\mathbf{x} 可能被同时分入多个类中。 假设我们的数据集 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} 符合 (3) (3) 定义的数据分布特征。我们的基本要求是，在我们不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 的情况下，使用大量 (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} 样本训练一个线性分类器，使得当我们给定任意一个新样本 \\mathbf{x} \\mathbf{x} 的时候，分类器能推断出其对应的 \\mathbf{y} \\mathbf{y} 来（亦即是否属于该分类）。 在这个问题里，我们虽然不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，但我们知道由 (3) (3) 确定的线性关系，因此，我们可以随机生成一组 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ，构建线性模型： \\begin{align} \\tilde{\\mathbf{y}} = \\sigma ( \\mathbf{W} \\mathbf{x} + \\mathbf{b} ). \\end{align} 其中，可微函数 \\sigma \\sigma 是一个将实数空间 \\mathbb{R}^n \\mathbb{R}^n 映射到有限范围的实数空间 [0,~1]^n [0,~1]^n 内的函数。特别地， \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 。因此，可以将 \\sigma \\sigma 看作是二分类布尔函数的插值函数。理论上，只要我们找到 \\mathbf{W}=\\mathbf{A} \\mathbf{W}=\\mathbf{A} ， \\mathbf{b}=\\mathbf{c} \\mathbf{b}=\\mathbf{c} ，则该线性分类器可以直接拟合出原分布来。","text_tokens":["一个多","i",")","leqslant","特别","线性","法","找到","退化","二","了","t","各自","未","看作","varepsilon","3","mathbb","&&","）","显式","线性关系","式",">","可知","一条","每个","关系","二维","交点","r","这个","tilde","可微","end","元素","测度","互不","y","能","infty","布尔","到","推断",",","对应","一维","样本","(","直线","独立","向量","与","如果","同时","定义","认为","一个","right","来","被","+","{","vdots","。","知道","align","内",".","看成","但","性关系","a","的","出原","-","实数","并且","aligned","即","d","使得","由此","结果","frac","给定","新","时候","有限","拟合","一个二维","如下","c","~","：","可以","则","范围","表述","数据","模型","在","由于","0.5","构成","将","平面","问题","尽管","bmatrix","equation","多个","left","假设","把","分布","里","\\","特征","；","^","boldsymbol","不","begin","1","2","&","要求","未知","hat","为","符合","生成","某","类","截距"," ","=","数据分布","b","大量","cdots","分入","训练","其","in","轴","一组","（","n","上","哪","由","类中","对","x","插值","由此可知","属于","基本","我们","标准","函数","虽然","mathbf","，","因此","使用","只要","有","]","直接","_","构建","k","确定","相关","一个点","是","可能","若","当","亦","是否","sigma","分类","地","}","空间","其中","映射","推断出","理论","超平面","一侧","噪声","集","情况","分类器","和","标准化","任意","随机","标量","|","w","下","该","[","常数","考虑","那么","0"],"title":"问题描述","title_tokens":["描述","问题"]},{"location":"book-1-x/chapter-1/linear-classification/#_4","text":"我们将 (4) (4) 定义的线性模型称为 单层感知机 (Single-layer perceptron) 模型。它包含一个权重矩阵 \\mathbf{W} \\mathbf{W} 和一个偏置矩阵 \\mathbf{b} \\mathbf{b} 。事实上，可以将 (4) (4) 改写成如下形式 \\begin{align} \\tilde{\\mathbf{y}} = \\sigma \\left( \\begin{bmatrix} \\mathbf{W} & \\mathbf{b} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ 1 \\end{bmatrix} \\right). \\end{align} 可见偏置本身可以看成是输入向量多了一个常数元素的等价模型。 感知机是最早的神经网络形式，它非常孱弱，只能解线性问题，但却为神经网络后来的发展开了先河。在单层感知机里，我们视输入向量 \\mathbf{x} \\mathbf{x} 的每个元素为一个“神经元”，矩阵 \\mathbf{W} \\mathbf{W} 和偏置 \\mathbf{b} \\mathbf{b} 将我们的输入映射到输出层 \\mathbf{y} \\mathbf{y} ，输出层的每个元素也视为一个神经元。在这个过程中， W_{ij} W_{ij} 作为 i i 行 j j 列的元素，代表了连接两个神经元的权重。我们用红线代表正值，蓝线代表负值，感知机可以被图示为 线性感知机的输出也是输入的线性组合，但我们可以添加激活函数，即 \\sigma(\\cdot) \\sigma(\\cdot) 将其映射到非线性空间。这要求我们添加的激活函数是一个非线性函数。 事实上，将单层感知机层叠，前一层的输出作为后一层的输入，就构建出早期的神经网络。这种网络每一层都是全连接的（两个神经元之间总是有权重，尽管值可能为0），每一层都有激活函数。理论上，任意一个两层堆叠的感知机，只要神经元数目足够多，就可以拟合出任意一个非线性函数。然而，实际测试中，这一理论的效果并不尽如人意，因此又有陆续地改进，才有了后来的深度学习。饮水思源，鉴往知来，我们也将从这个简简单单的单层模型开始，走上学习“深度学习”之旅。","text_tokens":["i",")","就","线性","称为","多","尽如人意","神经元","了","单单","单层","然而","perceptron","简单","列","）","知机","才","鉴往知来","开","layer","每个","感知机","感知","偏置","tilde","这个","end","尽如","元素","“","堆叠","y","两层","深度","输出","到","代表","连接","思源","它","(","行","事实","向量","早期","最早","一层","红线","定义","开始","效果","一个","网络","并","陆续","right","被","{","。","align",".","都","看成","但","矩阵","饮水思源","非线性","的","j","成","-","即","只能","视","拟合","神经网","走上","非常","如下","可以","这一","模型","却","在","等价","先河","将","本身","也","足够","问题","尽管","bmatrix","学习","总是","left","里","\\","不","从","两个","begin","1","输入","&","要求","事实上","之旅","改写","线性组合","为"," ","b","=","cdot","视为","层叠","作为","神经","之间","中","添加","这","其","（","组合","上","每","人意","可见","x","后","我们","激活","ij","改进","层","函数","权重","mathbf","single","，","数目","因此","只要","4","解","有","_","过程","后来","构建","用","孱弱","是","可能","图示","饮水","神经网络","发展","sigma","地","包含","}","”","正值","形式","蓝线","空间","映射","全","理论","值","前","实际","和","任意","w","测试","出","负值","常数","这种","简简单单","又","0"],"title":"感知机","title_tokens":["感知","感知机","知机"]},{"location":"book-1-x/chapter-1/linear-classification/#sigmoid","text":"在上述介绍中，我们没有解决的两个问题是， 如何定义插值函数 \\sigma \\sigma ？ 如何找到合适的 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ？ 我们首先讨论第一个问题。一般地，多分类问题中，如果各个分类彼此并非相斥，且不一定要将结果分入任一类的话，我们可以用 Sigmoid 函数来定义 \\sigma \\sigma ，亦即 \\begin{align} \\sigma(\\mathbf{x}) = \\frac{1}{ 1 + e^{-\\mathbf{x}}}. \\end{align} 它同时满足 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 ，且是一个单调函数。以下代码向我们展示了这种函数的特性： Python 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import matplotlib.pyplot as plt def test_sigmoid (): x = np . linspace ( - 5 , 5 , 100 ) y = 1 / ( 1 + np . exp ( - x ) ) plt . plot ( x , y ) plt . xlabel ( 'x' ), plt . ylabel ( 'y' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () if __name__ == '__main__' : test_sigmoid () Output 使用sigmoid函数的一大好处是，它的导数求解非常简单，很适合用来做神经网络这样一个复杂模型的激活函数。注意虽然 \\sigma(\\mathbf{x}) \\sigma(\\mathbf{x}) 和 \\mathbf{x} \\mathbf{x} 都是向量，这意味着导数是Jacobian矩阵，但由于 \\sigma \\sigma 是一个对 \\mathbf{x} \\mathbf{x} 各元素独立的解析函数，这个Jacobian矩阵实际上是一个对角矩阵，对角线上第j个元素的值为 \\begin{align} \\left. \\frac{ \\partial \\sigma(x) }{ \\partial x } \\right|_{x=x_j} = \\left. - e^{-x} \\left( - \\frac{1}{\\left( 1+e^{-x} \\right)^2} \\right) \\right|_{x=x_j} = - \\sigma(x_j) \\sigma(1 - x_j). \\end{align} 可见，该函数的导数和计算函数本身的复杂度相若，可以做到快速求导。","text_tokens":["linspace",")","多","show","找到","if","求解","了",":","任","exp","3","简单","plot","'","用来","各","满足","这个","向","一类","end","jacobian","元素","infty","y","意味着","9","首先","100","np",",","各个","导数","它","(","做","独立","适合","单调","向量","def","如果","同时","inches","定义","对角线","介绍","要","一个","numpy","name","网络","right","特性","来","+","{","讨论","。","align",".","都","但","import","矩阵","的","这样","j","-","e","即","6","个","结果","frac","神经网","非常","可以","~","：","10","12","求导","彼此","第一个","plt","模型","在","由于","0.5","将","本身","python","问题","xlabel","/","复杂","left","解析","\\","size","^","很","如何","不","展示","两个","没有","begin","1","2","解决","相若","相斥","as","为","sigmoid"," ","b","=","并非","快速","神经","中","分入","这","上","？","代码","可见","对","x","插值","output","我们","做到","激活","合适","函数","8","matplotlib","__","虽然","，","mathbf","gcf","注意","使用","的话","第","4","_","7","用","计算","上述","test","非常简单","是","复杂度","一定","亦","神经网络","sigma","11","分类","地","第一","}","set","一大","好处","partial","以下","main","值","实际上","5","一般","和","意味","实际","w","|","该","ylabel","对角","这种","且","pyplot","0"],"title":"Sigmoid函数","title_tokens":["函数","sigmoid"]},{"location":"book-1-x/chapter-1/linear-classification/#_5","text":"接下来，我们需要解决第二个问题，亦即找到 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} 。这一问题通常可以写成反问题的形式： \\begin{align} \\arg \\min_\\limits{\\mathbf{W},~\\mathbf{b}} \\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}^{(k)},~ \\sigma ( \\mathbf{W} \\mathbf{x}^{(k)} + \\mathbf{b} ) \\right). \\end{align} 最简单的情况下，我们可以把 损失函数(loss function) 定义为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = \\lVert \\mathbf{y} - \\tilde{\\mathbf{y}} \\rVert_2^2. \\end{align} 我们称 (7) (7) 为 逻辑斯蒂回归(logistic regression) 。有趣的是，虽然这个术语叫“回归”，但它解的其实是个分类问题。但是，既然这是一个分类问题，我们可以不使用这个损失函数，而是从概率论的角度看待这个问题。由此，我们引出一个新的损失函数：“交叉熵”。","text_tokens":[")","找到","概率","交叉","而是","下来","接下来","简单","概率论","引出","tilde","这个","最","end","“","l","y","逻辑",",","它","(","定义","一个","right","+","{","。","align",".","但","lvert","的","-","有趣","即","个","角度","由此","其实","limits","新","可以","~","：","这一","mathcal","loss","但是","问题","二个","这是","反","sum","left","把","\\","^","不","从","需要","begin","1","2","叫","解决","rvert","损失","为"," ","b","=","称","n","arg","x","function","我们","第二","min","通常","函数","熵","虽然","，","mathbf","第二个","regression","使用","解","_","k","7","是","回归","既然","亦","sigma","分类","}","”","形式","logistic","写成","看待","情况","接下","w","斯蒂","下","术语"],"title":"求解问题","title_tokens":["求解","问题"]},{"location":"book-1-x/chapter-1/linear-classification/#_6","text":"我们视sigmoid函数输出的值为一个概率，表示分类器对预测结果的确信程度，记 \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} ，则 \\begin{equation} \\begin{aligned} \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}), \\\\ \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) &= 1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}). \\end{aligned} \\end{equation} 注意这里的概率向量的含义是，其中第i个元素表明第i个超平面分类结果的确信程度。 然而，这个概率只是分类器对分类结果的确信程度，却并非是分类准确度的概率，实际上，分类准确度的概率，应当表述为 \\begin{equation} \\begin{aligned} \\mathbf{p}(\\mathbf{y}|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta})^{1-\\mathbf{y}}\\\\ &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\left(1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}) \\right)^{1-\\mathbf{y}}. \\end{aligned} \\end{equation} 注意 这里 \\mathbf{x}^{\\mathbf{y}} \\mathbf{x}^{\\mathbf{y}} 表示的是对每个元素一一求取指数，即函数第i个元素的返回值应当为 {x_i}^{y_i} {x_i}^{y_i} 。 我们使用真实值 \\mathbf{y} \\mathbf{y} 作为指数给概率向量加权。当 \\mathbf{y}=1 \\mathbf{y}=1 时，以预测值为1的可信度作为概率；反之则以预测值为0的可信度作为概率。这就是最大似然估计方法。至此，我们可以写出似然估计函数 \\begin{align} L(\\boldsymbol{\\Theta}) = \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}). \\end{align} 对似然估计函数取对数，则有 \\begin{equation} \\begin{aligned} l(\\boldsymbol{\\Theta}) &= \\sum_{k=1}^N \\log \\left( \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}) \\right) \\\\ &= \\sum_{k=1}^N \\mathbf{y}^{(k)} \\cdot \\log\\left(\\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right) + \\left(1 - \\mathbf{y}^{(k)} \\right) \\cdot \\log\\left(1 - \\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right). \\end{aligned} \\end{equation} 我们最终的目的是要最大化似然函数，亦即 \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) ，这等价于最小化 -l(\\boldsymbol{\\Theta}) -l(\\boldsymbol{\\Theta}) 。对比 (7) (7) 和 (8) (8) ，于是我们可以定义交叉熵为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = \\mathbf{y} \\cdot \\log\\left( \\tilde{\\mathbf{y}} \\right) + \\left(1 - \\mathbf{y} \\right) \\cdot \\log\\left(1 - \\tilde{\\mathbf{y}} \\right). \\end{align} 若我们记 \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) ，代入sigmoid函数，为了确保该损失函数的稳定性，我们可以将 (13) (13) 整理为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{x}} \\right) = \\max(\\tilde{\\mathbf{x}}, \\mathbf{0}) - \\tilde{\\mathbf{x}} \\cdot \\mathbf{y} + \\log\\left(1 + e^{-|\\tilde{\\mathbf{x}}|} \\right). \\end{align} 提示 这里交叉熵整理的推导过程参见 Tensorflow-API官方文档 。 实际情况下，我们使用 (14) (14) 来求取sigmoid函数激活下的交叉熵。","text_tokens":["方法","i",")","求取","表示","一一","概率","交叉","整理","然而","反之","似然","含义","应当","对比","每个","记","这个","tilde","信度","只是","end","加权","元素","给","l","y","输出",",","(","向量","准确度","定义","一个","要","right","最大化","来","+","{","api","。","align",".","可信度","时","的","估计","-","为了","预测","e","aligned","即","个","结果","以","确保","limits","视","至此","文档","max","代入","可以","~","则","可信","log","表述","p","mathcal","却","程度","大化","等价","提示","将","平面","就是","equation","sum","left","\\","；","于是","^","boldsymbol","官方","begin","1","&","指数","损失","这里","theta","为","sigmoid"," ","b","=","并非","cdot","作为","稳定性","这","真实","in","返回值","于","n","arg","对","x","取","定性","最小化","我们","激活","最小","对数","13","预测值","函数","8","准确","返回","熵","，","mathbf","注意","使用","第","有","_","过程","tensorflow","k","7","表明","最大","是","当","若","亦","sigma","分类",";","}","其中","超平面","14","值","实际上","推导","目的","情况","分类器","确信","实际","参见","和","w","|","稳定","下","该","最终","写出","0"],"title":"交叉熵","title_tokens":["交叉","熵"]},{"location":"book-1-x/chapter-1/linear-classification/#_7","text":"","text_tokens":[],"title":"解线性多分类问题","title_tokens":["问题","线性","多","解","分类"]},{"location":"book-1-x/chapter-1/linear-classification/#_8","text":"建立一个具有较强可读性的Tensorflow工程需要我们活用python的模块化设计。我们通常推荐以下的结构 . ├─ data/ # where we store our data │ └─ ... ├─ tools.py # codes for post-processing and analyzing records. ├─ extension.py # codes for extending the tensorflow model. ├─ dparser.py # data parser └─ main.py # main module where we define our tensorflow model. 除了保存数据的文件夹，我们应当有三个子模块。其中 tool : 用来处理、分析生成的数据，通常与Tensorflow无关； extension : 用来扩展tensorflow，例如在这里自定义网络层和操作符； dparser : 数据处理器，用来读取并预处理送入网络的数据； main : 主模块，只定义跟Tensorflow模型有关的内容，需要引用 extension 和 dparser 。 视情况可以灵活调整结构，但建议将定义Tensorflow模型的代码单独放在主模块里，和其他外围代码分离。 撰写各个模块时，建议使用类封装各组功能相同的函数。具有良好使用习惯的coder应当注意给各个面向用户的类、函数撰写（哪怕简短的）说明文字，在一些较长的函数、方法的定义中，适当注释各部分的功能，以便读者能正确理解代码意义。 另外，在对象命名上，python有如下必须遵守或不成文的规定，和C/C++用户熟悉的蛇形命名法不同，它大致包括 类与函数多用驼峰命名法，变量可以采用驼峰或蛇形命名法。 驼峰命名法指的是用大小写区分每个单词块，例如 alphaBetaFunction () ； 蛇形命名法指的是用下划线区分每个单词块，例如 alpha_beta_function = 10 ； 宏变量使用全字大写+蛇形命名法 函数/方法，还有模块均是首字母小写，但类的首字母大写。 用单下划线 _ 表示临时存储器，或省略参数，例如一个函数 func () 有两个返回值时，可以用 _ , b = func () 表示我们只需要第二个返回值；单下划线还可以与星号连用省略多个返回值； 以单下划线开头的方法，表示模块级的私有方法，在模块以外使用 import 导入类时，不会导入这些方法，例如 def _alphaBeta ( self ): ； 以单下划线结尾的对象，用来和python的关键字区分，例如 func ( x , class_ ) ; 以双下划线开头的方法，如果不以双下划线结尾，则表示类级的私有方法，只有类内部的方法能调用这些方法，在类外部、包括继承的子类里都原则上不能调用（但其实也有办法调用），例如 def _alphaBeta ( self ): ； 以双下划线同时开头和结尾的方法，一般是用来 重写 (override) 特殊功能，例如 def __getattribute__ (): 将重写获得类属性的方法。","text_tokens":["方法","调用",")","表示","关键","子","法","self","大小写","网络层","特殊","放在","c++","├","正确理解","法指",":","类时","codes","私有","─","module","用来","）","活用","处理器","应当","以双","必须","各","其他","每个","内部","扩展","首字母","用户","星号","划线","func","级","model","给","and","能","不成文","变量","getattribute","关键字","采用","还有","各个","块","它","自定","(",",","与","def","如果","同时","省略","定义","封装","预处理","一个","网络","并","简短","正确","区分","引用","部分","+","大小","蛇形","。","coder","单词","只",".","都","但","时","import","不会","子类","模块","存储器","面向","的","tools","store","、","还","-","习惯","处理","文字","以便","其实","原则上","全字","视","分离","原则","post","define","可以","如下","c","10","则","大写","#","这些","较强","哪怕","获得","数据","模型","在","参数","调整","大致","送入","不以","开头","无关","将","也","python","件夹","自定义","外围","二个","跟","/","多个","不能","注释","里","读者","包括","相同","；","遵守","例如","字母","for","保存","两个","需要","存储","用单","一些","办法","这里","命名","灵活","生成","宏","连用","三个","类"," ","=","└","py","we","熟悉","下划","b","中","the","文件","内容","有关","具有","理器","返回值","小写","（","模块化","功能","上","调整结构","多用","驼峰","撰写","records","processing","主","可读性","代码","alpha","data","function","x","alphabeta","除了","外部","我们","成文","第二","...","override","单","tool","向用","通常","函数","较长","建议","面向用户","理解","extending","可读","对象","不同","返回","__","，","推荐","注意","以外","使用","第二个","只有","有","属性","_","结尾","下划线","tensorflow","用","临时","是","beta","类级","或","操作符","另外","class","均",";","设计","文件夹","重写","以下","工程","其中","单独","main","不成","读取","alphabetafunction","where","意义","以单","规定","导入","操作","双","结构","│","analyzing","适当","分析","建立","情况","extension","一般","和","良好","our","说明","dparser","parser","各组","继承","但类","首字"],"title":"代码规范","title_tokens":["规范","代码"]},{"location":"book-1-x/chapter-1/linear-classification/#_9","text":"在本项目里，我们不需要扩展Tensorflow。但是，我们需要以随机生成数据代替数据集。因此，首先，通过以下代码定义数据生成器 dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class TestDataSet : ''' A generator of the data set for testing the linear model. ''' def __init__ ( self , scale_x , A , c ): ''' Initialize the data generator. scale_x: the scale of input vector. A, c: the linear transformation. ''' self . s_x = 2 * scale_x self . A = A self . c = c self . len_x = A . shape [ 0 ] self . config () def config ( self , train = True , batch = 100 , noise = 0.1 ): ''' Configuration train: a flag for controlling the iterator mode. batch: the number of samples in a batch noise: std. of the error added to the y. ''' self . train = bool ( train ) self . batch = batch self . noise = noise def next_train ( self ): ''' Get the next train batch: (x, y) ''' x = self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) y = np . matmul ( x , self . A ) + self . c if self . noise > 1e-3 : y = y + np . random . normal ( 0 , self . noise , size = y . shape ) y = np . greater ( y , 0.0 ) . astype ( np . float32 ) return x , y def next_test ( self ): ''' Get the next test batch x. ''' return self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) def __iter__ ( self ): while True : samp = self . __next__ () yield samp def __next__ ( self ): if self . train : return self . next_train () else : return self . next_test () 该生成器输入一组 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，以及相关配置，之后就可以通过 迭代器 (iterator) 或 方法 (method) 随机生成数据。这种数据集写法我们在后面还会用到， model . fit 允许我们不是馈入样本（或样本批次），而是馈入一个 生成器(generator) 。因此我们重写了 __iter__ 方法，并使其通过 yield 返回一个生成器。这样我们定义的数据集类就可以被Keras的训练函数 model . fit 使用。接下来，调用如下测试代码： dparser.py 1 2 3 4 5 6 7 8 9 10 def test_dataset (): A = np . random . normal ( 0 , 10 , [ 10 , 6 ]) c = np . random . uniform ( 1 , 3 , [ 1 , 6 ]) dataSet = TestDataSet ( 10 , A , c ) dIter = iter ( dataSet ) for i in range ( 10 ): x , y = next ( dIter ) print ( np . sum ( y , axis = 0 ) / 100 ) test_dataset () Output [ 0.47 0.57 0.58 0.56 0.5 0.38 ] [ 0.6 0.61 0.47 0.48 0.38 0.52 ] [ 0.5 0.61 0.49 0.42 0.45 0.53 ] [ 0.59 0.52 0.44 0.44 0.49 0.51 ] [ 0.54 0.59 0.48 0.5 0.51 0.47 ] [ 0.49 0.57 0.56 0.49 0.53 0.4 ] [ 0.5 0.61 0.51 0.54 0.51 0.52 ] [ 0.5 0.51 0.61 0.5 0.44 0.5 ] [ 0.44 0.46 0.53 0.45 0.56 0.52 ] [ 0.52 0.46 0.51 0.52 0.49 0.44 ] 我们随机生成了 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的数据，每组数据100个，并且测试了10组。输出结果是各组测试中， \\mathbf{y} \\mathbf{y} 在对应维度上分类为1的概率估计。结果基本都在0.5左右，说明我们的这种数据生成模式产生的点能均匀分布在各个超平面两侧，适合进行后续测试。","text_tokens":["方法","i","astype","41",")","return","就","42","集类","调用","20","self","生成器","print","0.38","0.47","if","概率","定义数据","了","而是","dataset","0.58",":","axis","使","36","53","3","下来","35","'","）","接下来","mathbb","vector",">","0.53","34","r","扩展","18","0.59","configuration","左右","30","model","error","y","config","39","9","normal","首先","45","samp","能","21","100","46",",","np","用到","输出","testing","(","样本","对应","各个","适合","def","定义","47","52","24","linear","get","一个","并","0.42","number","17","项目","19","被","+","{","0.57","。","of","true",".","flag","都","54","a","shape","这样","的","批次","估计","-","49","并且","点","6","40","uniform","len","个","0.4","48","结果","以","0.1","train","每组","greater","float32","c","12","~","10","可以","如下","：","0.46","else","数据","27","0.61","组","在","scale","但是","15","43","0.5","不是","平面","32","controlling","0.0","均匀","random","batch","sum","/","input","0.54","后面","里","分布","\\","25","37","size","std","馈入","0.48","^","不","testdataset","for","31","需要","1","输入","2","26","模式","*","iterator","生成","为"," ","=","py","通过","noise","中","the","next","之后","bool","训练","其","33","测试代码","in","一组","fit","后续","（","上","samples","还会","代替","代码","器","38","data","x","基本","1e","output","我们","23","0.56","13","均匀分布","iter","while","函数","0.52","8","init","diter","写法","配置","mode","__","返回","0.51","，","因此","mathbf","50","method","使用","4","29","44","initialize","]","_","yield","28","0.6","tensorflow","22","added","7","允许","16","相关","test","range","是","51","0.45","或","迭代","本","class","to","mapsto","11","以及","set","分类","进行","}","重写","以下","超平面","14","0.44","集","维度","5","keras","成器","随机","transformation","接下","说明","dparser","generator","测试","两侧","各组","产生","该","[","这种","s","matmul","0.49","0"],"title":"数据生成","title_tokens":["生成","数据"]},{"location":"book-1-x/chapter-1/linear-classification/#_10","text":"顺序(sequential) 模型是一个单输入单输出模型，网络结构较为简单，也不存在跨层短接（残差连接）。在大多数情况下，已经上手的Tensorflow用户不使用这个模型，故而作为我们入门的第一个project，我们姑且用之，但我们将不再使用顺序模型来实现后续的project。一个顺序模型大致可以描述为下图的模式： graph LR st(输<br/>入) --> l1[层<br/>1] l1 --> l2[层<br/>2] l2 --> l3[层<br/>3] l3 --> ldots[层<br/>...] ldots --> ed(输<br/>出) classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,ed styStart 由于我们完成的是一个线性分类器，故而我们使用单层的序列模型即可。 接下来，我们来定义一个类， class LinClsHandle : 。定义一个类的时候，我们通常需要定义的内容包括 在初始化方法 __init__ 里定义传入网络的固定参数，例如学习速率，存取路径等； 在方法 construct 里定义网络的构造和使用的优化器； 在方法 train 里定义训练网络的过程，主要需要调用 model . fit 。如果我们在数据集的定义非常完善，则这一环节不需要花费太多的功夫； 在方法 test 里定义测试网络的过程，主要需要调用 model . evaluate 。如果有必要，可以通过 model . predict 返回测试结果。","text_tokens":["方法","调用",")","线性","之","网络结构",":","单层","3","简单","下来","）","接下来","速率",">","linclshandle","lr","这个","ldots","用户","model","姑且","输出","连接",",","(","输","stystart","如果","定义","一个","网络","来","。","初始",".","大多数","但","序列","fae6a9","的","上手","predict","classdef","-","路径","结果","故而","时候","train","非常","下图","可以","：","即可","construct","则","#","第一个","数据","模型","在","由于","参数","大致","也","将","残差","描述","fill","不再","学习","l1","/","包括","里","evaluate","功夫","；","例如","较为简单","不","跨层","输入","1","2","ed","模式","需要","必要","ba9132","为","太多","类"," ","通过","作为","st","内容","训练","这","后续","fit","（","顺序","器","l2","已经","花费","<","多数","我们","入","...","单","层","通常","graph","主要","init","sequential","__","返回","入门","，","大多","使用","有","]","实现","stroke","过程","tensorflow","短接","用","test","传入","是","构造","完善","完成","初始化","class","固定",";","较为","第一","分类","project","结构","l3","存在","存取","等","集","情况","分类器","和","接下","测试","一","br","出","下","优化","[","环节"],"title":"定义线性顺序模型","title_tokens":["定义","顺序","线性","模型"]},{"location":"book-1-x/chapter-1/linear-classification/#_11","text":"首先，定义初始化方法： lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 def __init__ ( self , learning_rate = 0.01 , epoch = 10 , steppe = 30 ): ''' Initialization and pass fixed parameters. learning_rate: the learning rate for optimizer. epoch: training epochs. steppe: steps per epoch ''' self . lr = learning_rate self . epoch = epoch self . steppe = steppe 由于目前我们的project还非常简单，这里只需要有学习速率( learning_rate )，轮次数( epoch )和每轮迭代次数( steppe )即可。","text_tokens":["方法",")","每轮","self",":","3","简单","'","速率","linclshandle","lr","steps","30","and","9","首先",",","(","def","定义","learning","initialization","。","只","初始",".","的","还","-","6","per","非常","：","10","即可","由于","lin","学习","epoch","for","rate","steppe","轮","需要","1","2","这里"," ","=","py","the","次数","epochs","cls","我们","pass","8","init","__","，","4","目前","有","_","7","非常简单","parameters","迭代","初始化","class","project","0.01","optimizer","5","和","training","fixed"],"title":"初始化方法","title_tokens":["方法","初始化","初始"]},{"location":"book-1-x/chapter-1/linear-classification/#_12","text":"接下来定义网络构造 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def construct ( self ): ''' Construct a linear model and set the optimizer as Adam ''' # Construction self . model = tf . keras . Sequential () self . dense1 = tf . keras . layers . Dense ( LABEL_SHAPE , use_bias = True , input_shape = ( INPUT_SHAPE ,), kernel_initializer = tf . keras . initializers . RandomNormal ( 0.0 , stddev = 10.0 ), bias_initializer = tf . keras . initializers . Constant ( 2 ), activation = None ) self . model . add ( self . dense1 ) # Set optimizer self . model . compile ( optimizer = tf . train . AdamOptimizer ( self . lr ), loss = self . loss , metrics = [ self . accuracy ] ) @staticmethod def loss ( y_true , y_pred ): return tf . nn . sigmoid_cross_entropy_with_logits ( labels = y_true , logits = y_pred ) @staticmethod def accuracy ( y_true , y_pred ): return tf . keras . backend . mean ( tf . keras . backend . equal ( y_true , tf . keras . backend . round ( tf . keras . backend . sigmoid ( y_pred )))) 须知 这里 LABEL_SHAPE 和 INPUT_SHAPE 为两个宏变量，分别为输出和输入的向量维度。 我们使用 Dense 定义全连接层，它的用法请参照 这里 。由于我们已经知道 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 可能的取值范围，这里我们重定义了 \\mathbf{W} \\mathbf{W} 和 \\mathbf{b} \\mathbf{b} 的初始化方式。 另外，注意我们这里构造网络的时候有如下技巧： 我们定义的网络输出是 \\mathbf{W}\\mathbf{x} + \\mathbf{b} \\mathbf{W}\\mathbf{x} + \\mathbf{b} ，而非 \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) 。这是因为我们需要通过还未被激活的输出用来计算sigmoid交叉熵，亦即式 (14) (14) ； 我们通过静态方法，直接调用Tensorflow自带的 sigmoid交叉熵 函数来作为Keras模型的损失函数 self..loss ； 我们通过静态方法，调用Keras的后端API，自己定义了预测准确度的测度函数 self.accuracy ； 我们将网络层的关键字 self.dense1 保留在了实例中，这是为了确保接下来我们能通过实例抽取该层的参数。 之所以煞费周折地进行这些处理，盖因为Keras的内建API里目前还没有提供对互不相斥的多分类的支持。例如，无论是 tf.keras.metrics.categorical_accuracy 还是 tf.keras.metrics.categorical_crossentropy ，都要求分类的真实值为one-hot类型的向量组，因而它们只适合用在softmax分类器上。为了解决这一问题，我们自己实现了sigmoid分类器。","text_tokens":["方法","调用",")","return","技巧","关键","多","该层","20","self","round","网络层","with","交叉","支持","了","initializers",":","未","抽取","下来","接下来","3","'","用来","静态","metrics","label","linclshandle","lr","之所以","bias","adamoptimizer","18","backend","model","测度","and","y","变量","能","9","而","分别","关键字","互不","类型","21","输出",",","连接","它","(","适合","use","向量","用法","def","准确度","定义","提供","linear","网络","24","17","来","被","19","知道","{","+","。","api","none","true","初始",".","都","只","@","a","shape","的","还","为了","-","预测","处理","6","layers","确保","labels","train","时候","如下","c","12","construct","10","：","#","范围","这些","组","即式","这一","construction","模型","在","参照","loss","由于","参数","15","将","lin","问题","这是","0.0","内建","nn","pred","重","input","里","\\","还是","无论","25","；","方式","randomnormal","例如","自带","constant","因为","两个","..","没有","请","1","输入","2","26","entropy","需要","静态方法","保留","是因为","categorical","crossentropy","要求","损失","这里","宏","须知","解决","as","为","周折","相斥","adam","sigmoid"," ","=","py","b","通过","staticmethod","作为","中","the","dense1","这","真实","add","logits","上","实例","无论是","accuracy","对","x","后","cls","已经","我们","所以","23","激活","13","层","函数","dense","8","准确","sequential","熵","因而","，","mean","mathbf","注意","使用","4","目前","有","10.0","]","直接","费周折","_","值为","它们","实现","stddev","tensorflow","22","7","用","16","计算","煞","是","可能","构造","kernel","初始化","tf","class","另外","亦","端","11","sigma","set","地","进行","}","分类","全","非","cross","14","optimizer","softmax","维度","hot","分类器","取值","5","activation","keras","和","盖","接下","w","initializer","equal","自己","[","one","compile"],"title":"构造方法","title_tokens":["方法","构造方法","构造"]},{"location":"book-1-x/chapter-1/linear-classification/#_13","text":"最后定义的式训练和测试方法。由于我们目前的project还比较简单，关于这两部分都直接调用现有的API即可。使用的API在之前已经说明。 model.fit 在没有额外设置的情况下，默认会返回一个 History回调器 ； model.evaluate 返回的是测试样本给出的损失函数和准确值测度。 model.predict 返回的是测试样本给出的网络输出。详情请参照 顺序模型API 。 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def train ( self , dataSet ): ''' Use a data set to train the network. ''' return self . model . fit ( dataSet , epochs = self . epoch , steps_per_epoch = self . steppe ) def test ( self , data , labels ): ''' Use (data, label) pairs to test the results. ''' loss , accu = self . model . evaluate ( data , labels ) print ( 'Evaluated loss =' , loss ) print ( 'Evaluated accuracy =' , accu ) return self . model . predict ( data )","text_tokens":["方法","调用",")","return","self","print","dataset","pairs",":","3","简单","'","式","label","linclshandle","steps","model","之前","测度","accu","9","输出",",","样本","(","最后","use","关于","def","详情","定义","一个","网络","详情请","部分","api","。",".","都","设置","a","的","还","predict","-","回调","6","per","labels","train","12","即可","10","模型","results","在","由于","参照","loss","lin","history","epoch","evaluate","；","steppe","没有","1","比较","2","额外","损失","现有"," ","=","py","默认","the","训练","这","fit","evaluated","顺序","会","给出","accuracy","器","epochs","data","已经","cls","我们","13","函数","8","比较简单","准确","返回","，","使用","目前","4","直接","_","7","test","是","network","class","to","11","set","project","14","值","两","情况","测试方法","5","和","说明","测试","下"],"title":"训练和测试方法","title_tokens":["方法","测试","测试方法","训练","和"]},{"location":"book-1-x/chapter-1/linear-classification/#_14","text":"首先，训练网络。我们随机生成 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的线性变换，并且设置好数据集，给定噪声扰动为 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 。设定20个epoch，每个epoch迭代500次，每次馈入32个样本构成的batch，然后开始训练： lin-cls.py 1 2 3 4 5 6 7 8 A = np . random . normal ( 0 , 10 , [ INPUT_SHAPE , LABEL_SHAPE ]) c = np . random . uniform ( 1 , 3 , [ 1 , LABEL_SHAPE ]) dataSet = dp . TestDataSet ( 10 , A , c ) dataSet . config ( batch = 32 , noise = 0.1 ) # Construct the model and train it. h = LinClsHandle ( learning_rate = 0.01 , epoch = 20 , steppe = 500 ) h . construct () record = h . train ( iter ( dataSet )) Output Epoch 1 /20 500 /500 [==============================] - 3s 5ms/step - loss: 48 .2269 - accuracy: 0 .5458 Epoch 2 /20 500 /500 [==============================] - 1s 2ms/step - loss: 25 .5149 - accuracy: 0 .6491 Epoch 3 /20 500 /500 [==============================] - 1s 2ms/step - loss: 11 .9822 - accuracy: 0 .7607 Epoch 4 /20 500 /500 [==============================] - 1s 2ms/step - loss: 5 .6580 - accuracy: 0 .8513 Epoch 5 /20 500 /500 [==============================] - 1s 2ms/step - loss: 2 .7230 - accuracy: 0 .9106 Epoch 6 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .1082 - accuracy: 0 .9462 Epoch 7 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .3278 - accuracy: 0 .9708 Epoch 8 /20 500 /500 [==============================] - 1s 3ms/step - loss: 0 .0618 - accuracy: 0 .9878 Epoch 9 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0149 - accuracy: 0 .9963 Epoch 10 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9979 Epoch 11 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0124 - accuracy: 0 .9976 Epoch 12 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9978 Epoch 13 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9973 Epoch 14 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0120 - accuracy: 0 .9974 Epoch 15 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0121 - accuracy: 0 .9970 Epoch 16 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0116 - accuracy: 0 .9971 Epoch 17 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0120 - accuracy: 0 .9967 Epoch 18 /20 500 /500 [==============================] - 1s 3ms/step - loss: 0 .0114 - accuracy: 0 .9971 Epoch 19 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0114 - accuracy: 0 .9969 Epoch 20 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0113 - accuracy: 0 .9970 接下来，从训练返回的 History 类型的回调器中抽取对loss和accuracy的记录。 lin-cls.py 1 2 showCurve ( record . epoch , record . history [ 'loss' ], xlabel = 'epoch' , ylabel = 'Cross entropy' , log = True ) showCurve ( record . epoch , record . history [ 'accuracy' ], xlabel = 'epoch' , ylabel = 'Accuracy' ) Output 重新设定数据集的产生方式，变为每个batch含10个样本。使用这组重新随机生成的数据测试网络输出， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 12 # Generate a group of testing samples: dataSet . config ( batch = 10 ) x , y = next ( dataSet ) # Check the testing results yp = dp . sigmoid ( h . test ( x , y )) _ , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 ) ax1 . imshow ( y , interpolation = 'nearest' , aspect = 'auto' ) ax1 . set_title ( 'True class' ) ax2 . imshow ( yp , interpolation = 'nearest' , aspect = 'auto' ) ax2 . set_title ( 'Predicted class' ) plt . gcf () . set_size_inches ( 10 , 5 ), plt . show () Output 注意我们未对测量的结果阈值化，因此显示出来的测量结果和理想值略有差别，但从图可知，阈值化后则测量结果全部准确。 通过抽取 h.dense1 的参数，我们可以对比 \\mathbf{A} \\mathbf{A} 和 \\mathbf{W} \\mathbf{W} ，以及 \\mathbf{c} \\mathbf{c} 和 \\mathbf{b} \\mathbf{b} ， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 # Check the regressed values W , b = h . dense1 . get_weights () plt . imshow ( A , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'A' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . imshow ( W , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'W' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . plot ( c . T , label = 'c' ) plt . plot ( b . T , label = 'b' ) plt . legend () plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () Output 可以发现，虽然我们训练的分类器十分有效，但其权值和预期的 \\mathbf{A} \\mathbf{A} , \\mathbf{c} \\mathbf{c} 并不相同。这是由于我们训练的样本加入了噪声。这种技术常用于神经网络的训练，被认为是一种提高鲁棒性、减小过拟合、避免不稳定解的一个有效手段。可以看出真实值 \\mathbf{A} \\mathbf{A} 存在偏高值，但 \\mathbf{W} \\mathbf{W} 的数值更加均匀。","text_tokens":["gca","0116",")","0114","线性","6580","2ms","interpolation","nearest","20","title","show","aspect","了","dataset","t","9976",":","抽取","未","全部","varepsilon","5458","3","mathbb","下来","接下来","9963","'","plot","label","1082","对比","linclshandle","可知","含","step","每个","一种","weights","r","9979","predicted","18","generate","model","and","y","normal","config","9","类型","首先","记录","2269","7607","输出","0618","np",",","sim","9878","样本","(","testing","化后","出来","9822","regressed","技术","inches","开始","数据测试","认为","一个","网络","learning","略有","get","17","手段","9708","19","被","{","。","true","of",".","设置","9971","差别","但","看出","a","shape","的","ax1","发现","权值","阈值","好","、","-","这组","数值","并且","回调","理想","6","有效","uniform","个","变换","48","结果","0.1","给定","拟合","train","神经网","均匀","record","yp","c","~","：","10","construct","12","#","log","则","plt","可以","数据","mathcal","9106","results","参数","loss","由于","偏高值","15","构成","6491","lin","imshow","32","5ms","9973","ax2","0113","xlabel","h","random","batch","history","显示","这是","auto","epoch","/","input","重新","\\","8513","values","相同","25","方式","^","size","馈入","boldsymbol","并不相同","testdataset","rate","steppe","从","不","1","2","更加","9462","9969","每次","entropy","线性变换","9967","加入","生成","为","7230","sigmoid","colorbar"," ","=","py","0120","noise","通过","b","神经","it","the","next","训练","dense1","其","真实","于","n","5149","然后","samples","accuracy","3278","化","对","x","cls","3ms","output","变为","我们","0149","13","iter","9970","500","8","0121","0124","准确","返回","虽然","，","mathbf","gcf","注意","使用","因此","避免","4","dp","解","]","_","legend","9978","7","设定","16","check","常用","test","是","测量","减小","迭代","预期","class","神经网络","提高","mapsto","1s","11","set","以及","分类","}","group","showcurve","0.01","cross","14","值","噪声","存在","集","鲁棒性","次","扰动","分类器","图","5","和","十分","3s","过","随机","接下","器中","subplots","测试","w","稳定","产生","[","ylabel","这种","9974","0"],"title":"调试","title_tokens":["调试"]}]}