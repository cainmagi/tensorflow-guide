{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"扉页 ¶ 摘要 Tensorflow总纲，写给初学者们。本页面将简述Tensorflow的基本原理，结构设计以及版本更新。读者亦可在本页找到本教程将提供给读者对Tensorflow怎样的认识。 注意 由于技术限制，现在中文搜索功能无法完善，请注意当搜索关键词的时候自行分词，例如： 开放源代码软件库 无法搜到任何内容。但可以搜索 开放 源代码 软件 库 即可确保索引到上述内容。 Tensorflow总说 ¶ Tensorflow官网 TensorFlow™ 是一个开放源代码软件库，用于进行高性能数值计算。借助其灵活的架构，用户可以轻松地将计算工作部署到多种平台（CPU、GPU、TPU）和设备（桌面设备、服务器集群、移动设备、边缘设备等）。TensorFlow™ 最初是由 Google Brain 团队（隶属于 Google 的 AI 部门）中的研究人员和工程师开发的，可为机器学习和深度学习提供强力支持，并且其灵活的数值计算核心广泛应用于许多其他科学领域。 Tensorflow治学 ¶ 写在所有内容之前，读者不得不看以下几个页面，本教程所述内容大略来自于对这些资料的研读。 Tensorflow官网(中/英文) : https://www.tensorflow.org/ Keras中文文档 : https://keras-zh.readthedocs.io/ Tensorflow 2.0前瞻(英文) : https://medium.com/tensorflow/effective-tensorflow-2-0-best-practices-and-whats-changed-a0ca48767aff Tensorflow 2.0前瞻(中文) : https://zhuanlan.zhihu.com/p/50049041 提示 特别值得注意的是，现在官方文档至少在教程部分， 已经支持中英双语 。相比官方文档，我们的教程更侧重于以搭建工程为导向的设计。然而现在官方文档的翔实可读程度，确实不可错过。另一方面，Keras的 官方中文文档 似乎已不再更新，上面提供的原官方中文文档译者现在正在更新、维护的版本。 大略来说，学习Tensorflow主要应当依赖于官方文档的介绍。敝人自r1.4版开始入门Tensorflow，即是通过学习官方文档的教程来快速上手。相比四处蒐集资料，官方文档能提供一个完整、系统、完全贴合时下最新API的例子，帮助用户建立一个与Tensorflow各个功能合宜的使用习惯。若是通过在Github上检索他人的project，固然是一个很好的上手办法，但是往往就会遗漏某些重要的功能而不学。例如，Tensorflow自带的网络存取和Tensorboard API，在时兴的一些project中，经常会因方便之故，以numpy的IO来代替。笔者以为，如此培养起来的使用习惯，可谓走入偏门。当然，从这一点来说，本教程也亦复如是，即使笔者写下这些内容的时候，参照的乃是时下最新的API，时间一久，难免会过时。倘若笔者懒惰一些，不再时时更新本教程，那么本教程也就入不足取之流了。 虽然如此，教程也有并不合宜的地方。最大的问题莫过于Tensorflow本身，它有着日新月异的变化，此时能圆转如意地使用的API，到了彼时也许就成了废案。这就导致Tensorflow的官方教程也不断更新。r1.4时，教程还主要集中在如何使用“底层”API上，到了r1.9.0，就已经变成基本围绕着 tf.keras 设计的思路了。最新消息显示，即将上线的Tensorflow 2.0，将会彻底抛弃过去的“中层”API以下的全部方法， tf.layers , tf.contrib 都将被移除，倘若长期墨守成规，局限在入门时候的一套技术上，迟早会被官方库如此迅速的更新所弃。这也是无可奈何之事。毕竟Tensorflow仍然处于不断改进之中。倘若希望追求一个更加稳定、长期可用的库，keras或许是一个不错的选择。 故而，本教程将基本基于Tensorflow现在的版本(r1.13)展开介绍。本教程既可以看作一个入门教程，也可以看作是本人重新自学新版Tensorflow规范化API的一个手记，故而命名本教程为“手札”。读者不妨跟着笔者的思路，有如与笔者共学一般读下来本教程。 Tensorflow原理 ¶ 一个标准的Tensorflow工作流可以表示成这样： graph TD st(开始) --> Sess[启动Session] Sess --> ConNet[构建网络] subgraph 构造流程 ioNet>读取网络参数] --> ConNet end subgraph 执行流程 ioDat>导入数据] --> Run ConNet --> Run[执行网络] Run --> ioNetS>保存网络参数] Run --> ioRes>导出结果] end ioRes --> SessCl[关闭Session] SessCl --> ed(结束) classDef styStart fill:#FAE6A9,stroke:#BA9132; classDef styIO fill:#cde498,stroke:#13540c; class st,ed styStart class ioNet,ioNetS,ioDat,ioRes styIO 与一般的计算库不同，Tensorflow的执行流程大体可以分为两步： 构造流程 : 在这一步，Tensorflow根据用户代码构造一个 数据流图(dataflow graph) 。所谓数据流图，指的是由一系列 张量(Tensor) 构成的符号运算集合。就如同一张流程图一样，在这一阶段，尽管用户定义了每一步的运算（从简单的加减乘除到复杂的网络单元），但是没有任何运算被执行。就像一个程序员撰写代码一样，Tensorflow在这一步，将用户的代码转换成它的“机器语言”，但是网络还没有进入被使用的阶段。 执行流程 : 在这一步，Tensorflow将计算用户指定的某个 Tensor 的输出结果。要得到一个Tensor的输出，则必须得计算它一系列的依赖变量。例如，我们已知 y = x_1 + x_2 y = x_1 + x_2 , x_2 = z_1 \\times z_2 x_2 = z_1 \\times z_2 。那么，如果Tensorflow要得到 y y 的结果，它就必须先计算 x_2 x_2 。这个过程被完全地封装起来，从用户看来，我们只需要调用 y y 的输出即可，不需要关心Tensorflow是怎样按照流图完成计算的。 因此，典型的Tensorflow式的语言风格也可以这样划分： 构造流程 : 用户定义整个网络的符号运算，指定网络各个节点的属性、输入和输出。这些代码往往被写成一个函数（例如 def construct (): ） 执行流程 : 创建一个Session，在Session内调用构造函数，然后输入数据，得到并保存输出结果。必要情况下，还需要导入导出网络参数。 这里提到 会话(Session) 。Session如同一个Tensorflow虚拟机，在一个Session打开的时候，设备的计算资源（GPU, CPU, 带宽等）才被加载。Session又如同一个工人，用户撰写的网络构造代码如同车间，输入的数据如同商品，“工人（Session）”利用预定义好的“车间（流图）”将可以快速、批量地生产这些“商品（数据）”。这样地设计保证了一些外围的代码，例如指定网络参数的名称、函数的属性等操作，不需要重复进行，从而确保处理数据的时候，总是执行必要的代码，加快运算速度。 另一方面，构造-执行的结构还封装了许多提高运算效率的特性，例如多线程。尽管用户定义网络构造的时候，所写的代码是按照逻辑顺序的、线性的。但实际运行的时候，Tensorflow可以自行发现网络的哪些部分可以被同时运行，从而利用多核系统的计算资源。这些过程也是被完全封装起来的，用户并不需要花费心思去专门进行这些调整。 须知 在Tensorflow已经转变为Keras导向的现在，Session的调用被Keras API封装了起来，从用户的角度来看，现在已经不再需要手动调用Session。 Tensorflow API架构 ¶ 下图显示了当前Tensorflow-API的组织形式 Tensorflow在多个平台上均有部署，包括Python, C++, Java, Javascript, Go等，未来还可能支持更多的语言。然而，不同平台上，API的使用方式和代码风格是大不相同的。例如，Python的语言风格目前已经朝向Keras转变，而Javascript仍倾向于使用中层API；C++和Java偏向于使用底层API编写程序。本教程只针对时兴Python的API编写，除非涉及到特定的专题，不会讨论其他语言上Tensorflow的用法。 一般而言，Tensorflow将API面向划分为三个层次： 高层API (High level) : 包括Estimators和Keras； 中层API (Mid level) : 包括layers, datasets, loss和metrics等具有功能性的函数，例如网络层的定义，Loss Function，对结果的测量函数等； 底层API (Low level) : 包括具体的加减乘除、具有解析式的数学函数、卷积、对Tensor属性的测量等。 从r1.4入门的用户，所接受的训练往往是从底层API开始，使用自己的代码风格构建对应中层API的函数，然后再用自己定义好的中层API构建网络。在这种代码风格的驱使下，用户除了需要定义各个网络层具体的表达式，还需要设计输入输出的接口，用来调用 优化器(optimizer) 的接口等操作。 另一种使用方式，是从中层API开始，直接使用预定义好的网络层构建网络，这样的代码风格会节省一部分时间，相当于几乎不再需要接触底层API，但是在输入输出、网络训练和测试等更高层的模块设计上，和底层API用户的代码风格相似。 Estimators是从r1.4就已经存在的API了，它可以被看成是已经集成在Tensorflow里的完整的网络。因此，Estimators往往适合分发、需求不高的应用，但是并不适合使用Tensorflow的研究者。尽管至今为止，Tensorflow仍然对Estimators倍加推崇，但本教程将完全不涉及这方面的内容。 另一个高层API是 Eager Execution ，从r1.9起，Tensorflow就加入了这种新的使用模式，并且在Tensorflow 2.0中，它仍然将会被保留。Eager摈弃了上述的构造-执行流程，任何对Tensor的定义会被立时计算、并可得到结果。对于需要进行小规模的调试、检查API功能的用户而言更加方便。但是，据一些用户的反响，现在Eager还没有做到和之前API的完全兼容，本教程将基本不涉及Eager的用法。 因此，Keras API将是本教程的重中之重。使用本教程的用户，可以对照目前的官方文档，跟进本教程的思路，自己逐步实现各个project的设计。使用Keras风格的设计，代码量会明显少于前面提到的各种风格（Estimators除外）。事实上， tf.keras 和底层API之间具有一定的兼容，这样的编写风格能让我们在满足自己设计的派生功能的基础上，尽可能使用规范化、预定义的API单元，从而减少代码出错的可能性。通过对本教程TF 1.x版的学习，对未来2.0版的上手也会带来莫大帮助。 金宇琛( @cainmagi )，2019年3月2日 Yuchen Jin( @cainmagi ), Mar. 2, 2019 教程导读 ¶ 接下来，本教程将会涉及 从线性问题入门 : 如何使用Tensorflow完成一个简单的线性分类设计，我们将从Hello World开始，逐步过渡到一个具有核函数的非线性问题project。 Hello world: 第一个Tensorflow程序。 线性分类: 一个简单的二分类问题。 线性回归: 一个同样简单的，线性拟合问题。 非线性回归: 拟合一个简单的，可以表达出解析式的非线性函数。 非线性分类: 使用线性分类器对非线性分布的数据进行分类。 训练与测试分立 : 从这一章开始，并试图解决几个更加复杂的实际问题。这里涉及到的project相对更大、更完整，训练往往需要一定时间才能完成。因此，在本章，所有项目的训练、测试环节都会分开，我们将从这里开始，使用TensorBoard跟踪我们的训练情况，并介绍如何存取神经网络。 Super Resolution: 使用神经网络进行图像像素插值。 Sparse Coding: 使用线性的词典学习(dictionary learning)来进行图像像素插值。 Generitive Model: 使用时兴的生成模型(Generitive Model)来完成图像风格的转换。 原生的数据管理 : 从这一章开始，我们将纳入Tensorflow自带的数据管理API。实际上，很多用户并不习惯使用这些API，往往倾向于自己完成数据的导入导出。虽然这并非一个复杂的工作，但据Tensorflow的文档，原生的数据管理内部实现了多线程，本身更适合用于提高数据IO的效率。读者可以酌情选择是否需要阅读本章。 使用MINIST数据集: 使用原生的数据管理来IO一个现成的MINIST数据集，这一节改自 官方文档教程 。 使用RNN处理文章: 通过LSTM模型来进行文本分类，并使用原生的数据管理来IO一个现成的IMDB数据集，这一节改自 官方文档教程 。 使用RBM生成音频: 通过限制玻尔兹曼机(RBM)来编写一个简单的音乐生成器，这一节改自 llSourcell/Music_Generator_Demo 。 其他的高级技巧 : 本章将介绍一些特殊的、利用底层API技巧，使得用户能更灵活地定制Tensorflow-keras，从而实现一些仅依靠原生库不能实现的功能。","text_tokens":["www","数学","指定","输入","集合","一个","提到","利用","库","原","构设","官方","开始","出","系列","一系列","存在","一步","这方","resolution","方式","版","页面","然后","第一个","成规","双语","翔实","本人","minist","读者","例如","指","非线性","/","low","跟踪","同样","通过","得到","loss","很多","本页","时候","神经网","读","可能","创建","生成器","一些","规范化","之间","导出","当然","网络层","器","lstm","稳定","均","，","不断","最新消息","被","ba9132","imdb","已经","手动","线程","平台","之中","式","与","部分","着","数值","明显","足取","music","practices","维护","计算资源","参照","外围","打开","最新","速度","generitive","派生","回归","也","多","抛弃","这些","写","cpu","人员","系统","源代码","iodat","成","或许","又","td","小规模","generator","成器","¶","标准","编写程序","立时","时间","方法","导向","能圆转","函数","资料","至今","移除","定义","除了","核心","无法","不妨","即可","预定","全部","兼容","一方","选择","风格","并","用来","大体","的","高性能","移动","集中","cde498","fae6a9","subgraph","所弃","看","图","加入","手","4","定制","mar","进行","所有","当于","功能","如同","13540c","给","第一","内容","不","偏门","estimators","神经","错过","建立","多线","环节","模型","灵活","cainmagi","桌面","故而","新版","不高","值得注意","入","sesscl","如此","科学","注意","玻尔","当前","追求","义","操作","层次","总说","墨守成规","另一方面","隶属于","medium","义好","笔者","守成","以","保存","商品","本章","tensorflow","跟着","0","ionet","1","表示","自己","涉及","但是","带宽","来自","检查","酌情","工程师","layers","变成","专题","机器语言","来看","learning","车间","再用","更","参数","而言","月","可","所","大略","批量","stroke","是从","以为","研究者","学","方面","驱使","保留","另","二","加减","固然","据","分立","可以","至少","分开","流程","github","按照","倍加","用户","治学","进入","完全","程序员","构成","-","接下","扉页","无可","从这一点","虚拟","eager","须知","但据","符号","high","分类","分类器","使得","赖于","官网","接受","方便","许多","处于","命名","加减乘除","展开","但","前瞻","session","下图","生成","原生","玻尔兹曼","a0ca48767aff","需要","多个","阅读","llsourcell","和","（","显示","实际","优化","connet","限制","基于","并不需要","不合宜","解析","摈弃","另一方","应用","9.0","处理","特性","写给","架构","读取","中文","编写","确保","功能性","务器","未来","这种","设备","依赖","下来","办法","试图","既","部门","最大","共","墨守成","加载","现成","性能","依赖于","更新","加快","模块","敝",";","可能性","学习","每","结构设计",":","形式","内部","同时","网络","从而","入门教程","问题","看成","r1","一张","搜","numpy","启动","google","c++","从","tensor","为","资源","def","之故","y","学者","机器","找到","往往","无可奈何","简述","思路","范化","生产","rbm","之流","金宇琛","运算","莫过","输入输出","等","这个","初学","规范","结束","似乎","两步","需求","事实","一种","rnn","讨论","音乐","入门","execution","先","一样","角度","节点","高层","贴合","情况","帮助","由于","会话","也许","执行","结果","基本原理","了","文本","13","若","彼时","项目","多种","索引","其他","包括","节省","彻底","遗漏","整个","可用","io","上述","我们","过于","不错","只","level","api","仍然","借助","使用","一方面","\\","所谓","一部","过程","经常","graph","+","起","并非","看来","这里","它","像素","可谓","重新","拟合","go","]","输出","@","coding","完成","对照","不再","原理","总是","三个","这","下","例子","optimizer","本原","减少","如何","迟早会","不得","工程","2019","以及","接下来","构造函数","一般而言","yuchen","java","某些","要","fill","相当于","封装","来说","mid","快速","部署","新月","过去","隶属","hello","过渡","到","工作","metrics","少于","倾向","一定","事实上","2.0","变量","文档","底层","tensorboard","强力","花费","摘要","迟早","™","各种","集成","不会","对","一节","上面","自带","手札","https","即将","iores","ai","）","更大","关键词","2","心思","[","表达式","神经网络","best","stystart","根据","9","然而","去","关键","能","墨守","。","手记","st","前面","相对","典型","function","应当","集群","即使","起来","ed","在","表达","看作","具体","莫过于","分为","project","教程","规模","为止","不合","除外","所写","不可","深度","测量","时时","所述","brain","转换成","已知","流图","核","插值","设计","各个","研究","有如","集","对本","技巧","蒐集","特殊","实际上","过时","变化","研读","and","发现","卷积","实现","python","纳入","关闭","转换","都","复杂","接触","网络单元","四处","直接","tf","边缘","仍","调试","已","难免会","那么","像","jin","效率","总纲","哪些","几个","contrib","并且","写下","tpu","习惯","classdef","readthedocs","亦可","名称","x","模式","当","局限","阶段","目前","一章","whats","流程图","dataflow","存取","调整","提供","end","组织",">","培养","构造","满足","提示","基本","自行","是否","广泛应用","轻松","服务","朝向",")","时兴","times","一套","相同","、","用法","基础","地方","：","软件","是","构建","重要","会因","内","属性","数据管理","“","特别","奈何","尽可能","怎样","具有","changed","亦复如是","其","达式","这样","重中之重"," ","不断更新","上线","改进","这方面","认识","迅速","construct","必要","相似","zh","=","做到","日","词典","因此","之前","确实","里","消息","不得不","单元","出错","代替","即","但本","javascript","导入","自","才能","此时","检索","译者","一点","”","来","不断改进","转变","不能","新","英文","主要","日新月异","搭建","事","一般","程序","侧重于","久","虽然","调用","语言","datasets","分词","z","换成","文章","流","一系","介绍","面向","倘若","某个","地","自学","super","重复","广泛","划分","值得","model","由","必须","上","p","正在",",","effective","以下","zhuanlan","如意","如是","才","中层","关心","推崇","乘除","完善","更加","初学者","运行","不足","图像","人","懒惰","不学","难免","结构","会","本身","简单","请","改","很","逐步","styio",".","之","开发","适合","支持","于","量会","分布","得","训练","不同","专门","提高","一","让","50049041","他人","class","虚拟机","多核","zhihu","一部分","莫大","走入","sparse","们","尽可","高级","没有","org","代码","长期","偏向","导读","测试","不足取","dictionary","现在","对应","音频","分发","用于","重于","3","开放","数据流","相当","_","接口","解决","兹曼","英双语","侧重","年","demo","工人","com","线性","乃是","尽管","将会","对于","希望","数据","而","有着","属于","#","高性","除非","技术","中","时","写成","撰写","带来","特定","则","管理","计算","sess","合宜","围绕","针对","本","任何","ionets","毕竟","顺序","keras","版本","就","好","还","有","最初","world","仅","反响","程度","将","跟进","；","相比","中文搜索","服务器","废案","几乎","run","领域","导致","完整","(","团队","多线程","如果","保证","大不相同","机","逻辑","依靠","张量","搜索","gpu"],"title":"扉页","title_tokens":["扉页"]},{"location":"#_1","text":"摘要 Tensorflow总纲，写给初学者们。本页面将简述Tensorflow的基本原理，结构设计以及版本更新。读者亦可在本页找到本教程将提供给读者对Tensorflow怎样的认识。 注意 由于技术限制，现在中文搜索功能无法完善，请注意当搜索关键词的时候自行分词，例如： 开放源代码软件库 无法搜到任何内容。但可以搜索 开放 源代码 软件 库 即可确保索引到上述内容。","text_tokens":["：","软件","。","技术","由于","，","注意","怎样","限制","分词","库","搜","到","可以","原理","在","构设","无法","写给","本"," ","学者","任何","基本原理","中文","总纲","本原","找到","们","认识","摘要","教程","版本","简述","tensorflow","确保","以及","代码","索引","的","对","页面","亦可","上述","将","初学","现在","当","关键","读者","例如","中文搜索","更新","提供","开放","完善","设计","初学者","功能","关键词","但","给","结构设计","基本","自行","内容","本页","结构","时候","源代码","请","搜索","即可"],"title":"扉页","title_tokens":["扉页"]},{"location":"#tensorflow","text":"Tensorflow官网 TensorFlow™ 是一个开放源代码软件库，用于进行高性能数值计算。借助其灵活的架构，用户可以轻松地将计算工作部署到多种平台（CPU、GPU、TPU）和设备（桌面设备、服务器集群、移动设备、边缘设备等）。TensorFlow™ 最初是由 Google Brain 团队（隶属于 Google 的 AI 部门）中的研究人员和工程师开发的，可为机器学习和深度学习提供强力支持，并且其灵活的数值计算核心广泛应用于许多其他科学领域。","text_tokens":["软件","是","一个","可以","库","其","用户"," ","工程","官网","许多","部署","和","（","隶属","，","到","工作","应用","地","架构","平台","广泛","强力","™","务器","由","设备","数值","部门","ai","性能","）","学习","cpu","人员","源代码","开发","。","支持","于","集群","google","核心","为","机器","代码","高性能","的","移动","等","深度","brain","用于","开放","进行","研究","灵活","桌面","属于","高性","中","科学","计算","边缘","隶属于","tensorflow","多种","并且","tpu","其他","最初","将","服务器","借助","工程师","提供","领域","团队","广泛应用","轻松","服务","可","、","gpu"],"title":"Tensorflow总说","title_tokens":["总说","tensorflow"]},{"location":"#tensorflow_1","text":"写在所有内容之前，读者不得不看以下几个页面，本教程所述内容大略来自于对这些资料的研读。 Tensorflow官网(中/英文) : https://www.tensorflow.org/ Keras中文文档 : https://keras-zh.readthedocs.io/ Tensorflow 2.0前瞻(英文) : https://medium.com/tensorflow/effective-tensorflow-2-0-best-practices-and-whats-changed-a0ca48767aff Tensorflow 2.0前瞻(中文) : https://zhuanlan.zhihu.com/p/50049041 提示 特别值得注意的是，现在官方文档至少在教程部分， 已经支持中英双语 。相比官方文档，我们的教程更侧重于以搭建工程为导向的设计。然而现在官方文档的翔实可读程度，确实不可错过。另一方面，Keras的 官方中文文档 似乎已不再更新，上面提供的原官方中文文档译者现在正在更新、维护的版本。 大略来说，学习Tensorflow主要应当依赖于官方文档的介绍。敝人自r1.4版开始入门Tensorflow，即是通过学习官方文档的教程来快速上手。相比四处蒐集资料，官方文档能提供一个完整、系统、完全贴合时下最新API的例子，帮助用户建立一个与Tensorflow各个功能合宜的使用习惯。若是通过在Github上检索他人的project，固然是一个很好的上手办法，但是往往就会遗漏某些重要的功能而不学。例如，Tensorflow自带的网络存取和Tensorboard API，在时兴的一些project中，经常会因方便之故，以numpy的IO来代替。笔者以为，如此培养起来的使用习惯，可谓走入偏门。当然，从这一点来说，本教程也亦复如是，即使笔者写下这些内容的时候，参照的乃是时下最新的API，时间一久，难免会过时。倘若笔者懒惰一些，不再时时更新本教程，那么本教程也就入不足取之流了。 虽然如此，教程也有并不合宜的地方。最大的问题莫过于Tensorflow本身，它有着日新月异的变化，此时能圆转如意地使用的API，到了彼时也许就成了废案。这就导致Tensorflow的官方教程也不断更新。r1.4时，教程还主要集中在如何使用“底层”API上，到了r1.9.0，就已经变成基本围绕着 tf.keras 设计的思路了。最新消息显示，即将上线的Tensorflow 2.0，将会彻底抛弃过去的“中层”API以下的全部方法， tf.layers , tf.contrib 都将被移除，倘若长期墨守成规，局限在入门时候的一套技术上，迟早会被官方库如此迅速的更新所弃。这也是无可奈何之事。毕竟Tensorflow仍然处于不断改进之中。倘若希望追求一个更加稳定、长期可用的库，keras或许是一个不错的选择。 故而，本教程将基本基于Tensorflow现在的版本(r1.13)展开介绍。本教程既可以看作一个入门教程，也可以看作是本人重新自学新版Tensorflow规范化API的一个手记，故而命名本教程为“手札”。读者不妨跟着笔者的思路，有如与笔者共学一般读下来本教程。","text_tokens":["www","一个","库","原","官方","开始","版","页面","成规","双语","翔实","本人","读者","例如","/","通过","时候","读","一些","规范化","当然","稳定","，","不断","最新消息","被","已经","之中","与","部分","着","足取","practices","维护","参照","最新","也","抛弃","这些","写","系统","成","或许","时间","方法","导向","能圆转","资料","移除","不妨","全部","一方","选择","并","的","集中","所弃","看","手","4","所有","功能","内容","偏门","错过","建立","故而","新版","值得注意","入","如此","注意","追求","墨守成规","另一方面","medium","笔者","守成","以","tensorflow","跟着","0","但是","来自","layers","变成","更","可","大略","以为","学","方面","固然","可以","至少","github","用户","完全","-","无可","从这一点","赖于","官网","方便","处于","命名","展开","前瞻","a0ca48767aff","和","显示","基于","不合宜","另一方","9.0","中文","依赖","下来","办法","既","最大","共","墨守成","依赖于","更新","敝","学习",":","网络","入门教程","问题","r1","numpy","为","之故","往往","无可奈何","思路","范化","之流","莫过","规范","似乎","入门","贴合","帮助","也许","了","13","若","彼时","彻底","遗漏","可用","io","我们","过于","不错","api","仍然","使用","一方面","经常","它","可谓","重新","不再","这","下","例子","如何","迟早会","不得","工程","某些","来说","快速","新月","过去","到","2.0","文档","tensorboard","迟早","对","上面","自带","手札","https","即将","2","best","然而","能","墨守","。","手记","应当","即使","起来","在","看作","莫过于","project","教程","不合","不可","时时","所述","设计","各个","有如","蒐集","过时","变化","研读","and","都","四处","tf","已","难免会","几个","contrib","写下","习惯","readthedocs","局限","whats","存取","提供","培养","提示","基本",")","时兴","一套","、","地方","是","重要","会因","“","特别","奈何","changed","亦复如是"," ","不断更新","上线","改进","迅速","zh","之前","确实","消息","不得不","代替","即","自","此时","检索","译者","一点","”","来","不断改进","英文","主要","日新月异","搭建","事","一般","侧重于","久","虽然","介绍","倘若","地","自学","值得","上","p","正在",",","effective","以下","zhuanlan","如意","如是","中层","更加","不足","人","懒惰","不学","难免","会","本身","很",".","之","于","支持","一","50049041","他人","zhihu","走入","org","长期","不足取","现在","重于","英双语","侧重","com","乃是","将会","希望","而","有着","技术","中","时","合宜","围绕","本","毕竟","keras","版本","就","好","还","有","程度","将","相比","废案","导致","完整","(","那么","底层"],"title":"Tensorflow治学","title_tokens":["治学","tensorflow"]},{"location":"#tensorflow_2","text":"一个标准的Tensorflow工作流可以表示成这样： graph TD st(开始) --> Sess[启动Session] Sess --> ConNet[构建网络] subgraph 构造流程 ioNet>读取网络参数] --> ConNet end subgraph 执行流程 ioDat>导入数据] --> Run ConNet --> Run[执行网络] Run --> ioNetS>保存网络参数] Run --> ioRes>导出结果] end ioRes --> SessCl[关闭Session] SessCl --> ed(结束) classDef styStart fill:#FAE6A9,stroke:#BA9132; classDef styIO fill:#cde498,stroke:#13540c; class st,ed styStart class ioNet,ioNetS,ioDat,ioRes styIO 与一般的计算库不同，Tensorflow的执行流程大体可以分为两步： 构造流程 : 在这一步，Tensorflow根据用户代码构造一个 数据流图(dataflow graph) 。所谓数据流图，指的是由一系列 张量(Tensor) 构成的符号运算集合。就如同一张流程图一样，在这一阶段，尽管用户定义了每一步的运算（从简单的加减乘除到复杂的网络单元），但是没有任何运算被执行。就像一个程序员撰写代码一样，Tensorflow在这一步，将用户的代码转换成它的“机器语言”，但是网络还没有进入被使用的阶段。 执行流程 : 在这一步，Tensorflow将计算用户指定的某个 Tensor 的输出结果。要得到一个Tensor的输出，则必须得计算它一系列的依赖变量。例如，我们已知 y = x_1 + x_2 y = x_1 + x_2 , x_2 = z_1 \\times z_2 x_2 = z_1 \\times z_2 。那么，如果Tensorflow要得到 y y 的结果，它就必须先计算 x_2 x_2 。这个过程被完全地封装起来，从用户看来，我们只需要调用 y y 的输出即可，不需要关心Tensorflow是怎样按照流图完成计算的。 因此，典型的Tensorflow式的语言风格也可以这样划分： 构造流程 : 用户定义整个网络的符号运算，指定网络各个节点的属性、输入和输出。这些代码往往被写成一个函数（例如 def construct (): ） 执行流程 : 创建一个Session，在Session内调用构造函数，然后输入数据，得到并保存输出结果。必要情况下，还需要导入导出网络参数。 这里提到 会话(Session) 。Session如同一个Tensorflow虚拟机，在一个Session打开的时候，设备的计算资源（GPU, CPU, 带宽等）才被加载。Session又如同一个工人，用户撰写的网络构造代码如同车间，输入的数据如同商品，“工人（Session）”利用预定义好的“车间（流图）”将可以快速、批量地生产这些“商品（数据）”。这样地设计保证了一些外围的代码，例如指定网络参数的名称、函数的属性等操作，不需要重复进行，从而确保处理数据的时候，总是执行必要的代码，加快运算速度。 另一方面，构造-执行的结构还封装了许多提高运算效率的特性，例如多线程。尽管用户定义网络构造的时候，所写的代码是按照逻辑顺序的、线性的。但实际运行的时候，Tensorflow可以自行发现网络的哪些部分可以被同时运行，从而利用多核系统的计算资源。这些过程也是被完全封装起来的，用户并不需要花费心思去专门进行这些调整。 须知 在Tensorflow已经转变为Keras导向的现在，Session的调用被Keras API封装了起来，从用户的角度来看，现在已经不再需要手动调用Session。","text_tokens":["：","方面","构建","是","]","指定","输出","它","属性","集合","“","一个","加减","完成","输入","提到","可以","怎样","利用","库","流程","不再","总是","开始","这","系列","一系列","一步","这样","用户"," ","进入","完全","程序员","按照","下","构成","-","construct","必要","虚拟","=","构造函数","须知","符号","因此","然后","指","单元","例如","许多","导入","加减乘除","得到","但","要","fill","session","封装","”","时候","转变","快速","创建","需要","一些","导出","程序","（","和","一般","实际","调用","ba9132","connet","，","被","语言","换成","到","工作","z","流","一系","另一方","并不需要","已经","手动","某个","地","线程","处理","特性","变量","重复","读取","花费","划分","确保","式","与","由","必须","设备","依赖","部分",",","计算资源","外围","iores","打开","加载","）","关心","才","速度","乘除","加快","也","运行",";","2","这里","这些","每","心思","[",":","cpu","结构","stystart","根据","同时","iodat","成","简单","网络","td","又","系统","从而","去","标准","styio","。","一张","st","典型","导向","函数","得","启动","ed","不同","在","定义","从","起来","tensor","提高","专门","一","资源","为","def","虚拟机","class","机器","y","多核","分为","即可","预定","往往","批量","一方","没有","风格","生产","代码","并","运算","大体","的","所写","这个","等","图","cde498","fae6a9","subgraph","现在","转换成","已知","结束","流图","两步","数据流","设计","_","进行","各个","如同","13540c","工人","不","线性","尽管","多线","数据","先","一样","角度","发现","节点","#","情况","sesscl","关闭","写成","撰写","内","转换","则","会话","操作","复杂","执行","计算","sess","网络单元","任何","ionets","结果","像","了","那么","另一方面","义好","效率","顺序","保存","哪些","商品","keras","就","tensorflow","还","classdef","ionet","1","表示","整个","名称","x","但是","将","我们","带宽","阶段","只","api","调整","流程图","dataflow","使用","end","run","一方面","\\",">","所谓","构造","(","机器语言","过程","自行","graph","多线程","来看","如果","+","车间",")","参数","times","保证","逻辑","看来","、","张量","stroke","gpu"],"title":"Tensorflow原理","title_tokens":["tensorflow","原理"]},{"location":"#tensorflow-api","text":"下图显示了当前Tensorflow-API的组织形式 Tensorflow在多个平台上均有部署，包括Python, C++, Java, Javascript, Go等，未来还可能支持更多的语言。然而，不同平台上，API的使用方式和代码风格是大不相同的。例如，Python的语言风格目前已经朝向Keras转变，而Javascript仍倾向于使用中层API；C++和Java偏向于使用底层API编写程序。本教程只针对时兴Python的API编写，除非涉及到特定的专题，不会讨论其他语言上Tensorflow的用法。 一般而言，Tensorflow将API面向划分为三个层次： 高层API (High level) : 包括Estimators和Keras； 中层API (Mid level) : 包括layers, datasets, loss和metrics等具有功能性的函数，例如网络层的定义，Loss Function，对结果的测量函数等； 底层API (Low level) : 包括具体的加减乘除、具有解析式的数学函数、卷积、对Tensor属性的测量等。 从r1.4入门的用户，所接受的训练往往是从底层API开始，使用自己的代码风格构建对应中层API的函数，然后再用自己定义好的中层API构建网络。在这种代码风格的驱使下，用户除了需要定义各个网络层具体的表达式，还需要设计输入输出的接口，用来调用 优化器(optimizer) 的接口等操作。 另一种使用方式，是从中层API开始，直接使用预定义好的网络层构建网络，这样的代码风格会节省一部分时间，相当于几乎不再需要接触底层API，但是在输入输出、网络训练和测试等更高层的模块设计上，和底层API用户的代码风格相似。 Estimators是从r1.4就已经存在的API了，它可以被看成是已经集成在Tensorflow里的完整的网络。因此，Estimators往往适合分发、需求不高的应用，但是并不适合使用Tensorflow的研究者。尽管至今为止，Tensorflow仍然对Estimators倍加推崇，但本教程将完全不涉及这方面的内容。 另一个高层API是 Eager Execution ，从r1.9起，Tensorflow就加入了这种新的使用模式，并且在Tensorflow 2.0中，它仍然将会被保留。Eager摈弃了上述的构造-执行流程，任何对Tensor的定义会被立时计算、并可得到结果。对于需要进行小规模的调试、检查API功能的用户而言更加方便。但是，据一些用户的反响，现在Eager还没有做到和之前API的完全兼容，本教程将基本不涉及Eager的用法。 因此，Keras API将是本教程的重中之重。使用本教程的用户，可以对照目前的官方文档，跟进本教程的思路，自己逐步实现各个project的设计。使用Keras风格的设计，代码量会明显少于前面提到的各种风格（Estimators除外）。事实上， tf.keras 和底层API之间具有一定的兼容，这样的编写风格能让我们在满足自己设计的派生功能的基础上，尽可能使用规范化、预定义的API单元，从而减少代码出错的可能性。通过对本教程TF 1.x版的学习，对未来2.0版的上手也会带来莫大帮助。 金宇琛( @cainmagi )，2019年3月2日 Yuchen Jin( @cainmagi ), Mar. 2, 2019","text_tokens":["数学","输入","一个","提到","官方","开始","存在","这方","方式","版","然后","例如","low","通过","得到","loss","可能","一些","规范化","之间","网络层","器","均","，","被","已经","平台","式","部分","明显","派生","也","多","小规模","编写程序","立时","时间","函数","至今","定义","除了","预定","兼容","风格","用来","并","的","加入","手","4","mar","进行","当于","功能","不","内容","estimators","cainmagi","不高","当前","义","操作","层次","义好","tensorflow","1","自己","涉及","但是","检查","layers","专题","再用","更","而言","月","可","所","是从","研究者","驱使","方面","保留","另","加减","据","可以","流程","倍加","用户","完全","-","eager","high","接受","方便","加减乘除","下图","多个","需要","和","（","显示","优化","摈弃","解析","应用","编写","功能性","未来","这种","模块","可能性","学习",":","形式","网络","从而","看成","r1","c++","从","tensor","为","往往","思路","范化","金宇琛","输入输出","等","规范","事实","需求","一种","讨论","入门","execution","高层","帮助","执行","结果","了","包括","其他","节省","上述","我们","只","level","api","仍然","使用","一部","起","它","go","@","输出","对照","不再","三个","下","optimizer","减少","2019","一般而言","yuchen","java","相当于","mid","部署","到","metrics","少于","倾向","一定","事实上","2.0","文档","各种","集成","不会","对","）","2","表达式","9","然而","能","。","前面","function","在","表达","具体","project","教程","规模","为止","除外","测量","设计","各个","研究","对本","卷积","实现","python","接触","直接","tf","仍","调试","jin","并且","x","模式","目前","组织","构造","满足","基本","朝向","时兴",")","相同","用法","、","基础","：","是","构建","属性","尽可能","具有","达式","这样","重中之重"," ","这方面","相似","做到","日","因此","之前","里","单元","出错","但本","javascript","转变","新","程序","一般","调用","语言","datasets","面向","划分","上",",","中层","推崇","乘除","更加","会","逐步",".","量会","适合","支持","于","训练","不同","让","一部分","莫大","尽可","没有","代码","偏向","测试","对应","现在","分发","3","相当","接口","年","尽管","将会","对于","而","除非","中","带来","特定","计算","针对","本","任何","keras","就","好","还","有","反响","将","跟进","；","几乎","完整","(","大不相同","底层"],"title":"Tensorflow API架构","title_tokens":["tensorflow"," ","架构","api"]},{"location":"#_2","text":"接下来，本教程将会涉及 从线性问题入门 : 如何使用Tensorflow完成一个简单的线性分类设计，我们将从Hello World开始，逐步过渡到一个具有核函数的非线性问题project。 Hello world: 第一个Tensorflow程序。 线性分类: 一个简单的二分类问题。 线性回归: 一个同样简单的，线性拟合问题。 非线性回归: 拟合一个简单的，可以表达出解析式的非线性函数。 非线性分类: 使用线性分类器对非线性分布的数据进行分类。 训练与测试分立 : 从这一章开始，并试图解决几个更加复杂的实际问题。这里涉及到的project相对更大、更完整，训练往往需要一定时间才能完成。因此，在本章，所有项目的训练、测试环节都会分开，我们将从这里开始，使用TensorBoard跟踪我们的训练情况，并介绍如何存取神经网络。 Super Resolution: 使用神经网络进行图像像素插值。 Sparse Coding: 使用线性的词典学习(dictionary learning)来进行图像像素插值。 Generitive Model: 使用时兴的生成模型(Generitive Model)来完成图像风格的转换。 原生的数据管理 : 从这一章开始，我们将纳入Tensorflow自带的数据管理API。实际上，很多用户并不习惯使用这些API，往往倾向于自己完成数据的导入导出。虽然这并非一个复杂的工作，但据Tensorflow的文档，原生的数据管理内部实现了多线程，本身更适合用于提高数据IO的效率。读者可以酌情选择是否需要阅读本章。 使用MINIST数据集: 使用原生的数据管理来IO一个现成的MINIST数据集，这一节改自 官方文档教程 。 使用RNN处理文章: 通过LSTM模型来进行文本分类，并使用原生的数据管理来IO一个现成的IMDB数据集，这一节改自 官方文档教程 。 使用RBM生成音频: 通过限制玻尔兹曼机(RBM)来编写一个简单的音乐生成器，这一节改自 llSourcell/Music_Generator_Demo 。 其他的高级技巧 : 本章将介绍一些特殊的、利用底层API技巧，使得用户能更灵活地定制Tensorflow-keras，从而实现一些仅依靠原生库不能实现的功能。","text_tokens":["拟合","数据管理","coding","二","一个","完成","分立","利用","可以","库","分开","具有","官方","开始","出","这","用户"," ","resolution","如何","-","接下","接下来","但据","分类","分类器","使得","因此","第一个","minist","读者","非线性","/","跟踪","导入","同样","自","通过","才能","很多","生成","来","神经网","原生","玻尔兹曼","不能","生成器","需要","阅读","llsourcell","一些","程序","导出","lstm","实际","hello","虽然","，","过渡","限制","到","解析","工作","文章","imdb","介绍","倾向","一定","地","线程","处理","super","文档","tensorboard","编写","式","与","model","对","下来","一节","自带","music","试图","现成","generitive","更大","词典","回归","更加","图像","学习","这些",":","内部","神经网络","会","本身","简单","网络","改","从而","generator","能","成器","问题","逐步","时间","。","于","相对","分布","适合","函数","训练","在","从","表达","提高","sparse","project","往往","教程","选择","高级","风格","rbm","并","的","测试","dictionary","音频","定制","用于","核","插值","设计","进行","所有","_","解决","兹曼","功能","rnn","demo","第一","线性","不","音乐","集","将会","神经","技巧","特殊","环节","多线","实际上","数据","入门","模型","灵活","情况","实现","纳入","转换","管理","玻尔","都","复杂","本","文本","了","效率","keras","几个","本章","tensorflow","项目","其他","习惯","world","像素","自己","仅","涉及","io","将","我们","酌情","一章","api","存取","使用","完整","(","是否","learning","多线程","机","更",")","时兴","并非","依靠","、","这里","底层"],"title":"教程导读","title_tokens":["导读","教程"]},{"location":"licenses/","text":"协议 (Licenses) ¶ 本站协议 (中文版) ¶ MIT 开源许可协议 版权所有 © 2019, 金宇琛 (cainmagi) 特此向任何得到本软件副本或相关文档的人授权：被授权人有权使用、复制、修改、 合并、出版、发布、散布、再授权和/或贩售软件及软件的副本，及授予被供应人 同等权利，只需服从以下义务： 在软件和软件的所有副本中都必须包含以上版权声明和本许可声明。 该软件是\"按原样\"提供的，没有任何形式的明示或暗示，包括但不限于为特定目的和 不侵权的适销性和适用性的保证担保。在任何情况下，作者或版权持有人，都无权要求 任何索赔，或有关损害赔偿的其他责任。无论在本软件的使用上或其他买卖交易中， 是否涉及合同，侵权或其他行为。 License of this website (English version) ¶ MIT License Copyright © 2019 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. 本站相关项目的协议 ¶ 下面介绍的诸多协议，原则上并无必要列在本条目中（例如MIT License）。列在下侧，主要是为了向诸位支持本文档的开发者致以真诚的谢意。亦将荣耀与在下的感谢致以高天之上的上帝，愿你保守这份文档，成为众人的帮助。 License of Material ¶ MIT License Copyright © 2016 - 2019 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of MkDocs ¶ BSD 2-Clause \"Simplified\" License Copyright © 2014, Tom Christie. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. License of Jieba3K ¶ The MIT License (MIT) Copyright © 2013 Sun Junyi Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of Simple Lightbox ¶ The MIT License (MIT) Copyright © 2018 Damir Brekalo Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of MathJax ¶ Apache License 2.0 See the full license here: MathJax license License of mermaid ¶ The MIT License (MIT) Copyright © 2014 - 2018 Knut Sveidqvist Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["liable","subject","要求","conditions","文版","to","purpose","publish","associated","implied","do","例如","/","有关","得到","或","permission","loss","亦","interruption","notice","on","limitation","，","被","list","\"","any","infringement","indirect","你","与","copy","侵权","原样","高天","mermaid","再","knut","2014","按","暗示","供应","permit","this","及","¶","strict","person","out","granted","下侧","damages","material","并","mathjax","kind","的","需","reserved","consequential","条目","all","所有","full","simple","不","particular","cainmagi","theory","redistribution","sun","exemplary","之上","software","涉及","中文版","为了","有人","contributors","真诚","including","special","direct","开源","persons","向","散布","-","damir","包含","even","materials","advised","whom","source","connection","here","但","该软件","交易","成为","sublicense","协议","副本","jieba3k","和","（","持有人","特此","an","disclaimer","warranty","中文","simplified","保守","开发者","substantial","but","无","mit","free","license","forms","as",";",":","形式","行为","is","provided","must","授权人","明示","non","为","赔偿","with","brekalo","use","金宇琛","whether","适销","services","a","合并","holder","business","情况","帮助","version","clause","furnished","substitute","no","索赔","项目","无论","包括","其他","negligence","way","other","只","authors","使用","有权","许可","noninfringement","without","modify","相关","出版","列","christie","limited","贩售","诸位","sveidqvist","众人","下","2019","modification","服从","yuchen","restriction","in","列在","dealings","声明","files","donath","hereby","liability","if","2013","charge","2.0","文档","included","同等","tort","copyright","the","disclaimed","）","sell","2016","for","2","复制","portions","binary","code","感谢","原则","2018","。","event","在","from","see","修改","lightbox","无权","持有","shall","义务","or","适用性","荣耀","本站","possibility","deal","goods","met","and","such","by","obtaining","都","not","jin","procurement","retain","data","incidental","so","licenses","warranties","tom","express","下面","提供","fitness","是否","following","授权","form",")","bsd","、","损害赔偿","：","contract","软件","是","合同","版权所有","谢意","授予","that","merge","distribution"," ","适用","担保","必要","distribute","merchantability","责任","martin","愿","profits","©","be","主要","作者","介绍","损害","版权","claim","mkdocs","必须","上","redistributions","english","这份",",","arising","以下","are","原则上","人","reproduce","致以","rights","买卖","限于","action","发布","of","目的",".","开发","支持","documentation","however","permitted","没有","damage","诸多","caused","中","特定","otherwise","website","以上","本","任何","apache","copies","权利","性","上帝","将","holders","junyi","(","above","保证"],"title":"协议","title_tokens":["协议"]},{"location":"licenses/#licenses","text":"","text_tokens":[],"title":"协议 (Licenses)","title_tokens":["licenses","("," ",")","协议"]},{"location":"licenses/#_1","text":"MIT 开源许可协议 版权所有 © 2019, 金宇琛 (cainmagi) 特此向任何得到本软件副本或相关文档的人授权：被授权人有权使用、复制、修改、 合并、出版、发布、散布、再授权和/或贩售软件及软件的副本，及授予被供应人 同等权利，只需服从以下义务： 在软件和软件的所有副本中都必须包含以上版权声明和本许可声明。 该软件是\"按原样\"提供的，没有任何形式的明示或暗示，包括但不限于为特定目的和 不侵权的适销性和适用性的保证担保。在任何情况下，作者或版权持有人，都无权要求 任何索赔，或有关损害赔偿的其他责任。无论在本软件的使用上或其他买卖交易中， 是否涉及合同，侵权或其他行为。","text_tokens":["损害赔偿","向","：","软件","是","合同","版权所有","要求","授予","贩售","散布"," ","适用","下","2019","担保","包含","服从","/","有关","得到","但","该软件","或","责任","交易","©","协议","副本","声明","和","持有人","特此","作者","，","被","\"","损害","文档","版权","同等","必须","侵权","上","原样","mit",",","以下","再","人","买卖","形式","复制","按","暗示","供应","行为","限于","发布","及","目的","。","授权人","明示","在","为","赔偿","修改","无权","持有","没有","义务","金宇琛","的","需","适用性","适销","所有","不","合并","cainmagi","情况","中","特定","都","以上","本","任何","索赔","无论","包括","其他","权利","性","涉及","只","有人","提供","使用","有权","许可","(","是否","保证","授权",")","、","出版","开源","相关"],"title":"本站协议 (中文版)","title_tokens":["文版","("," ","本站","中文",")","中文版","协议"]},{"location":"licenses/#license-of-this-website-english-version","text":"MIT License Copyright © 2019 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["contract","liable","subject","conditions","limited","merge"," ","to","purpose","publish","2019","associated","distribute","implied","do","whom","yuchen","connection","restriction","/","in","merchantability","dealings","permission","sublicense","©","notice","be","limitation","files","\"","hereby","liability","any","an","warranty","charge","included","claim","tort","substantial","copy","but","mit",",","copyright","free","license","arising","the","sell","as","for","rights",":","is","portions","action","permit","this","of",".","provided","person","documentation","event","out","granted","from","damages","with","use","shall","or","kind","whether","all","deal","a","particular","cainmagi","and","obtaining","otherwise","not","furnished","jin","software","no","copies","so","warranties","express","holders","other","authors","fitness","noninfringement","(","without","following","above","modify",")","including","persons"],"title":"License of this website (English version)","title_tokens":["this","website","of","english","("," ","license",")","version"]},{"location":"licenses/#_2","text":"下面介绍的诸多协议，原则上并无必要列在本条目中（例如MIT License）。列在下侧，主要是为了向诸位支持本文档的开发者致以真诚的谢意。亦将荣耀与在下的感谢致以高天之上的上帝，愿你保守这份文档，成为众人的帮助。","text_tokens":["原则","主要","向","列","（","是","帮助","开发","。","支持","中","谢意","，","感谢","介绍","在","诸位","众人","之上","本"," ","下侧","文档","下","保守","你","开发者","与","必要","并","的","上帝","无","这份","mit","荣耀","将","license","诸多","例如","高天","为了","）","条目","下面","列在","原则上","真诚","致以","亦","愿","成为","协议"],"title":"本站相关项目的协议","title_tokens":["的","本站","相关","项目","协议"]},{"location":"licenses/#license-of-material","text":"MIT License Copyright © 2016 - 2019 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["contract","liable","subject","conditions","limited","merge"," ","to","purpose","-","publish","2019","associated","distribute","implied","do","whom","connection","restriction","/","in","merchantability","dealings","permission","martin","sublicense","©","notice","be","limitation","files","\"","donath","hereby","liability","any","an","infringement","warranty","charge","included","claim","tort","substantial","copy","but","mit",",","copyright","free","license","arising","the","sell","2016","as","for","rights",":","is","portions","action","permit","this","of",".","provided","person","documentation","event","non","out","granted","from","damages","with","use","shall","or","kind","whether","all","deal","a","particular","and","obtaining","otherwise","not","furnished","software","no","copies","so","warranties","express","holders","other","authors","fitness","(","without","following","above","modify",")","including","persons"],"title":"License of Material","title_tokens":["license","material"," ","of"]},{"location":"licenses/#license-of-mkdocs","text":"BSD 2-Clause \"Simplified\" License Copyright © 2014, Tom Christie. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","text_tokens":["contract","christie","liable","conditions","limited","that","distribution"," ","to","purpose","-","modification","implied","even","materials","advised","source","/","in","merchantability","loss","interruption","profits","©","notice","be","on","list","\"","liability","disclaimer","any","if","indirect","simplified","tort","but","redistributions",",","copyright","arising","license","are","the","disclaimed","forms","as",";","2","reproduce","for","rights","2014",":","is","code","binary","this","of",".","strict","provided","must","documentation","event","out","however","permitted","damages","with","use","shall","damage","or","whether","reserved","consequential","possibility","all","services","goods","met","a","particular","caused","holder","business","and","theory","such","by","redistribution","otherwise","clause","exemplary","not","substitute","procurement","retain","data","software","no","incidental","negligence","warranties","tom","express","way","other","holders","contributors","fitness","without","(","following","above","form","bsd","including",")","special","direct"],"title":"License of MkDocs","title_tokens":["license"," ","of","mkdocs"]},{"location":"licenses/#license-of-jieba3k","text":"The MIT License (MIT) Copyright © 2013 Sun Junyi Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["contract","liable","subject","conditions","limited","merge"," ","to","purpose","publish","associated","distribute","implied","do","whom","connection","restriction","/","in","merchantability","dealings","permission","sublicense","©","notice","be","limitation","files","\"","hereby","liability","any","an","2013","warranty","charge","included","claim","tort","substantial","copy","but","mit",",","copyright","free","license","arising","the","sell","as","for","rights",":","is","portions","action","permit","this","of",".","provided","person","documentation","event","out","granted","from","damages","with","use","shall","or","kind","whether","all","deal","a","particular","and","obtaining","sun","otherwise","not","furnished","software","no","copies","so","warranties","express","holders","other","junyi","authors","fitness","noninfringement","(","without","following","above","modify",")","including","persons"],"title":"License of Jieba3K","title_tokens":["license","jieba3k"," ","of"]},{"location":"licenses/#license-of-simple-lightbox","text":"The MIT License (MIT) Copyright © 2018 Damir Brekalo Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["contract","liable","subject","conditions","limited","merge"," ","to","purpose","publish","damir","associated","distribute","implied","do","whom","connection","restriction","/","in","merchantability","dealings","permission","sublicense","©","notice","be","limitation","files","\"","hereby","liability","any","an","warranty","charge","included","claim","tort","substantial","copy","but","mit",",","copyright","free","license","arising","the","sell","as","for","rights",":","is","portions","action","permit","this","of","2018",".","provided","person","documentation","event","out","granted","from","damages","brekalo","with","use","shall","or","kind","whether","all","deal","a","particular","and","obtaining","otherwise","not","furnished","software","no","copies","so","warranties","express","holders","other","authors","fitness","noninfringement","(","without","following","above","modify",")","including","persons"],"title":"License of Simple Lightbox","title_tokens":["of","simple"," ","license","lightbox"]},{"location":"licenses/#license-of-mathjax","text":"Apache License 2.0 See the full license here: MathJax license","text_tokens":["here","mathjax","full","2.0","see"," ",":","license","apache","the"],"title":"License of MathJax","title_tokens":["license","mathjax"," ","of"]},{"location":"licenses/#license-of-mermaid","text":"The MIT License (MIT) Copyright © 2014 - 2018 Knut Sveidqvist Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","text_tokens":["contract","liable","subject","conditions","limited","sveidqvist","merge"," ","to","purpose","-","publish","associated","distribute","implied","do","whom","connection","restriction","/","in","merchantability","dealings","permission","sublicense","©","notice","be","limitation","files","\"","hereby","liability","any","an","warranty","charge","included","claim","tort","substantial","copy","but","mit",",","copyright","free","license","arising","the","sell","as","for","knut","rights","2014",":","is","portions","action","permit","this","of","2018",".","provided","person","documentation","event","out","granted","from","damages","with","use","shall","or","kind","whether","all","deal","a","particular","and","obtaining","otherwise","not","furnished","software","no","copies","so","warranties","express","holders","other","authors","fitness","noninfringement","(","without","following","above","modify",")","including","persons"],"title":"License of mermaid","title_tokens":["license","mermaid"," ","of"]},{"location":"release-notes/","text":"更新记录 ¶ 大版本更新 ¶ 在此记录本文档的主要更新，读者可以在此确认经过更新后，本文档添加了哪些主要的内容、话题。 0.1 @ February 25, 2019 ¶ 正式立项，开始本文档的撰写工作。目前尚处于未完成状态，计划包括： Tensorflow 1.12 入门教程 从线性问题入门 60% 训练与测试分立 0% 原生的数据管理 0% 其他的高级技巧 0% Tensorflow 2.0 入门教程 0% Tensorflow 第三方扩展库 入门教程 0% 局部更新记录 ¶ 0.20 @ March 12, 2019 ¶ 完成“从线性问题入门”专题下的 线性回归 ； 微调样式表(stylesheet)文件； 修正前文的一些笔误。 0.18-r1.13 @ March 9, 2019 ¶ 由于Tensorflow的新版 r1.13 发行版预编译包开始支持CUDA 10，本文的内容全部根据 r1.13 版进行调整。特别注意这个星期是Tensorflow 2.0-alpha 横空出世的日子，可喜可贺，可喜可贺。 0.18 @ March 6, 2019 ¶ 完善“从线性问题入门”专题下的 线性分类 ，补充了一些概念便于初学者理解。 0.17 @ March 5, 2019 ¶ 完成“从线性问题入门”专题下的 线性分类 ； 修正前文的一些笔误。 0.15 @ March 4, 2019 ¶ 完成“从线性问题入门”专题下的 本章总说 和 Hello world! ； 微调图片链接， MathJax 的显示特性。 0.12 @ March 3, 2019 ¶ 补完扉页。未来可能会调整 教程导读 ； 修复 Arithmatex 对 MathJax 的引入； 引入 mermaid 库，用于绘制示意图。 0.11 @ February 25, 2019 ¶ 提交 Google Analytics 和 Google Search Console 的账户关联资料； 提交 Disqus 关联资料。 0.10 @ February 25, 2019 ¶ 正式立项，并撰写扉页的一部分。","text_tokens":["计划","本文","：","数据管理","0.20","是","@","星期","理解","!","“","特别","完成","分立","发行","可以","后","库","%","arithmatex","开始","前文"," ","下","状态","-","stylesheet","扉页","2019","笔误","补完","链接","版","分类","0.1","february","读者","第三方","6","处于","样式","10","”","原生","局部","5","意图","微调","可能","第三","0.11","一些","主要","和","0.18","显示","hello","图片","，","search","工作","60","修正","发行版","特性","2.0","添加","文档","横空出世","与","未来","march","立项","对","部分",",","横空","此","示意图","mermaid","更新","回归","完善","初学者","编译","预","引入","0.12","正式","根据","会","出世","9","概念","入门教程","示意","问题","¶",".","r1","经过","。","支持","资料","补充","大","训练","google","在","账户","从","alpha","便于","绘制","图片链接","学者","三方","disqus","一部分","记录本","全部","提交","教程","高级","analytics","导读","并","mathjax","的","测试","这个","初学","0.17","4","空出","25","3","用于","进行","0.10","包","内容","线性","话题","技巧","0.15","数据","关联","入门","确认","新版","尚","由于","扩展","撰写","注意","管理","修复","日子","1.12","未","总说","样式表","本","console","了","哪些","13","本章","可喜可贺","版本","tensorflow","包括","其他","0","cuda","可贺","world","文件","；","目前","调整","记录","12","专题","一部","(","可喜",")","、"],"title":"更新记录","title_tokens":["记录","更新"]},{"location":"release-notes/#_1","text":"","text_tokens":[],"title":"更新记录","title_tokens":["记录","更新"]},{"location":"release-notes/#_2","text":"在此记录本文档的主要更新，读者可以在此确认经过更新后，本文档添加了哪些主要的内容、话题。","text_tokens":["确认","主要","经过","。","，","可以","后","在","添加","本","文档","了","记录本","哪些","的","此","读者","更新","记录","内容","话题","、"],"title":"大版本更新","title_tokens":["版本","大","更新"]},{"location":"release-notes/#01-february-25-2019","text":"正式立项，开始本文档的撰写工作。目前尚处于未完成状态，计划包括： Tensorflow 1.12 入门教程 从线性问题入门 60% 训练与测试分立 0% 原生的数据管理 0% 其他的高级技巧 0% Tensorflow 2.0 入门教程 0% Tensorflow 第三方扩展库 入门教程 0%","text_tokens":["计划","：","问题","数据管理","尚","。","完成","扩展","，","撰写","分立","管理","库","工作","训练","1.12","60","从","%","开始","未","2.0","本"," ","文档","三方","状态","教程","高级","与","tensorflow","包括","其他","0","立项","的","测试","目前","第三方","处于","线性","正式","技巧","原生","数据","入门","第三","入门教程"],"title":"0.1 @ February 25, 2019","title_tokens":["0.1","@",","," ","february","25","2019"]},{"location":"release-notes/#_3","text":"","text_tokens":[],"title":"局部更新记录","title_tokens":["记录","更新","局部"]},{"location":"release-notes/#020-march-12-2019","text":"完成“从线性问题入门”专题下的 线性回归 ； 微调样式表(stylesheet)文件； 修正前文的一些笔误。","text_tokens":["一些","问题","。","“","完成","从","修正","前文","样式表"," ","下","stylesheet","笔误","的","文件","；","回归","专题","(","样式","线性","”",")","微调","入门"],"title":"0.20 @ March 12, 2019","title_tokens":["march","12","0.20","@",","," ","2019"]},{"location":"release-notes/#018-r113-march-9-2019","text":"由于Tensorflow的新版 r1.13 发行版预编译包开始支持CUDA 10，本文的内容全部根据 r1.13 版进行调整。特别注意这个星期是Tensorflow 2.0-alpha 横空出世的日子，可喜可贺，可喜可贺。","text_tokens":["新版","本文","是",".","r1","星期","。","支持","由于","特别","，","发行","注意","日子","开始","alpha","发行版","2.0"," ","横空出世","-","13","全部","可喜可贺","tensorflow","可贺","cuda","的","版","这个","横空","调整","空出","进行","编译","预","包","内容","10","可喜","根据","出世"],"title":"0.18-r1.13 @ March 9, 2019","title_tokens":["march","0.18",".","r1","@",","," ","-","13","9","2019"]},{"location":"release-notes/#018-march-6-2019","text":"完善“从线性问题入门”专题下的 线性分类 ，补充了一些概念便于初学者理解。","text_tokens":["一些","问题","理解","。","“","，","补充","从","便于"," ","下","学者","了","的","分类","初学","完善","初学者","专题","线性","”","入门","概念"],"title":"0.18 @ March 6, 2019","title_tokens":["march","0.18","@","2019",","," ","6"]},{"location":"release-notes/#017-march-5-2019","text":"完成“从线性问题入门”专题下的 线性分类 ； 修正前文的一些笔误。","text_tokens":["从","一些","修正","问题","专题","的","分类","前文","线性"," ","下","“","。","”","完成","；","入门","笔误"],"title":"0.17 @ March 5, 2019","title_tokens":["march","@",","," ","0.17","5","2019"]},{"location":"release-notes/#015-march-4-2019","text":"完成“从线性问题入门”专题下的 本章总说 和 Hello world! ； 微调图片链接， MathJax 的显示特性。","text_tokens":["和","问题","!","显示","。","“","hello","完成","图片","，","从","特性","总说"," ","下","图片链接","本章","链接","mathjax","的","world","；","专题","线性","”","微调","入门"],"title":"0.15 @ March 4, 2019","title_tokens":["march","@",","," ","0.15","4","2019"]},{"location":"release-notes/#012-march-3-2019","text":"补完扉页。未来可能会调整 教程导读 ； 修复 Arithmatex 对 MathJax 的引入； 引入 mermaid 库，用于绘制示意图。","text_tokens":["示意","。","，","修复","库","arithmatex","绘制"," ","教程","扉页","补完","未来","导读","mathjax","的","对","；","调整","用于","示意图","mermaid","引入","会","意图","可能"],"title":"0.12 @ March 3, 2019","title_tokens":["march","@","0.12"," ",",","3","2019"]},{"location":"release-notes/#011-february-25-2019","text":"提交 Google Analytics 和 Google Search Console 的账户关联资料； 提交 Disqus 关联资料。","text_tokens":["和","的"," ","console","disqus","资料","。","；","提交","search","analytics","关联","google","账户"],"title":"0.11 @ February 25, 2019","title_tokens":["@",","," ","february","25","2019","0.11"]},{"location":"release-notes/#010-february-25-2019","text":"正式立项，并撰写扉页的一部分。","text_tokens":["并","立项","的","部分","一部","正式","一部分","。","，","撰写","扉页"],"title":"0.10 @ February 25, 2019","title_tokens":["0.10","@",","," ","february","25","2019"]},{"location":"book-1-x/chapter-1/","text":"从线性问题入门 ¶ 摘要 本章将从线性问题入手，讨论最简单的分类与回归问题。我们将随机生成指定分布的数据，通过理论值，检查实验效果。这些简单的、解析的问题，可以提供读者一个直观的感受，并帮助读者快速上手Tensorflow的基本概念与运用。本章中，我们不会涉及任何数据存取、跟踪测度方面的概念，也不会将训练和测试分开，旨在使读者能集中心思到代码规范和Tensorflow的基本功能上。 漫谈线性问题 ¶ 在机器学习领域，线性问题既简单也不简单。目前神经网络主要是指多层、非凸的网络结构，常常用来解复杂的、难以推导的问题。但这并不意味着线性问题容易解决。相反，在很多情况下，线性问题是解不唯一的(undetermined)，解不稳定的(ill-posed/ill-conditioned)，条件的(constrained)。同时，为了得到一个快速收敛的、高质量的解，即使对线性问题，人们也在不断提出、改进解法。例如ISTA, AMP, PGD, LISTA, vAMP等算法，都用来解线性问题。在此不作详细展开。 我们已经知道，一个线性函数具有可加性，和一次齐次性，亦即 \\begin{align} f(x_1 + x_2) &= f(x_1) + f(x_2), \\\\ f(\\alpha x) &= \\alpha f(x). \\end{align} 因此，求解一个线性问题，我们需要将问题纯粹以 线性函数 进行描述。例如， \\begin{align} \\mathbf{y} \\sim \\mathbf{A}\\mathbf{x}. \\end{align} 具体而言， \\mathbf{x} \\mathbf{x} 是我们的已知数据， \\mathbf{y} \\mathbf{y} 是我们的未知量，我们需要找到一个合适的 \\mathbf{A} \\mathbf{A} 来确保 \\mathbf{x} \\mathbf{x} 能拟合到 \\mathbf{y} \\mathbf{y} 。如果此处 \\mathbf{y} \\in \\{0,~1\\}^p \\mathbf{y} \\in \\{0,~1\\}^p 是一个代表p类-分类的向量，那么这就是一个线性分类问题；相反，如果此处 \\mathbf{y} \\in \\mathbb{R}^p \\mathbf{y} \\in \\mathbb{R}^p 在连续p维空间取值，那么这就是一个线性回归问题。 线性问题与凸问题 ¶ 请注意，虽然我们在此处提到“ 线性问题 (Linear problem) ”，但我们指的并非“ 线性规划 (Linear programming) ”。虽然严格意义上，线性规划才是真正的线性问题，但我们在此处尚不讨论线性规划相关的内容，而是着眼于机器学习应用最普遍的两个领域， 分类 和 回归 上。实际上，这两种问题虽然求解的是线性函数，但本质上是凸问题。 例如，如果我们要求解回归问题，通常可以表述成 \\begin{equation} \\begin{aligned} \\arg \\min_{\\mathbf{A}}~& \\sum_{i=1}^N \\mathcal{L}(\\mathbf{A},~\\mathbf{x}_i,~\\mathbf{y}_i), \\\\ \\mathcal{L}(\\mathbf{A},~\\mathbf{x},~\\mathbf{y}) &= \\lVert \\mathbf{y} - \\mathbf{A}\\mathbf{x} \\rVert^2_2. \\end{aligned} \\end{equation} 虽然我们求解的模型 \\mathbf{A} \\mathbf{A} 是线性的，但我们优化的函数对象 \\mathcal{L} \\mathcal{L} 是一个 凸函数 (convex function) 。在此，我们可以将这个标量函数(同时也是凸函数)表述为 \\begin{align} \\forall~\\alpha,~\\beta,~\\mathbf{x}_1,~\\mathbf{x}_2,~\\mathcal{L}(\\alpha\\mathbf{x}_1 + \\beta\\mathbf{x}_2) \\leqslant \\alpha\\mathcal{L}(\\mathbf{x}_1) + \\beta\\mathcal{L}(\\mathbf{x}_2). \\end{align} 我们将这个问题表述为 最小二乘问题(Least-square problem) 。正是由于 \\mathbf{A} \\mathbf{A} 是线性的， \\mathcal{L} \\mathcal{L} 才能被确保为一个凸函数，进而，我们才能确保上述问题能得到精确的全局最小值解。 知悉Tensorflow ¶ 在本章接下来的内容里，我们将探讨Tensorflow如何求解一些简单的问题。对于初次上手的读者而言，在安装Tensorflow，亲自开始写一些project之前，笔者推荐你到这个游乐场“抢鲜体验”一番， Tensorflow Playground 在这个在线页面里，用户不需要有任何编程知识，可以通过直观的操作界面，建立一个简单的 多层感知机网络(有时也叫做Artificial Neural Network, ANN) ，并且实时观测网络的性能和测度。它提供了几个简单的二维数据集，供用户体验不同的数据集下，各种分类、回归问题的合宜解法。如果读者已经对神经网络的基本原理有所了解，相信能通过这个小小的实验场达到心有灵犀。我们在本章所做的project和demo，大体不跳出这个试验场的范畴，只是会略微复杂一点而已。 本章要点 ¶ 下图展示了通过本章学习，能了解到的概念： graph LR st(Hello world!) --> linclas(线性分类) linclas --> linreg(线性回归) linreg --> nonlinreg(非线性回归) nonlinreg --> ed(非线性分类) lp[感知机] --> linclas sigma[Logsitc回归] --> linclas ce[交叉熵] --> linclas opt[优化器] --> linreg argpar[项目选项] --> linreg para[参数回归] --> nonlinreg sdlayer[自定义层] --> nonlinreg kernel[核函数] --> ed classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,linclas,linreg,nonlinreg,ed styStart Hello world : 首先，在本节，读者将了解Tensorflow的安装方法，并编写通过第一个简单的Tensorflow程序。 线性分类 : 本节通过解一个简单的二分类问题，我们将引入单层感知机、Logistic回归和交叉熵的概念，并且实验结果进行可视化。 线性回归 : 本节通过解一个与上一节难度相仿的，简单的回归问题，另读者能对比不同优化器的性能、特点，并介绍如何在项目中进行arugument parsing(引入项目的可选项)。 非线性回归 : 本节将修改上一节回归问题的激活函数，将线性的回归问题推广到解析函数参数回归的范畴。同时介绍自行编写网络层(类API)的方法。 非线性分类 : 本节将通过上一节的参数回归，引入核函数的概念，将线性分类问题推广到非线性空间里。","text_tokens":["指定","合适","一个","详细","要求","提到","arg","人们","开始","纯粹","nonlinreg","页面","第一个","运用","读者","指","例如","非线性","推导","/","跟踪","forall","通过","得到","两个","mathbf","很多","亦","神经网","一些","器","网络层","稳定","小小","，","不断","被","ba9132","已经","你","与","感受","mathbb","跳出","回归","也","这些","写","n","成","¶","amp","方法","线性规划","函数","最小值","定义","beta","并","用来","大体","的","集中","fae6a9","向量","正是","sdlayer","para","equation","进行","未知量","功能","第一","不","内容","神经","建立","可加","模型","入手","不作","注意","界面","操作","{","严格","标量","笔者","本章","以","tensorflow","集下","0","1","涉及","可视","只是","算法","检查","为了","意味","可视化","全局","自定","kernel","常常","而言","参数","所","stroke","方面","!","有时","相信","二","另","试验场","aligned","推广","可以","分开","lvert","着眼于","难度","用户","-","接下","解法","进而","理论","有所","分类","undetermined","此处","artificial","展开","但","}","下图","生成","使","f","需要","和","函数参数","sim","实际","优化","初次","解析","应用","灵犀","linclas","探讨","编写","确保","节","空间","lr","下来","要点","此","既","性能","漫谈",";","学习","programming","align",":","同时","网络","唯一","问题","质量","凸函数","范畴","规划","从","维空间","begin","为","y","机器","二维","熵","找到","试验","linear","高质量","等","这个","在线","编程","实时","规范","l","讨论","a","入门","难以","首先","情况","帮助","由于","知机","解","结果","基本原理","了","体验","项目","network","constrained","上述","我们","了解","api","而已","\\","playground","r","graph","精确","+","并非","相关","vamp","它","拟合","]","知悉","相仿","亲自","原理","这","真正","下","直观","leqslant","如何","本原","二乘","特点","接下来","提出","收敛","展示","可选项","in","fill","旨在","快速","实验","linreg","square","hello","对象","取值","到","凸","基本功能","ill","摘要","各种","意味着","不会","对","一节","高质","rvert","引入","小小的","2","lp","心思","[","神经网络","stystart","概念","能","而是","一番","。","st","function","即使","ed","在","opt","类","具体","修改","理论值","project","激活","测度","convex","观测","安装","齐次","略微","已知","交叉","核","ista","集","实际上","logistic","^","代表","最小","叫做","都","可加性","描述","复杂","推荐","就是","几个","供","一次","基本功","做","并且","classdef","x","可选","层","目前","pgd","存取","提供","end",">","操作界面","基本","自行",")","本质","最","、","通常","：","是","&","未知","“","随机","具有","对比","两种"," ","普遍","改进","=","problem","因此","之前","里","argpar","连续","即","着眼","lista","才能","”","多层","来","一点","logsitc","主要","程序","parsing","posed","虽然","ce","mathcal","介绍","表述","上","p",",","sum","相反","min","才","达到","游乐场","结构","会","简单","请",".","~","neural","分布","conditioned","训练","不同","alpha","抢鲜","class","基本概念","容易","代码","解不","测试","感知","_","知道","解决","sigma","非凸","i","demo","线性","对于","数据","arugument","心有灵犀","#","尚","选项","中","效果","条件","单层","自定义","知识","合宜","实验场","本","任何","网络结构","least","意义","有","性","world","ann","求解","本节","将","；","感知机","上手","领域","(","如果","游乐","那么"],"title":"本章总说","title_tokens":["总说","本章"]},{"location":"book-1-x/chapter-1/#_1","text":"摘要 本章将从线性问题入手，讨论最简单的分类与回归问题。我们将随机生成指定分布的数据，通过理论值，检查实验效果。这些简单的、解析的问题，可以提供读者一个直观的感受，并帮助读者快速上手Tensorflow的基本概念与运用。本章中，我们不会涉及任何数据存取、跟踪测度方面的概念，也不会将训练和测试分开，旨在使读者能集中心思到代码规范和Tensorflow的基本功能上。","text_tokens":["能","和","问题","方面","指定","帮助","。","分布","效果","一个","入手","，","中","可以","随机","解析","分开","训练","到","从","基本功能"," ","任何","理论值","直观","基本概念","本章","摘要","实验","与","tensorflow","基本功","代码","并","测度","理论","不会","的","分类","上","测试","感受","简单","集中","涉及","将","我们","数据","运用","检查","读者","上手","规范","存取","提供","回归","跟踪","也","通过","功能","这些","讨论","旨在","线性","基本","心思","生成","使","快速","最","、","概念"],"title":"从线性问题入门","title_tokens":["从","入门","线性","问题"]},{"location":"book-1-x/chapter-1/#_2","text":"在机器学习领域，线性问题既简单也不简单。目前神经网络主要是指多层、非凸的网络结构，常常用来解复杂的、难以推导的问题。但这并不意味着线性问题容易解决。相反，在很多情况下，线性问题是解不唯一的(undetermined)，解不稳定的(ill-posed/ill-conditioned)，条件的(constrained)。同时，为了得到一个快速收敛的、高质量的解，即使对线性问题，人们也在不断提出、改进解法。例如ISTA, AMP, PGD, LISTA, vAMP等算法，都用来解线性问题。在此不作详细展开。 我们已经知道，一个线性函数具有可加性，和一次齐次性，亦即 \\begin{align} f(x_1 + x_2) &= f(x_1) + f(x_2), \\\\ f(\\alpha x) &= \\alpha f(x). \\end{align} 因此，求解一个线性问题，我们需要将问题纯粹以 线性函数 进行描述。例如， \\begin{align} \\mathbf{y} \\sim \\mathbf{A}\\mathbf{x}. \\end{align} 具体而言， \\mathbf{x} \\mathbf{x} 是我们的已知数据， \\mathbf{y} \\mathbf{y} 是我们的未知量，我们需要找到一个合适的 \\mathbf{A} \\mathbf{A} 来确保 \\mathbf{x} \\mathbf{x} 能拟合到 \\mathbf{y} \\mathbf{y} 。如果此处 \\mathbf{y} \\in \\{0,~1\\}^p \\mathbf{y} \\in \\{0,~1\\}^p 是一个代表p类-分类的向量，那么这就是一个线性分类问题；相反，如果此处 \\mathbf{y} \\in \\mathbb{R}^p \\mathbf{y} \\in \\mathbb{R}^p 在连续p维空间取值，那么这就是一个线性回归问题。","text_tokens":["拟合","是","&","未知","合适","一个","详细","人们","具有","这","纯粹"," ","下","改进","-","解法","=","分类","提出","收敛","因此","undetermined","连续","此处","指","例如","推导","/","即","in","lista","得到","但","展开","}","mathbf","很多","多层","亦","神经网","来","快速","f","需要","主要","和","sim","稳定","posed","，","不断","取值","到","已经","ill","确保","意味着","空间","p","对",",","此","相反","既","高质","mathbb","回归","也","2","学习","align","神经网络","结构","同时","简单","网络","唯一","能","问题",".","~","。","amp","质量","函数","conditioned","即使","在","类","alpha","维空间","begin","具体","y","机器","找到","容易","用来","并","解不","的","高质量","等","向量","齐次","已知","ista","_","知道","解决","进行","未知量","非凸","线性","不","神经","a","可加","数据","难以","^","代表","情况","条件","不作","解","都","可加性","描述","复杂","{","那么","网络结构","就是","以","一次","0","constrained","性","1","求解","x","将","我们","；","算法","目前","pgd","为了","意味","end","领域","\\","(","r","如果","+","常常",")","而言","、","vamp"],"title":"漫谈线性问题","title_tokens":["漫谈","线性","问题"]},{"location":"book-1-x/chapter-1/#_3","text":"请注意，虽然我们在此处提到“ 线性问题 (Linear problem) ”，但我们指的并非“ 线性规划 (Linear programming) ”。虽然严格意义上，线性规划才是真正的线性问题，但我们在此处尚不讨论线性规划相关的内容，而是着眼于机器学习应用最普遍的两个领域， 分类 和 回归 上。实际上，这两种问题虽然求解的是线性函数，但本质上是凸问题。 例如，如果我们要求解回归问题，通常可以表述成 \\begin{equation} \\begin{aligned} \\arg \\min_{\\mathbf{A}}~& \\sum_{i=1}^N \\mathcal{L}(\\mathbf{A},~\\mathbf{x}_i,~\\mathbf{y}_i), \\\\ \\mathcal{L}(\\mathbf{A},~\\mathbf{x},~\\mathbf{y}) &= \\lVert \\mathbf{y} - \\mathbf{A}\\mathbf{x} \\rVert^2_2. \\end{aligned} \\end{equation} 虽然我们求解的模型 \\mathbf{A} \\mathbf{A} 是线性的，但我们优化的函数对象 \\mathcal{L} \\mathcal{L} 是一个 凸函数 (convex function) 。在此，我们可以将这个标量函数(同时也是凸函数)表述为 \\begin{align} \\forall~\\alpha,~\\beta,~\\mathbf{x}_1,~\\mathbf{x}_2,~\\mathcal{L}(\\alpha\\mathbf{x}_1 + \\beta\\mathbf{x}_2) \\leqslant \\alpha\\mathcal{L}(\\mathbf{x}_1) + \\beta\\mathcal{L}(\\mathbf{x}_2). \\end{align} 我们将这个问题表述为 最小二乘问题(Least-square problem) 。正是由于 \\mathbf{A} \\mathbf{A} 是线性的， \\mathcal{L} \\mathcal{L} 才能被确保为一个凸函数，进而，我们才能确保上述问题能得到精确的全局最小值解。","text_tokens":["是","&","“","一个","要求","aligned","提到","可以","arg","lvert","两种","着眼于","这"," ","真正","普遍","leqslant","-","二乘","=","进而","problem","分类","此处","指","例如","forall","着眼","得到","但","}","两个","mathbf","才能","”","square","和","实际","虽然","优化","，","被","对象","凸","mathcal","应用","表述","确保","上",",","sum","此","min","才","rvert","回归","也","2","programming","学习","align","n","同时","成","请","能","问题","而是",".","~","。","线性规划","function","函数","凸函数","最小值","规划","在","alpha","begin","为","y","机器","beta","linear","的","这个","convex","正是","l","equation","_","讨论","i","线性","不","内容","a","实际上","模型","^","尚","由于","最小","注意","解","{","严格","标量","least","意义","1","求解","x","上述","将","我们","end","领域","\\","全局","(","精确","如果","+",")","并非","最","本质","相关","通常"],"title":"线性问题与凸问题","title_tokens":["凸","与","线性","问题"]},{"location":"book-1-x/chapter-1/#tensorflow","text":"在本章接下来的内容里，我们将探讨Tensorflow如何求解一些简单的问题。对于初次上手的读者而言，在安装Tensorflow，亲自开始写一些project之前，笔者推荐你到这个游乐场“抢鲜体验”一番， Tensorflow Playground 在这个在线页面里，用户不需要有任何编程知识，可以通过直观的操作界面，建立一个简单的 多层感知机网络(有时也叫做Artificial Neural Network, ANN) ，并且实时观测网络的性能和测度。它提供了几个简单的二维数据集，供用户体验不同的数据集下，各种分类、回归问题的合宜解法。如果读者已经对神经网络的基本原理有所了解，相信能通过这个小小的实验场达到心有灵犀。我们在本章所做的project和demo，大体不跳出这个试验场的范畴，只是会略微复杂一点而已。","text_tokens":["有时","相信","“","一个","试验场","可以","亲自","原理","开始","用户"," ","直观","本原","如何","接下","解法","接下来","有所","分类","页面","之前","里","读者","artificial","通过","”","多层","一点","神经网","实验","需要","一些","和","小小","，","初次","到","已经","灵犀","探讨","你","各种","对","下来",",","性能","跳出","达到","回归","也","小小的","写","游乐场","神经网络","会","简单","网络","能","问题","一番","。","neural","范畴","不同","在","抢鲜","二维","试验","project","测度","大体","的","这个","在线","编程","实时","感知","观测","安装","略微","demo","内容","不","集","对于","神经","建立","数据","心有灵犀","知机","叫做","界面","操作","复杂","推荐","知识","合宜","实验场","任何","基本原理","了","笔者","几个","本章","供","tensorflow","体验","network","集下","并且","做","有","ann","求解","将","我们","感知机","只是","了解","上手","而已","提供","playground","操作界面","(","基本","如果","游乐","而言",")","所","、","它"],"title":"知悉Tensorflow","title_tokens":["知悉","tensorflow"]},{"location":"book-1-x/chapter-1/#_4","text":"下图展示了通过本章学习，能了解到的概念： graph LR st(Hello world!) --> linclas(线性分类) linclas --> linreg(线性回归) linreg --> nonlinreg(非线性回归) nonlinreg --> ed(非线性分类) lp[感知机] --> linclas sigma[Logsitc回归] --> linclas ce[交叉熵] --> linclas opt[优化器] --> linreg argpar[项目选项] --> linreg para[参数回归] --> nonlinreg sdlayer[自定义层] --> nonlinreg kernel[核函数] --> ed classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,linclas,linreg,nonlinreg,ed styStart Hello world : 首先，在本节，读者将了解Tensorflow的安装方法，并编写通过第一个简单的Tensorflow程序。 线性分类 : 本节通过解一个简单的二分类问题，我们将引入单层感知机、Logistic回归和交叉熵的概念，并且实验结果进行可视化。 线性回归 : 本节通过解一个与上一节难度相仿的，简单的回归问题，另读者能对比不同优化器的性能、特点，并介绍如何在项目中进行arugument parsing(引入项目的可选项)。 非线性回归 : 本节将修改上一节回归问题的激活函数，将线性的回归问题推广到解析函数参数回归的范畴。同时介绍自行编写网络层(类API)的方法。 非线性分类 : 本节将通过上一节的参数回归，引入核函数的概念，将线性分类问题推广到非线性空间里。","text_tokens":["：","!","]","相仿","二","另","一个","推广","对比","难度"," ","nonlinreg","-","如何","特点","分类","第一个","展示","argpar","可选项","里","读者","非线性","通过","fill","下图","linreg","实验","logsitc","器","程序","和","parsing","函数参数","网络层","hello","优化","，","ba9132","到","ce","解析","介绍","linclas","编写","与","节","空间","上","lr",",","一节","性能","回归","引入",";","学习","lp","[",":","stystart","同时","简单","网络","概念","能","问题","。","st","方法","函数","范畴","ed","定义","opt","在","不同","类","class","熵","修改","激活","并","的","fae6a9","感知","sdlayer","安装","para","交叉","核","进行","sigma","第一","线性","logistic","首先","arugument","#","选项","中","知机","解","单层","自定义","本","结果","了","本章","tensorflow","项目","并且","classdef","world","本节","将","层","我们","感知机","可视","了解","可选","api","可视化",">","(","自定","kernel","自行","graph",")","参数","、","stroke"],"title":"本章要点","title_tokens":["要点","本章"]},{"location":"book-1-x/chapter-1/hello-world/","text":"Hello world! ¶ 摘要 本节包含一个简易的安装指导，主要提供给Windows上的GPU用户。之后，用户可以按照本节指导编写第一个测试程序。 安装Tensorflow ¶ 本节针对Windows使用GPU的用户，提供一个安装最新版Tensorflow的方法。大致的步骤符合以下两个教程： 通过pip安装Tensorflow Tensorflow的GPU支持 但是，需要指出的是，官方教程里有些部分已经不符合当前用户的实际情况，笔者经过摸索，总结以下的安装过程。 总体而言，安装Tensorflow需要用户确保以下条件 NVIDIA驱动已经达到411.31及以上 CUDA 10.0 (10.1不确定能否正常使用) CUDNN已经达到7.3.1及以上 CUPTI版本与CUDA相配（不需要专门安装，在安装CUDA的同时已经自动集成） 除此之外，涉及多GPU支持的库(NCCL和TensorRT)，这里不考虑。因此，我们主要需要确保前三条正常安装。 提示 Tensorflow官方提供的安装包是基于CUDA 9.0，CUDNN >= 7.2的基础上编译的。然而现在NVIDIA已经不再提供CUDA 9.0了。Windows用户可以安装的版本包括过时的8.1和最新的10.1。在这种情况下，官方还提供一个令用户自行从源码编译Tensorflow库的方案，但是对于Windows用户而言过于繁琐，容易出错，这里不建议按照官方教程。 更新NVIDIA驱动 ¶ 首先，我们可以开始更新我们的NVIDIA驱动。任何载有NVIDIA-GPU显卡的电脑，都应当在系统里已经预装好了 GeForce Experience 。点击如下图标开启GeForce Experience（如果是低版本的GeForce Experience，图标可能会不同）， 如左图，在主界面上，可以观察到，当驱动版本不够高时，会自动出现更新提示，依照提示更新即可。建议在更新驱动时，关闭其他所有程序，以免造成干扰。 更新NVIDIA driver 检查更新结果 如果更新完成，如右图，可以在设置界面检查到当前的版本已经达到最新。在安装过程中，由于驱动更新，屏幕可能会闪烁数次。在看到更新提示完成后建议重启。 提示 如果GeForce Experience提示无法更新驱动，可以考虑将机器恢复出厂设置，可能可以解决这一问题。 安装CUDA ¶ 驱动更新并重启后，需要安装最新版的Visual Studio(VS)，这是因为CUDA库本身对VS具有一定的支持，如果先安装CUDA，则这部分支持无法安装到位。这对我们使用Tensorflow并不造成妨害，但是对CUDA的用户并不是一个推荐的做法，所以无论如何，建议在第一步，安装VS，下面提供VS社区版（免费）的链接： Visual Studio官网 接下来，可以开始安装CUDA。进入CUDA安装包的下载链接 下载最新版CUDA CUDA官方指南 提示 须知，并非最新版就是最适合的版本。实际能使用的CUDA版本取决于后续步骤中预编译包的支持版本。就笔者写到这里时，最新的CUDA 10.1已然不匹配最新版预编译包支持的CUDA 10.0，由于版本差距很小，不确定是否会由此引发问题，但读者可以自行尝试。 现在的安装包已经支持在线安装包和离线安装包两种模式，读者可以根据自己的实际情况选择对应的版本。注意，安装CUDA的时候，CUDA可能要求我们翻新驱动版本，但通过GeForce Experience安装的驱动一般应已达到最新，新于CUDA内部提供的驱动。因此，我们需要选择定制安装模式，根据我们需要对比版本号，将驱动更新的选项取消，如下图所示 安装CUDA后，建议重启。 危险 特别需要注意的是，如果你的机器无法通过GeForce Experience更新驱动，那么在这里选择安装驱动也是枉然。并且，这种强制更新驱动的行为很可能导致系统崩溃。因此特别建议在安装CUDA之前一定要解决驱动更新的问题，哪怕是通过恢复出厂设置。 安装CUDNN ¶ 安装CUDA的同时，已经自带安装上了CUPTI。但是CUDNN并没有包含在CUDA中，因此，我们需要到以下链接下载CUDNN并查看安装说明 下载最新版CUDNN CUDNN官方指南 不同的是，CUDNN没有安装包，是以压缩包的形式下载到本地。解压后，其中应当包含如下目录及文件 . └─ cuda/ ├─ bin/ # Binary library │ └─ cudnn64_7.dll ├─ include/ # C++ Include file │ └─ cudnn.h ├─ lib/ # C++ Lib file │ └─ x64/ │ └─ cudnn.lib └─ NVIDIA_SLA_cuDNN_Support.txt # Readme file 假设我们CUDA的安装目录在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，将上述解压的 bin , include 和 lib 三个文件夹，直接拷贝覆盖到该安装目录下，即可完成CUDNN的安装。 进行完上述步骤后，我们还需要确保几个环境变量正确设置： 存在 CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，是我们的安装目录。理论上在CUDA安装后，该目录已经自动设置好。 环境变量 PATH 中，存在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin , C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/extras/CUPTI/libx64 两个路径，由于CUDNN已经配置在CUDA安装目录下，我们不需要像官方文档那样，添加第三个CUDNN的安装路径。 安装Anaconda ¶ Windows用户建议使用Anaconda管理python环境。作为一个开源的项目，Anaconda发行版已经集成了我们所需要的大多数python包，其中有些包是我们自己难以安装上的，例如支持python3的PIL。我们选用python3版的Anaconda，下载路径如下： Anaconda 最新版下载链接 注意选择x64版的安装包。安装结束后，我们已经有python3.7的环境了。鉴于有些情况下我们可能需要使用更早的python版本，接下来我们可以安装python3.6的虚环境。 Anaconda可以安装在用户或系统目录下。注意，如果安装在系统目录下，如果不在虚环境下，安装任何包都需要管理员模式。不过这不影响本教程的示例，因为我们将安装虚环境。直接打开 Anaconda Prompt ，并键入以下命令： conda create -n py36 python = 3 .6 anaconda 按照引导流程安装，经过一段时间后，我们将得到python3.6版的Anaconda。以后我们可以直接从开始菜单 Anaconda Prompt (py36) 进入该虚环境，也可以在基环境中键入 activate py36 转换到虚环境中。 提示 如果用户愿意，还可以通过 conda install -c conda-forge opencv 安装 opencv 。opencv是一个强大的图像处理库，这里python版的opencv比C++版更容易上手。一般该命令安装的是conda库中最新版的opencv（截至笔者写到这里时是opencv3.4.4）。然而，这样安装的并非是支持GPU的版本。尽管conda还提供一个GPU的opencv版本，可以通过 conda install -c patricksnape opencv-gpu 安装 opencv2 ，但它的版本还停留在2.4.9，和opencv3的用法殊有不同，请读者注意这一点。 安装预编译好的Tensorflow ¶ 可以通过 pip install tensorflow-gpu == 查看Tensorflow是否有官方发行的新版。当然，使用GPU的用户要特别注意最新版是否和你预装的驱动匹配，尤其是CUDA是否匹配，否则Tensorflow可能无法正常工作。 在官方发行版不适合我们使用的时候，我们也可以查看如下第三方发行的项目： fo40225/tensorflow-windows-wheel 该项目的维护者在不断根据Tensorflow的更新，预编译出适合不同版本的Tensorflow安装包。截至笔者写到这里为止，最新支持到r1.12版。 原则上来讲，我们选择尽可能新的版本。有时候官方发行版对CUDA的支持滞后，因此我们可以选择第三方版。无论选择哪种发行方，要安装Tensorflow，我们需要选择对应的GPU版，并在虚环境下执行以下命令： 官方版 pip install --upgrade tensorflow-gpu 第三方版 pip install https://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.12.0/py36/GPU/cuda100cudnn73sse2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl 第三方版(CPU AVX2加强) curl -o tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.001 https://raw.githubusercontent.com/fo40225/tensorflow-windows-wheel/master/1.12.0/py36/GPU/cuda100cudnn73avx2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.001 curl -o tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.002 https://raw.githubusercontent.com/fo40225/tensorflow-windows-wheel/master/1.12.0/py36/GPU/cuda100cudnn73avx2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.002 \"C:/Program Files/7-Zip/7z.exe\" x tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.001 # Note that you need to specify where your 7-zip gets installed. pip install tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl 等待一段时间后，安装将会结束。 Hello world! 测试 ¶ 撰写如下代码，保存到 hello-world.py 文件， Python 1 2 3 4 5 6 7 8 9 if __name__ == '__main__' : import tensorflow as tf test_str = tf . constant ( 'Hello, world!' ) test_cal = tf . reduce_sum ( tf . random_normal ([ 1000 , 1000 ])) with tf . Session () as sess : print ( 'Current Tensorflow version is:' , tf . __version__ ) print ( 'Test string: ' , sess . run ( test_str )) print ( 'Test calculation:' , sess . run ( test_cal )) Output Current Tensorflow version is: 1 .13.1 Test string: b 'Hello, world!' Test calculation: 955 .5544 在保存目录下，执行 python hello-world.py 第一次运行可能需要等待较长的初始化时间（硬件配置）。如果能正常运行，我们将看到如下结果： 其中，显示结果之前的记录信息反映了GPU已经可以正常工作。而“Test calculation”显示的结果是 10^6 10^6 个服从 \\mathcal{N}(0,1) \\mathcal{N}(0,1) 分布的随机数之和。根据相互独立的随机正态分布互不相关的特性，我们可以推算出，该结果服从 \\mathcal{N}(0,10^3) \\mathcal{N}(0,10^3) 的正态分布。","text_tokens":["8","一个","要求","库","windows","官方","开始","出","存在","一步","to","无论如何","等待","已然","开始菜单","import","版","第一个","取决于","955","读者","例如","压缩","第三方","/","6","通过","得到","install","两个","或","studio","10","main","符合","时候","5","可能","cudnn","当然","比","installed","是因为","，","不断","\"","1000","已经","选用","10.1","computing","amd64","你","与","部分","维护","正态分布","打开","最新","也","编译","py","多","写","危险","本地","cpu","屏幕","系统","n","txt","测试程序","str","及","¶","时间","方法","右图","无法","菜单","cp36m","即可","管理员","7","相互","选择","新于","并","的","三条","摸索","命令","图","环境变","愿意","手","forge","4","定制","反映","normal","进行","所有","给","第一","不","总结","x64","新版","support","test","中预","注意","当前","│","界面","1.12","所以","{","toolkit","h","笔者","保存","以","tensorflow","libx64","0","1","文件","自己","指导","涉及","tensorrt","但是","visual","path","检查","记录","12","include","zip","而言","更","路径","geforce","所","vs","开源","wheel","尤其","pil","!","有时","哪","experience","发行","可以","出厂","流程","github","按照","用户","进入","-","接下","须知","理论","包含","program","官网","activate","win","但","版本号","尝试","}","session","大多","源码","githubusercontent","需要","考虑","由此","和","（","显示","实际","基于","机数","string","9.0","处理","特性","配置","随机数","取消","编写","确保","造成","节","name","这种","独立","下来","upgrade","10.0","更新","as","内部","形式",":","同时","行为","is","如","问题","8.1","r1","经过","应已","干扰","正确","nvidia","c++","从","'","7z","机器","因为","三方","完","with","影响","一段时间","硬件","强大","引发","在线","确定","段时间","观察","预装","解压","结束","强制","其中","master","殊有","calculation","whl","所示","sla","这部","└","难以","首先","先","情况","where","由于","种","version","做法","file","py36","大多数","执行","you","匹配","结果","了","specify","项目","推算出","无论","包括","其他","环境变量","初始","上述","我们","过于","开启","使用","cal","个","\\","过程","否则","driver","并非","相关","这里","多数","它","v10","包都","]","完成","后","停留","library","不再","以后","三个","这","之外","dll","图标","下","如何","算出","低版本","pip","鉴于","接下来","7.2","差距","服从","查看","官方版","总体而言","__","不过","raw","要","正常","conda","哪怕","不够","信息","枉然","第三","键入","hello","依照","files","lib","到","引导","工作","一定","if","令","前","第三个","发行版","基","变量","添加","文档","摘要","压缩包","包是","集成","reduce","对","能否","自带","cp36","https","）","这部分","2","cuda100cudnn73avx2","重启","[","根据","nccl","然而","闪烁","binary","文件夹","示例","9","能","原则","相配","。","最新版","应当","在","第一次","除此","该","大致","教程","为止","anaconda","安装","current","推算","库中","安装包","note","包","gets","滞后","除此之外","第一步","过时","取决","^","这一","curl","python","关闭","作为","转换","都","以免","推荐","直接","tf","opencv2","显卡","prompt","方","13.1","一段","readme","像","就是","载有","几个","一次","并且","cuda","x","模式","当","驱动","下面","o","提供",">","提示","自行","是否","环境",")","fo40225","左图","最","用法","有些","基础","：","是","如下","图像处理","“","blob","数次","特别","尽可能","翻新","that","随机","具有","对比","两种","7.3","虚","这样"," ","bin","主","=","电脑","不是","链接","件夹","因此","之前","加强","里","社区","出错","很小","一点","自动","”","假设","妨害","新","001","互不","主要","程序","一般","来讲","mathcal","your","python3","看到","离线","点击","指出","有时候","免费","411.31","较长","c","上","该项","维护者",",","sum","以下","create","4.4","cuda100cudnn73sse2","达到","运行","原则上","出现","预","图像","print","拷贝","截至","会","本身","步骤","请","很","cupti",".","目的","之","适合","支持","avx2","002","分布","设置","不同","专门","目录","下载","简易","尽可","容易","没有","早","constant","代码","测试","现在","对应","低版","之后","3","patricksnape","系统目录","b","_","解决","exe","后续","├","总体","com","尽管","崩溃","对于","─","将会","random","繁琐","而","#","need","选项","条件","时","中","opencv","output","撰写","那样","则","管理","2.4","指南","opencv3","sess","以上","说明","针对","本","任何","extras","版本","建议","就","好","还","方案","初始化","有","world","将","导致系统","覆盖","run","到位","导致","恢复","5544","(","cudnn64","如果","高时","那么","gpu"],"title":"Hello world!","title_tokens":[" ","world","hello","!"]},{"location":"book-1-x/chapter-1/hello-world/#hello-world","text":"摘要 本节包含一个简易的安装指导，主要提供给Windows上的GPU用户。之后，用户可以按照本节指导编写第一个测试程序。","text_tokens":["测试程序","主要","程序","。","一个","，","可以","windows","按照","用户","本"," ","简易","编写","摘要","节","上","包含","的","测试","指导","第一个","安装","之后","提供","给","第一","gpu"],"title":"Hello world!","title_tokens":[" ","world","hello","!"]},{"location":"book-1-x/chapter-1/hello-world/#tensorflow","text":"本节针对Windows使用GPU的用户，提供一个安装最新版Tensorflow的方法。大致的步骤符合以下两个教程： 通过pip安装Tensorflow Tensorflow的GPU支持 但是，需要指出的是，官方教程里有些部分已经不符合当前用户的实际情况，笔者经过摸索，总结以下的安装过程。 总体而言，安装Tensorflow需要用户确保以下条件 NVIDIA驱动已经达到411.31及以上 CUDA 10.0 (10.1不确定能否正常使用) CUDNN已经达到7.3.1及以上 CUPTI版本与CUDA相配（不需要专门安装，在安装CUDA的同时已经自动集成） 除此之外，涉及多GPU支持的库(NCCL和TensorRT)，这里不考虑。因此，我们主要需要确保前三条正常安装。 提示 Tensorflow官方提供的安装包是基于CUDA 9.0，CUDNN >= 7.2的基础上编译的。然而现在NVIDIA已经不再提供CUDA 9.0了。Windows用户可以安装的版本包括过时的8.1和最新的10.1。在这种情况下，官方还提供一个令用户自行从源码编译Tensorflow库的方案，但是对于Windows用户而言过于繁琐，容易出错，这里不建议按照官方教程。","text_tokens":["：","是","一个","可以","库","windows","不再","官方","7.3","按照","用户","之外"," ","下","pip","=","7.2","因此","里","总体而言","出错","通过","两个","正常","源码","符合","自动","需要","考虑","cudnn","和","主要","（","实际","，","基于","已经","10.1","令","前","9.0","指出","确保","411.31","与","节","集成","上","这种","部分","能否","10.0","以下","最新","）","达到","编译","多","同时","步骤","nccl","然而","及","cupti","相配","8.1",".","经过","。","支持","方法","nvidia","最新版","在","从","专门","除此","大致","容易","教程","的","三条","摸索","确定","现在","安装","安装包","总结","不","总体","对于","除此之外","繁琐","过时","新版","情况","条件","当前","以上","针对","本","了","笔者","版本","建议","tensorflow","包括","cuda","还","方案","1","涉及","tensorrt","但是","我们","过于","驱动","提供","使用",">","过程","(","提示","自行","而言",")","有些","基础","这里","gpu"],"title":"安装Tensorflow","title_tokens":["安装","tensorflow"]},{"location":"book-1-x/chapter-1/hello-world/#nvidia","text":"首先，我们可以开始更新我们的NVIDIA驱动。任何载有NVIDIA-GPU显卡的电脑，都应当在系统里已经预装好了 GeForce Experience 。点击如下图标开启GeForce Experience（如果是低版本的GeForce Experience，图标可能会不同）， 如左图，在主界面上，可以观察到，当驱动版本不够高时，会自动出现更新提示，依照提示更新即可。建议在更新驱动时，关闭其他所有程序，以免造成干扰。 更新NVIDIA driver 检查更新结果 如果更新完成，如右图，可以在设置界面检查到当前的版本已经达到最新。在安装过程中，由于驱动更新，屏幕可能会闪烁数次。在看到更新提示完成后建议重启。 提示 如果GeForce Experience提示无法更新驱动，可以考虑将机器恢复出厂设置，可能可以解决这一问题。","text_tokens":["如下","是","数次","完成","experience","可以","后","出厂","开始","图标"," ","主","-","低版本","电脑","里","自动","不够","可能","考虑","程序","（","依照","，","到","已经","看到","点击","造成","上","最新","）","更新","达到","出现","屏幕","重启","系统","会","闪烁","如","问题","。","干扰","设置","nvidia","应当","不同","在","右图","无法","机器","即可","的","安装","观察","预装","低版","所有","解决","首先","这一","中","时","关闭","由于","当前","都","界面","以免","显卡","任何","结果","了","载有","版本","建议","好","其他","将","我们","当","检查","驱动","开启","恢复","过程","提示","如果","高时","driver","左图","geforce","gpu"],"title":"更新NVIDIA驱动","title_tokens":["驱动","更新","nvidia"]},{"location":"book-1-x/chapter-1/hello-world/#cuda","text":"驱动更新并重启后，需要安装最新版的Visual Studio(VS)，这是因为CUDA库本身对VS具有一定的支持，如果先安装CUDA，则这部分支持无法安装到位。这对我们使用Tensorflow并不造成妨害，但是对CUDA的用户并不是一个推荐的做法，所以无论如何，建议在第一步，安装VS，下面提供VS社区版（免费）的链接： Visual Studio官网 接下来，可以开始安装CUDA。进入CUDA安装包的下载链接 下载最新版CUDA CUDA官方指南 提示 须知，并非最新版就是最适合的版本。实际能使用的CUDA版本取决于后续步骤中预编译包的支持版本。就笔者写到这里时，最新的CUDA 10.1已然不匹配最新版预编译包支持的CUDA 10.0，由于版本差距很小，不确定是否会由此引发问题，但读者可以自行尝试。 现在的安装包已经支持在线安装包和离线安装包两种模式，读者可以根据自己的实际情况选择对应的版本。注意，安装CUDA的时候，CUDA可能要求我们翻新驱动版本，但通过GeForce Experience安装的驱动一般应已达到最新，新于CUDA内部提供的驱动。因此，我们需要选择定制安装模式，根据我们需要对比版本号，将驱动更新的选项取消，如下图所示 安装CUDA后，建议重启。 危险 特别需要注意的是，如果你的机器无法通过GeForce Experience更新驱动，那么在这里选择安装驱动也是枉然。并且，这种强制更新驱动的行为很可能导致系统崩溃。因此特别建议在安装CUDA之前一定要解决驱动更新的问题，哪怕是通过恢复出厂设置。","text_tokens":["：","如下","是","一个","特别","要求","experience","翻新","可以","后","库","出厂","具有","对比","官方","两种","开始","这","用户","一步","进入"," ","无论如何","如何","接下","已然","不是","链接","接下来","须知","版","差距","因此","取决于","之前","官网","读者","社区","geforce","通过","但","版本号","很小","尝试","要","studio","时候","哪怕","妨害","可能","枉然","需要","由此","和","（","一般","实际","是因为","，","到","已经","一定","10.1","离线","取消","免费","你","造成","这种","对","部分","下来","10.0","最新","）","更新","达到","也","编译","预","这部分","写","危险","重启","内部","根据","会","本身","行为","步骤","系统","很","能","问题","。","支持","适合","应已","设置","最新版","在","无法","下载","机器","因为","选择","新于","并","的","引发","在线","图","确定","现在","对应","安装","安装包","定制","强制","解决","包","后续","第一","不","所示","崩溃","第一步","这部","取决","先","新版","情况","选项","时","中预","由于","注意","则","做法","指南","所以","推荐","匹配","就是","笔者","建议","版本","tensorflow","就","无论","并且","cuda","自己","但是","我们","模式","将","visual","导致系统","驱动","下面","提供","使用","到位","导致","恢复","(","提示","自行","是否","如果",")","并非","最","vs","这里","那么"],"title":"安装CUDA","title_tokens":["cuda","安装"]},{"location":"book-1-x/chapter-1/hello-world/#cudnn","text":"安装CUDA的同时，已经自带安装上了CUPTI。但是CUDNN并没有包含在CUDA中，因此，我们需要到以下链接下载CUDNN并查看安装说明 下载最新版CUDNN CUDNN官方指南 不同的是，CUDNN没有安装包，是以压缩包的形式下载到本地。解压后，其中应当包含如下目录及文件 . └─ cuda/ ├─ bin/ # Binary library │ └─ cudnn64_7.dll ├─ include/ # C++ Include file │ └─ cudnn.h ├─ lib/ # C++ Lib file │ └─ x64/ │ └─ cudnn.lib └─ NVIDIA_SLA_cuDNN_Support.txt # Readme file 假设我们CUDA的安装目录在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，将上述解压的 bin , include 和 lib 三个文件夹，直接拷贝覆盖到该安装目录下，即可完成CUDNN的安装。 进行完上述步骤后，我们还需要确保几个环境变量正确设置： 存在 CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 ，是我们的安装目录。理论上在CUDA安装后，该目录已经自动设置好。 环境变量 PATH 中，存在 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin , C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/extras/CUPTI/libx64 两个路径，由于CUDNN已经配置在CUDA安装目录下，我们不需要像官方文档那样，添加第三个CUDNN的安装路径。","text_tokens":["v10","：","是","如下","完成","后","library","官方","三个","存在","dll"," ","bin","下","=","链接","理论","包含","件夹","program","因此","查看","压缩","/","两个","自动","假设","第三","需要","cudnn","和","files","，","lib","到","已经","第三个","变量","配置","computing","添加","文档","确保","压缩包","c","上",",","自带","以下","最新","本地",":","形式","拷贝","同时","步骤","binary","文件夹","txt","及","cupti",".","。","正确","设置","nvidia","最新版","应当","c++","不同","在","目录","该","下载","即可","7","完","没有","并","的","环境变","安装","安装包","解压","其中","进行","_","├","不","─","sla","x64","└","新版","support","#","中","由于","那样","│","指南","file","直接","toolkit","说明","readme","h","了","像","extras","几个","以","好","libx64","还","cuda","0","环境变量","文件","上述","但是","我们","将","path","覆盖","include","环境","cudnn64","路径","gpu"],"title":"安装CUDNN","title_tokens":["cudnn","安装"]},{"location":"book-1-x/chapter-1/hello-world/#anaconda","text":"Windows用户建议使用Anaconda管理python环境。作为一个开源的项目，Anaconda发行版已经集成了我们所需要的大多数python包，其中有些包是我们自己难以安装上的，例如支持python3的PIL。我们选用python3版的Anaconda，下载路径如下： Anaconda 最新版下载链接 注意选择x64版的安装包。安装结束后，我们已经有python3.7的环境了。鉴于有些情况下我们可能需要使用更早的python版本，接下来我们可以安装python3.6的虚环境。 Anaconda可以安装在用户或系统目录下。注意，如果安装在系统目录下，如果不在虚环境下，安装任何包都需要管理员模式。不过这不影响本教程的示例，因为我们将安装虚环境。直接打开 Anaconda Prompt ，并键入以下命令： conda create -n py36 python = 3 .6 anaconda 按照引导流程安装，经过一段时间后，我们将得到python3.6版的Anaconda。以后我们可以直接从开始菜单 Anaconda Prompt (py36) 进入该虚环境，也可以在基环境中键入 activate py36 转换到虚环境中。 提示 如果用户愿意，还可以通过 conda install -c conda-forge opencv 安装 opencv 。opencv是一个强大的图像处理库，这里python版的opencv比C++版更容易上手。一般该命令安装的是conda库中最新版的opencv（截至笔者写到这里时是opencv3.4.4）。然而，这样安装的并非是支持GPU的版本。尽管conda还提供一个GPU的opencv版本，可以通过 conda install -c patricksnape opencv-gpu 安装 opencv2 ，但它的版本还停留在2.4.9，和opencv3的用法殊有不同，请读者注意这一点。","text_tokens":["：","pil","包都","如下","是","图像处理","这里","一个","发行","可以","后","停留","库","windows","流程","以后","开始","这","按照","用户","虚","进入"," ","下","这样","-","接下","鉴于","=","接下来","链接","开始菜单","版","读者","例如","6","不过","activate","通过","得到","install","但","或","大多","conda","一点","可能","需要","和","（","比","键入","一般","，","到","引导","已经","python3","选用","发行版","基","处理","开源","包是","c","集成","上","用法","下来","以下","create","打开","最新","）","4.4","也","图像","写","截至","系统","n","9","然而","请","示例",".","时间","经过","。","支持","最新版","c++","不同","在","从","目录","该","下载","菜单","因为","管理员","7","容易","教程","选择","早","影响","并","一段时间","强大","的","anaconda","命令","段时间","愿意","安装","手","forge","安装包","库中","结束","3","patricksnape","其中","系统目录","包","殊有","不","尽管","x64","难以","新版","情况","python","中","opencv","作为","时","注意","转换","管理","2.4","py36","大多数","直接","opencv3","opencv2","prompt","本","任何","一段","了","笔者","建议","版本","项目","还","有","自己","将","我们","模式","提供","使用","(","提示","环境","如果","更",")","路径","并非","所","有些","多数","它","gpu"],"title":"安装Anaconda","title_tokens":["安装","anaconda"]},{"location":"book-1-x/chapter-1/hello-world/#tensorflow_1","text":"可以通过 pip install tensorflow-gpu == 查看Tensorflow是否有官方发行的新版。当然，使用GPU的用户要特别注意最新版是否和你预装的驱动匹配，尤其是CUDA是否匹配，否则Tensorflow可能无法正常工作。 在官方发行版不适合我们使用的时候，我们也可以查看如下第三方发行的项目： fo40225/tensorflow-windows-wheel 该项目的维护者在不断根据Tensorflow的更新，预编译出适合不同版本的Tensorflow安装包。截至笔者写到这里为止，最新支持到r1.12版。 原则上来讲，我们选择尽可能新的版本。有时候官方发行版对CUDA的支持滞后，因此我们可以选择第三方版。无论选择哪种发行方，要安装Tensorflow，我们需要选择对应的GPU版，并在虚环境下执行以下命令： 官方版 pip install --upgrade tensorflow-gpu 第三方版 pip install https://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.12.0/py36/GPU/cuda100cudnn73sse2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl 第三方版(CPU AVX2加强) curl -o tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.001 https://raw.githubusercontent.com/fo40225/tensorflow-windows-wheel/master/1.12.0/py36/GPU/cuda100cudnn73avx2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.001 curl -o tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.002 https://raw.githubusercontent.com/fo40225/tensorflow-windows-wheel/master/1.12.0/py36/GPU/cuda100cudnn73avx2/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.002 \"C:/Program Files/7-Zip/7z.exe\" x tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.7z.001 # Note that you need to specify where your 7-zip gets installed. pip install tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl 等待一段时间后，安装将会结束。","text_tokens":["尤其","：","是","如下","有时","哪","blob","特别","尽可能","发行","可以","后","that","windows","官方","出","github","用户","虚"," ","下","to","-","等待","pip","=","版","program","因此","加强","查看","官方版","第三方","/","raw","通过","win","install","要","正常","时候","githubusercontent","可能","第三","新","需要","001","和","当然","installed","files","，","不断","来讲","\"","到","工作","your","发行版","amd64","有时候","你","c","对","该项","维护者","upgrade","cp36","维护","以下","https","最新","更新","cuda100cudnn73sse2","也","原则上","编译","预","写","cpu","截至",":","cuda100cudnn73avx2","根据","原则","目的","r1",".","时间","。","适合","支持","avx2","002","最新版","不同","在","无法","7z","三方","cp36m","7","尽可","选择","为止","并","一段时间","的","命令","对应","段时间","安装","安装包","预装","结束","master","note","_","exe","gets","滞后","不","com","whl","将会","新版","#","need","curl","where","种","注意","1.12","执行","py36","you","方","匹配","一段","笔者","specify","版本","tensorflow","项目","无论","cuda","0","有","x","我们","驱动","o","使用","12","(","zip","环境","是否","否则",")","fo40225","这里","wheel","gpu"],"title":"安装预编译好的Tensorflow","title_tokens":["编译","预","的","安装","tensorflow","好"]},{"location":"book-1-x/chapter-1/hello-world/#hello-world_1","text":"撰写如下代码，保存到 hello-world.py 文件， Python 1 2 3 4 5 6 7 8 9 if __name__ == '__main__' : import tensorflow as tf test_str = tf . constant ( 'Hello, world!' ) test_cal = tf . reduce_sum ( tf . random_normal ([ 1000 , 1000 ])) with tf . Session () as sess : print ( 'Current Tensorflow version is:' , tf . __version__ ) print ( 'Test string: ' , sess . run ( test_str )) print ( 'Test calculation:' , sess . run ( test_cal )) Output Current Tensorflow version is: 1 .13.1 Test string: b 'Hello, world!' Test calculation: 955 .5544 在保存目录下，执行 python hello-world.py 第一次运行可能需要等待较长的初始化时间（硬件配置）。如果能正常运行，我们将看到如下结果： 其中，显示结果之前的记录信息反映了GPU已经可以正常工作。而“Test calculation”显示的结果是 10^6 10^6 个服从 \\mathcal{N}(0,1) \\mathcal{N}(0,1) 分布的随机数之和。根据相互独立的随机正态分布互不相关的特性，我们可以推算出，该结果服从 \\mathcal{N}(0,10^3) \\mathcal{N}(0,10^3) 的正态分布。","text_tokens":["：","8","!","如下","是","]","“","可以","随机"," ","下","-","算出","等待","=","import","之前","服从","955","6","__","}","正常","session","10","main","”","5","信息","可能","需要","互不","和","（","显示","hello","，","到","1000","已经","工作","mathcal","机数","看到","if","string","特性","配置","随机数","较长","name","reduce","独立",",","sum","正态分布","）","运行","print","py","as","2","[",":","根据","n","is","9","str","能",".","之","时间","。","分布","在","第一次","目录","该","'","7","相互","with","constant","代码","硬件","的","current","推算","4","b","3","反映","其中","normal","_","calculation","第一","random","而","^","python","test","output","撰写","version","执行","{","sess","tf","13.1","结果","了","保存","tensorflow","一次","推算出","初始化","0","world","1","初始","文件","将","我们","记录","cal","个","run","\\","5544","(","如果",")","相关","gpu"],"title":"Hello world! 测试","title_tokens":["world","!","测试"," ","hello"]},{"location":"book-1-x/chapter-1/linear-classification/","text":"线性分类 ¶ 摘要 本节介绍如何使用顺序模型(sequential model)来编写一个线性分类器，使用sigmoid函数激活，并验证其效果。在本节我们将编写第一个Project，并介绍一些基本概念、和一个推荐的Tensorflow Project的编写格式。 理论 ¶ 问题描述 ¶ 考虑我们有如下的二分类数据集 (\\mathbf{x},~y_i) \\in \\mathbb{D}_i (\\mathbf{x},~y_i) \\in \\mathbb{D}_i ，并且有一个未知的常数向量 \\mathbf{a}_i \\mathbf{a}_i 和未知的常数标量 c_i c_i ，使得： \\begin{equation} y_i = \\left\\{ \\begin{aligned} 0, && \\mathbf{a}_i^T \\mathbf{x} + c_i \\leqslant 0, \\\\ 1, && \\mathbf{a}_i^T \\mathbf{x} + c_i > 0. \\end{aligned} \\right. \\end{equation} 其中， \\mathbf{a} \\mathbf{a} 可以看成是某超平面的（未标准化的）法向量，那么 \\mathbf{a}^T \\mathbf{x} + c = 0 \\mathbf{a}^T \\mathbf{x} + c = 0 是该超平面的截距式定义，亦即该平面与 x_i x_i 轴的交点可以显式表述为 x_i^{(0)} = - \\frac{c}{a_i} x_i^{(0)} = - \\frac{c}{a_i} 。由此可知，式 (1) (1) 显式定义了一个点在超平面的哪一侧。特别地，若 \\mathbf{x} \\mathbf{x} 是一个二维向量，则该超平面退化为一维平面；若 \\mathbf{x} \\mathbf{x} 是一个标量，则该超平面退化为一条直线。 若我们定义 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} ，有 \\begin{equation} \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, ~~ \\mathbf{A} = \\begin{bmatrix} \\mathbf{a}^T_1 \\\\ \\mathbf{a}^T_2 \\\\ \\vdots \\\\ \\mathbf{a}^T_n \\end{bmatrix}, ~~ \\mathbf{c} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}. \\end{equation} 则我们可以认为 \\begin{align} \\mathbf{y} = \\left\\{ \\begin{bmatrix}\\hat{y}_1 > 0 & \\hat{y}_2 > 0 & \\cdots & \\hat{y}_n > 0\\end{bmatrix}^T, ~ \\left| ~ \\hat{\\mathbf{y}} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon} \\right. \\right\\}, \\end{align} 其中 \\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon} 是一个定义噪声的向量。 我们可以把向量 \\mathbf{y} \\mathbf{y} 的元素看成是互不相关的多个超平面对向量 \\mathbf{x} \\mathbf{x} 各自独立的分类结果。即 y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} 。由于每个超平面构成一个二分类，如果把每个二分类看作是向量是否属于这个类的测度，那么 \\mathbf{y} \\mathbf{y} 可以被看作是一个多分类的结果，尽管向量 \\mathbf{x} \\mathbf{x} 可能被同时分入多个类中。 假设我们的数据集 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} 符合 (3) (3) 定义的数据分布特征。我们的基本要求是，在我们不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 的情况下，使用大量 (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} 样本训练一个线性分类器，使得当我们给定任意一个新样本 \\mathbf{x} \\mathbf{x} 的时候，分类器能推断出其对应的 \\mathbf{y} \\mathbf{y} 来（亦即是否属于该分类）。 须知 请注意我们在这里说到“线性分类器”，虽然使用“线性”一词，但准确来说，这是一个仿射变换。因为线性变换要求有齐次性，即 f(x) = \\alpha f(x) f(x) = \\alpha f(x) ，但仿射变换允许我们引入一个平移向量 \\mathbf{b} \\mathbf{b} 。当然，我们的求解的线性问题本身也是一个仿射变换。 在这个问题里，我们虽然不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，但我们知道由 (3) (3) 确定的线性关系，因此，我们可以随机生成一组 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ，构建线性模型： \\begin{align} \\tilde{\\mathbf{y}} = \\sigma ( \\mathbf{W} \\mathbf{x} + \\mathbf{b} ). \\end{align} 其中，可微函数 \\sigma \\sigma 是一个将实数空间 \\mathbb{R}^n \\mathbb{R}^n 映射到有限范围的实数空间 [0,~1]^n [0,~1]^n 内的函数。特别地， \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 。因此，可以将 \\sigma \\sigma 看作是二分类布尔函数的插值函数。理论上，只要我们找到 \\mathbf{W}=\\mathbf{A} \\mathbf{W}=\\mathbf{A} ， \\mathbf{b}=\\mathbf{c} \\mathbf{b}=\\mathbf{c} ，则该线性分类器可以直接拟合出原分布来。 感知机 ¶ 我们将 (4) (4) 定义的线性模型称为 单层感知机 (Single-layer perceptron) 模型。它包含一个权重矩阵 \\mathbf{W} \\mathbf{W} 和一个偏置矩阵 \\mathbf{b} \\mathbf{b} 。事实上，可以将 (4) (4) 改写成如下形式 \\begin{align} \\tilde{\\mathbf{y}} = \\sigma \\left( \\begin{bmatrix} \\mathbf{W} & \\mathbf{b} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ 1 \\end{bmatrix} \\right). \\end{align} 可见偏置本身可以看成是输入向量多了一个常数元素的等价模型。 感知机是最早的神经网络形式，它非常孱弱，只能解线性问题，但却为神经网络后来的发展开了先河。在单层感知机里，我们视输入向量 \\mathbf{x} \\mathbf{x} 的每个元素为一个“神经元”，矩阵 \\mathbf{W} \\mathbf{W} 和偏置 \\mathbf{b} \\mathbf{b} 将我们的输入映射到输出层 \\mathbf{y} \\mathbf{y} ，输出层的每个元素也视为一个神经元。在这个过程中， W_{ij} W_{ij} 作为 i i 行 j j 列的元素，代表了连接两个神经元的权重。我们用红线代表正值，蓝线代表负值，感知机可以被图示为 线性感知机的输出也是输入的线性组合，但我们可以添加激活函数，即 \\sigma(\\cdot) \\sigma(\\cdot) 将其映射到非线性空间。这要求我们添加的激活函数是一个非线性函数。 事实上，将单层感知机层叠，前一层的输出作为后一层的输入，就构建出早期的神经网络。这种网络每一层都是全连接的（两个神经元之间总是有权重，尽管值可能为0），每一层都有激活函数。理论上，任意一个两层堆叠的感知机，只要神经元数目足够多，就可以拟合出任意一个非线性函数。然而，实际测试中，这一理论的效果并不尽如人意，因此又有陆续地改进，才有了后来的深度学习。饮水思源，鉴往知来，我们也将从这个简简单单的单层模型开始，走上学习“深度学习”之旅。 Sigmoid函数 ¶ 在上述介绍中，我们没有解决的两个问题是， 如何定义插值函数 \\sigma \\sigma ？ 如何找到合适的 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ？ 我们首先讨论第一个问题。一般地，多分类问题中，如果各个分类彼此并非相斥，且不一定要将结果分入任一类的话，我们可以用 Sigmoid 函数来定义 \\sigma \\sigma ，亦即 \\begin{align} \\sigma(\\mathbf{x}) = \\frac{1}{ 1 + e^{-\\mathbf{x}}}. \\end{align} 它同时满足 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 ，且是一个单调函数。以下代码向我们展示了这种函数的特性： Python 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import matplotlib.pyplot as plt def test_sigmoid (): x = np . linspace ( - 5 , 5 , 100 ) y = 1 / ( 1 + np . exp ( - x ) ) plt . plot ( x , y ) plt . xlabel ( 'x' ), plt . ylabel ( 'y' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () if __name__ == '__main__' : test_sigmoid () Output 使用sigmoid函数的一大好处是，它的导数求解非常简单，很适合用来做神经网络这样一个复杂模型的激活函数。注意虽然 \\sigma(\\mathbf{x}) \\sigma(\\mathbf{x}) 和 \\mathbf{x} \\mathbf{x} 都是向量，这意味着导数是Jacobian矩阵，但由于 \\sigma \\sigma 是一个对 \\mathbf{x} \\mathbf{x} 各元素独立的解析函数，这个Jacobian矩阵实际上是一个对角矩阵，对角线上第j个元素的值为 \\begin{align} \\left. \\frac{ \\partial \\sigma(x) }{ \\partial x } \\right|_{x=x_j} = \\left. - e^{-x} \\left( - \\frac{1}{\\left( 1+e^{-x} \\right)^2} \\right) \\right|_{x=x_j} = - \\sigma(x_j) ( 1 - \\sigma(x_j) ). \\end{align} 可见，该函数的导数和计算函数本身的复杂度相若，可以做到快速求导。 求解问题 ¶ 接下来，我们需要解决第二个问题，亦即找到 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} 。这一问题通常可以写成反问题的形式： \\begin{align} \\arg \\min_\\limits{\\mathbf{W},~\\mathbf{b}} \\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}^{(k)},~ \\sigma ( \\mathbf{W} \\mathbf{x}^{(k)} + \\mathbf{b} ) \\right). \\end{align} 最简单的情况下，我们可以把 损失函数(loss function) 定义为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = \\lVert \\mathbf{y} - \\tilde{\\mathbf{y}} \\rVert_2^2. \\end{align} 我们称 (8) (8) 为 逻辑斯蒂回归(logistic regression) 。有趣的是，虽然这个术语叫“回归”，但它解的其实是个分类问题。但是，既然这是一个分类问题，我们可以不使用这个损失函数，而是从概率论的角度看待这个问题。由此，我们引出一个新的损失函数：“交叉熵”。 交叉熵 ¶ 我们视sigmoid函数输出的值为一个概率，表示分类器对预测结果的确信程度，记 \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} ，则 \\begin{equation} \\begin{aligned} \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}), \\\\ \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) &= 1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}). \\end{aligned} \\end{equation} 注意这里的概率向量的含义是，其中第i个元素表明第i个超平面分类结果的 确信程度 。 然而，这个概率只是分类器对分类结果的确信程度，却并非是分类准确度的概率，实际上，分类 准确度 的概率，应当表述为 \\begin{equation} \\begin{aligned} \\mathbf{p}(\\mathbf{y}|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta})^{1-\\mathbf{y}}\\\\ &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\left(1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}) \\right)^{1-\\mathbf{y}}. \\end{aligned} \\end{equation} 注意 这里 \\mathbf{x}^{\\mathbf{y}} \\mathbf{x}^{\\mathbf{y}} 表示的是对每个元素一一求取指数，即函数第i个元素的返回值应当为 {x_i}^{y_i} {x_i}^{y_i} 。 我们使用真实值 \\mathbf{y} \\mathbf{y} 作为指数给概率向量加权。当 \\mathbf{y}=1 \\mathbf{y}=1 时，乘积项的第二个因子消去，该函数退化为以预测值为1的可信度 \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) ；反之则第一个因子消去，退化为以预测值为0的可信度 \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) 。这就是最大似然估计方法。至此，我们可以写出似然估计函数 \\begin{align} L(\\boldsymbol{\\Theta}) = \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}). \\end{align} 对似然估计函数取对数，则有 \\begin{equation} \\begin{aligned} l(\\boldsymbol{\\Theta}) &= \\sum_{k=1}^N \\log \\left( \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}) \\right) \\\\ &= \\sum_{k=1}^N \\mathbf{y}^{(k)} \\cdot \\log\\left(\\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right) + \\left(1 - \\mathbf{y}^{(k)} \\right) \\cdot \\log\\left(1 - \\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right). \\end{aligned} \\end{equation} 我们最终的目的是要最大化似然函数，亦即 \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) ，这等价于最小化 -l(\\boldsymbol{\\Theta}) -l(\\boldsymbol{\\Theta}) 。对比 (8) (8) 和 (9) (9) ，于是我们可以定义交叉熵为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = - \\mathrm{mean}\\left[ \\mathbf{y} \\cdot \\log\\left( \\tilde{\\mathbf{y}} \\right) + \\left(1 - \\mathbf{y} \\right) \\cdot \\log\\left(1 - \\tilde{\\mathbf{y}} \\right) \\right]. \\end{align} 注意这里我们使用 \\mathrm{mean}\\left[ \\cdot \\right] \\mathrm{mean}\\left[ \\cdot \\right] 表示求取一个向量所有元素的平均值。实际上，Tensorflow允许我们定义损失函数的输出为一个和输出向量维度相同的向量，Tensorflow自带的交叉熵也是这样定义的。实际应用时，Tensorflow会自动在向量维度上求均值，并压缩成上述 (14) (14) 的形式。 若我们记 \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) ，代入sigmoid函数，为了确保该损失函数的稳定性，我们可以将 (14) (14) 整理为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{x}} \\right) = \\mathrm{mean}\\left[ \\max(\\tilde{\\mathbf{x}}, \\mathbf{0}) - \\tilde{\\mathbf{x}} \\cdot \\mathbf{y} + \\log\\left(1 + e^{-|\\tilde{\\mathbf{x}}|} \\right) \\right]. \\end{align} 提示 这里交叉熵整理的推导过程参见 Tensorflow-API官方文档 。 实际情况下，我们使用 (15) (15) 来求取sigmoid函数激活下的交叉熵。 解线性多分类问题 ¶ 接下来，我们将开始实战上手，编写我们的第一个Project。虽然一个Project的格式并无定法，每个人按照自己的喜好会选择不同的风格，但一个从无受过训练的人，往往写出的Project几乎完全不具有可读性。实际上，学习任何语言， 变量命名规范 、 缩进规范 以及 模块化 、 面向对象 等都被认为是编写一个具有可读性的代码所不得不知的概念。本教程所推荐的代码格式，均具有统一的风格，读者在了解每个Project和其对应的教程时，会慢慢熟悉这种风格的特点。愿读者能从这样的风格中得到启发，得到 代码可读性 的神髓。 代码规范 ¶ 建立一个具有较强可读性的Tensorflow工程需要我们活用python的模块化设计。我们通常推荐以下的结构 . ├─ data/ # where we store our data │ └─ ... ├─ tools.py # codes for post-processing and analyzing records. ├─ extension.py # codes for extending the tensorflow model. ├─ dparser.py # data parser └─ main.py # main module where we define our tensorflow model. 除了保存数据的文件夹，我们应当有三个子模块。其中 tool : 用来处理、分析生成的数据，通常与Tensorflow无关； extension : 用来扩展tensorflow，例如在这里自定义网络层和操作符； dparser : 数据处理器，用来读取并预处理送入网络的数据； main : 主模块，只定义跟Tensorflow模型有关的内容，需要引用 extension 和 dparser 。 视情况可以灵活调整结构，但建议将定义Tensorflow模型的代码单独放在主模块里，和其他外围代码分离。 撰写各个模块时，建议使用类封装各组功能相同的函数。具有良好使用习惯的coder应当注意给各个面向用户的类、函数撰写（哪怕简短的）说明文字，在一些较长的函数、方法的定义中，适当注释各部分的功能，以便读者能正确理解代码意义。 另外，在对象命名上，python有如下必须遵守或不成文的规定，和C/C++用户熟悉的蛇形命名法不同，它大致包括 类与函数多用驼峰命名法，变量可以采用驼峰或蛇形命名法。 驼峰命名法指的是用大小写区分每个单词块，例如 alphaBetaFunction () ； 蛇形命名法指的是用下划线区分每个单词块，例如 alpha_beta_function = 10 ； 宏变量使用全字大写+蛇形命名法 函数/方法，还有模块均是首字母小写，但类的首字母大写。 用单下划线 _ 表示临时存储器，或省略参数，例如一个函数 func () 有两个返回值时，可以用 _ , b = func () 表示我们只需要第二个返回值；单下划线还可以与星号连用省略多个返回值； 以单下划线开头的方法，表示模块级的私有方法，在模块以外使用 import 导入类时，不会导入这些方法，例如 def _alphaBeta ( self ): ； 以单下划线结尾的对象，用来和python的关键字区分，例如 func ( x , class_ ) ; 以双下划线开头的方法，如果不以双下划线结尾，则表示类级的私有方法，只有类内部的方法能调用这些方法，在类外部、包括继承的子类里都原则上不能调用（但其实也有办法调用），例如 def _alphaBeta ( self ): ； 以双下划线同时开头和结尾的方法，一般是用来 重写 (override) 特殊功能，例如 def __getattribute__ (): 将重写获得类属性的方法。 Tensorflow的数据概念 ¶ 在Tensorflow中，我们把变量都称为“ 张量 (Tensor) ”。这是因为我们有零维的标量，一维的向量，二维的矩阵，更高维的我们都称为张量。作为一个更大的概念，张量当然也可以用来包括标量、向量和矩阵了。在Tensorflow中，有的张量是 可以训练 (trainable) 的，有的则不是。比如一个张量的形状（指各维大小），当然可以是一个 < tf . Tensor 'Shape:0' shape = ( 1 ,) dtype = int32 > 类型的张量，但它不是变量，当然就不可训练。我们也可以人为控制某些张量可以训练或不可以训练，但本节、乃至本章所介绍的凡是我们接触到的张量，都是可以训练的。 特别地，对于神经网络而言，在网络内计算（或者说流动、传播）的一个n维数据，通常按照以下形式组织： tensor [ batch , dim1 , dim2 , ... , dimn , channel ] 其中，第一个维度 batch 一定存在，它表示的是单个batch中的某一个样本。如果一个batch只有一个样本，那么 batch 只能取0。 从 dim1 到 dimn 指的是实际的n维数据的各个维度； channel 指的是数据的通道，例如，一个二维RGB图像，每种颜色代表一个通道，因此有三个通道。 channel 通常用在卷积网络里，我们经常需要在深度卷积网络里不断增大通道数的同时，缩小数据尺寸。 在某些特殊情况下， channel 维度可以不存在，例如我们使用的是全连接层而不是卷积网络， tf.keras.layer.Flatten 可以用来将一个有通道的张量压缩成一个没有通道的一维向量（但是注意 batch 维度仍然存在，不会被压缩）。 因此，我们知道一个n维的数据，在神经网络中通常被描述为一个n+2维的矩阵，而一个一维向量，在卷积网络里是三维的： vector [ batch , length , channel ] 但是在全连接网络里，是二维的： vector [ batch , channel ] 在本节，乃至本章里，我们还不讨论卷积网络，因此我们都是使用二维张量（一维向量组）作为我们的数据。 数据生成 ¶ 在本项目里，我们不需要扩展Tensorflow。但是，我们需要以随机生成数据代替数据集。因此，首先，通过以下代码定义数据生成器 dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class TestDataSet : ''' A generator of the data set for testing the linear model. ''' def __init__ ( self , scale_x , A , c ): ''' Initialize the data generator. scale_x: the scale of input vector. A, c: the linear transformation. ''' self . s_x = 2 * scale_x self . A = A self . c = c self . len_x = A . shape [ 0 ] self . config () def config ( self , train = True , batch = 100 , noise = 0.1 ): ''' Configuration train: a flag for controlling the iterator mode. batch: the number of samples in a batch noise: std. of the error added to the y. ''' self . train = bool ( train ) self . batch = batch self . noise = noise def next_train ( self ): ''' Get the next train batch: (x, y) ''' x = self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) y = np . matmul ( x , self . A ) + self . c if self . noise > 1e-3 : y = y + np . random . normal ( 0 , self . noise , size = y . shape ) y = np . greater ( y , 0.0 ) . astype ( np . float32 ) return x , y def next_test ( self ): ''' Get the next test batch x. ''' return self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) def __iter__ ( self ): while True : samp = self . __next__ () yield samp def __next__ ( self ): if self . train : return self . next_train () else : return self . next_test () 该生成器输入一组 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，以及相关配置，之后就可以通过 迭代器 (iterator) 或 方法 (method) 随机生成数据。这种数据集写法我们在后面还会用到， model . fit 允许我们不是馈入样本（或样本批次），而是馈入一个 生成器(generator) 。因此我们重写了 __iter__ 方法，并使其通过 yield 返回一个生成器。这样我们定义的数据集类就可以被Keras的训练函数 model . fit 使用。接下来，调用如下测试代码： dparser.py 1 2 3 4 5 6 7 8 9 10 def test_dataset (): A = np . random . normal ( 0 , 10 , [ 10 , 6 ]) c = np . random . uniform ( 1 , 3 , [ 1 , 6 ]) dataSet = TestDataSet ( 10 , A , c ) dIter = iter ( dataSet ) for i in range ( 10 ): x , y = next ( dIter ) print ( np . sum ( y , axis = 0 ) / 100 ) test_dataset () Output [ 0.47 0.57 0.58 0.56 0.5 0.38 ] [ 0.6 0.61 0.47 0.48 0.38 0.52 ] [ 0.5 0.61 0.49 0.42 0.45 0.53 ] [ 0.59 0.52 0.44 0.44 0.49 0.51 ] [ 0.54 0.59 0.48 0.5 0.51 0.47 ] [ 0.49 0.57 0.56 0.49 0.53 0.4 ] [ 0.5 0.61 0.51 0.54 0.51 0.52 ] [ 0.5 0.51 0.61 0.5 0.44 0.5 ] [ 0.44 0.46 0.53 0.45 0.56 0.52 ] [ 0.52 0.46 0.51 0.52 0.49 0.44 ] 我们随机生成了 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的数据，每组数据100个，并且测试了10组。输出结果是各组测试中， \\mathbf{y} \\mathbf{y} 在对应维度上分类为1的概率估计。结果基本都在0.5左右，说明我们的这种数据生成模式产生的点能均匀分布在各个超平面两侧，适合进行后续测试。 定义线性顺序模型 ¶ 顺序 (sequential) 模型是一个单输入单输出模型，网络结构较为简单，也不存在跨层短接（残差连接）。在大多数情况下，已经上手的Tensorflow用户不使用这个模型，故而作为我们入门的第一个project，我们姑且用之，但我们将不再使用顺序模型来实现后续的project。一个顺序模型大致可以描述为下图的模式： graph LR st(输<br/>入) --> l1[层<br/>1] l1 --> l2[层<br/>2] l2 --> l3[层<br/>3] l3 --> ldots[层<br/>...] ldots --> ed(输<br/>出) classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,ed styStart 由于我们完成的是一个线性分类器，故而我们使用单层的序列模型即可。 接下来，我们来定义一个类， class LinClsHandle : 。定义一个类的时候，我们通常需要定义的内容包括 在初始化方法 __init__ 里定义传入网络的固定参数，例如学习速率，存取路径等； 在方法 construct 里定义网络的构造和使用的优化器； 在方法 train 里定义训练网络的过程，主要需要调用 model . fit 。如果我们在数据集的定义非常完善，则这一环节不需要花费太多的功夫； 在方法 test 里定义测试网络的过程，主要需要调用 model . evaluate 。如果有必要，可以通过 model . predict 返回测试结果。 初始化方法 ¶ 首先，定义初始化方法： lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 def __init__ ( self , learning_rate = 0.01 , epoch = 10 , steppe = 30 ): ''' Initialization and pass fixed parameters. learning_rate: the learning rate for optimizer. epoch: training epochs. steppe: steps per epoch ''' self . lr = learning_rate self . epoch = epoch self . steppe = steppe 由于目前我们的project还非常简单，这里只需要有学习速率( learning_rate )，轮次数( epoch )和每轮迭代次数( steppe )即可。 构造方法 ¶ 接下来定义网络构造 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def construct ( self ): ''' Construct a linear model and set the optimizer as Adam ''' # Construction self . model = tf . keras . Sequential () self . dense1 = tf . keras . layers . Dense ( LABEL_SHAPE , use_bias = True , input_shape = ( INPUT_SHAPE ,), kernel_initializer = tf . keras . initializers . RandomNormal ( 0.0 , stddev = 10.0 ), bias_initializer = tf . keras . initializers . Constant ( 2 ), activation = tf . nn . sigmoid ) self . model . add ( self . dense1 ) # Set optimizer self . model . compile ( optimizer = tf . train . AdamOptimizer ( self . lr ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ self . accuracy , tf . keras . metrics . BinaryAccuracy ()] ) @staticmethod def accuracy ( y_true , y_pred ): return tf . keras . backend . mean ( tf . keras . backend . equal ( y_true , tf . keras . backend . round ( y_pred ))) 须知 这里 LABEL_SHAPE 和 INPUT_SHAPE 为两个宏变量，分别为输出和输入的向量维度。 我们使用 Dense 定义全连接层，它的用法请参照 这里 。由于我们已经知道 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 可能的取值范围，这里我们重定义了 \\mathbf{W} \\mathbf{W} 和 \\mathbf{b} \\mathbf{b} 的初始化方式。 信息: Dense API tf . keras . layers . Dense ( shape , ** kwargs ) 指全连接层，其输入一组已知形状的向量，输出一组形状为 shape 的向量。可用的API如下： shape : 正整数，输出空间维度。 activation : 激活函数。 若不指定，则不使用激活函数 (即，线性激活: a(\\mathbf{y}) = \\mathbf{y} a(\\mathbf{y}) = \\mathbf{y} )。该函数可以定义为任何元素级操作的Tensorflow函数。 use_bias : 布尔值，该层是否使用偏置向量。 True 则网络定义为 \\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} \\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} ， False 则定义为 \\mathbf{y} = \\mathbf{W}\\mathbf{x} \\mathbf{y} = \\mathbf{W}\\mathbf{x} 。 kernel_initializer : kernel 权值矩阵的初始化器，自定义的初始化器需要使用Keras后端API编写。 bias_initializer : 偏置向量的初始化器，同上。 kernel_regularizer : 运用到 kernel 权值矩阵的正则化函数，自定义的正则化函数需要使用Keras后端API编写。 bias_regularizer : 运用到偏置向的的正则化函数，同上。 activity_regularizer : 运用到层的输出的正则化函数，同上。 kernel_constraint : 运用到 kernel 权值矩阵的约束函数，只能使用Keras备选的几种方案，不能自定义。 bias_constraint : 运用到偏置向量的约束函数，同上。 信息: model.compile API model . compile ( optimizer , ** kwargs ) 在这里指的是顺序模型的编译函数，其可用的API如下： optimizer : 优化器，可以使用Tensorflow内置的优化器。 loss : 损失函数，也是目标函数。顺序模型只有一个输出，因此只能传入一个损失函数。可以使用形式为 func ( y_true , y_pred ) 的Tensorflow函数。 metrics : 测度函数，一般是一组函数，如果是一个函数则定义为 [ func ] 即可。自定义的测度函数目前还需要使用Keras后端API编写。 loss_weights : 损失的权重，顺序模型只有一个损失函数，因此只有一个权重，但要使用一维列表 [ value ] 定义。可以使用张量来控制可变权重。 sample_weight_mode : 按时间步采样权重，默认不提供。相比上面的损失权重，该选项会随着迭代次数使用不同的权重，因此输入的是二维列表。 weighted_metrics : 测度的权重，和损失权重类似，用来加给不同的测度函数。由于我们可以使用不只一个测度函数，这里的权重是个一维列表。 target_tensors : 默认情况下，Keras 将为模型的目标创建一个占位符，在训练过程中将使用目标数据。相反，如果你想使用自己的目标张量（反过来说，Keras在训练期间不会载入这些目标张量的外部 Numpy数据），您可以通过 target_tensors 参数指定它们。对于单输出的顺序模型，它应该是单个张量。 ** kwargs : 其他参量，会传递给 tf . Session . run 。 另外，注意我们这里构造网络的时候有如下技巧： Tensorflow在导入Keras模式以后，已经不再使用 (15) (15) 的形式定义 sigmoid交叉熵 ，而是采取更通用的定义 (14) (14) ； 我们使用Tensorflow重新封装过的类， 二分类交叉熵 ( BinaryCrossentropy ) 来作为Keras模型的损失函数 self.loss ，该函数与 多分类交叉熵 ( CategoricalCrossentropy ) 不同，乃是对两组对比张量的每个元素分别计算交叉熵，再求取均值，正符合本应用的需求； 我们通过静态方法，调用Keras的后端API，自己定义了预测准确度的测度函数 self.accuracy ，同时也使用另一个来自Tensorflow封装好的测度类 二分类准确度 ( BinaryAccuracy ) ，这是为了比照两个准确度的区别，以便我们更好理解该测度类； 我们将网络层的关键字 self.dense1 保留在了实例中，这是为了确保接下来我们能通过实例抽取该层的参数。 需要注意的是，由于 二分类交叉熵 ， 二分类准确度 和 多分类交叉熵 等都是类（从它们的定义都是大写字母开头也可以看出来），因此我们需要在使用的时候后面加上括号以实例化；由于这些类都定义了 __call__ 方法，我们可以像使用函数一样使用它们的实例。 训练和测试方法 ¶ 最后定义的式训练和测试方法。由于我们目前的project还比较简单，关于这两部分都直接调用现有的API即可。使用的API在之前已经说明。 model.fit 在没有额外设置的情况下，默认会返回一个 History回调器 ； model.evaluate 返回的是测试样本给出的损失函数和准确值测度。 model.predict 返回的是测试样本给出的网络输出。详情请参照 顺序模型API 。 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def train ( self , dataSet ): ''' Use a data set to train the network. ''' return self . model . fit ( dataSet , epochs = self . epoch , steps_per_epoch = self . steppe ) def test ( self , data , labels ): ''' Use (data, label) pairs to test the results. ''' loss , accu = self . model . evaluate ( data , labels ) print ( 'Evaluated loss =' , loss ) print ( 'Evaluated accuracy =' , accu ) return self . model . predict ( data ) 信息: model.fit API model . fit ( dataSet , ** kwargs ) 是训练函数，注意这个函数虽然支持输入一组 x,~y x,~y 用来代替 dataSet ，我们还是建议在任何情况下都用dataSet馈送数据，以免内存中数据集占用过多。 dataSet : 数据集，其本身应当是一个 tf.data.Dataset 类型的类，或者是一个能不断迭代产生新数据的生成器。数据的 batch 大小由 dataSet 本身决定。 epochs : 整数，终止训练时经历的世代(轮次)数，通常一个epoch表示遍历整个数据集一回。 verbose : 0, 1或2。日志显示模式。 0=安静模式, 1=进度条, 2=每轮一行。默认是1。 callbacks : 回调器，它是 tf.keras.callbacks 模块下的类，用来在训练中进行记录保存和数据检查点更新。默认是 tf.keras.callbacks.History 。 validation_split : 在 0 和 1 之间浮动。用作验证集的训练数据的比例。模型将分出一部分不会被训练的验证数据，并将在每一轮结束时评估这些验证数据的误差和任何其他模型指标。验证数据是混洗之前 x 和 y 数据的最后一部分样本中。 validation_data : 元组 (x_val，y_val) 或元组 (x_val，y_val，val_sample_weights) ，用来评估损失，以及在每轮结束时的任何模型度量指标。模型将不会在这个数据上进行训练。这个参数会覆盖 validation_split 。 shuffle : 布尔值（是否在每轮迭代之前混洗数据）。当 steps_per_epoch 非 None 时，这个参数无效。 class_weight : 可选的字典，用来映射类索引（整数）到权重（浮点）值，用于加权损失函数（仅在训练期间）。这可能有助于告诉模型 「更多关注」来自代表性不足的类的样本。 sample_weight : 用来给损失函数添加权重，作用类似 model.compile 的同一参数。 initial_epoch : 开始训练的轮次（有助于恢复之前的训练）。 steps_per_epoch : 在声明一个轮次完成并开始下一个轮次之前的总步数（样品批次）。使用TensorFlow数据张量等输入张量进行训练时，默认值 None 等于数据集中样本的数量除以 batch 的大小，如果无法确定，则为1。 validation_steps : 只有在指定了 steps_per_epoch 时才有用。停止前要验证的总步数（批次样本）。 该函数会返回 callbacks 定义的实例。 信息 model.evaluate API model . evaluate ( x , y , ** kwargs ) 是测试函数，需要传入 label 即 y 来验证性能。 x , y : Numpy 数组，分别是输入和输出的真实参照值。 batch_size : 计算的 batch 大小，该函数会将输入的数据组拆分成许多 batch 分别计算，并组合起来，这个设定值对效果不产生影响，只影响计算过程。 verbose : 0, 1。日志显示模式。0=安静模式, 1=进度条。默认是1。 sample_weight : 用来给损失函数添加权重，作用类似 model.compile 的同一参数。 steps : 整数或 None 。 声明评估结束之前的总步数（批次样本）。默认值 None 。 该函数会返回损失函数和测度（列表）。 信息 model.predict API model . predict ( x , ** kwargs ) 是预测函数，用在只知道输入 x 的场合。 x : Numpy 数组，输入值。 batch_size : 计算的 batch 大小，该函数会将输入的数据组拆分成许多 batch 分别计算，并组合起来，这个设定值对效果不产生影响，只影响计算过程。 verbose : 0, 1。日志显示模式。0=安静模式, 1=进度条。默认是1。 steps : 整数或 None 。 声明评估结束之前的总步数（批次样本）。默认值 None 。 该函数返回预测结果。 调试 ¶ 首先，训练网络。我们随机生成 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的仿射变换，并且设置好数据集，给定噪声扰动为 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 。设定20个epoch，每个epoch迭代500次，每次馈入32个样本构成的batch，然后开始训练： lin-cls.py 1 2 3 4 5 6 7 8 9 # Initialization A = np . random . normal ( 0 , 10 , [ INPUT_SHAPE , LABEL_SHAPE ]) c = np . random . uniform ( 1 , 3 , [ 1 , LABEL_SHAPE ]) dataSet = dp . TestDataSet ( 10 , A , c ) dataSet . config ( batch = 32 , noise = 0.1 ) # Construct the model and train it. h = LinClsHandle ( learning_rate = 0.01 , epoch = 20 , steppe = 500 ) h . construct () record = h . train ( iter ( dataSet )) Output Epoch 1 /20 500 /500 [==============================] - 1s 3ms/step - loss: 6 .3005 - accuracy: 0 .5884 - binary_accuracy: 0 .5884 Epoch 2 /20 500 /500 [==============================] - 1s 2ms/step - loss: 5 .4671 - accuracy: 0 .6407 - binary_accuracy: 0 .6407 Epoch 3 /20 500 /500 [==============================] - 1s 2ms/step - loss: 4 .5711 - accuracy: 0 .6957 - binary_accuracy: 0 .6957 Epoch 4 /20 500 /500 [==============================] - 1s 2ms/step - loss: 3 .6789 - accuracy: 0 .7519 - binary_accuracy: 0 .7519 Epoch 5 /20 500 /500 [==============================] - 1s 2ms/step - loss: 2 .7101 - accuracy: 0 .8127 - binary_accuracy: 0 .8127 Epoch 6 /20 500 /500 [==============================] - 1s 2ms/step - loss: 2 .0059 - accuracy: 0 .8627 - binary_accuracy: 0 .8627 Epoch 7 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .6403 - accuracy: 0 .8894 - binary_accuracy: 0 .8894 Epoch 8 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .3663 - accuracy: 0 .9066 - binary_accuracy: 0 .9066 Epoch 9 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .0466 - accuracy: 0 .9274 - binary_accuracy: 0 .9274 Epoch 10 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .8377 - accuracy: 0 .9418 - binary_accuracy: 0 .9418 Epoch 11 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .6465 - accuracy: 0 .9546 - binary_accuracy: 0 .9546 Epoch 12 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .4492 - accuracy: 0 .9667 - binary_accuracy: 0 .9667 Epoch 13 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .2795 - accuracy: 0 .9779 - binary_accuracy: 0 .9779 Epoch 14 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .1624 - accuracy: 0 .9861 - binary_accuracy: 0 .9861 Epoch 15 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0680 - accuracy: 0 .9926 - binary_accuracy: 0 .9926 Epoch 16 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0128 - accuracy: 0 .9971 - binary_accuracy: 0 .9971 Epoch 17 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0055 - accuracy: 0 .9981 - binary_accuracy: 0 .9981 Epoch 18 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0052 - accuracy: 0 .9986 - binary_accuracy: 0 .9986 Epoch 19 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0055 - accuracy: 0 .9981 - binary_accuracy: 0 .9981 Epoch 20 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0049 - accuracy: 0 .9985 - binary_accuracy: 0 .9985 接下来，从训练返回的 History 类型的回调器中抽取对loss和accuracy的记录。 lin-cls.py 1 2 3 4 5 # Show records plt . semilogy ( record . epoch , record . history [ 'loss' ]), plt . xlabel ( 'epoch' ), plt . ylabel ( 'Cross entropy' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () plt . plot ( record . epoch , record . history [ 'accuracy' ], label = 'self defined' ), plt . plot ( record . epoch , record . history [ 'binary_accuracy' ], label = 'from tensorflow' ), plt . xlabel ( 'epoch' ), plt . ylabel ( 'Accuracy' ), plt . legend () plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () Output 结果显示，我们自定义的准确度测度和Tensorflow内置的 二分类准确度 完全相同，这说明其本身的定义就是求取所有元素阈值化后，各自分类结果是否正确的平均值。这个实验也让我们对自定义测度函数有了一定的认识。 重新设定数据集的产生方式，变为每个batch含10个样本。使用这组重新随机生成的数据测试网络输出， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 12 # Generate a group of testing samples: dataSet . config ( batch = 10 ) x , y = next ( dataSet ) # Check the testing results yp = h . test ( x , y ) _ , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 ) ax1 . imshow ( y , interpolation = 'nearest' , aspect = 'auto' ) ax1 . set_title ( 'True class' ) ax2 . imshow ( yp , interpolation = 'nearest' , aspect = 'auto' ) ax2 . set_title ( 'Predicted class' ) plt . gcf () . set_size_inches ( 10 , 5 ), plt . show () Output Evaluated loss ( losses.BinaryCrossentropy ) = 0 .0023145806044340134 Evaluated accuracy ( self defined ) = 1 .0 Evaluated accuracy ( metrics.BinaryAccuracy ) = 1 .0 注意我们未对测量的结果阈值化，因此显示出来的测量结果和理想值略有差别，但从图可知，阈值化后则测量结果全部准确。 通过抽取 h.dense1 的参数，我们可以对比 \\mathbf{A} \\mathbf{A} 和 \\mathbf{W} \\mathbf{W} ，以及 \\mathbf{c} \\mathbf{c} 和 \\mathbf{b} \\mathbf{b} ， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 # Check the regressed values W , b = h . dense1 . get_weights () plt . imshow ( A , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'A' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . imshow ( W , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'W' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . plot ( c . T , label = 'c' ) plt . plot ( b . T , label = 'b' ) plt . legend () plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () Output 可以发现，虽然我们训练的分类器十分有效，但其权值和预期的 \\mathbf{A} \\mathbf{A} , \\mathbf{c} \\mathbf{c} 并不完全相同。这是由于sigmoid函数激活的特性，使得当预测值偏向最小或最大的情况下， |\\sigma(x)| \\rightarrow 1 |\\sigma(x)| \\rightarrow 1 ，根据 (7) (7) ，可知其梯度 |\\sigma(x)(1-\\sigma(x))| \\rightarrow 0 |\\sigma(x)(1-\\sigma(x))| \\rightarrow 0 ，因此那些分类结果已经比较确信的样本，其梯度消失，对训练网络的影响忽略不计（这是合理的，因为我们不希望极端样本干扰结果，更希望对分类结果不确切的样本进行训练）。故而，我们虽然可以求解出这个分类问题，但求解到的 \\mathbf{W} \\mathbf{W} , \\mathbf{b} \\mathbf{b} 不会回归到 \\mathbf{A} \\mathbf{A} , \\mathbf{c} \\mathbf{c} 上。关于回归问题，我们会在下一节进一步讨论。","text_tokens":["前要","extension","8","忽略不计","our","50","视","合适","一个","无定法","arg","现有","正确理解","术语","activity","蛇形","存在","方式","后面","第一个","线性组合","熟悉","指","/","布尔值","通过","得到","时候","5","matmul","生成器","创建","验证","稳定","0055","可变","backend","不断","把","check","step","pass","测试代码","私有","分入","分成","回调","store","静态方法","mathbb","详情","...","0.56","再","编译","多","matplotlib","alphabetafunction","返回","ax1","最后","¶","平面","时间","固定","方法","else","除了","即可","7","beta","选择","块","并","batch","fae6a9","向量","看","equation","samp","给","不","数据测试","建立","环节","模型","weighted","每轮","样品","星号","既然","元素","exp","人意","predicted","14","以","1","单词","来自","检查","意味","关注","告诉","正则","反之","所","vector","比照","ax2","预测","不计","self","扰动","两侧","target","范围","l3","-","接下","linclshandle","|","一个点","parser","但","生成","缩小","33","过程中将","组","mapsto","5711","和","br","（","写字","等价","6957","pyplot","读取","name","独立","类级","性能","0.5","加","学习","缩成","形式","1624","3005","看成","干扰","对角","init","以单","regularizer","看待","aspect","'","y","set","往往","use","信度","等","staticmethod","混洗","l","讨论","浮点","a","以外","testdataset","入门","首先","神髓","next","analyzing","内置","where","44","扩展","9779","解","大写字母","yield","大多数","0.38","静态","13","initialize","比例","values","network","包括","evaluated","索引","accuracy","我们","9861","了解","只","超平面","馈送","个","\\","经常","两","先河","sigmoid","相关","这里","它","拟合","压缩成","43","期间","8627","总是","losses","leqslant","工程","一层","0466","向用","数据分布","第二","某些","梯度","快速","tensors","9418","对象","出来","零维","认为","if","前","变量","分出","法","大小","initializer","对","类似","引入","2","45","0680","根据","定义数据","9","去","概念","文件夹","目标","正值","while","能","通道","st","不以","类","0052","非","红线","激活","尽如人意","齐次","tools","插值","input","设计","各个","称","启发","module","项","^","损失","代表","卷积","transformation","其实","准确","控制","直接","概率论","乃至","传入","做","并且","number","负值","2ms","一回","确切","调整","想","概率","diter",">","构造","最","相同","阈值","通常","是","tool","cdots","0059","可信","这样"," ","改进","人为","适当","列表","平均值","备选","rightarrow","写出","2795","形状","假设","几种","7101","类中","实数","varepsilon","称为","次数","经历","training","c",",","sum","以下","0.42","print","多用","steps","~","受过","0.51","22","不同","测试方法","步","应该","一部分","records","比较","送入","代码","子类","true","split","之后","group","同一","后续","字母","对于","希望","random","xlabel","output","get","本","出原","方案","求解","将","；","感知机","fit","run","权值","activation","e","于是","指定","一大","非常","要求","最小化","尽如","开始","extending","binaryaccuracy","浮动","一步","to","rgb","子","一侧","import","train","然后","读者","非线性","推导","10","main","慢慢","大量","getattribute","指全","一些","网络层","9546","是因为","，","被","generate","47","predict","思源","参见","vdots","略有","有限","部分","8127","参照","一条","预期","回归","也","简短","这些","完全相同","各组","又","相斥","函数","饮水","视为","8894","全部","成文","limits","风格","用来","的","j","化后","100","手","4","分离","normal","进行","所有","另外","test","直线","大化","未","func","保存","本章","tensorflow","gcf","自己","左右","但是","记录","layers","12","kernel","后来","参数","更","28","bmatrix","向","有效","哪","保留","fixed","aligned","正整数","含","构成","5884","仿射变换","须知","包含","使得","数","0.1","weights","42","命名","session","大多","下图","需要","端","遍历","dimn","测试函数","说","lr","详情请","最大","s","更新","组合","as","align","网络","反过","斯蒂","乘积","begin","参量","熵","找到","nearest","无定","0.53","l1","影响","0.61","9926","预处理","模块化","这个","太多","ldots","linspace","除以","结束","error","compile","」","连用","一样","角度","情况","堆叠","21","引出","float32","infty","label","initialization","了","项目","其他","误差","dense","初始","终止","整理","dim1","仍然","整数","theta","一部","graph","一词","均匀分布","+","发展","]","放在","跟","速率","流动","行","一轮","单个","仿射","指各维","理想","in","pairs","fill","信息","实验","变换","声明","分析","事实上","序列","花费","lin","给出","还会","这是","不会","shuffle","一节","上面","由此可知","）","更大","文字","神经网络","然而","binary","关键","原则","总","鉴往知来","function","epoch","应当","ed","看作","一个多","蓝线","rate","project","教程","测度","极端","随着","不可","反过来","已知","25","regressed","26","一维","集","好处","boldsymbol","and","callbacks","连接","发现","colorbar","实现","作为","常数","52","以免","指标","复杂","接触","调试","<","就是","均匀","忽略","x","较为","层","目前","kwargs","权重","基本","三维",")","层叠","samples","结果显示","construction","重写","iter","如下","内","&","约束","退化","特别","具有","history","其","method","主","认识","不是","做到","指数","有助","之前","正","即","导入","每次","默认值","6465","数目","互不","一般","initial","虽然","调用","34","符","最终","35","p","规定","相反","mode","完善","不足","0.57","it","会","简单","第二个","of","之","支持","sample","alpha","引用","0023145806044340134","class","record","没有","显式","感知","np","宏","代入","用于","3","类时","_","sigma","解决","0.54","0.47","d","结尾","但类","属于","一类","写成","时","trainable","撰写","过来","单层","功夫","dim2","那些","每个","好","还","randomnormal","加上","法指","cross","上手","覆盖","彼此","小写","恢复","0.48","输入","区别","设定","有助于","统一","一一","出","4671","且","例如","54","面向对象","mathbf","平移","神经网","可能","处理器","器","hat","均","ij","defined","你","式","与","18","构造方法","确信","外围","作用","世代","py","按","n","generator","成器","标准","有趣","定义","controlling","叫","维度","0.01","集中","残差","图","0049","采取","加权","validation","第一","内容","0.0","0.58","神经","过","故而","关系","采用","regression","抽取","含义","后端","操作","define","最早","h","log","文件","表示","16","合理","只是","ylabel","processing","某","定性","自定","&&","而言","步数","30","stroke","initializers","不成文","通用","40","legend","二","另","开头","categoricalcrossentropy","可以","按照","完全","scale","一行","分类","维","颜色","size","可读性","许多","样本","检查点","取","全字","考虑","由此","较强","perceptron","实际","差别","0128","优化","单单","调整结构","dtype","两组","特性","处理","9667","偏置","semilogy","确保","可读","uniform","空间","layer","无","下来","办法","模块",";","传播","每",":","sequential","特征","同时","布尔","c++","tensor","为","可信度","或者说","均值","linear","adam","tilde","规范","mathrm","以双","需求","活用","17","上求","└","数量","停止","1e","pred","evaluate","各","结果","dataset","格式","整个","we","configuration","api","0.49","额外","之旅","6789","r","实战","epochs","27","多数","9985","flag","重新","@","输出","完成","38","8377","这","下","optimizer","len","特点","以及","下划线","大小写","要","cls","20","4492","跨层","到","metrics","6407","the","注释","for","inches","9981","labels","进度","器中","48","。","查点","在","括号","大致","frac","对数","std","给定","技巧","特殊","实际上","logistic","mean","python","都","进一步","推荐","tf","像","data","习惯","首字母","载入","可选","模式","override","标准化","无效","似然","组织","满足","提示","字典","是否","馈入","、","孱弱","cdot","：","left","未知","「","“","31","估计","定法","采样","必要","=","7519","53","件夹","plot","里","不知","集类","每种","partial","类型","codes","51","23","下划","imshow","凡是","36","主要","语言","重","改写","46","地","表述","model","等于","才","写法","人","线性变换","请","很","可微","适合","训练","9066","让","还有","基本概念","可见","偏向","对应","级","b","title","知道","乃是","而","？","noise","#","astype","交点","同","效果","29","饮水思源","则","自定义","任何","testing","none","建议","就","准确度","意义","性","constraint","第","几乎","存储","(","如果","9971","表明","张量","那么","中将","噪声","官方","auto","最大化","性关系","config","这组","相若","运用","压缩","开","6","有关","两个","loss","或","亦","符合","还是","之间","当然","nn","ba9132","轮次","已经","首字","简简单单","dparser","它们","9274","反过来说","500","次","成","允许","entropy","足够","single","15","迭代","无法","用单","*","分别","占用","轴","单调","推断出","功能","l2","更好","equal","灵活","range","alphabeta","greater","却","24","入","注意","区分","│","bool","per","{","标量","41","映射","int32","0","19","0.45","为了","49","非常简单","高维","实例","返回值","learning","代表性","32","路径","遵守","dp","理解","驼峰","lvert","设定值","w","陆续","用户","jacobian","理论","真实","分类器","}","使","f","评估","用","多个","矩阵","accu","显示","sim","length","解析","11","应用","数组","配置","编写","以便","dense1","节","关键字","反","这种","1s","10.0","短接","无关","stddev","return","记","内部","两层","问题","默认","正确","t","numpy","从","call","的话","def","二维","因为","缩进","shape","省略","value","确定","导数","iterator","元组","其中","round","事实","安静","单","0.59","单独","有用","由于","知机","post","较为简单","0.46","组拆","right","max","若","6403","可用","上述","false","使用","占位","面向用户","过程","大写","并非","至此","列","比较简单","尺寸","后","稳定性","不再","9986","以后","三个","parameters","助于","如何","不得","接下来","走上","展示","推断","姑且","__","add","轮","封装","化","flatten","来说","哪怕","37","十分","binarycrossentropy","3ms","取值","一定","截距","添加","plt","每组","文档","只要","摘要","意味着","3663","自带","复杂度","rvert","[","stystart","不成","良好","而是","获得","对角线","全","外部","起来","axis","比如","该","该层","from","coder","深度","测量","各自","交叉","产生","增大","可知","平均","val","变为","操作符","这一","最小","决定","描述","进度条","39","added","classdef","当","点","线性关系","存取","提供","end","消","adamoptimizer","用作","用到","用法","消失","构建","weight","属性","或者","随机","对比","预测值","construct","subplots","因此","日志","一组","show","steppe","代替","存储器","内存","”","自动","来","任","愿","不能","新","二个","mathcal","面向","介绍","只能","求导","verbose","较长","由","必须","上","0.44","因子","min","临时","原则上","图像","0.4","结构","本身","results","理器","0.6",".","目的","channel","于","分布","设置","早期","双","一","神经元","一个二维","constant","传递","图示","测试","值","├","i","线性","批次","尽管","─","k","0.52","数据","选项","中","bias","您","输","任意","计算","说明","场合","只有","网络结构","过多","顺序","keras","继承","yp","gca","初始化","有","仅","本节","程度","划线","度量","相比","关于","interpolation","喜好","逻辑","求取","时才"],"title":"线性分类","title_tokens":["线性","分类"]},{"location":"book-1-x/chapter-1/linear-classification/#_1","text":"摘要 本节介绍如何使用顺序模型(sequential model)来编写一个线性分类器，使用sigmoid函数激活，并验证其效果。在本节我们将编写第一个Project，并介绍一些基本概念、和一个推荐的Tensorflow Project的编写格式。","text_tokens":["一些","和","验证","。","、","一个","效果","，","函数","介绍","在","其","推荐","本"," ","顺序","如何","编写","project","摘要","激活","基本概念","tensorflow","节","model","格式","并","的","分类","分类器","第一个","本节","将","我们","使用","(","第一","线性","基本","sequential","来",")","sigmoid","模型","概念"],"title":"线性分类","title_tokens":["线性","分类"]},{"location":"book-1-x/chapter-1/linear-classification/#_2","text":"","text_tokens":[],"title":"理论","title_tokens":["理论"]},{"location":"book-1-x/chapter-1/linear-classification/#_3","text":"考虑我们有如下的二分类数据集 (\\mathbf{x},~y_i) \\in \\mathbb{D}_i (\\mathbf{x},~y_i) \\in \\mathbb{D}_i ，并且有一个未知的常数向量 \\mathbf{a}_i \\mathbf{a}_i 和未知的常数标量 c_i c_i ，使得： \\begin{equation} y_i = \\left\\{ \\begin{aligned} 0, && \\mathbf{a}_i^T \\mathbf{x} + c_i \\leqslant 0, \\\\ 1, && \\mathbf{a}_i^T \\mathbf{x} + c_i > 0. \\end{aligned} \\right. \\end{equation} 其中， \\mathbf{a} \\mathbf{a} 可以看成是某超平面的（未标准化的）法向量，那么 \\mathbf{a}^T \\mathbf{x} + c = 0 \\mathbf{a}^T \\mathbf{x} + c = 0 是该超平面的截距式定义，亦即该平面与 x_i x_i 轴的交点可以显式表述为 x_i^{(0)} = - \\frac{c}{a_i} x_i^{(0)} = - \\frac{c}{a_i} 。由此可知，式 (1) (1) 显式定义了一个点在超平面的哪一侧。特别地，若 \\mathbf{x} \\mathbf{x} 是一个二维向量，则该超平面退化为一维平面；若 \\mathbf{x} \\mathbf{x} 是一个标量，则该超平面退化为一条直线。 若我们定义 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} ，有 \\begin{equation} \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, ~~ \\mathbf{A} = \\begin{bmatrix} \\mathbf{a}^T_1 \\\\ \\mathbf{a}^T_2 \\\\ \\vdots \\\\ \\mathbf{a}^T_n \\end{bmatrix}, ~~ \\mathbf{c} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}. \\end{equation} 则我们可以认为 \\begin{align} \\mathbf{y} = \\left\\{ \\begin{bmatrix}\\hat{y}_1 > 0 & \\hat{y}_2 > 0 & \\cdots & \\hat{y}_n > 0\\end{bmatrix}^T, ~ \\left| ~ \\hat{\\mathbf{y}} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon} \\right. \\right\\}, \\end{align} 其中 \\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon} 是一个定义噪声的向量。 我们可以把向量 \\mathbf{y} \\mathbf{y} 的元素看成是互不相关的多个超平面对向量 \\mathbf{x} \\mathbf{x} 各自独立的分类结果。即 y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} y_i = \\{ \\hat{y}_i>0 ~ | ~ \\hat{y}_i = \\mathbf{a}_i^T \\mathbf{x} + c_i + \\varepsilon_i \\} 。由于每个超平面构成一个二分类，如果把每个二分类看作是向量是否属于这个类的测度，那么 \\mathbf{y} \\mathbf{y} 可以被看作是一个多分类的结果，尽管向量 \\mathbf{x} \\mathbf{x} 可能被同时分入多个类中。 假设我们的数据集 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} 符合 (3) (3) 定义的数据分布特征。我们的基本要求是，在我们不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 的情况下，使用大量 (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} (\\mathbf{x}^{(k)},~\\mathbf{y}^{(k)}) \\in \\mathbb{D} 样本训练一个线性分类器，使得当我们给定任意一个新样本 \\mathbf{x} \\mathbf{x} 的时候，分类器能推断出其对应的 \\mathbf{y} \\mathbf{y} 来（亦即是否属于该分类）。 须知 请注意我们在这里说到“线性分类器”，虽然使用“线性”一词，但准确来说，这是一个仿射变换。因为线性变换要求有齐次性，即 f(x) = \\alpha f(x) f(x) = \\alpha f(x) ，但仿射变换允许我们引入一个平移向量 \\mathbf{b} \\mathbf{b} 。当然，我们的求解的线性问题本身也是一个仿射变换。 在这个问题里，我们虽然不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，但我们知道由 (3) (3) 确定的线性关系，因此，我们可以随机生成一组 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ，构建线性模型： \\begin{align} \\tilde{\\mathbf{y}} = \\sigma ( \\mathbf{W} \\mathbf{x} + \\mathbf{b} ). \\end{align} 其中，可微函数 \\sigma \\sigma 是一个将实数空间 \\mathbb{R}^n \\mathbb{R}^n 映射到有限范围的实数空间 [0,~1]^n [0,~1]^n 内的函数。特别地， \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 。因此，可以将 \\sigma \\sigma 看作是二分类布尔函数的插值函数。理论上，只要我们找到 \\mathbf{W}=\\mathbf{A} \\mathbf{W}=\\mathbf{A} ， \\mathbf{b}=\\mathbf{c} \\mathbf{b}=\\mathbf{c} ，则该线性分类器可以直接拟合出原分布来。","text_tokens":["：","left","拟合","如下","是","未知","哪","&","构建","二","退化","“","一个","特别","]","aligned","要求","可以","随机","噪声","cdots","其","w","范围"," ","下","构成","leqslant","-","仿射变换","性关系","一侧","=","须知","理论","分类","使得","分类器","因此","一组","推断","|","里","仿射","一个点","数据分布","样本","即","in","但","}","mathbf","平移","亦","符合","时候","来","假设","”","大量","来说","f","生成","可能","新","互不","考虑","由此","多个","变换","和","类中","（","hat","当然","实数","虽然","varepsilon","，","被","到","把","认为","地","截距","表述","只要","分入","式","与","vdots","空间","法","c","这是","由","说","有限","上","对","独立",",","由此可知","mathbb","一条","）","0.5","也","引入","2","align","[","特征","n","同时","线性变换","本身","请","允许","能","问题","看成","标准","可微",".","~","平面","。","分布","t","函数","布尔","定义","训练","在","类","该","看作","一个多","begin","为","alpha","y","二维","frac","因为","找到","一个二维","显式","测度","的","这个","向量","确定","对应","齐次","轴","各自","b","3","tilde","其中","插值","equation","_","可知","知道","推断出","sigma","i","一维","不","线性","尽管","集","d","k","给定","a","boldsymbol","数据","模型","^","属于","情况","交点","关系","由于","直线","常数","内","注意","则","元素","准确","任意","{","未","infty","标量","直接","结果","了","right","若","每个","映射","并且","出原","0","有","性","1","求解","x","将","我们","；","当","标准化","超平面","线性关系","使用","end","某","\\",">","(","r","基本","&&","是否","一词","如果","+",")","相关","这里","那么","bmatrix"],"title":"问题描述","title_tokens":["描述","问题"]},{"location":"book-1-x/chapter-1/linear-classification/#_4","text":"我们将 (4) (4) 定义的线性模型称为 单层感知机 (Single-layer perceptron) 模型。它包含一个权重矩阵 \\mathbf{W} \\mathbf{W} 和一个偏置矩阵 \\mathbf{b} \\mathbf{b} 。事实上，可以将 (4) (4) 改写成如下形式 \\begin{align} \\tilde{\\mathbf{y}} = \\sigma \\left( \\begin{bmatrix} \\mathbf{W} & \\mathbf{b} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ 1 \\end{bmatrix} \\right). \\end{align} 可见偏置本身可以看成是输入向量多了一个常数元素的等价模型。 感知机是最早的神经网络形式，它非常孱弱，只能解线性问题，但却为神经网络后来的发展开了先河。在单层感知机里，我们视输入向量 \\mathbf{x} \\mathbf{x} 的每个元素为一个“神经元”，矩阵 \\mathbf{W} \\mathbf{W} 和偏置 \\mathbf{b} \\mathbf{b} 将我们的输入映射到输出层 \\mathbf{y} \\mathbf{y} ，输出层的每个元素也视为一个神经元。在这个过程中， W_{ij} W_{ij} 作为 i i 行 j j 列的元素，代表了连接两个神经元的权重。我们用红线代表正值，蓝线代表负值，感知机可以被图示为 线性感知机的输出也是输入的线性组合，但我们可以添加激活函数，即 \\sigma(\\cdot) \\sigma(\\cdot) 将其映射到非线性空间。这要求我们添加的激活函数是一个非线性函数。 事实上，将单层感知机层叠，前一层的输出作为后一层的输入，就构建出早期的神经网络。这种网络每一层都是全连接的（两个神经元之间总是有权重，尽管值可能为0），每一层都有激活函数。理论上，任意一个两层堆叠的感知机，只要神经元数目足够多，就可以拟合出任意一个非线性函数。然而，实际测试中，这一理论的效果并不尽如人意，因此又有陆续地改进，才有了后来的深度学习。饮水思源，鉴往知来，我们也将从这个简简单单的单层模型开始，走上学习“深度学习”之旅。","text_tokens":["left","列","拟合","如下","是","&","输入","发展","输出","非常","视","“","一个","要求","可以","后","其","总是","出","尽如","开始","这","w","陆续"," ","改进","-","=","一层","理论","包含","走上","因此","行","里","线性组合","开","非线性","即","但","}","两个","mathbf","”","神经网","可能","用","数目","之间","和","矩阵","（","perceptron","实际","，","ij","等价","被","单单","到","只能","称为","改写","地","前","事实上","添加","简简单单","偏置","只要","思源","空间","上","layer","这种","）","才","也","组合","多","学习","align","每","形式","神经网络","本身","两层","成","又","网络","然而","简单","正值","问题","看成",".","。","鉴往知来","足够","构建","函数","全","single","定义","在","早期","从","begin","为","饮水","蓝线","y","神经元","视为","红线","激活","可见","并","的","图示","这个","测试","j","深度","向量","感知","尽如人意","4","b","tilde","值","事实","_","sigma","i","线性","不","尽管","神经","模型","连接","这一","代表","堆叠","却","中","知机","作为","效果","常数","饮水思源","解","元素","都","单层","任意","cdot","{","人意","最早","了","right","就","每个","映射","0","有","1","负值","x","将","我们","层","感知机","end","\\","之旅","权重","(","过程","后来","先河",")","层叠","孱弱","它","bmatrix"],"title":"感知机","title_tokens":["知机","感知机","感知"]},{"location":"book-1-x/chapter-1/linear-classification/#sigmoid","text":"在上述介绍中，我们没有解决的两个问题是， 如何定义插值函数 \\sigma \\sigma ？ 如何找到合适的 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ？ 我们首先讨论第一个问题。一般地，多分类问题中，如果各个分类彼此并非相斥，且不一定要将结果分入任一类的话，我们可以用 Sigmoid 函数来定义 \\sigma \\sigma ，亦即 \\begin{align} \\sigma(\\mathbf{x}) = \\frac{1}{ 1 + e^{-\\mathbf{x}}}. \\end{align} 它同时满足 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 \\sigma(-\\infty)=0,~\\sigma(0)=0.5,~\\sigma(+\\infty)=1 ，且是一个单调函数。以下代码向我们展示了这种函数的特性： Python 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import matplotlib.pyplot as plt def test_sigmoid (): x = np . linspace ( - 5 , 5 , 100 ) y = 1 / ( 1 + np . exp ( - x ) ) plt . plot ( x , y ) plt . xlabel ( 'x' ), plt . ylabel ( 'y' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () if __name__ == '__main__' : test_sigmoid () Output 使用sigmoid函数的一大好处是，它的导数求解非常简单，很适合用来做神经网络这样一个复杂模型的激活函数。注意虽然 \\sigma(\\mathbf{x}) \\sigma(\\mathbf{x}) 和 \\mathbf{x} \\mathbf{x} 都是向量，这意味着导数是Jacobian矩阵，但由于 \\sigma \\sigma 是一个对 \\mathbf{x} \\mathbf{x} 各元素独立的解析函数，这个Jacobian矩阵实际上是一个对角矩阵，对角线上第j个元素的值为 \\begin{align} \\left. \\frac{ \\partial \\sigma(x) }{ \\partial x } \\right|_{x=x_j} = \\left. - e^{-x} \\left( - \\frac{1}{\\left( 1+e^{-x} \\right)^2} \\right) \\right|_{x=x_j} = - \\sigma(x_j) ( 1 - \\sigma(x_j) ). \\end{align} 可见，该函数的导数和计算函数本身的复杂度相若，可以做到快速求导。","text_tokens":["e","向","：","8","是","left","一大","非常","合适","一个","可以","这","w","这样"," ","如何","-","jacobian","且","=","import","做到","分类","相若","plot","第一个","展示","size","show","|","即","6","/","__","但","}","两个","mathbf","要","partial","10","main","亦","来","任","5","神经网","快速","用","和","矩阵","一般","实际","虽然","，","解析","介绍","11","一定","if","地","pyplot","特性","plt","求导","分入","意味着","name","上","这种","对","独立",",","以下","复杂度","0.5","as","多","2","align","matplotlib","inches",":","神经网络","同时","本身","简单","9","网络","很","问题",".","~","。","适合","相斥","对角线","函数","对角","numpy","定义","在","该","begin","'","的话","为","def","y","frac","找到","7","set","激活","没有","可见","代码","用来","的","这个","j","向量","np","100","linspace","导数","4","b","3","插值","单调","值","_","解决","sigma","各个","讨论","第一","不","神经","好处","实际上","模型","首先","？","^","xlabel","python","一类","test","中","由于","output","注意","元素","都","各","复杂","{","计算","infty","exp","结果","了","right","gcf","做","0","1","求解","第","x","上述","将","我们","ylabel","彼此","非常简单","意味","使用","end","个","\\","12","满足","(","如果","+",")","sigmoid","并非","它"],"title":"Sigmoid函数","title_tokens":["sigmoid","函数"]},{"location":"book-1-x/chapter-1/linear-classification/#_5","text":"接下来，我们需要解决第二个问题，亦即找到 \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} 。这一问题通常可以写成反问题的形式： \\begin{align} \\arg \\min_\\limits{\\mathbf{W},~\\mathbf{b}} \\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}^{(k)},~ \\sigma ( \\mathbf{W} \\mathbf{x}^{(k)} + \\mathbf{b} ) \\right). \\end{align} 最简单的情况下，我们可以把 损失函数(loss function) 定义为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = \\lVert \\mathbf{y} - \\tilde{\\mathbf{y}} \\rVert_2^2. \\end{align} 我们称 (8) (8) 为 逻辑斯蒂回归(logistic regression) 。有趣的是，虽然这个术语叫“回归”，但它解的其实是个分类问题。但是，既然这是一个分类问题，我们可以不使用这个损失函数，而是从概率论的角度看待这个问题。由此，我们引出一个新的损失函数：“交叉熵”。","text_tokens":["：","left","8","是","它","“","一个","arg","可以","术语","lvert","w"," ","下","-","接下","=","接下来","分类","即","第二","但","}","mathbf","loss","亦","”","新","需要","由此","二个","虽然","，","mathcal","把","这是","反","下来",",","sum","min","rvert","回归","2","align","形式","n","简单","问题","第二个","而是",".","~","有趣","。","斯蒂","function","函数","定义","看待","从","begin","为","y","熵","叫","找到","limits","的","这个","b","tilde","交叉","l","_","解决","sigma","不","k","称","logistic","^","损失","角度","这一","情况","既然","regression","写成","其实","解","引出","{","概率论","right","1","x","但是","我们","概率","使用","end","个","\\","(","+",")","逻辑","最","通常"],"title":"求解问题","title_tokens":["求解","问题"]},{"location":"book-1-x/chapter-1/linear-classification/#_6","text":"我们视sigmoid函数输出的值为一个概率，表示分类器对预测结果的确信程度，记 \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} \\mathbf{W},~\\mathbf{b} \\in \\boldsymbol{\\Theta} ，则 \\begin{equation} \\begin{aligned} \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}), \\\\ \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) &= 1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}). \\end{aligned} \\end{equation} 注意这里的概率向量的含义是，其中第i个元素表明第i个超平面分类结果的 确信程度 。 然而，这个概率只是分类器对分类结果的确信程度，却并非是分类准确度的概率，实际上，分类 准确度 的概率，应当表述为 \\begin{equation} \\begin{aligned} \\mathbf{p}(\\mathbf{y}|\\mathbf{x};~\\boldsymbol{\\Theta}) &= \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta})^{1-\\mathbf{y}}\\\\ &= \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta})^{\\mathbf{y}} \\left(1 - \\sigma(\\mathbf{x};~\\boldsymbol{\\Theta}) \\right)^{1-\\mathbf{y}}. \\end{aligned} \\end{equation} 注意 这里 \\mathbf{x}^{\\mathbf{y}} \\mathbf{x}^{\\mathbf{y}} 表示的是对每个元素一一求取指数，即函数第i个元素的返回值应当为 {x_i}^{y_i} {x_i}^{y_i} 。 我们使用真实值 \\mathbf{y} \\mathbf{y} 作为指数给概率向量加权。当 \\mathbf{y}=1 \\mathbf{y}=1 时，乘积项的第二个因子消去，该函数退化为以预测值为1的可信度 \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) \\mathbf{p}(y_i=1|\\mathbf{x};~\\boldsymbol{\\Theta}) ；反之则第一个因子消去，退化为以预测值为0的可信度 \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) \\mathbf{p}(y_i=0|\\mathbf{x};~\\boldsymbol{\\Theta}) 。这就是最大似然估计方法。至此，我们可以写出似然估计函数 \\begin{align} L(\\boldsymbol{\\Theta}) = \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}). \\end{align} 对似然估计函数取对数，则有 \\begin{equation} \\begin{aligned} l(\\boldsymbol{\\Theta}) &= \\sum_{k=1}^N \\log \\left( \\mathbf{p}(\\mathbf{y}^{(k)}|\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta}) \\right) \\\\ &= \\sum_{k=1}^N \\mathbf{y}^{(k)} \\cdot \\log\\left(\\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right) + \\left(1 - \\mathbf{y}^{(k)} \\right) \\cdot \\log\\left(1 - \\sigma(\\mathbf{x}^{(k)};~\\boldsymbol{\\Theta})\\right). \\end{aligned} \\end{equation} 我们最终的目的是要最大化似然函数，亦即 \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) \\mathbf{W},~\\mathbf{b} = \\arg\\max\\limits_{\\boldsymbol{\\Theta}} l(\\boldsymbol{\\Theta}) ，这等价于最小化 -l(\\boldsymbol{\\Theta}) -l(\\boldsymbol{\\Theta}) 。对比 (8) (8) 和 (9) (9) ，于是我们可以定义交叉熵为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) = - \\mathrm{mean}\\left[ \\mathbf{y} \\cdot \\log\\left( \\tilde{\\mathbf{y}} \\right) + \\left(1 - \\mathbf{y} \\right) \\cdot \\log\\left(1 - \\tilde{\\mathbf{y}} \\right) \\right]. \\end{align} 注意这里我们使用 \\mathrm{mean}\\left[ \\cdot \\right] \\mathrm{mean}\\left[ \\cdot \\right] 表示求取一个向量所有元素的平均值。实际上，Tensorflow允许我们定义损失函数的输出为一个和输出向量维度相同的向量，Tensorflow自带的交叉熵也是这样定义的。实际应用时，Tensorflow会自动在向量维度上求均值，并压缩成上述 (14) (14) 的形式。 若我们记 \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) \\tilde{\\mathbf{y}} = \\sigma(\\tilde{\\mathbf{x}}) ，代入sigmoid函数，为了确保该损失函数的稳定性，我们可以将 (14) (14) 整理为 \\begin{align} \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{x}} \\right) = \\mathrm{mean}\\left[ \\max(\\tilde{\\mathbf{x}}, \\mathbf{0}) - \\tilde{\\mathbf{x}} \\cdot \\mathbf{y} + \\log\\left(1 + e^{-|\\tilde{\\mathbf{x}}|} \\right) \\right]. \\end{align} 提示 这里交叉熵整理的推导过程参见 Tensorflow-API官方文档 。 实际情况下，我们使用 (15) (15) 来求取sigmoid函数激活下的交叉熵。","text_tokens":["至此","于是","相同","left","8","e","是","&","输出","]","退化","视","预测","一个","压缩成","aligned","arg","可以","稳定性","对比","预测值","一一","估计","最小化","官方","这","w","可信","这样"," ","最大化","下","-","=","平均值","指数","分类","真实","分类器","第一个","|","压缩","即","第二","推导","in","}","写出","mathbf","取","要","亦","自动","来","和","二个","实际","稳定","，","等价","mathcal","应用","表述","文档","确保","参见","最终","p","对","因子","确信",",","sum","自带","最大","也",";","align","记","缩成","[","形式","n","返回","会","9","然而","去","允许","第二个",".","~","平面","目的","。","于","方法","乘积","函数","应当","定义","15","在","该","begin","为","可信度","y","熵","均值","limits","激活","维度","对数","并","的","信度","这个","向量","代入","b","加权","交叉","l","其中","tilde","值","equation","mathrm","_","所有","sigma","上求","i","给","第一","平均","k","boldsymbol","实际上","项","^","损失","情况","却","mean","时","作为","最小","注意","大化","则","含义","元素","准确","{","结果","14","right","就是","max","以","若","log","每个","准确度","tensorflow","0","有","1","表示","第","x","程度","上述","将","我们","只是","当","；","超平面","整理","为了","概率","api","使用","theta","个","end","消","\\","似然","(","定性","返回值","提示","过程","+","反之",")","sigmoid","并非","求取","表明","这里","cdot"],"title":"交叉熵","title_tokens":["交叉","熵"]},{"location":"book-1-x/chapter-1/linear-classification/#_7","text":"接下来，我们将开始实战上手，编写我们的第一个Project。虽然一个Project的格式并无定法，每个人按照自己的喜好会选择不同的风格，但一个从无受过训练的人，往往写出的Project几乎完全不具有可读性。实际上，学习任何语言， 变量命名规范 、 缩进规范 以及 模块化 、 面向对象 等都被认为是编写一个具有可读性的代码所不得不知的概念。本教程所推荐的代码格式，均具有统一的风格，读者在了解每个Project和其对应的教程时，会慢慢熟悉这种风格的特点。愿读者能从这样的风格中得到启发，得到 代码可读性 的神髓。","text_tokens":["是","无定法","一个","统一","具有","其","开始","定法","按照","这样"," ","完全","接下","不得","特点","以及","接下来","第一个","不知","熟悉","可读性","读者","命名","面向对象","得到","但","写出","愿","慢慢","和","实际","均","虽然","，","语言","对象","被","面向","认为","变量","编写","可读","上","这种","无","下来","模块","人","学习","会","概念","能","受过","。","训练","不同","在","从","缩进","project","无定","往往","选择","教程","风格","代码","并","的","模块化","等","对应","手","规范","第一","不","实际上","启发","神髓","中","时","都","推荐","本","任何","格式","每个","自己","将","我们","了解","几乎","喜好","实战","所","、"],"title":"解线性多分类问题","title_tokens":["问题","多","分类","线性","解"]},{"location":"book-1-x/chapter-1/linear-classification/#_8","text":"建立一个具有较强可读性的Tensorflow工程需要我们活用python的模块化设计。我们通常推荐以下的结构 . ├─ data/ # where we store our data │ └─ ... ├─ tools.py # codes for post-processing and analyzing records. ├─ extension.py # codes for extending the tensorflow model. ├─ dparser.py # data parser └─ main.py # main module where we define our tensorflow model. 除了保存数据的文件夹，我们应当有三个子模块。其中 tool : 用来处理、分析生成的数据，通常与Tensorflow无关； extension : 用来扩展tensorflow，例如在这里自定义网络层和操作符； dparser : 数据处理器，用来读取并预处理送入网络的数据； main : 主模块，只定义跟Tensorflow模型有关的内容，需要引用 extension 和 dparser 。 视情况可以灵活调整结构，但建议将定义Tensorflow模型的代码单独放在主模块里，和其他外围代码分离。 撰写各个模块时，建议使用类封装各组功能相同的函数。具有良好使用习惯的coder应当注意给各个面向用户的类、函数撰写（哪怕简短的）说明文字，在一些较长的函数、方法的定义中，适当注释各部分的功能，以便读者能正确理解代码意义。 另外，在对象命名上，python有如下必须遵守或不成文的规定，和C/C++用户熟悉的蛇形命名法不同，它大致包括 类与函数多用驼峰命名法，变量可以采用驼峰或蛇形命名法。 驼峰命名法指的是用大小写区分每个单词块，例如 alphaBetaFunction () ； 蛇形命名法指的是用下划线区分每个单词块，例如 alpha_beta_function = 10 ； 宏变量使用全字大写+蛇形命名法 函数/方法，还有模块均是首字母小写，但类的首字母大写。 用单下划线 _ 表示临时存储器，或省略参数，例如一个函数 func () 有两个返回值时，可以用 _ , b = func () 表示我们只需要第二个返回值；单下划线还可以与星号连用省略多个返回值； 以单下划线开头的方法，表示模块级的私有方法，在模块以外使用 import 导入类时，不会导入这些方法，例如 def _alphaBeta ( self ): ； 以单下划线结尾的对象，用来和python的关键字区分，例如 func ( x , class_ ) ; 以双下划线开头的方法，如果不以双下划线结尾，则表示类级的私有方法，只有类内部的方法能调用这些方法，在类外部、包括继承的子类里都原则上不能调用（但其实也有办法调用），例如 def _alphaBeta ( self ): ； 以双下划线同时开头和结尾的方法，一般是用来 重写 (override) 特殊功能，例如 def __getattribute__ (): 将重写获得类属性的方法。","text_tokens":["不成文","重写","extension","tool","如下","our","理解","是","它","放在","视","开头","一个","属性","可以","驼峰","正确理解","具有","蛇形","self","三个","extending","用户"," ","主","-","子","工程","跟","适当","=","import","件夹","里","向用","可读性","例如","读者","熟悉","/","parser","下划线","第二","__","大小写","有关","命名","导入","但","存储器","或","两个","封装","10","main","codes","生成","哪怕","下划","全字","不能","用","getattribute","需要","多个","处理器","一些","网络层","和","（","较强","一般","二个","均","调用","，","对象","分析","调整结构","面向","处理","首字","变量","私有","读取","可读","以便","与","model","较长","必须","c","上","法","大小","store","不会","关键字","dparser","部分","类级",",","办法","以下","规定","外围","the","无关","...","）","注释","临时","模块","原则上","也","py","文字","for",";","简短","这些",":","内部","结构","各组","不成","alphabetafunction","返回","同时","网络","文件夹","多用","理器","关键","能","良好","原则","第二个",".","。","正确","方法","function","获得","函数","外部","应当","以单","不以","定义","c++","在","不同","除了","类","alpha","双","引用","大致","def","用单","class","还有","records","成文","beta","省略","块","送入","coder","代码","用来","并","预处理","的","子类","模块化","宏","级","tools","分离","b","其中","以双","设计","类时","_","活用","各个","├","功能","给","另外","单","内容","字母","─","单独","结尾","建立","特殊","以外","数据","└","模型","灵活","操作符","module","连用","and","但类","星号","alphabeta","#","情况","analyzing","python","采用","where","中","时","扩展","撰写","注意","其实","则","区分","│","post","都","操作","各","推荐","define","自定义","说明","只有","func","data","继承","保存","建议","tensorflow","每个","包括","其他","习惯","有","意义","还","首字母","文件","表示","法指","划线","x","将","我们","we","；","单词","override","只","processing","调整","使用","存储","小写","面向用户","自定","(","返回值","如果","+","大写",")","参数","相同","、","这里","通常","遵守"],"title":"代码规范","title_tokens":["规范","代码"]},{"location":"book-1-x/chapter-1/linear-classification/#tensorflow","text":"在Tensorflow中，我们把变量都称为“ 张量 (Tensor) ”。这是因为我们有零维的标量，一维的向量，二维的矩阵，更高维的我们都称为张量。作为一个更大的概念，张量当然也可以用来包括标量、向量和矩阵了。在Tensorflow中，有的张量是 可以训练 (trainable) 的，有的则不是。比如一个张量的形状（指各维大小），当然可以是一个 < tf . Tensor 'Shape:0' shape = ( 1 ,) dtype = int32 > 类型的张量，但它不是变量，当然就不可训练。我们也可以人为控制某些张量可以训练或不可以训练，但本节、乃至本章所介绍的凡是我们接触到的张量，都是可以训练的。 特别地，对于神经网络而言，在网络内计算（或者说流动、传播）的一个n维数据，通常按照以下形式组织： tensor [ batch , dim1 , dim2 , ... , dimn , channel ] 其中，第一个维度 batch 一定存在，它表示的是单个batch中的某一个样本。如果一个batch只有一个样本，那么 batch 只能取0。 从 dim1 到 dimn 指的是实际的n维数据的各个维度； channel 指的是数据的通道，例如，一个二维RGB图像，每种颜色代表一个通道，因此有三个通道。 channel 通常用在卷积网络里，我们经常需要在深度卷积网络里不断增大通道数的同时，缩小数据尺寸。 在某些特殊情况下， channel 维度可以不存在，例如我们使用的是全连接层而不是卷积网络， tf.keras.layer.Flatten 可以用来将一个有通道的张量压缩成一个没有通道的一维向量（但是注意 batch 维度仍然存在，不会被压缩）。 因此，我们知道一个n维的数据，在神经网络中通常被描述为一个n+2维的矩阵，而一个一维向量，在卷积网络里是三维的： vector [ batch , length , channel ] 但是在全连接网络里，是二维的： vector [ batch , channel ] 在本节，乃至本章里，我们还不讨论卷积网络，因此我们都是使用二维张量（一维向量组）作为我们的数据。","text_tokens":["：","是","内","]","“","压缩成","一个","特别","或者","尺寸","可以","三个","这","按照","存在"," ","下","人为","rgb","=","不是","维","数","流动","第一个","颜色","因此","里","单个","指各维","指","例如","样本","压缩","某些","每种","但","或","取","flatten","”","类型","缩小","形状","神经网","组","凡是","用","需要","和","矩阵","当然","（","实际","length","是因为","，","不断","被","到","把","介绍","零维","称为","一定","dtype","地","只能","变量","dimn","大小","layer","不会",",","以下","）","...","更大","也","图像","传播","2","vector","缩成","[",":","形式","神经网络","n","同时","网络","概念",".","通道","。","channel","全","训练","通常","在","比如","从","tensor","'","为","二维","或者说","因为","一个二维","shape","维度","没有","用来","的","batch","不可","深度","向量","其中","增大","知道","各个","讨论","一维","不","第一","对于","神经","特殊","数据","而","连接","代表","情况","卷积","中","作为","trainable","注意","则","都","控制","描述","接触","计算","标量","tf","dim2","<","只有","那么","了","乃至","keras","本章","就","tensorflow","int32","包括","还","0","有","1","表示","本节","将","我们","层","；","但是","dim1","仍然","使用","某","高维",">","组织","(","经常","三维","如果","+",")","更","而言","所","、","张量","它"],"title":"Tensorflow的数据概念","title_tokens":["概念","tensorflow","的","数据"]},{"location":"book-1-x/chapter-1/linear-classification/#_9","text":"在本项目里，我们不需要扩展Tensorflow。但是，我们需要以随机生成数据代替数据集。因此，首先，通过以下代码定义数据生成器 dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class TestDataSet : ''' A generator of the data set for testing the linear model. ''' def __init__ ( self , scale_x , A , c ): ''' Initialize the data generator. scale_x: the scale of input vector. A, c: the linear transformation. ''' self . s_x = 2 * scale_x self . A = A self . c = c self . len_x = A . shape [ 0 ] self . config () def config ( self , train = True , batch = 100 , noise = 0.1 ): ''' Configuration train: a flag for controlling the iterator mode. batch: the number of samples in a batch noise: std. of the error added to the y. ''' self . train = bool ( train ) self . batch = batch self . noise = noise def next_train ( self ): ''' Get the next train batch: (x, y) ''' x = self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) y = np . matmul ( x , self . A ) + self . c if self . noise > 1e-3 : y = y + np . random . normal ( 0 , self . noise , size = y . shape ) y = np . greater ( y , 0.0 ) . astype ( np . float32 ) return x , y def next_test ( self ): ''' Get the next test batch x. ''' return self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) def __iter__ ( self ): while True : samp = self . __next__ () yield samp def __next__ ( self ): if self . train : return self . next_train () else : return self . next_test () 该生成器输入一组 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，以及相关配置，之后就可以通过 迭代器 (iterator) 或 方法 (method) 随机生成数据。这种数据集写法我们在后面还会用到， model . fit 允许我们不是馈入样本（或样本批次），而是馈入一个 生成器(generator) 。因此我们重写了 __iter__ 方法，并使其通过 yield 返回一个生成器。这样我们定义的数据集类就可以被Keras的训练函数 model . fit 使用。接下来，调用如下测试代码： dparser.py 1 2 3 4 5 6 7 8 9 10 def test_dataset (): A = np . random . normal ( 0 , 10 , [ 10 , 6 ]) c = np . random . uniform ( 1 , 3 , [ 1 , 6 ]) dataSet = TestDataSet ( 10 , A , c ) dIter = iter ( dataSet ) for i in range ( 10 ): x , y = next ( dIter ) print ( np . sum ( y , axis = 0 ) / 100 ) test_dataset () Output [ 0.47 0.57 0.58 0.56 0.5 0.38 ] [ 0.6 0.61 0.47 0.48 0.38 0.52 ] [ 0.5 0.61 0.49 0.42 0.45 0.53 ] [ 0.59 0.52 0.44 0.44 0.49 0.51 ] [ 0.54 0.59 0.48 0.5 0.51 0.47 ] [ 0.49 0.57 0.56 0.49 0.53 0.4 ] [ 0.5 0.61 0.51 0.54 0.51 0.52 ] [ 0.5 0.51 0.61 0.5 0.44 0.5 ] [ 0.44 0.46 0.53 0.45 0.56 0.52 ] [ 0.52 0.46 0.51 0.52 0.49 0.44 ] 我们随机生成了 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的数据，每组数据100个，并且测试了10组。输出结果是各组测试中， \\mathbf{y} \\mathbf{y} 在对应维度上分类为1的概率估计。结果基本都在0.5左右，说明我们的这种数据生成模式产生的点能均匀分布在各个超平面两侧，适合进行后续测试。","text_tokens":["iter","flag","重写","8","：","40","如下","]","50","输入","输出","是","43","一个","31","可以","38","随机","self","其","两侧","估计","method","这样"," ","to","config","-","len","接下","scale","后续","=","以及","train","后面","53","不是","接下来","分类","因此","0.1","一组","size","里","集类","42","样本","代替","6","/","__","in","54","通过","}","或","20","mathbf","10","生成","23","33","5","51","matmul","使","37","组","生成器","36","需要","mapsto","器","（","调用","，","被","34","11","46","47","if","测试代码","配置","每组","uniform","18","还会","model","c","0.44","上","35","这种","dparser","下来",",","sum","以下","mathbb","the","s","）","0.5","mode","0.56","0.42","print","py","写法","2","45","for","return","0.4","0.57","[",":","各组","返回","定义数据","9","允许","generator","while","能","0.6","成器","48","而是","of",".","~","平面","。","适合","分布","方法","0.51","else","函数","22","init","定义","15","在","训练","controlling","axis","该","'","迭代","为","def","class","*","y","7","set","shape","0.53","维度","linear","代码","并","0.61","的","batch","测试","true","np","100","对应","iterator","std","4","产生","3","25","error","input","之后","normal","进行","_","26","各个","17","samp","0.59","i","0.54","0.0","不","批次","0.47","集","0.58","0.52","27","a","random","数据","testdataset","range","首先","^","noise","greater","transformation","astype","next","1e","24","test","中","29","扩展","44","output","21","bool","get","52","float32","yield","都","{","0.46","39","说明","本","41","14","0.38","了","结果","keras","data","13","initialize","以","testing","added","就","tensorflow","项目","dataset","并且","0","number","1","均匀","16","左右","x","但是","我们","模式","19","configuration","0.45","fit","点","超平面","49","diter","0.49","概率","使用","个","\\","12",">","(","r","基本","均匀分布","+","馈入",")","用到","32","samples","30","vector","相关","28","0.48"],"title":"数据生成","title_tokens":["生成","数据"]},{"location":"book-1-x/chapter-1/linear-classification/#_10","text":"顺序 (sequential) 模型是一个单输入单输出模型，网络结构较为简单，也不存在跨层短接（残差连接）。在大多数情况下，已经上手的Tensorflow用户不使用这个模型，故而作为我们入门的第一个project，我们姑且用之，但我们将不再使用顺序模型来实现后续的project。一个顺序模型大致可以描述为下图的模式： graph LR st(输<br/>入) --> l1[层<br/>1] l1 --> l2[层<br/>2] l2 --> l3[层<br/>3] l3 --> ldots[层<br/>...] ldots --> ed(输<br/>出) classDef styStart fill:#FAE6A9,stroke:#BA9132; class st,ed styStart 由于我们完成的是一个线性分类器，故而我们使用单层的序列模型即可。 接下来，我们来定义一个类， class LinClsHandle : 。定义一个类的时候，我们通常需要定义的内容包括 在初始化方法 __init__ 里定义传入网络的固定参数，例如学习速率，存取路径等； 在方法 construct 里定义网络的构造和使用的优化器； 在方法 train 里定义训练网络的过程，主要需要调用 model . fit 。如果我们在数据集的定义非常完善，则这一环节不需要花费太多的功夫； 在方法 test 里定义测试网络的过程，主要需要调用 model . evaluate 。如果有必要，可以通过 model . predict 返回测试结果。","text_tokens":["：","是","]","输入","输出","非常","一个","完成","可以","不再","出","这","存在","用户"," ","下","l3","-","接下","linclshandle","construct","必要","接下来","train","分类","分类器","速率","第一个","里","姑且","例如","/","__","通过","但","fill","大多","下图","时候","跨层","来","用","需要","br","和","（","器","主要","调用","优化","，","ba9132","已经","predict","序列","花费","model","lr","下来",",","短接","）","...","完善","也",";","2","学习","[",":","sequential","结构","stystart","返回","简单","网络","之",".","。","st","固定","方法","init","ed","定义","在","通常","训练","类","一","大致","为","class","即可","project","l1","的","这个","等","太多","测试","残差","ldots","fae6a9","3","单","后续","l2","第一","不","线性","内容","集","环节","数据","入门","模型","连接","故而","情况","#","实现","入","test","由于","作为","输","则","evaluate","单层","功夫","描述","较为简单","大多数","<","网络结构","结果","顺序","传入","tensorflow","包括","classdef","初始化","有","初始","1","较为","我们","将","模式","层","；","上手","fit","存取","使用",">","构造","(","过程","graph","如果",")","参数","路径","多数","stroke"],"title":"定义线性顺序模型","title_tokens":["模型","顺序","线性","定义"]},{"location":"book-1-x/chapter-1/linear-classification/#_11","text":"首先，定义初始化方法： lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 def __init__ ( self , learning_rate = 0.01 , epoch = 10 , steppe = 30 ): ''' Initialization and pass fixed parameters. learning_rate: the learning rate for optimizer. epoch: training epochs. steppe: steps per epoch ''' self . lr = learning_rate self . epoch = epoch self . steppe = steppe 由于目前我们的project还非常简单，这里只需要有学习速率( learning_rate )，轮次数( epoch )和每轮迭代次数( steppe )即可。","text_tokens":["：","8","非常","fixed","self","parameters"," ","optimizer","-","linclshandle","=","速率","steppe","6","__","轮","cls","10","5","需要","和","，","pass","次数","lin","training","lr",",","the","py","2","for","学习",":","简单","9",".","steps","。","方法","epoch","init","定义","'","迭代","def","class","rate","即可","7","project","0.01","的","4","3","_","首先","每轮","and","由于","per","initialization","还","初始化","有","初始","1","我们","目前","只","非常简单","(","learning",")","epochs","30","这里"],"title":"初始化方法","title_tokens":["初始化","初始","方法"]},{"location":"book-1-x/chapter-1/linear-classification/#_12","text":"接下来定义网络构造 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def construct ( self ): ''' Construct a linear model and set the optimizer as Adam ''' # Construction self . model = tf . keras . Sequential () self . dense1 = tf . keras . layers . Dense ( LABEL_SHAPE , use_bias = True , input_shape = ( INPUT_SHAPE ,), kernel_initializer = tf . keras . initializers . RandomNormal ( 0.0 , stddev = 10.0 ), bias_initializer = tf . keras . initializers . Constant ( 2 ), activation = tf . nn . sigmoid ) self . model . add ( self . dense1 ) # Set optimizer self . model . compile ( optimizer = tf . train . AdamOptimizer ( self . lr ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ self . accuracy , tf . keras . metrics . BinaryAccuracy ()] ) @staticmethod def accuracy ( y_true , y_pred ): return tf . keras . backend . mean ( tf . keras . backend . equal ( y_true , tf . keras . backend . round ( y_pred ))) 须知 这里 LABEL_SHAPE 和 INPUT_SHAPE 为两个宏变量，分别为输出和输入的向量维度。 我们使用 Dense 定义全连接层，它的用法请参照 这里 。由于我们已经知道 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 可能的取值范围，这里我们重定义了 \\mathbf{W} \\mathbf{W} 和 \\mathbf{b} \\mathbf{b} 的初始化方式。 信息: Dense API tf . keras . layers . Dense ( shape , ** kwargs ) 指全连接层，其输入一组已知形状的向量，输出一组形状为 shape 的向量。可用的API如下： shape : 正整数，输出空间维度。 activation : 激活函数。 若不指定，则不使用激活函数 (即，线性激活: a(\\mathbf{y}) = \\mathbf{y} a(\\mathbf{y}) = \\mathbf{y} )。该函数可以定义为任何元素级操作的Tensorflow函数。 use_bias : 布尔值，该层是否使用偏置向量。 True 则网络定义为 \\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} \\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} ， False 则定义为 \\mathbf{y} = \\mathbf{W}\\mathbf{x} \\mathbf{y} = \\mathbf{W}\\mathbf{x} 。 kernel_initializer : kernel 权值矩阵的初始化器，自定义的初始化器需要使用Keras后端API编写。 bias_initializer : 偏置向量的初始化器，同上。 kernel_regularizer : 运用到 kernel 权值矩阵的正则化函数，自定义的正则化函数需要使用Keras后端API编写。 bias_regularizer : 运用到偏置向的的正则化函数，同上。 activity_regularizer : 运用到层的输出的正则化函数，同上。 kernel_constraint : 运用到 kernel 权值矩阵的约束函数，只能使用Keras备选的几种方案，不能自定义。 bias_constraint : 运用到偏置向量的约束函数，同上。 信息: model.compile API model . compile ( optimizer , ** kwargs ) 在这里指的是顺序模型的编译函数，其可用的API如下： optimizer : 优化器，可以使用Tensorflow内置的优化器。 loss : 损失函数，也是目标函数。顺序模型只有一个输出，因此只能传入一个损失函数。可以使用形式为 func ( y_true , y_pred ) 的Tensorflow函数。 metrics : 测度函数，一般是一组函数，如果是一个函数则定义为 [ func ] 即可。自定义的测度函数目前还需要使用Keras后端API编写。 loss_weights : 损失的权重，顺序模型只有一个损失函数，因此只有一个权重，但要使用一维列表 [ value ] 定义。可以使用张量来控制可变权重。 sample_weight_mode : 按时间步采样权重，默认不提供。相比上面的损失权重，该选项会随着迭代次数使用不同的权重，因此输入的是二维列表。 weighted_metrics : 测度的权重，和损失权重类似，用来加给不同的测度函数。由于我们可以使用不只一个测度函数，这里的权重是个一维列表。 target_tensors : 默认情况下，Keras 将为模型的目标创建一个占位符，在训练过程中将使用目标数据。相反，如果你想使用自己的目标张量（反过来说，Keras在训练期间不会载入这些目标张量的外部 Numpy数据），您可以通过 target_tensors 参数指定它们。对于单输出的顺序模型，它应该是单个张量。 ** kwargs : 其他参量，会传递给 tf . Session . run 。 另外，注意我们这里构造网络的时候有如下技巧： Tensorflow在导入Keras模式以后，已经不再使用 (15) (15) 的形式定义 sigmoid交叉熵 ，而是采取更通用的定义 (14) (14) ； 我们使用Tensorflow重新封装过的类， 二分类交叉熵 ( BinaryCrossentropy ) 来作为Keras模型的损失函数 self.loss ，该函数与 多分类交叉熵 ( CategoricalCrossentropy ) 不同，乃是对两组对比张量的每个元素分别计算交叉熵，再求取均值，正符合本应用的需求； 我们通过静态方法，调用Keras的后端API，自己定义了预测准确度的测度函数 self.accuracy ，同时也使用另一个来自Tensorflow封装好的测度类 二分类准确度 ( BinaryAccuracy ) ，这是为了比照两个准确度的区别，以便我们更好理解该测度类； 我们将网络层的关键字 self.dense1 保留在了实例中，这是为了确保接下来我们能通过实例抽取该层的参数。 需要注意的是，由于 二分类交叉熵 ， 二分类准确度 和 多分类交叉熵 等都是类（从它们的定义都是大写字母开头也可以看出来），因此我们需要在使用的时候后面加上括号以实例化；由于这些类都定义了 __call__ 方法，我们可以像使用函数一样使用它们的实例。","text_tokens":["8","中将","指定","输入","一个","区别","activity","binaryaccuracy","方式","train","后面","运用","指","6","布尔值","通过","两个","loss","mathbf","10","时候","符合","5","可能","创建","指全","器","网络层","nn","可变","backend","，","已经","你","18","与","静态方法","它们","参照","反过来说","也","再","编译","py","多","这些","按","时间","方法","函数","定义","15","迭代","*","分别","即可","7","维度","用来","的","向量","看","采取","4","另外","给","0.0","不","过","更好","equal","模型","weighted","抽取","注意","元素","后端","操作","{","14","func","以","tensorflow","1","16","自己","19","来自","为了","layers","12","实例","自定","kernel","正则","参数","更","比照","initializers","通用","向","理解","二","另","预测","保留","开头","categoricalcrossentropy","可以","self","target","正整数","范围","w","-","接下","linclshandle","须知","分类","weights","但","}","session","过程中将","需要","和","矩阵","（","优化","写字","11","应用","两组","偏置","端","编写","确保","以便","dense1","空间","关键字","lr","下来","10.0","加","as","stddev","return",":","形式","sequential","同时","网络","反过","默认","布尔","numpy","regularizer","从","call","'","参量","为","def","y","二维","熵","均值","set","shape","use","value","linear","adam","等","staticmethod","round","需求","单","17","compile","a","一样","情况","内置","由于","pred","21","大写字母","label","了","静态","13","若","其他","dense","初始","accuracy","可用","我们","false","只","api","使用","整数","个","占位","\\","过程","+","大写","sigmoid","这里","它","重新","]","@","输出","期间","后","不再","以后","losses","下","optimizer","接下来","单个","__","add","cls","要","20","封装","化","来说","信息","tensors","binarycrossentropy","取值","到","metrics","出来","变量","lin","这是","不会","initializer","对","类似","上面","the","）","2","[","9","目标","关键","能","而是","。","全","外部","在","括号","类","该","该层","激活","测度","随着","反过来","已知","交叉","input","一维","技巧","and","连接","损失","mean","作为","准确","控制","都","tf","像","传入","载入","x","层","模式","目前","想","提供","kwargs","adamoptimizer","权重","构造","是否",")","用法","construction","：","如下","是","weight","约束","对比","其"," ","采样","construct","=","列表","因此","一组","备选","正","即","导入","来","形状","不能","几种","一般","调用","重","只能","符","次数","model","c","上",",","相反","mode","会","请",".","22","sample","不同","训练","步","应该","class","constant","传递","true","宏","级","b","3","_","知道","线性","字母","乃是","对于","数据","#","同","选项","中","bias","您","则","过来","计算","自定义","只有","任何","本","顺序","keras","每个","准确度","好","还","方案","randomnormal","初始化","constraint","有","加上","将","；","相比","run","(","如果","权值","求取","张量","activation"],"title":"构造方法","title_tokens":["构造","方法","构造方法"]},{"location":"book-1-x/chapter-1/linear-classification/#_13","text":"最后定义的式训练和测试方法。由于我们目前的project还比较简单，关于这两部分都直接调用现有的API即可。使用的API在之前已经说明。 model.fit 在没有额外设置的情况下，默认会返回一个 History回调器 ； model.evaluate 返回的是测试样本给出的损失函数和准确值测度。 model.predict 返回的是测试样本给出的网络输出。详情请参照 顺序模型API 。 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def train ( self , dataSet ): ''' Use a data set to train the network. ''' return self . model . fit ( dataSet , epochs = self . epoch , steps_per_epoch = self . steppe ) def test ( self , data , labels ): ''' Use (data, label) pairs to test the results. ''' loss , accu = self . model . evaluate ( data , labels ) print ( 'Evaluated loss =' , loss ) print ( 'Evaluated accuracy =' , accu ) return self . model . predict ( data ) 信息: model.fit API model . fit ( dataSet , ** kwargs ) 是训练函数，注意这个函数虽然支持输入一组 x,~y x,~y 用来代替 dataSet ，我们还是建议在任何情况下都用dataSet馈送数据，以免内存中数据集占用过多。 dataSet : 数据集，其本身应当是一个 tf.data.Dataset 类型的类，或者是一个能不断迭代产生新数据的生成器。数据的 batch 大小由 dataSet 本身决定。 epochs : 整数，终止训练时经历的世代(轮次)数，通常一个epoch表示遍历整个数据集一回。 verbose : 0, 1或2。日志显示模式。 0=安静模式, 1=进度条, 2=每轮一行。默认是1。 callbacks : 回调器，它是 tf.keras.callbacks 模块下的类，用来在训练中进行记录保存和数据检查点更新。默认是 tf.keras.callbacks.History 。 validation_split : 在 0 和 1 之间浮动。用作验证集的训练数据的比例。模型将分出一部分不会被训练的验证数据，并将在每一轮结束时评估这些验证数据的误差和任何其他模型指标。验证数据是混洗之前 x 和 y 数据的最后一部分样本中。 validation_data : 元组 (x_val，y_val) 或元组 (x_val，y_val，val_sample_weights) ，用来评估损失，以及在每轮结束时的任何模型度量指标。模型将不会在这个数据上进行训练。这个参数会覆盖 validation_split 。 shuffle : 布尔值（是否在每轮迭代之前混洗数据）。当 steps_per_epoch 非 None 时，这个参数无效。 class_weight : 可选的字典，用来映射类索引（整数）到权重（浮点）值，用于加权损失函数（仅在训练期间）。这可能有助于告诉模型 「更多关注」来自代表性不足的类的样本。 sample_weight : 用来给损失函数添加权重，作用类似 model.compile 的同一参数。 initial_epoch : 开始训练的轮次（有助于恢复之前的训练）。 steps_per_epoch : 在声明一个轮次完成并开始下一个轮次之前的总步数（样品批次）。使用TensorFlow数据张量等输入张量进行训练时，默认值 None 等于数据集中样本的数量除以 batch 的大小，如果无法确定，则为1。 validation_steps : 只有在指定了 steps_per_epoch 时才有用。停止前要验证的总步数（批次样本）。 该函数会返回 callbacks 定义的实例。 信息 model.evaluate API model . evaluate ( x , y , ** kwargs ) 是测试函数，需要传入 label 即 y 来验证性能。 x , y : Numpy 数组，分别是输入和输出的真实参照值。 batch_size : 计算的 batch 大小，该函数会将输入的数据组拆分成许多 batch 分别计算，并组合起来，这个设定值对效果不产生影响，只影响计算过程。 verbose : 0, 1。日志显示模式。0=安静模式, 1=进度条。默认是1。 sample_weight : 用来给损失函数添加权重，作用类似 model.compile 的同一参数。 steps : 整数或 None 。 声明评估结束之前的总步数（批次样本）。默认值 None 。 该函数会返回损失函数和测度（列表）。 信息 model.predict API model . predict ( x , ** kwargs ) 是预测函数，用在只知道输入 x 的场合。 x : Numpy 数组，输入值。 batch_size : 计算的 batch 大小，该函数会将输入的数据组拆分成许多 batch 分别计算，并组合起来，这个设定值对效果不产生影响，只影响计算过程。 verbose : 0, 1。日志显示模式。0=安静模式, 1=进度条。默认是1。 steps : 整数或 None 。 声明评估结束之前的总步数（批次样本）。默认值 None 。 该函数返回预测结果。","text_tokens":["前要","8","指定","输入","一个","设定","现有","有助于","开始","浮动","to","train","6","布尔值","或","loss","10","5","生成器","可能","还是","之间","器","验证","，","不断","轮次","被","已经","predict","分成","式","回调","部分","参照","作用","世代","详情","py","多","这些","返回","最后","成器","方法","函数","定义","迭代","无法","*","分别","即可","7","占用","用来","并","的","batch","集中","4","加权","validation","进行","给","不","模型","每轮","样品","test","注意","per","14","保存","tensorflow","映射","0","1","表示","来自","检查","记录","12","关注","实例","告诉","参数","更","代表性","步数","预测","设定值","self","-","linclshandle","一行","真实","数","weights","size","许多","样本","检查点","生成","评估","用","需要","和","（","accu","显示","11","数组","遍历","测试函数","详情请","性能","更新","模块","组合","return","每",":","网络","默认","布尔","numpy","'","为","def","y","set","use","影响","这个","等","确定","混洗","除以","结束","元组","安静","compile","」","浮点","a","有用","数量","情况","停止","由于","evaluate","label","组拆","结果","了","dataset","13","比例","network","evaluated","其他","索引","误差","accuracy","整个","我们","终止","只","api","额外","馈送","使用","整数","一部","过程","两","epochs","它","输出","比较简单","期间","完成","这","下","助于","以及","一轮","pairs","cls","信息","声明","到","添加","分出","lin","给出","大小","不会","对","shuffle","类似","the","）","2","9","labels","进度","能","。","总","epoch","应当","查点","起来","在","类","该","project","非","测度","产生","val","集","callbacks","损失","代表","决定","都","准确","以免","指标","直接","进度条","tf","data","传入","x","可选","模式","当","目前","一回","无效","kwargs","权重","字典","是否","用作",")","通常","是","weight","「","或者","history","其"," ","=","列表","有助","日志","之前","一组","steppe","代替","即","内存","类型","来","默认值","新","initial","调用","虽然","经历","verbose","model","由","上",",","等于","print","不足","会","本身","简单","results",".","steps","~","支持","设置","训练","测试方法","sample","class","一部分","比较","没有","测试","split","用于","3","值","_","知道","同一","批次","数据","中","时","效果","则","计算","说明","场合","只有","任何","过多","顺序","keras","none","建议","还","仅","将","度量","；","fit","覆盖","关于","恢复","(","如果","时才","张量"],"title":"训练和测试方法","title_tokens":["和","测试","方法","训练","测试方法"]},{"location":"book-1-x/chapter-1/linear-classification/#_14","text":"首先，训练网络。我们随机生成 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的仿射变换，并且设置好数据集，给定噪声扰动为 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,1)^6 。设定20个epoch，每个epoch迭代500次，每次馈入32个样本构成的batch，然后开始训练： lin-cls.py 1 2 3 4 5 6 7 8 9 # Initialization A = np . random . normal ( 0 , 10 , [ INPUT_SHAPE , LABEL_SHAPE ]) c = np . random . uniform ( 1 , 3 , [ 1 , LABEL_SHAPE ]) dataSet = dp . TestDataSet ( 10 , A , c ) dataSet . config ( batch = 32 , noise = 0.1 ) # Construct the model and train it. h = LinClsHandle ( learning_rate = 0.01 , epoch = 20 , steppe = 500 ) h . construct () record = h . train ( iter ( dataSet )) Output Epoch 1 /20 500 /500 [==============================] - 1s 3ms/step - loss: 6 .3005 - accuracy: 0 .5884 - binary_accuracy: 0 .5884 Epoch 2 /20 500 /500 [==============================] - 1s 2ms/step - loss: 5 .4671 - accuracy: 0 .6407 - binary_accuracy: 0 .6407 Epoch 3 /20 500 /500 [==============================] - 1s 2ms/step - loss: 4 .5711 - accuracy: 0 .6957 - binary_accuracy: 0 .6957 Epoch 4 /20 500 /500 [==============================] - 1s 2ms/step - loss: 3 .6789 - accuracy: 0 .7519 - binary_accuracy: 0 .7519 Epoch 5 /20 500 /500 [==============================] - 1s 2ms/step - loss: 2 .7101 - accuracy: 0 .8127 - binary_accuracy: 0 .8127 Epoch 6 /20 500 /500 [==============================] - 1s 2ms/step - loss: 2 .0059 - accuracy: 0 .8627 - binary_accuracy: 0 .8627 Epoch 7 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .6403 - accuracy: 0 .8894 - binary_accuracy: 0 .8894 Epoch 8 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .3663 - accuracy: 0 .9066 - binary_accuracy: 0 .9066 Epoch 9 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1 .0466 - accuracy: 0 .9274 - binary_accuracy: 0 .9274 Epoch 10 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .8377 - accuracy: 0 .9418 - binary_accuracy: 0 .9418 Epoch 11 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .6465 - accuracy: 0 .9546 - binary_accuracy: 0 .9546 Epoch 12 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .4492 - accuracy: 0 .9667 - binary_accuracy: 0 .9667 Epoch 13 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .2795 - accuracy: 0 .9779 - binary_accuracy: 0 .9779 Epoch 14 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .1624 - accuracy: 0 .9861 - binary_accuracy: 0 .9861 Epoch 15 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0680 - accuracy: 0 .9926 - binary_accuracy: 0 .9926 Epoch 16 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0128 - accuracy: 0 .9971 - binary_accuracy: 0 .9971 Epoch 17 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0055 - accuracy: 0 .9981 - binary_accuracy: 0 .9981 Epoch 18 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0052 - accuracy: 0 .9986 - binary_accuracy: 0 .9986 Epoch 19 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0055 - accuracy: 0 .9981 - binary_accuracy: 0 .9981 Epoch 20 /20 500 /500 [==============================] - 1s 2ms/step - loss: 0 .0049 - accuracy: 0 .9985 - binary_accuracy: 0 .9985 接下来，从训练返回的 History 类型的回调器中抽取对loss和accuracy的记录。 lin-cls.py 1 2 3 4 5 # Show records plt . semilogy ( record . epoch , record . history [ 'loss' ]), plt . xlabel ( 'epoch' ), plt . ylabel ( 'Cross entropy' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () plt . plot ( record . epoch , record . history [ 'accuracy' ], label = 'self defined' ), plt . plot ( record . epoch , record . history [ 'binary_accuracy' ], label = 'from tensorflow' ), plt . xlabel ( 'epoch' ), plt . ylabel ( 'Accuracy' ), plt . legend () plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () Output 结果显示，我们自定义的准确度测度和Tensorflow内置的 二分类准确度 完全相同，这说明其本身的定义就是求取所有元素阈值化后，各自分类结果是否正确的平均值。这个实验也让我们对自定义测度函数有了一定的认识。 重新设定数据集的产生方式，变为每个batch含10个样本。使用这组重新随机生成的数据测试网络输出， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 12 # Generate a group of testing samples: dataSet . config ( batch = 10 ) x , y = next ( dataSet ) # Check the testing results yp = h . test ( x , y ) _ , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 ) ax1 . imshow ( y , interpolation = 'nearest' , aspect = 'auto' ) ax1 . set_title ( 'True class' ) ax2 . imshow ( yp , interpolation = 'nearest' , aspect = 'auto' ) ax2 . set_title ( 'Predicted class' ) plt . gcf () . set_size_inches ( 10 , 5 ), plt . show () Output Evaluated loss ( losses.BinaryCrossentropy ) = 0 .0023145806044340134 Evaluated accuracy ( self defined ) = 1 .0 Evaluated accuracy ( metrics.BinaryAccuracy ) = 1 .0 注意我们未对测量的结果阈值化，因此显示出来的测量结果和理想值略有差别，但从图可知，阈值化后则测量结果全部准确。 通过抽取 h.dense1 的参数，我们可以对比 \\mathbf{A} \\mathbf{A} 和 \\mathbf{W} \\mathbf{W} ，以及 \\mathbf{c} \\mathbf{c} 和 \\mathbf{b} \\mathbf{b} ， lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 # Check the regressed values W , b = h . dense1 . get_weights () plt . imshow ( A , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'A' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . imshow ( W , interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( 'W' ) plt . gcf () . set_size_inches ( 6 , 5 ), plt . show () plt . plot ( c . T , label = 'c' ) plt . plot ( b . T , label = 'b' ) plt . legend () plt . gcf () . set_size_inches ( 5 , 5 ), plt . show () Output 可以发现，虽然我们训练的分类器十分有效，但其权值和预期的 \\mathbf{A} \\mathbf{A} , \\mathbf{c} \\mathbf{c} 并不完全相同。这是由于sigmoid函数激活的特性，使得当预测值偏向最小或最大的情况下， |\\sigma(x)| \\rightarrow 1 |\\sigma(x)| \\rightarrow 1 ，根据 (7) (7) ，可知其梯度 |\\sigma(x)(1-\\sigma(x))| \\rightarrow 0 |\\sigma(x)(1-\\sigma(x))| \\rightarrow 0 ，因此那些分类结果已经比较确信的样本，其梯度消失，对训练网络的影响忽略不计（这是合理的，因为我们不希望极端样本干扰结果，更希望对分类结果不确切的样本进行训练）。故而，我们虽然可以求解出这个分类问题，但求解到的 \\mathbf{W} \\mathbf{W} , \\mathbf{b} \\mathbf{b} 不会回归到 \\mathbf{A} \\mathbf{A} , \\mathbf{c} \\mathbf{c} 上。关于回归问题，我们会在下一节进一步讨论。","text_tokens":["8","忽略不计","设定","噪声","auto","开始","出","binaryaccuracy","一步","config","4671","这组","方式","train","然后","6","/","通过","或","mathbf","loss","10","5","9546","0055","，","已经","check","defined","generate","step","18","略有","回调","确信","9274","mathbb","8127","预期","回归","也","py","500","次","完全相同","n","返回","ax1","entropy","函数","定义","15","迭代","8894","7","全部","0.01","并","的","batch","图","化后","0049","4","normal","进行","所有","不","数据测试","故而","test","抽取","注意","元素","{","未","predicted","14","h","gcf","tensorflow","0","1","16","合理","19","ylabel","记录","12","自定","learning","参数","更","32","ax2","dp","有效","legend","二","预测","可以","不计","self","扰动","含","w","完全","构成","-","仿射变换","5884","接下","linclshandle","分类","分类器","使得","0.1","weights","size","|","样本","但","}","生成","mapsto","5711","和","（","显示","sim","差别","0128","6957","11","9667","特性","semilogy","uniform","dense1","1s","下来","最大",":","1624","网络","3005","问题","正确","干扰","t","从","aspect","'","为","y","因为","nearest","均值","set","shape","影响","9926","这个","17","讨论","a","testdataset","首先","情况","next","内置","由于","9779","label","initialization","结果","了","dataset","13","values","evaluated","accuracy","6403","我们","9861","使用","个","\\","6789","r","sigmoid","9985","重新","]","输出","8377","8627","9986","losses","这","下","以及","接下来","0466","仿射","理想","cls","20","梯度","4492","化","实验","十分","变换","binarycrossentropy","9418","3ms","到","metrics","出来","一定","6407","plt","lin","这是","不会","对","3663","一节","the","）","2","inches","[","0680","9981","根据","9","binary","器中","。","epoch","在","from","rate","0052","激活","测度","极端","测量","各自","产生","input","regressed","可知","给定","平均","集","变为","boldsymbol","and","^","发现","colorbar","最小","准确","进一步","就是","并且","忽略","x","2ms","当","确切","是否","馈入",")","samples","相同","阈值","结果显示","iter","：","消失","随机","对比","history","0059","其","预测值"," ","认识","construct","subplots","=","7519","平均值","plot","因此","show","steppe","rightarrow","2795","类型","imshow","每次","6465","7101","虽然","varepsilon","mathcal","model","c","上",",","it","会","本身","results","of",".","~","设置","训练","9066","让","0023145806044340134","class","record","records","比较","偏向","测试","true","np","b","3","group","title","值","_","sigma","希望","random","数据","noise","#","xlabel","output","则","get","自定义","说明","那些","testing","yp","每个","好","准确度","gca","有","求解","cross","关于","interpolation","(","9971","权值","求取"],"title":"调试","title_tokens":["调试"]},{"location":"book-1-x/chapter-1/linear-regression/","text":"线性回归 ¶ 摘要 本节介绍如何使用类模型(Model)来编写一个线性回归器，以拟合出一个线性模型。本节将第一次介绍如何编写一个带用户参数(选项)的Project，并且允许用户选择不同的优化器、对比验证不同优化算法的效果。 理论 ¶ 一般回归问题 ¶ 设存在一个多输出方程 \\mathbf{y} = \\mathcal{F}(x) \\mathbf{y} = \\mathcal{F}(x) ，当然 \\mathcal{F} \\mathcal{F} 可以是非线性函数，那么我们可以考虑使用一个带可调参数的模型 \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}) \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}) 来模拟它，其中 \\boldsymbol{\\Theta} \\boldsymbol{\\Theta} 是可调的参数。于是，该问题可以被表述为 \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\boldsymbol{\\Theta}} &\\sum_{k=1}^N \\mathcal{L} \\left( y_k,~ \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}_k) \\right),\\\\ \\mathrm{s.t.}~&y_k = \\mathcal{F}(x_k). \\end{aligned} \\end{equation} 在我们不知道 \\mathcal{F} \\mathcal{F} 的情况下，我们的目的是使用大量的 x_k,~y_k x_k,~y_k 样本，来调整出一个最优的近似模型 \\mathbf{D}_{\\boldsymbol{\\Theta}} \\mathbf{D}_{\\boldsymbol{\\Theta}} 。由于 \\mathcal{F} \\mathcal{F} 是非线性的，这要求我们的 \\mathbf{D}_{\\boldsymbol{\\Theta}} \\mathbf{D}_{\\boldsymbol{\\Theta}} 也可以是非线性的。实际情况下，这样的问题往往不容易求解，尤其是信号的非线性性极强时，该问题很容易陷入局部最优解，从而对求得一个可接受的解造成很大的障碍。 这里 \\mathcal{L} \\mathcal{L} 是损失函数。在回归问题中，很多情况下我们都只能选择 均方误差 (Mean squared error, MSE) 作为损失函数，这是因为回归问题的目的是模拟出一组信号来，而这些信号的分布范围可能是任意的。在一些特别的应用里，例如，如果我们的信号全部为正值，那么我们可以考虑使用 信噪比 (Signal-to-noise ratio, SNR) 来作为我们的损失函数。 线性回归 ¶ 继上一节的学习，我们知道如何解一个定义为 \\begin{align} \\mathbf{y} = \\left\\{ \\begin{bmatrix}\\hat{y}_1 > 0 & \\hat{y}_2 > 0 & \\cdots & \\hat{y}_n > 0\\end{bmatrix}^T, ~ \\left| ~ \\hat{\\mathbf{y}} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon} \\right. \\right\\}, \\end{align} 的分类模型。在本节，让我们考虑一个更简单的模型： \\begin{align} \\mathbf{y} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon}. \\end{align} 现在， \\mathbf{y} \\mathbf{y} 是关乎 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 的一个仿射函数，并且我们仍然保留噪声函数 \\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon} 。由于这是一个线性模型，我们可以想象到，存在一个线性回归器， \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ，使得预测结果为 \\begin{align} \\tilde{\\mathbf{y}} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}. \\end{align} 类似上一节，假设我们的数据集 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} 符合 (3) (3) 定义的数据分布特征。我们的基本要求是，在我们不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 的情况下，使用大量 (\\mathbf{x}_k,~\\mathbf{y}_k) \\in \\mathbb{D} (\\mathbf{x}_k,~\\mathbf{y}_k) \\in \\mathbb{D} 样本训练一个线性分类器，使得当我们给定任意一个新样本 \\mathbf{x} \\mathbf{x} 的时候，分类器能推断出其对应的 \\mathbf{y} \\mathbf{y} 。 故而，该问题可以描述为 \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\mathbf{W},~\\mathbf{b}} &\\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathbf{W} \\mathbf{x}_k + \\mathbf{b} \\right), \\\\ \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) &= \\lVert \\mathbf{y} - \\tilde{\\mathbf{y}} \\rVert_2^2. \\end{aligned} \\end{equation} 在本例中， \\mathbf{y} \\mathbf{y} 有正有负，因此我们使用均方误差来作为损失函数。 求解问题 ¶ 作为线性问题，该问题实际上可以写出其解析解。未免读者感到过于突兀，我们先从一个简单的问题开始入手： 例子：一次函数的线性回归 如果我们的矩阵 \\mathbf{A} \\mathbf{A} 退化为标量 a a ，向量 \\mathbf{c} \\mathbf{c} 退化为标量c，那么 (3) (3) 可以重新写为： \\begin{align} y = a x + c + \\varepsilon. \\end{align} 考虑我们拥有N个样本点 (x_k,~y_k) (x_k,~y_k) ，上述问题实际上可以求得解析解。设由这N个点构成了样本向量 \\mathbf{x}_d,~\\mathbf{y}_d \\mathbf{x}_d,~\\mathbf{y}_d (注意与前述的向量区分开来)，则问题可以写成 \\begin{align} \\arg \\min_\\limits{a,~c} \\lVert \\mathbf{y}_d - a \\mathbf{x}_d - c \\mathbf{1} \\rVert^2_2. \\end{align} 这就是附图所示的，拟合到直线的一次函数回归问题。将该损失函数展开，有 \\begin{equation} \\begin{aligned} \\mathcal{L}(a,~c) &= ( \\mathbf{y}_d - a \\mathbf{x}_d - c \\mathbf{1} )^T ( \\mathbf{y}_d - a \\mathbf{x}_d - c \\mathbf{1} )\\\\ &= \\mathbf{y}_d^T\\mathbf{y}_d + a^2 \\mathbf{x}_d^T \\mathbf{x}_d + c^2 + 2ac \\mathbf{1}^T \\mathbf{x}_d - 2 a \\mathbf{y}_d^T \\mathbf{x}_d - 2c \\mathbf{1}^T \\mathbf{y}_d. \\end{aligned} \\end{equation} 令 \\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial a}=0,~\\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial c}=0 \\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial a}=0,~\\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial c}=0 ，则我们得到一组二元一次方程组 \\begin{equation} \\left\\{ \\begin{aligned} a \\mathbf{x}_d^T \\mathbf{x}_d + c \\mathbf{1}^T \\mathbf{x}_d &= \\mathbf{y}_d^T \\mathbf{x}_d. \\\\ c + a \\mathbf{1}^T \\mathbf{x}_d &= \\mathbf{1}^T \\mathbf{y}_d. \\end{aligned} \\right. \\end{equation} 解之，得 \\begin{equation} \\left\\{ \\begin{aligned} a &= \\frac{ \\mathbf{y}_d^T \\mathbf{x}_d - ( \\mathbf{1}^T \\mathbf{y}_d ) ( \\mathbf{1}^T \\mathbf{x}_d ) }{ \\mathbf{x}_d^T \\mathbf{x}_d - (\\mathbf{1}^T \\mathbf{x}_d)^2 } = \\frac{ \\sum_k x_k y_k - \\sum_k y_k \\sum_k x_k }{ \\sum_k (x_k)^2 - \\left(\\sum_k x_k\\right)^2 }. \\\\ c &= \\mathbf{1}^T \\mathbf{y}_d - a ( \\mathbf{1}^T \\mathbf{x}_d ) = \\sum_k y_k - a \\left( \\sum_k x_k \\right). \\end{aligned} \\right. \\end{equation} 这个式子在诸多教材上都会出现，作为学生解回归问题的入门话题。可见，我们在本节讨论的问题并不是一个陌生的问题，相反，我们过去非常熟悉的一个问题，是这个问题的退化到标量下的特殊情况。另，计算该问题的相关系数，我们常使用 \\begin{align} \\rho = \\frac{ \\sum_k \\left(x_k - \\overline{x}\\right) \\left(y_k - \\overline{y}\\right) }{ \\sqrt{ \\sum_k \\left(x_k - \\overline{x}\\right)^2 \\sum_k \\left(y_k - \\overline{y}\\right)^2 } }, \\end{align} 其中 \\overline{x} = \\sum_k x_k ,~ \\overline{y} = \\sum_k y_k \\overline{x} = \\sum_k x_k ,~ \\overline{y} = \\sum_k y_k 。 有了解上述例子的基础，我们自然可以写出， \\begin{equation} \\begin{aligned} \\mathcal{L}(\\mathbf{A},~\\mathbf{c}) &= \\sum_k ( \\mathbf{y}_k - \\mathbf{A} \\mathbf{x}_k - \\mathbf{c} )^T ( \\mathbf{y}_k - \\mathbf{A} \\mathbf{x}_k - \\mathbf{c} )\\\\ &= \\sum_k \\left[ \\mathbf{y}_k^T\\mathbf{y}_k + \\mathbf{x}_k^T \\mathbf{A}^T\\mathbf{A} \\mathbf{x}_k + \\mathbf{c}^T \\mathbf{c} + 2 \\mathbf{c}^T \\mathbf{A} \\mathbf{x}_k - 2 \\mathbf{y}_k^T \\mathbf{A} \\mathbf{x}_k - 2 \\mathbf{y}_k^T \\mathbf{c} \\right]. \\end{aligned} \\end{equation} 提示 接下来的求导主要涉及单值对矩阵求导（导数仍是矩阵），单值对向量求导（导数仍是向量）。可以参考 The Matrix Cookbook 查到对应情况下的求导结果。 同理，令 \\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{A}}=0,~\\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{c}}=0 \\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{A}}=0,~\\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{c}}=0 ，则我们得到一组二元一次方程组 \\begin{equation} \\left\\{ \\begin{aligned} \\sum_k \\left[ \\mathbf{A} \\mathbf{x}_k \\mathbf{x}_k^T + \\mathbf{c} \\mathbf{x}_k^T \\right] &= \\sum_k \\left[ \\mathbf{y}_k \\mathbf{x}_k^T \\right]. \\\\ \\sum_k \\left[ \\mathbf{c} + \\mathbf{A} \\mathbf{x}_k \\right] &= \\sum_k \\left[ \\mathbf{y}_k \\right]. \\end{aligned} \\right. \\end{equation} 解之，得 \\begin{equation} \\left\\{ \\begin{aligned} \\mathbf{A} &= \\left[ N \\sum_k \\left[ \\mathbf{y}_k \\mathbf{x}_k^T \\right] - \\sum_k \\left[ \\mathbf{y}_k \\right] \\sum_k \\left[ \\mathbf{x}_k^T \\right] \\right] \\left[ N \\sum_k \\left[ \\mathbf{x}_k \\mathbf{x}_k^T \\right] - \\sum_k \\left[ \\mathbf{x}_k \\right] \\sum_k \\left[ \\mathbf{x}_k^T \\right] \\right]^{-1} \\\\ \\mathbf{c} &= \\frac{1}{N} \\sum_k \\left[ \\mathbf{y}_k - \\mathbf{A} \\mathbf{x}_k \\right]. \\end{aligned} \\right. \\end{equation} 可见，当上式中的逆不存在时（即低秩的情况），该方程还是有可能解不唯一。 同时，相关系数的计算可以表示为 \\begin{align} \\rho = \\mathrm{mean} \\left[ \\frac{ \\sum_k \\left(\\mathbf{x}_k - \\overline{\\mathbf{x}}\\right) \\cdot \\left(\\mathbf{y}_k - \\overline{\\mathbf{y}}\\right) }{ \\sqrt{ \\sum_k \\left[ \\left(\\mathbf{x}_k - \\overline{\\mathbf{x}}\\right) \\cdot \\left(\\mathbf{x}_k - \\overline{\\mathbf{x}}\\right) \\right] \\sum_k \\left[ \\left(\\mathbf{y}_k - \\overline{\\mathbf{y}}\\right) \\cdot \\left(\\mathbf{y}_k - \\overline{\\mathbf{y}}\\right) \\right] } } \\right]. \\end{align} 这就是 皮尔森相关系数 (Pearson's correlation) 。其中 \\overline{\\mathbf{x}} = \\sum_k \\mathbf{x}_k ,~ \\overline{\\mathbf{y}} = \\sum_k \\mathbf{y}_k \\overline{\\mathbf{x}} = \\sum_k \\mathbf{x}_k ,~ \\overline{\\mathbf{y}} = \\sum_k \\mathbf{y}_k ， \\cdot \\cdot 表示的是两个向量按元素各自相乘。它是式 (11) (11) 在多变量问题上的推广。相当于对向量的每一个元素，分别从统计上求取皮尔森相关系数，然后对向量每个元素对应的皮尔森相关系数求取平均值。 优化算法 ¶ 接下来，我们要介绍几种最常见的优化算法。关于更多这方面的内容，可以查考Google团队编写的在线电子书 Deep Learning 。笔者打算在未来为此开辟专题写文，因此这里只是介绍几种常见的 一阶梯度下降 算法。传统优化领域里，单靠一阶梯度下降往往难以满足对准确度的需求，但深度学习(Deep learning)往往必须使用这些简单的一阶梯度下降算法，就连使用一阶梯度近似二阶梯度的算法 共轭梯度下降 ，在很多情况下都被认为是费用(cost)过高。这是由于一个深度网络，往往具有大量的参数需要训练，因此一个Model的参数少则数十MB，多则上GB。一阶梯度下降算法所需的计算量小，能确保我们一次迭代的过程能迅速完成，因而备受青睐。为了提升其性能，深度学习领域内也对其进行了诸多改进。 注意 其实，论到优化算法，往往不得不提到 反向传播 。不过实际上，一个Tensorflow的入门者，其实完全不需要学习如何推导反向传播的过程。下面我们的叙述也完全不会提及反向传播相关的内容。关于为何我们不需要了解反向传播，在下一节我们会论到。但是，在本教程后期，介绍高级技巧的时候，我们会详细展开。事实上，笔者认为，一个Tensorflow的用户，如果只是为了编写代码，反向传播与ta其实无关痛痒；但只有真正掌握反向传播，我们才算是真正入门了神经网络的理论。 我们在这里说到优化算法，是用在训练网络上的。事实上，只有几种个别的机器学习应用，需要我们在测试阶段执行 迭代算法 (iterative algorithm) 。一般来说，深度学习的训练过程可以被普遍地描述为：已知一个带可调参数 \\boldsymbol{\\Theta} \\boldsymbol{\\Theta} 的模型 \\mathcal{D}_{\\boldsymbol{\\Theta}} \\mathcal{D}_{\\boldsymbol{\\Theta}} ，已知一组数据集 (\\mathbf{x}_i,~\\mathbf{y}_i) \\in \\mathbb{D} (\\mathbf{x}_i,~\\mathbf{y}_i) \\in \\mathbb{D} ，则我们的训练目标为 \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\boldsymbol{\\Theta}} \\mathbb{E}_{(\\mathbf{x}_i,~\\mathbf{y}_i) \\in \\mathbb{D}} \\left[ \\mathcal{L} \\left( \\mathbf{y}_i,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_i) \\right) \\right]. \\end{aligned} \\end{equation} 实际情况下，一般用均值估计来代替上式的期望函数。联系我们上一节的优化问题 (1) (1) 和本节的优化问题 (5) (5) ，都可以描述成上式的形式。也就是说，线性分类/回归器，是神经网络在解线性问题时的特例。 引入动量的优化算法 ¶ 接下来，让我们看看第一个算法， 随机梯度下降 (stochastic gradient descent, SGD) 。 随机梯度下降 记学习率为 \\epsilon \\epsilon ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\mathbf{g} 。 注意学习率一般需要设为一个较小的值，视情况而定。 由于梯度的期望满足 \\begin{equation} \\begin{aligned} \\mathbb{E} \\left[ \\mathbf{g} \\right] &= \\frac{1}{m} \\sum\\limits_{k=1}^m \\mathbb{E} \\left[ \\nabla_{\\boldsymbol{\\Theta}} \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\right] \\\\ &= \\mathbb{E} \\left[ \\nabla_{\\boldsymbol{\\Theta}} \\mathcal{L} \\left( \\mathbf{y},~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}) \\right) \\right] = \\nabla_{\\boldsymbol{\\Theta}} \\mathbb{E} \\left[ \\mathcal{L} \\left( \\mathbf{y},~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}) \\right) \\right]. \\end{aligned} \\end{equation} 可知随机抽取m个样本计算的梯度，在统计学上的期望等于全局梯度的期望。因此，这是一个有效的算法。 随机梯度下降存在明显的弊端，就是在收敛到（全局或局部）最优解的前提下，全局梯度为0，但通过随机选取batch得到的梯度（一般）可能不为0；并且，迭代受到个别极端样本梯度的影响较大，因此，我们有了第一个改进，即 带动量的随机梯度下降 (SGD with momentum) 。 带动量的随机梯度下降 参考文献 提出该算法的文章，可以在这里参考： On the momentum term in gradient descent learning algorithms. Neural Networks 记学习率为 \\epsilon \\epsilon ，惯性常数为 \\alpha \\alpha ，初始化动量 \\mathbf{v}=\\mathbf{v}_0 \\mathbf{v}=\\mathbf{v}_0 （不考虑继续训练的情况下 \\mathbf{v}_0 = \\mathbf{0} \\mathbf{v}_0 = \\mathbf{0} ），则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 计算带动量的更新值 \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} + \\mathbf{v} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} + \\mathbf{v} 。 显然，我们不难计算出， \\begin{equation} \\begin{aligned} \\mathbb{E} \\left[ \\mathbf{v} \\right] &= \\alpha \\mathbb{E} \\left[ \\mathbf{v} \\right] - \\epsilon \\mathbb{E} \\left[ \\mathbf{g} \\right]. \\\\ \\mathbb{E} \\left[ \\mathbf{v} \\right] &= - \\frac{\\epsilon}{1 - \\alpha} \\mathbb{E} \\left[ \\mathbf{g} \\right]. \\end{aligned} \\end{equation} 注意惯性通常需要设为 \\alpha \\in (0,~1) \\alpha \\in (0,~1) 。 这种改进的带来的好处是， 每次更新梯度时，上一次的梯度都会以指数衰减的形式残留在本次迭代中，从而确保新的梯度会被旧的梯度部分中和，避免极端梯度对更新参数影响过大； 当求解得到的梯度陷入局部最优时，如果该局部最优处的曲率较小，可以依靠动量的惯性，越过该局部最优解。 附图说明了使用这种算法的好处。黑色路径为SGD的更新轨迹，而红色路径为本算法的更新轨迹，可以看出随着迭代次数的增加，算法收敛的效果强于SGD。 有人从Nesterov在1983年的论文得到启发，提出了一个修正后的带动量随机梯度下降法，即 带Nesterov动量的随机梯度下降 (SGD with Nesterov momentum) 。 带Nesterov动量的随机梯度下降 参考文献 提出该算法的文章，可以在这里参考： A method for unconstrained convex minimization problem with the rate of convergence o\\left( \\frac{1}{k_2} \\right) o\\left( \\frac{1}{k_2} \\right) 记学习率为 \\epsilon \\epsilon ，惯性常数为 \\alpha \\alpha ，初始化动量 \\mathbf{v}=\\mathbf{v}_0 \\mathbf{v}=\\mathbf{v}_0 （不考虑继续训练的情况下 \\mathbf{v}_0 = \\mathbf{0} \\mathbf{v}_0 = \\mathbf{0} ），则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算惯性目标点的位置： \\boldsymbol{\\Theta}^{\\dagger} \\leftarrow \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} \\boldsymbol{\\Theta}^{\\dagger} \\leftarrow \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} ； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}^{\\dagger}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}^{\\dagger}} (\\mathbf{x}_k) \\right) ； 计算带动量的更新值 \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta}^{\\dagger} - \\epsilon \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta}^{\\dagger} - \\epsilon \\mathbf{g} 。 其实，该方法的更新量期望与前一种方法一样， 显然，我们不难计算出， \\begin{equation} \\begin{aligned} \\mathbb{E} \\left[ \\mathbf{v} \\right] &= - \\frac{\\epsilon}{1 - \\alpha} \\mathbb{E} \\left[ \\mathbf{g} \\right]\\\\ &= \\frac{\\epsilon}{1 - \\alpha} \\nabla_{\\boldsymbol{\\Theta}} \\mathbb{E} \\left[ \\mathcal{L} \\left( \\mathbf{y},~ \\mathcal{D}_{\\boldsymbol{\\Theta} + \\alpha \\mathbf{v}} (\\mathbf{x}) \\right) \\right]. \\end{aligned} \\end{equation} 当收敛到最优解时， \\mathbf{v} \\rightarrow 0 \\mathbf{v} \\rightarrow 0 ，同时有 \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} \\rightarrow \\boldsymbol{\\Theta} \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} \\rightarrow \\boldsymbol{\\Theta} 。我们在此不展开证明这个算法时能收敛的。但Nesterov的文献表明，它能将上面提到的带动量梯度下降算法的误差从 O\\left(\\frac{1}{K}\\right) O\\left(\\frac{1}{K}\\right) 下降到 O\\left(\\frac{1}{K^2}\\right) O\\left(\\frac{1}{K^2}\\right) 。其中 K K 为迭代次数。下图展示了这种方法的改进原理。它的梯度是在更新动量的惯性部分之后才计算出来的，因此新的梯度和之前的惯性是首尾相接的。实际实现时，按照上面的算法，每次迭代需要更新两次参数，计算一次梯度。合理调整算法的计算次序，可以改进为每次迭代更新一次参数，计算一次梯度。 引入可变学习率的优化算法 ¶ 上述几种算法共同的特点是，具有一个“学习率”。实际上，这个学习率非常不好处理，值过小时，收敛速度很慢；值过大时，在最优解附近又难以收敛。为了解决这一思路，我们可以令学习率可变。最简单的思路是，将学习率设为指数衰减的（当然也可以设置下界），这样当开始学习的时候，学习率较大；而即将收敛时，学习率又会较小。 但是，以上做法不过是一些小小的花招(trick)罢了，接下来介绍的几种算法，是根据当前计算出的梯度来自适应调整学习率的。理论上，使用这种算法，用户不再需要特别关注学习率对训练的影响，我们尽可以设置一个偏大的学习率，在训练过程中，它能被自适应调整到一个合适的区间上。 首先，我们来介绍一种初步的改进， Adagrad (Adaptive Subgradient) ， Adgrad 参考文献 提出该算法的文章，可以在这里参考： Adaptive Subgradient Methods for Online Learning and Stochastic Optimization 记学习率为 \\epsilon \\epsilon ，小量 \\delta \\delta ，初始化学习率参数对角矩阵为 \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新学习率为： \\mathbf{r} \\leftarrow \\mathbf{r} + \\mathrm{diag}(\\mathbf{g})^2 \\mathbf{r} \\leftarrow \\mathbf{r} + \\mathrm{diag}(\\mathbf{g})^2 ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} 。 注意文献中常用向量点积 \\odot \\odot 来表示学习率，这样学习率就不是对角矩阵而是向量了。我们这里不定义额外的符号，以便不熟悉相关定义的读者理解。 这一方法的思想是，学习率随着梯度的累计增大而逐渐减小，类似我们使用指数衰减的策略。所不同的是，在梯度小的地方，我们认为梯度平缓，所以学习率减小得慢，以便算法迅速地通过这一片区域；在梯度大地地方，由于梯度陡峭，为了防止我们因为学习率过大漏过该区域，学习率减小得快，以适应梯度的大小。 这个方法没有从根本上解决迭代次数过多时，梯度过小的问题。不难看出该算法学习率以 O\\left(\\frac{1}{\\mathbf{g}^T\\mathbf{g}}\\right) O\\left(\\frac{1}{\\mathbf{g}^T\\mathbf{g}}\\right) 的比率衰减，经验指出，这个算法在很多情况下是不好用的，只能解决一些比较特定的模型。 在这里，我们依然不给出收敛性的证明（或许在未来我们会在专题中讨论这一问题）。读者不必为这些算法的原理感到压力，我们只需要对其有一个直观的了解就好。 考虑到Adagrad学习率减小的速度未免太快了，我们可以考虑它的改进， RMSprop (root mean square proportion) ，注意它是另一个算法Adadelta的特例，不过在本节我们不会讨论Adadelta，有兴趣的读者可以自己去寻找参考资料。 RMSprop 参考文献 提出该算法的文章，可以在这里参考： Overview of mini-batch gradient descent 记学习率为 \\epsilon \\epsilon ，小量 \\delta \\delta ，衰减参数 \\rho \\rho ，初始化学习率参数对角矩阵为 \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新学习率为： \\mathbf{r} \\leftarrow \\rho \\mathbf{r} + (1 - \\rho) \\mathrm{diag}(\\mathbf{g})^2 \\mathbf{r} \\leftarrow \\rho \\mathbf{r} + (1 - \\rho) \\mathrm{diag}(\\mathbf{g})^2 ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} 。 和上一个算法相比，它唯一的改变就是引入了一个衰减参数 \\rho \\rho ，以指数衰减将之前收集的学习率遗忘。如此就可以控制指数过大的问题，这个trick真是令人一言难尽。但是有趣的是，实际经验中，这个方法真的是卓有成效，是现在常用的神经网络优化算法之一。 最后让我们来介绍当前最实用的算法（之一）， Adam (adaptive momentum estimation) 。顾名思义，它的基本原理是基于对动量的可变估计。实际上，在上一节的Project中，我们选用的优化器就是Adam，Tensorflow的官方教程中，也将Adam作为默认推荐的优化器。 Adam 参考文献 提出该算法的文章，可以在这里参考： Adam: a Method for Stochastic Optimization 特别需要注意的是，Adam的收敛性证明已经被后来者推翻，指出其中存在一个错误。改正后的版本称为AMSGrad，Tensorflow的Keras API支持我们在设置Adam的时候开启AMSGrad模式。关于AMSGrad，我们不在此展开讨论，有兴趣的读者可以参考： On the Convergence of Adam and Beyond 记 k k 为迭代次数，学习率为 \\epsilon \\epsilon ，小量 \\delta \\delta ，衰减参数 \\rho_1,~\\rho_2 \\rho_1,~\\rho_2 ，初始化动量为 \\mathbf{s} = \\mathbf{0} \\mathbf{s} = \\mathbf{0} ，学习率参数对角矩阵为 \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新动量为： \\mathbf{s} \\leftarrow \\rho_1 \\mathbf{s} + (1 - \\rho_1) \\mathbf{g} \\mathbf{s} \\leftarrow \\rho_1 \\mathbf{s} + (1 - \\rho_1) \\mathbf{g} ； 更新学习率为： \\mathbf{r} \\leftarrow \\rho_2 \\mathbf{r} + (1 - \\rho_2) \\mathrm{diag}(\\mathbf{g})^2 \\mathbf{r} \\leftarrow \\rho_2 \\mathbf{r} + (1 - \\rho_2) \\mathrm{diag}(\\mathbf{g})^2 ； 调整参数大小： \\hat{\\mathbf{s}} \\leftarrow \\dfrac{\\mathbf{s}}{1 - \\rho_1^k} \\hat{\\mathbf{s}} \\leftarrow \\dfrac{\\mathbf{s}}{1 - \\rho_1^k} , \\hat{\\mathbf{r}} \\leftarrow \\dfrac{\\mathbf{r}}{1 - \\rho_2^k} \\hat{\\mathbf{r}} \\leftarrow \\dfrac{\\mathbf{r}}{1 - \\rho_2^k} ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\dfrac{ \\hat{\\mathbf{s}} }{\\delta + \\sqrt{\\hat{\\mathbf{r}}}} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\dfrac{ \\hat{\\mathbf{s}} }{\\delta + \\sqrt{\\hat{\\mathbf{r}}}} 。 Adam不仅估计了学习率的可变性，还引入了可变的动量。这是迄今为止，我们见到的第一个将动量和可变学习率结合起来的算法。我们当然期望它能带来双份的 快乐 好处，可是…… 为什么会这样呢？ ，已经有文献指出，Adam存在原理上的失误，并提出了改正的算法AMSGrad，这正是我们未来将要在专题中讨论的内容。现在读者只需要知道，Adam的思路其实就是结合动量和可变学习率就行了。 注意 无论是我们没提到的Adadelta还是提到的Adam，其实都引入了动量的概念。那么一个自然而然的idea就是，使用Nesterov动量代替普通的动量。当然，毫无意外的是，已经有人做过了。例如，Adam的Nesterov动量版本叫Nadam，有兴趣的读者不妨去了解一下。 解线性回归问题 ¶ 代码规范 ¶ 重申我们之前提到的，我们建议一个完整的工程应当包括 . ├─ data/ # where we store our data │ └─ ... ├─ tools.py # codes for post-processing and analyzing records. ├─ extension.py # codes for extending the tensorflow model. ├─ dparser.py # data parser └─ main.py # main module where we define our tensorflow model. 除了保存数据的文件夹，我们应当有三个子模块。其中 tool : 用来处理、分析生成的数据，通常与Tensorflow无关； extension : 用来扩展tensorflow，例如在这里自定义网络层和操作符； dparser : 数据处理器，用来读取并预处理送入网络的数据； main : 主模块，只定义跟Tensorflow模型有关的内容，需要引用 extension 和 dparser 。 视情况可以灵活调整结构，但建议将定义Tensorflow模型的代码单独放在主模块里，和其他外围代码分离。 在上一节中，我们没有定义 tool.py 和 extension.py ，这是因为我们的工程还很简单，不需要扩展Tensoflow模型，也不需要专门的数据处理代码。相应地，我们把数据的后处理代码直接集成在了主模块 lin-cls.py 里。在这一节，我们要开始构造一个真正严格按照这四部分分离的工程，并且在接下来的各个例子实现里，都会遵照这个模式，读者应当熟悉类似我们所推荐的、这样一个高度分离的模块化设计的思路。 扩展模块 ¶ 此次是我们第一次写扩展模块，编写扩展模块的目的是，提供一个更复杂的支持库，以便我们能轻松地使用Tensorflow。因此，扩展模块编写地原则应当包括： 可适用性 : 它应当与我们某一个Project完全无关，就像我们自己基于Tensorflow编写一个扩展库一样，以后我们在任何项目都应该可以使用同一个扩展模块文件； 低依赖性 : 它应当最低限度地需要依赖库。 tensorflow 库本身当然是需要的，而 numpy ， matplotlib 甚或是读写数据的模块，都不宜出现在这里，以确保我们的扩展模块被其他任何模块调用时，依赖关系都是树状的； 强一致性 : 它的使用风格，应当尽可能和Tensorflow本身的API一致，使得一个之前不怎么接触它的人，也能快速上手。 在这个工程里，我们扩展的内容其实很简单，就是允许模型调用一个指定的优化器。让我们直接看以下代码： extension.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class AdvNetworkBase : ''' Base object of the advanced network APIs. ''' @staticmethod def optimizer ( name = 'adam' , l_rate = 0.01 , decay = 0.0 ): ''' Define the optimizer by default parameters except learning rate. Note that most of optimizers do not suggest users to modify their speically designed parameters. name: the name of optimizer (default='adam') (available: 'adam', 'amsgrad', 'adamax', 'nadam', 'adadelta', 'rms', 'adagrad', 'nmoment', 'sgd') l_rate: learning rate (default=0.01) decay: decay ratio ('adadeltaDA' do not support this option) ''' name = name . casefold () if name == 'adam' : return tf . keras . optimizers . Adam ( l_rate , decay = decay ) elif name == 'amsgrad' : return tf . keras . optimizers . Adam ( l_rate , decay = decay , amsgrad = True ) elif name == 'adamax' : return tf . keras . optimizers . Adamax ( l_rate , decay = decay ) elif name == 'nadam' : return tf . keras . optimizers . Nadam ( l_rate , schedule_decay = decay ) elif name == 'adadelta' : return tf . keras . optimizers . Adadelta ( l_rate , decay = decay ) elif name == 'rms' : return tf . keras . optimizers . RMSprop ( l_rate , decay = decay ) elif name == 'adagrad' : return tf . keras . optimizers . Adagrad ( l_rate , decay = decay ) elif name == 'nmoment' : return tf . keras . optimizers . SGD ( lr = l_rate , momentum = 0.6 , decay = decay , nesterov = True ) else : return tf . keras . optimizers . SGD ( l_rate , decay = decay ) 我们在这里几乎罗列了所有可能使用的优化器，全部来自Keras API。但我们也可以使用Tensorflow旧版API定义的优化器。目前Tensorflow允许使用两种API中的任意一种来定义，但是实验发现，旧版API系列的优化器要么已经在Keras中能找到对应的版本，要么就水土不服，无法正常调用。因此，上文提到的几种优化器，我们基本上全部在这里用Keras API定义出来。 优化器的参数尽可能应当选择默认参数，并且应当封装起来，不宜让用户自行操作。尤其是Adadelta，Adam这些优化器的 \\rho \\rho 变量，在 Keras文档 中，建议我们遵从默认值。 任何继承该类的子类，都可以通过 self . optimizer ( self . optimizerName , self . learning_rate ) 来将封装好的优化器API调用到主模块中。 项目选项：argparse ¶ 本节将第一次引入 argparse 模块。该模块是python本身继承的原生模块，用来给代码提供启动选项。作为一个完整的Project，我们不希望为了调整参数而频繁地修改代码，因此 argparse 对我们是不可或缺的。在后面所有的Project中，我们都会通过 argparse 模块支持项目选项。 argparse 的官方文档可以在此查阅： argparse — Parser for command-line options, arguments and sub-commands 调用 argparse 的一开始，我们需要定义如下内容： Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import argparse def str2bool ( v ): if v . casefold () in ( 'yes' , 'true' , 't' , 'y' , '1' ): return True elif v . casefold () in ( 'no' , 'false' , 'f' , 'n' , '0' ): return False else : raise argparse . ArgumentTypeError ( 'Unsupported value encountered.' ) parser = argparse . ArgumentParser ( description = 'A demo for linear regression.' , formatter_class = argparse . ArgumentDefaultsHelpFormatter ) Output usage: tools.py [ -h ] A demo for linear regression. optional arguments: -h, --help show this help message and exit 我们首先定义了 str2bool 函数，用来支持用户提供布尔类型的选项；之后，我们初始化了 parser ，一般地初始化 parser 时，我们主要定义三个参数： description : 项目描述，展示在参数用法之前的一段字符串； formatter_class : 格式化器 ，我们一般调用的都是 ArgumentDefaultsHelpFormatter ，因为它能支持自动换行，并在每个参数用法后展示该参数的默认值； epilog : 后记 ，这一段说明文字出现在所有参数用法之后。我们一般不太需要这个功能，但是有时候我们可以使用该功能提供一些用法范例给用户。 现在，我们来介绍几种典型的 argparse 可以提供的参数类型。 字符串选项 1 2 3 4 5 6 7 parser . add_argument ( '-o' , '--optimizer' , default = 'adam' , metavar = 'str' , help = ''' \\ The optimizer we use to train the model (available: 'adam', 'amsgrad', 'adamax', 'nadam', 'adadelta', 'rms', 'adagrad', 'nmoment', 'sgd') ''' ) 在这里我们定义了一个字符串选项，这是最常用的一类选项。用户可以像 python codes.py -o amsgrad 或者 python codes.py --optimizer amsgrad 一样，通过添加参数来覆盖默认值(定义在 default 字段下)。 数值选项 1 2 3 4 5 6 parser . add_argument ( '-lr' , '--learningRate' , default = 0.001 , type = float , metavar = 'float' , help = ''' \\ The learning rate for training the model. ''' ) 这里添加的参数类型是一个浮点数，虽然用户在输入参数的时候输入的是一个字符串，但 metavar 字段告诉了用户应该输入浮点数， type 决定了用户输入的字符串会被自动转换为浮点数。类似地，将两个字段的 float 改为 int ，我们就能提供一个整数作为参数选项 布尔选项 1 2 3 4 5 6 parser . add_argument ( '-if' , '--importFlag' , type = str2bool , nargs = '?' , const = True , default = False , metavar = 'bool' , help = ''' \\ The flag of importing pre-trained model. ''' ) 这里添加的是一个二值选项，它的默认值是 False ，用户可以通过输入 ( 'yes' , 'true' , 't' , 'y' , '1' ) 中的任何一个来指定该选项为真，或通过 ( 'no' , 'false' , 'f' , 'n' , '0' ) 中的任何一个指定该选项为假，不区分大小写。该功能由我们之前定义的 str2bool 函数提供。 特别值得注意的是，这个布尔选项还可以有这样的用法，例如： python codes.py -if -o amsgrad 我们如果指派了 -if ，在不指定它任何值的情况下，该选项就会被开启（值为真）了；如果我们去掉这一行的 -if ，则该选项关闭（值为假）。 多值选项 1 2 3 4 5 6 parser . add_argument ( '-ml' , '--mergedLabel' , default = None , type = int , nargs = '+' , metavar = 'int' , help = ''' \\ The merged label settings. ''' ) 上面的设置提供了一个可以输入任意多个 int 型值的选项，用法如下： python codes.py -ml 1 3 4 0 2 -o amsgrad 上述的输入会被解析成一个值为 [ 1 , 3 , 4 , 0 , 2 ] 的列表。当然，我们也可以输入任意多的值，但是特别值得注意的是，由于在 nargs 字段指定了 + ，一旦我们指派该选项，就要至少输入一个值方可。 上面的几种范例，并不是每一种都需要用在Project中。实际设置选项的时候，应当参照实际情况来处理。例如，本例中，就只使用 字符串选项 和 数值选项 两种。更多关于 add_argument 的用法，请参阅官方文档： argparse — add_argument() 在所有参数都设置好后，调用 args = parser . parse_args () 即可使参数选项生效。用户输入的参数选项将返回到 args 中，例如，如果用户制定了 -o ( --optimizer )，那么我们可以调用 args.optimizer 来取出该字段的值。 数据生成 ¶ 本节的数据也是自动生成出来的。参考上一节的数据生成器，重新定义数据生成类的迭代器： dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class TestDataRegSet ( TestDataSet ): ''' A generator of the data set for testing the linear regression model. ''' def next_train ( self ): ''' Get the next train batch: (x, y) ''' x = self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) y = np . matmul ( x , self . A ) + self . c if self . noise > 1e-3 : y = y + np . random . normal ( 0 , self . noise , size = y . shape ) else : np . random . normal ( 0 , self . noise , size = y . shape ) return x , y 提示 这里我们在没有噪声的情况下，仍然调用随机噪声函数，这是为了确保噪声函数被调用，使得随机数无论开关噪声，都能保持一致性。 该生成器同样是输入一组 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，以及相关配置，之后就可以通过 迭代器 (iterator) 或 方法 (method) 随机生成数据。与上一节不同的是，我们在本节可以尝试更进一步，令 \\mathbf{A} \\mathbf{A} 的 SVD分解 写作如下形式 \\begin{align} \\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^T. \\end{align} 其中， \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma} 是一个对角矩阵，对角线上的元素顺次排列，对应为矩阵 \\mathbf{A} \\mathbf{A} 的各个特征值。Numpy的库已经集成了 SVD分解 。我们知道，一个 M \\times N M \\times N 的矩阵经过SVD分解后，应当有 \\mathbf{U}_{M \\times M} \\mathbf{U}_{M \\times M} 和 \\mathbf{V}^T_{N \\times N} \\mathbf{V}^T_{N \\times N} 两个方阵。故而，矩阵 \\boldsymbol{\\Sigma}_{M \\times N} \\boldsymbol{\\Sigma}_{M \\times N} 并非方阵。由于它只有对角线上有元素，所以必定有多出来的空行或空列。因此，若我们设 K = \\min(M,~N) K = \\min(M,~N) ，则我们可以知道，SVD分解其实不需要矩阵 \\mathbf{U} \\mathbf{U} 和 \\mathbf{V}^T \\mathbf{V}^T 两个方阵都是方阵，因为当我们取矩阵 \\boldsymbol{\\Sigma}_{K \\times K} \\boldsymbol{\\Sigma}_{K \\times K} 这一对角部分后，可以只取部分行/列构成的矩阵 \\mathbf{U}_{M \\times K} \\mathbf{U}_{M \\times K} 和 \\mathbf{V}^T_{K \\times N} \\mathbf{V}^T_{K \\times N} 。这相当于我们略去了 \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma} 上的空行/空列，但是SVD分解仍然能保证恢复出原矩阵来。 在本例中，我们保留 \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma} 中的前 r r 个特征值，其后的特征值都丢弃，我们把这样的做法称为矩阵的低秩近似，于是有 dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 def gen_lowrank ( A , r ): ''' Generate a low rank approximation to matrix A. A: input matrix. r: output rank. ''' sze = A . shape r_min = np . amin ( sze ) assert r <= r_min and r > 0 , 'r should in the range of [1, {0}]' . format ( r_min ) u , s , v = np . linalg . svd ( A , full_matrices = False ) s = np . diag ( s [: r ]) return np . matmul ( np . matmul ( u [:,: r ], s ), v [: r ,:]) 一个低秩近似的矩阵，其定义的仿射变换 (3) (3) 满足不同的 \\mathbf{x} \\mathbf{x} 对应同一个值 \\mathbf{y} \\mathbf{y} ；反之， \\mathbf{y} \\mathbf{y} 将会对应多个不同的解 \\mathbf{x} \\mathbf{x} 。如果我们训练的线性分类器模拟的是 (3) (3) 的逆过程，可能我们会无法模拟出合适的解来；但是，由于我们定义的 (4) (4) 仍是在拟合正过程，故而我们仍然可以把这个问题看成是有解的。在后续的内容中，我们会适当地讨论当问题 解不唯一 时，我们可以进行哪些工作来处理这类问题。 接下来，我们即可测试低秩近似的效果， dparser.py 1 2 3 4 5 6 7 8 9 def test_lowrank (): A = np . random . normal ( 0 , 10 , [ 10 , 6 ]) for r in range ( 1 , 7 ): A_ = gen_lowrank ( A , r ) RMS = np . sqrt ( np . mean ( np . square ( A - A_ ))) R = np . linalg . matrix_rank ( A_ ) print ( 'Rank = {0}, RMS={1}' . format ( R , RMS )) test_lowrank () Output Rank = 1 , RMS = 6.8600432267325955 Rank = 2 , RMS = 4.677152938185369 Rank = 3 , RMS = 3.216810970685858 Rank = 4 , RMS = 1.8380598782932136 Rank = 5 , RMS = 0.9348520972791058 Rank = 6 , RMS = 9.736224609164252e-15 可见，对于一个标准差为10的矩阵，低秩近似的残差仍然是不超过随机高斯矩阵本身的标准差的。这里的秩是我们在调用低秩近似函数后，使用 np.linalg.matrix_rank 测量的结果。 定义类模型 ¶ 类模型 (Model class) ，在官方文档中也称为函数式API，是Tensorflow-Keras的用户大多数情况下应当使用的模型。它支持一些灵活的操作，使得我们可以 多输入多输出 : 类模型的输入和输出层，都是通过函数定义的。类模型在构建的时候，只需要给定输入和输出即可； 跨层短接 : 由于类模型的各层都由函数定义，可以轻松将不同的层连接起来，通常通过 融合层 完成这一工作； 多优化器 : 可以通过复用同一层对应的对象，构建多个不同的类模型，并分别对它们使用不同的训练数据、损失函数、优化器，以实现多优化目标。 一个顺序模型大致可以描述为下图的模式： graph LR st1(输<br/>入<br/>1) --> l11[层<br/>1-1] l11 --> l21[层<br/>1-2] l21 --> l31[层<br/>1-3] l31 --> ldots1[层<br/>...] st2(输<br/>入<br/>2) --> l12[层<br/>2-1] l12 --> l22[层<br/>2-2] l22 --> l32[层<br/>2-3] l32 --> ldots2[层<br/>...] ldots1 --> l3[层<br/>3] ldots2 --> l3 l3 --> l4[层<br/>4] l4 --> ed1(输<br/>出<br/>1) l4 --> ed2(输<br/>出<br/>2) l22 --> ed3(输出3) l21 --> l3 classDef styStart fill:#FAE6A9,stroke:#BA9132; class st1,ed1,st2,ed2,ed3 styStart 在本节中，尽管我们开始使用类模型，但我们定义的仍然是一个单线路的线性回归模型，换言之，这样的模型完全可以通过 顺序模型 实现出来。我们从这一节开始，不再使用顺序模型，其一，是因为顺序模型都可以写成类模型的形式，其二，是希望读者能够熟悉、灵活运用类模型的优势。 我们定义一个继承自 extension.py 的类， class LinRegHandle ( ext . AdvNetworkBase ): 。与上一节的情况相若，这里我们不再赘述需要定义哪些方法。并且，我们也不会介绍一些改动不大、或者不重要的方法，详情请读者参阅源码。 初始化方法 ¶ 首先，定义初始化方法： lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def __init__ ( self , learning_rate = 0.01 , epoch = 10 , steppe = 30 , optimizerName = 'adam' ): ''' Initialization and pass fixed parameters. learning_rate: the learning rate for optimizer. epoch: training epochs. steppe: steps per epoch optimizerName: the name of optimizer (available: 'adam', 'amsgrad', 'adamax', 'nadam', 'adadelta', 'rms', 'adagrad', 'nmoment', 'sgd') ''' self . lr = learning_rate self . epoch = epoch self . steppe = steppe self . optimizerName = optimizerName 与上一节相比，这里我们增加了一个参数， opmizerName ，用来指定我们选用的优化器名称，默认值为 adam 。 构造方法 ¶ 接下来定义网络构造 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def construct ( self ): ''' Construct a linear model and set the optimizer as Adam ''' # Construction input = tf . keras . Input ( shape = ( INPUT_SHAPE ,), dtype = tf . float32 ) dense1 = tf . keras . layers . Dense ( LABEL_SHAPE , use_bias = True , kernel_initializer = tf . keras . initializers . RandomNormal ( 0.0 , stddev = 10.0 ), bias_initializer = tf . keras . initializers . Constant ( 2 ), activation = None , name = 'dense1' )( input ) self . model = tf . keras . Model ( inputs = input , outputs = dense1 ) # Set optimizer self . model . compile ( optimizer = self . optimizer ( self . optimizerName , self . lr ), loss = tf . keras . losses . MeanSquaredError (), metrics = [ self . relation ] ) @staticmethod def relation ( y_true , y_pred ): m_y_true = tf . keras . backend . mean ( y_true , axis = 0 ) m_y_pred = tf . keras . backend . mean ( y_pred , axis = 0 ) s_y_true = tf . keras . backend . sqrt ( tf . keras . backend . mean ( tf . keras . backend . square ( y_true ), axis = 0 ) - tf . keras . backend . square ( m_y_true )) s_y_pred = tf . keras . backend . sqrt ( tf . keras . backend . mean ( tf . keras . backend . square ( y_pred ), axis = 0 ) - tf . keras . backend . square ( m_y_pred )) return tf . keras . backend . mean (( tf . keras . backend . mean ( y_true * y_pred , axis = 0 ) - m_y_true * m_y_pred ) / ( s_y_true * s_y_pred )) 使用类模型时，我们每定义一层，都调用对应的网络层函数，并返回层的输出结果。这就是为何它又叫“函数式API”。我们直接使用均方误差作为我们的损失函数，同时，我们还自行定义了一个评价函数， 皮尔森相关系数 ，该系数专门用来反映两组数据之间是否线性相关，上文我们已经叙述过它的定义。 注意 理想情况下，相关系数应当使用整个数据集来求取。但实际情况下做不到这一点，因此我们求取的相关系数只能看作是一个通过batch得到的估计。故此，我们可以发现，求相关系数要求我们每次输入的样本至少有2个。样本数目越多，相关系数的估计越准确。 注意 从式中可以发现，我们定义的皮尔森相关系数时，完全使用的时Tensorflow-Keras API，因此它当然可以用作我们的训练损失函数。但实际情况下，我们并不使用它。考虑一个反例，当两组数据的分布之间唯一的不同只是均值时，亦即 \\mathbf{y}_2 = \\mathbf{y}_1 + C \\mathbf{y}_2 = \\mathbf{y}_1 + C ，这种情况下皮尔森相关系数仍然为1。虽然我们可以考虑用 余弦相似度函数 (Cosine similarity) 来代替它，但经验显示，余弦相似度最大化到一定程度以后，其对应的均方误差反而上升。考虑另一个反例， \\mathbf{y}_2 = \\alpha \\mathbf{y}_1 \\mathbf{y}_2 = \\alpha \\mathbf{y}_1 ，显然 \\mathbf{y}_1 \\mathbf{y}_1 和 \\mathbf{y}_2 \\mathbf{y}_2 的余弦相似度是1。因此，实际应用中，无论是皮尔森相关系数还是余弦相似度，都适合用作评价函数而不是损失函数。 与上一节不同的是，由于这是一个线性回归器，我们不给它提供激活函数。 训练和测试方法 ¶ 类模型的 compile 、 fit 、 evaluate 、 predict 等API与顺序模型完全相同，详情请查看 Keras中文文档 - Model类 (函数式API) 。 调试 ¶ 上一节中，我们每次训练后，就当场显示分析结果。在本节中，我们会“再进一步”。即使用 tools.py 专门进行实验结果分析（后处理）。相对地，训练后，我们会讲 原始输出 (raw output) 保存到文件里。这是一种编写代码的思想，是为了便于我们批量分析测试数据。在后面的Project中，我们会看到，我们既会编写当场显示分析结果的测试代码，也会编写保存输出后使用 tools.py 分析的代码。究竟使用哪种方式分析数据，视具体情况而定。一般地，测试少量数据时，我们当场分析；批量测试大量数据时，或者需要比较不同选项（例如不同噪声）对结果的影响时，我们在 tools.py 中分析。本实验的情况属于后者。 使实验结果可复现 ¶ 由于我们本次实验需要对比不同设置下的回归器性能，我们希望随机生成的矩阵 \\mathbf{A} \\mathbf{A} ，向量 \\mathbf{c} \\mathbf{c} 应当可复现；换言之，我们希望我们的结果是可复现的。 关于这一问题，Keras的文档给出的建议可以在这里查阅： 如何在 Keras 开发过程中获取可复现的结果？ 我们只需要使 argparse 添加一个选项 -sd ( --seed )，并通过该选项控制： 1 2 3 4 5 6 def setSeed ( seed ): np . random . seed ( seed ) random . seed ( seed + 12345 ) tf . set_random_seed ( seed + 1234 ) if args . seed is not None : # Set seed for reproductable results setSeed ( args . seed ) 其中， np.random.seed ， random.seed ， tf.set_random_seed 分别来自Numpy，python原生的random库，以及Tensorflow。将这三个库的 随机种子 (seed) 设为三个不同的值，即可保证我们每次指定 -sd 后，从程序运行开始，得到的所有随机数都是固定的随机序列。当然， Keras文档 指出，即使如此，我们还不能保证我们的结果完完全全是可复现的。因为多线程算法并发的先后顺序随机性、GPU运算带来的先后顺序随机性等干扰因素，均会导致我们每次得到的结果有细微的偏差。但这些因素对于本实验验证可复现数据的要求几乎没有什么影响。 使实验代码保存输出 ¶ 首先，训练网络。我们同样随机生成 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的仿射变换，将该变换中的线性变换矩阵采用秩为4的低秩近似，并且设置好数据集，给定噪声扰动由用户决定。默认值下，噪声为 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,10)^6 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,10)^6 ，epoch为20个，每个epoch迭代500次，每次馈入32个样本构成的batch。我们将上一节的主函数输出部分修改成如下形式，并进行不加参数的调试： lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # Initialization A = dp . gen_lowrank ( np . random . normal ( 0 , 10 , [ INPUT_SHAPE , LABEL_SHAPE ]), RANK ) c = np . random . uniform ( 1 , 3 , [ 1 , LABEL_SHAPE ]) dataSet = dp . TestDataRegSet ( 10 , A , c ) dataSet . config ( noise = args . noise ) # Generate a group of testing samples. if args . seed is not None : setSeed ( args . seed + 1000 ) dataSet . config ( batch = args . testBatchNum ) x , y = next ( dataSet ) # Set the data set for training. dataSet . config ( batch = args . trainBatchNum ) # Construct the model and train it. h = LinRegHandle ( learning_rate = args . learningRate , epoch = args . epoch , steppe = args . steppe , optimizerName = args . optimizer ) h . construct () print ( 'Begin to train:' ) print ( '---------------' ) record = h . train ( iter ( dataSet )) # Check the testing results print ( 'Begin to test:' ) print ( '---------------' ) yp , loss_p , corr_p = h . test ( x , y ) # Check the regressed values W , b = h . model . get_layer ( name = 'dense1' ) . get_weights () # Save if args . outputData is not None : np . savez_compressed ( args . outputData , epoch = record . epoch , loss = record . history [ 'loss' ], corr = record . history [ 'relation' ], test_x = x , test_y = y , pred_y = yp , pred_loss = loss_p , pred_corr = corr_p , W = W , b = b , A = A , c = c ) Output Begin to train: --------------- Epoch 1 /20 500 /500 [==============================] - 1s 2ms/step - loss: 29084 .6994 - relation: 0 .3472 Epoch 2 /20 500 /500 [==============================] - 1s 2ms/step - loss: 15669 .9579 - relation: 0 .5597 Epoch 3 /20 500 /500 [==============================] - 1s 2ms/step - loss: 8145 .8705 - relation: 0 .7134 Epoch 4 /20 500 /500 [==============================] - 1s 2ms/step - loss: 4000 .0838 - relation: 0 .8130 Epoch 5 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1856 .1477 - relation: 0 .8801 Epoch 6 /20 500 /500 [==============================] - 1s 2ms/step - loss: 799 .4556 - relation: 0 .9354 Epoch 7 /20 500 /500 [==============================] - 1s 2ms/step - loss: 336 .8600 - relation: 0 .9700 Epoch 8 /20 500 /500 [==============================] - 1s 2ms/step - loss: 166 .5899 - relation: 0 .9813 Epoch 9 /20 500 /500 [==============================] - 1s 2ms/step - loss: 113 .2465 - relation: 0 .9831 Epoch 10 /20 500 /500 [==============================] - 1s 2ms/step - loss: 102 .0431 - relation: 0 .9834 Epoch 11 /20 500 /500 [==============================] - 1s 2ms/step - loss: 99 .6678 - relation: 0 .9838 Epoch 12 /20 500 /500 [==============================] - 1s 2ms/step - loss: 99 .8547 - relation: 0 .9833 Epoch 13 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .1278 - relation: 0 .9834 Epoch 14 /20 500 /500 [==============================] - 1s 2ms/step - loss: 99 .6048 - relation: 0 .9835 Epoch 15 /20 500 /500 [==============================] - 1s 2ms/step - loss: 101 .1930 - relation: 0 .9832 Epoch 16 /20 500 /500 [==============================] - 1s 2ms/step - loss: 101 .6636 - relation: 0 .9835 Epoch 17 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .6665 - relation: 0 .9834 Epoch 18 /20 500 /500 [==============================] - 1s 2ms/step - loss: 101 .2459 - relation: 0 .9832 Epoch 19 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .9701 - relation: 0 .9836 Epoch 20 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .7719 - relation: 0 .9836 Begin to test: --------------- 10 /10 [==============================] - 0s 5ms/sample - loss: 94 .8883 - relation: 0 .9897 Evaluated loss ( losses.MeanSquaredError ) = 94 .88829040527344 Evaluated metric ( Pearson ' s correlation ) = 0 .9897396 以上结果是不加任何参数的前提下，直接以默认参数运行程序得到的。结果显明，MSE最后收敛在100左右，因为我们馈入的label添加了标准差为10的白噪声，对应的方差为100。可知，实验结果与预期一致。另一方面，我们可以看到，相关系数在这里可以充当类似准确度的作用，考虑到我们默认的噪声为10，这一相关系数的收敛结果是符合我们的预期的。 我们还可以注意到，这段代码中，生成测试集的代码被提前了，这是为了确保每次运行程序，只要指定了种子，生成的测试集总是一致的。 现在，我们可以导出生成数据了，首先，我们改变不同的优化器，其他参数全部一致，例如，学习率均为0.01（Adadelta除外，其初始参数一般推荐为1.0）。调用代码时的参数设置如下 python lin-reg.py -e 25 -sd 1 -do test/algorithm/ { optimizer } -o { optimizer } 其中我们用 {optimizer} 来指代我们选用的优化算法。同时，我们固定测试的epoch数量为25，这是因为有些算法的收敛速度不足以保证20个epoch收敛。 接下来，我们固定优化器为Adam，改变不同的噪声，分别令标准差为0, 1, 5, 10, 50, 100，产生多组结果。 python lin-reg.py -sd 1 -do test/noise/ { noise } -is { noise } 在 tools.py 中分析比较结果 ¶ 首先，在 tools.py 中定义数据解析函数 tools.py 1 2 3 4 5 6 7 8 9 10 11 12 def parseData ( path , keys ): keys_list = dict (( k , []) for k in keys ) name_list = [] for f in os . scandir ( path ): if f . is_file (): name , _ = os . path . splitext ( f . name ) name_list . append ( name . replace ( '_' , ' ' )) data = np . load ( os . path . join ( path , f . name )) for key in keys : keys_list [ key ] . append ( data [ key ]) epoch = data [ 'epoch' ] return name_list , epoch , keys_list 该函数的作用是，给定保存输出文件的文件夹路径，能够自动读取文件夹下所有数据文件，并将不同文件的结果列在列表的不同元素中。 keys 关键字能帮助我们指派我们关心的数据字段。 接下来，我们通过如下代码，对比不同优化器条件下的损失函数和测度函数，对比不同噪声条件下的损失函数和测度函数，输出的曲线反映了对训练过程的跟踪。 tools.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def showCurves ( path , prefix = '{0}' , converter = str ): ''' Show curves from different tests in a same folder. ''' name_list , epoch , keys_list = parseData ( path , [ 'loss' , 'corr' ]) loss_list = keys_list [ 'loss' ] corr_list = keys_list [ 'corr' ] if ( not loss_list ) or ( not corr_list ): raise FileExistsError ( 'No data found, could not draw curves.' ) for i in range ( len ( loss_list )): plt . semilogy ( loss_list [ i ], label = prefix . format ( converter ( name_list [ i ])), marker = MARKERS [ i % 9 ], markevery = 3 ) plt . legend () plt . xlabel ( 'epoch' ), plt . ylabel ( 'MSE' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . tight_layout (), plt . show () for i in range ( len ( corr_list )): plt . plot ( corr_list [ i ], label = prefix . format ( converter ( name_list [ i ])), marker = MARKERS [ i % 9 ], markevery = 3 ) plt . legend () plt . xlabel ( 'epoch' ), plt . ylabel ( 'Pearson \\' s correlation' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . tight_layout (), plt . show () showCurves ( './test/algorithm' ) showCurves ( './test/noise' , prefix = 'ε=N(0,{0})' , converter = int ) Output (优化器) 损失函数 (MSE) 测度函数 (相关系数) Output (噪声) 损失函数 (MSE) 测度函数 (相关系数) 可见，损失曲线反映了训练的进度，而测度曲线反映了当前的准确度。我们可以得到如下结论： 令人意外的是，SGD和Nesterov动量法收敛速度最快。这是由于这两种方法没有引入对学习率的调整。我们使用的损失函数初始点梯度非常大，这使得简单的方法，形如SGD和动量法在一开头就取得了非常迅速的下降；而对那些需要调整学习率的算法而言，初始梯度在很大的情况下，会导致初始学习率被降到较小的水准。这就是为何Adagrad几乎不收敛的原因，因为一开始这一算法的学习率就被大梯度抑制到将近0的水平了，导致训练无法为继； 在调整学习率的算法里，收敛速度有 RMSprop > Adam = NAdam > Adamax = AMSgrad > Adadelta。从AMSgrad以上的这些算法都可资利用，Adadelta的原理和RMSprop几乎相同但效果相差甚巨，这是由于参数不同引起的，我们虽然将Adadelta的学习率特地设为 1.0 ，仍然远远不如RMSprop，可见一个合适的参数对算法的重要性。 噪声的输出结果并不令人意外，所有噪声条件下的MSE最后都收敛到对应的噪声方差上。 为了检查测试集的情况，我们通过以下函数来绘制比较不同样本在不同优化器、不同噪声条件下的RMSE（均方根误差）， tools.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def showBars ( path , prefix = '{0}' , converter = str , ylim = None ): ''' Show bar graphs for RMSE for each result ''' name_list , epoch , keys_list = parseData ( path , [ 'test_y' , 'pred_y' ]) #print(keys_list) ytrue_list = keys_list [ 'test_y' ] ypred_list = keys_list [ 'pred_y' ] def RMSE ( y_true , y_pred ): return np . sqrt ( np . mean ( np . square ( y_true - y_pred ), axis = 1 )) N = ytrue_list [ 0 ] . shape [ 0 ] NG = len ( ytrue_list ) for i in range ( NG ): plt . bar ([ 0.6 + j + 0.8 * i / NG + 0.4 / NG for j in range ( - 1 , 9 , 1 )], RMSE ( ytrue_list [ i ], ypred_list [ i ]), width = 0.8 / NG , label = prefix . format ( converter ( name_list [ i ]))) plt . legend ( ncol = 5 ) plt . xlabel ( 'sample' ), plt . ylabel ( 'RMSE' ) if ylim is not None : plt . ylim ([ 0 , ylim ]) plt . gcf () . set_size_inches ( 12 , 5 ), plt . tight_layout (), plt . show () showBars ( './test/algorithm' , ylim = 70 ) showBars ( './test/noise' , prefix = 'ε=N(0,{0})' , converter = int ) Output (优化器) Output (噪声) 上述结果反映了 测试结果和训练情况相仿，这是由于我们的训练集和测试机完全独立同分布； Adadelta和Adagrad还没有训练好，它们的误差明显大于其他算法。且Adagrad已经无法收敛，可见这种算法不实用。 再接下来，我们要分别展示不同测试下的输出。下面列举的所有输出由该函数所产生： tools.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def saveResults ( path , opath , oprefix , datakeys , title = '' , xlabel = None , ylabel = None , onlyFirst = False , plot = False , prefix = ' ({0})' , converter = str ): ''' Save result graphs to a folder. ''' name_list , _ , data_list = parseData ( path , datakeys ) if plot : # show curves c_list = data_list [ 'c' ] b_list = data_list [ 'b' ] NG = len ( b_list ) for i in range ( NG ): plt . plot ( c_list [ i ] . T , label = 'c' ) plt . plot ( b_list [ i ] . T , label = 'b' ) plt . legend () plt . gca () . set_title ( title + prefix . format ( converter ( name_list [ i ]))) if xlabel is not None : plt . xlabel ( xlabel ) if ylabel is not None : plt . ylabel ( ylabel ) plt . gcf () . set_size_inches ( 5 , 5 ) if onlyFirst : formatName = '' else : formatName = name_list [ i ] . replace ( ' ' , '_' ) plt . savefig ( os . path . join ( opath , oprefix + '{0}.svg' . format ( formatName ))) plt . close ( plt . gcf ()) if onlyFirst : return else : # show images data_list = data_list [ datakeys [ 0 ]] NG = len ( data_list ) for i in range ( NG ): plt . imshow ( data_list [ i ], interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( title + prefix . format ( converter ( name_list [ i ]))) if xlabel is not None : plt . xlabel ( xlabel ) if ylabel is not None : plt . ylabel ( ylabel ) plt . gcf () . set_size_inches ( 6 , 5 ) if onlyFirst : formatName = '' else : formatName = name_list [ i ] . replace ( ' ' , '_' ) plt . savefig ( os . path . join ( opath , oprefix + '{0}.png' . format ( formatName ))) plt . close ( plt . gcf ()) if onlyFirst : return 测试代码 1 2 3 4 5 6 7 8 9 10 11 12 13 def saveAllResults (): saveResults ( './test/algorithm' , './record/algorithm' , 'alg-A-' , [ 'A' ], title = 'A' , prefix = '' , onlyFirst = True ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-yt-' , [ 'test_y' ], title = 'True values' , prefix = '' , onlyFirst = True ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-y-' , [ 'pred_y' ], title = 'Predicted values' ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-W-' , [ 'W' ], title = 'W' ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-cb-' , [ 'c' , 'b' ], title = 'Biases' , plot = True ) saveResults ( './test/noise' , './record/noise' , 'noi-A-' , [ 'A' ], title = 'A' , prefix = '' , onlyFirst = True ) saveResults ( './test/noise' , './record/noise' , 'noi-yt-' , [ 'test_y' ], title = 'True values' , prefix = ' (ε=N(0,{0}))' , converter = int ) saveResults ( './test/noise' , './record/noise' , 'noi-y-' , [ 'pred_y' ], title = 'Predicted values' , prefix = ' (ε=N(0,{0}))' , converter = int ) saveResults ( './test/noise' , './record/noise' , 'noi-W-' , [ 'W' ], title = 'W' , prefix = ' (ε=N(0,{0}))' , converter = int ) saveResults ( './test/noise' , './record/noise' , 'noi-cb-' , [ 'c' , 'b' ], title = 'Biases' , plot = True , prefix = ' (ε=N(0,{0}))' , converter = int ) saveAllResults () 首先考虑不同优化器的测试，在这些测试里，我们确保 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 对所有测试相同，且产生的随机数据真值 \\mathbf{y}_{\\mathrm{true}} \\mathbf{y}_{\\mathrm{true}} 对所有测试也相同，亦即： \\mathbf{A} \\mathbf{A} \\mathbf{y} \\mathbf{y} 的真实值 于是我们可得到所有的数据 优化器 \\mathbf{y} \\mathbf{y} 的预测值 \\mathbf{W} \\mathbf{W} \\mathbf{b} \\mathbf{b} 与 \\mathbf{c} \\mathbf{c} Adadelta Adagrad Adam Adamax AMSgrad Nesterov Adam Nesterov Moment RMSprop SGD 接下来考虑不同噪声的测试，在这些测试里，我们确保 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 对所有测试相同，但由于噪声大小的不同，随机数据真值 \\mathbf{y}_{\\mathrm{true}} \\mathbf{y}_{\\mathrm{true}} 会有所偏差： \\mathbf{A} \\mathbf{A} 于是我们可得到所有的数据 \\boldsymbol{\\varepsilon} \\sim N(0,~?) \\boldsymbol{\\varepsilon} \\sim N(0,~?) \\mathbf{y} \\mathbf{y} 的真实值 \\mathbf{y} \\mathbf{y} 的预测值 \\mathbf{W} \\mathbf{W} \\mathbf{b} \\mathbf{b} 与 \\mathbf{c} \\mathbf{c} 0 1 5 10 50 100 我们最为看重的，其实是是否拟合出 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 。一系列实验表明， \\mathbf{W} \\mathbf{W} 和 \\mathbf{b} \\mathbf{b} 的拟合效果甚好。由于我们建立的仿射变换模型和原始仿射变换模型有着完全一致的结构，优化结果反映这一问题的解相当准确。至此，我们已经掌握了一个完整的Project应当具有的模块结构，以及对不同的优化器有了理论和实际的体验。在后续的章节里，除非有特别的应用，我们不再探讨不同的优化器对结果的影响，在绝大多数情况下，我们都将使用AMSgrad。","text_tokens":["parsedata","extension","8","式子","our","50","视","合适","一个","详细","arg","库","模拟","一系列","存在","their","exit","showcurves","小时","ypred","方式","青睐","后面","root","第一个","熟悉","/","low","通过","得到","时候","5","matmul","生成器","导出","验证","可变","空列","backend","1000","把","check","频繁","linalg","curves","step","pass","测试代码","corr","附图","const","连接起来","store","mathbb","段","l11","详情","受到","...","再","多","卓有","8883","提前","写","matplotlib","共轭","一般来说","返回","importing","最后","¶","固定","snr","方法","衰减","else","8130","除了","为此","markevery","依然","即可","7","选择","并","看重","batch","fae6a9","向量","看","正是","不如","ml","equation","时能","trainbatchnum","当于","importflag","full","给","不","matrix","schedule","建立","后来者","因素","模型","most","5ms","各层","support","灵活运用","真","9701","元素","svg","只取","predicted","14","markers","以","designed","1","svd","涉及","过大","来自","区间","检查","相关系数","全局","专题","关注","告诉","相乘","反之","可","所","v","角","方面","预测","即使如此","self","扰动","范围","9834","l3","-","接下","savez","linclshandle","6994","|","parser","上升","但","生成","故此","33","法在","mapsto","9700","和","br","（","5597","字符","陷入","amsgrad","读写","1278","感到","deep","接起","单值","2c","读取","中文","一旦","未来","name","unsupported","旧版","累计","先后","依赖","大地","独立","之一","性能","文献","方程组","0.5","加","学习","l4","二元","形式","testdataregset","动量","从而","看成","os","干扰","对角","启动","init","aspect","'","y","往往","set","秩","use","统计学","少量","等","staticmethod","networks","双份","l","讨论","浮点","a","testdataset","入门","首先","略去","next","analyzing","帮助","where","44","扩展","保持","先后顺序","解","大多数","ytrue","经验","13","values","no","beyond","network","包括","evaluated","iterative","我们","了解","只","advnetworkbase","开启","个","\\","线路","showbars","计算出来","相关","这里","它","皮尔森","水土","融合","replace","拟合","相仿","43","原理","总是","losses","1.8380598782932136","本原","工程","一层","评价","无关痛痒","收敛","数据分布","raw","方可","梯度","快速","高斯","apis","对象","上式","9.736224609164252","出来","available","可调","工作","认为","convergence","if","前","开关","变量","4000","反例","except","法","单靠","集成","大小","initializer","对","nmoment","类似","罢了","ng","引入","2","45","指派","optimization","根据","定义数据","目标","去","概念","文件夹","9","远不如","正值","能","argparse","相对","甚或","处","4556","类","绘制","激活","后记","为止","除外","users","方差","得慢","种子","tools","该类","input","设计","各个","称","启发","本例","module","^","损失","by","小量","关闭","70","其实","准确","控制","0431","直接","方","系数","哪些","biases","一次","做","并且","key","squared","怎么","2ms","调整","o","真是",">","构造","轻松","9836","最","相同","8547","通常","继续","是","tool","that","trained","cdots","%","这样","点数"," ","费用","改进","tests","迅速","适当","相似","width","列表","problem","就行了","平均值","1477","残留","rightarrow","approximation","写出","比率","为本","假设","9897396","几种","标准差","程序","varepsilon","大漏","初步","文章","一系","称为","关乎","信号","次数","training","指代","c",",","sum","以下","reg","本次","print","steps","~","开发","得","22","寻找","不同","测试方法","专门","adamax","应该","records","比较","送入","代码","解不","子类","tensoflow","自然","true","5899","之后","group","113","88829040527344","同一","后续","term","scandir","对于","希望","random","参数设置","9832","overline","除非","xlabel","output","get","本","子书","版本","绝大多数","能够","出原","求解","多值","将","；","fit","settings","8705","树状","完整","见到","率","多线程","保证","改变","gpu","activation","于是","e","指定","非常","adaptive","提及","要求","stochastic","开始","extending","一步","to","idea","子","import","train","dfrac","然后","再进一步","带动","读者","非线性","推导","跟踪","10","main","很多","ed1","大量","formatter","一些","网络层","epilog","小小","是因为","完全一致","，","被","值为","generate","predict","拥有","suggest","inputs","部分","l32","参考","参照","不太","预期","proportion","回归","也","这些","15669","完全相同","又","estimation","教材","一言难尽","this","ncol","8801","函数","资料","章节","不妨","0838","全部","limits","风格","结合","用来","的","j","需","写为","100","手","4","分离","错误","normal","进行","所有","或缺","多线","下降","入门者","写作","test","突兀","直线","入手","大化","所以","严格","1930","另一方面","学生","笔者","保存","tensorflow","gcf","自己","左右","但是","elif","opmizername","layers","12","kernel","后来","参数","更","批量","28","bmatrix","有效","9354","哪","保留","fixed","object","aligned","拟出","大时","graphs","构成","仿射变换","prefix","有所","使得","minimization","推翻","description","weights","不足以","st2","前提","接受","会论","42","有解","失误","展开","大多","下图","源码","需要","限度","0s","随机数","下界","阶梯","说","附近","lr","详情请","最大","s","更新","found","赘述","converter","as","align","is","网络","分解","位置","这四","assert","mb","begin","args","秩为","找到","nearest","影响","字符串","预处理","模块化","这个","在线","水土不服","强于","adgrad","适应","error","compile","字","话题","求","float","为什么","难以","先","一样","情况","21","float32","file","执行","label","pre","越过","initialization","了","体验","项目","无论","其他","误差","dense","初始","甚巨","偏大","不到","仍然","整数","theta","一方面","graph","+","argument","type","偏差","load","]","放在","不必","看看","tight","真正","99","直观","算出","跟","常用","行","真值","查看","仿射","理想","当场","in","ed3","共同","正常","fill","快","u","linreghandle","实验","square","变换","模拟出","分析","修正","事实上","序列","lin","9831","给出","这是","其后","不会","不仅","一节","上面","similarity","）","较","append","文字","度","3472","神经网络","然而","6048","关键","原则","epoch","应当","看作","一个多","rate","方根","project","教程","learningrate","开来","options","陡峭","different","测度","轨迹","极端","随着","不可","convex","优势","ext","已知","25","regressed","26","reproductable","集","好处","?","rmse","boldsymbol","意外","and","发现","连接","colorbar","adadeltada","实现","作为","常数","旧","复杂","接触","调试","not","<","yt","就是","draw","x","量","层","阶段","目前","base","想象","基本",")","samples","设","基础","construction","gen","iter","同一个","内","如下","&","退化","由该","特别","具有","history","其","两种","随机性","method","主","要么","不是","should","指数","率过","之前","正","不得不","即","自","统计","arguments","每次","默认值","数目","一片","一般","曲率","遵照","选取","调用","虽然","34","optimizername","9813","过多时","顾名","指出","cb","35","p","计学","9833","相反","2ac","不足","it","会","复用","简单","抑制","浮点数","既会","of","ylim","支持","neural","点积","rho","细微","sample","alpha","引用","class","record","容易","6636","没有","范例","np","cosine","3","相当","_","解决","sigma","demo","d","属于","diag","一类","条件","写成","时","3.216810970685858","metavar","特定","曲线","白","以上","数据处理","那些","每个","好","还","randomnormal","提升","覆盖","小写","常见","恢复","l31","原始","9838","取得","输入","电子书","oprefix","线性相关","出","pearson","制定","均方","系列","dict","这方","未免","且","此次","do","例如","save","12345","罗列","1.0","同样","掌握","mathbf","神经网","leftarrow","一致","可能","处理器","器","hat","on","均","期望","线程","一次函数","依赖性","越","式","与","18","构造方法","明显","数值","打算","dagger","迄今","外围","速度","实用","作用","same","py","远远","按","率为","n","或许","generator","str","成器","标准","首尾相接","有趣","改为","定义","叫","0.01","集中","残差","第一","内容","0.0","神经","过","个点","algorithm","故而","关系","采用","regression","格式化","抽取","当前","0.001","0.9348520972791058","操作","查考","define","迄今为止","saveresults","h","难看","特征值","文件","表示","合理","16","参考文献","nadam","只是","rmsprop","ylabel","processing","某","自定","而言","30","stroke","initializers","outputdata","40","有时","联系","legend","另","开头","推广","可以","带","一致性","按照","当上","完全","最快","一行","6.8600432267325955","分类","l22","size","样本","2465","images","尝试","取","性极","局部","小","考虑","实际","优化","基于","型值","调整结构","另一方","dtype","两组","处理","help","signal","semilogy","确保","uniform","开辟","layer","下来","变性","此","ta","器有","close","argumentparser","模块",";","传播","每","两次","无论是","meansquarederror",":","结论","特征","同时","mse","g","option","唯一","电子","可资利用","布尔","google","率设","为","cost","均值","避免","思路","遵从","linear","adam","运算","参阅","—","tilde","trick","规范","raise","mathrm","optional","毫无","需求","17","顺次","└","数量","ed2","1e","pred","evaluate","就是说","解之","结果","基本原理","nabla","dataset","不宜","格式","correlation","整个","we","过于","率均","api","率以","解时","额外","r","epochs","27","多数","flag","重新","@","输出","方程","平缓","后者","多组","完成","38","写文","这","下","例子","optimizer","上文","sqrt","len","相差","parse","特点","以及","程序运行","rank","不过","大小写","相当于","要","较大","cls","20","29084","metric","跨层","花招","而定","过去","到","metrics","0.8","前述","超过","online","即将","the","nesterov","小小的","for","inches","进度","。","典型","4.677152938185369","求得","在","大致","令人","frac","可变性","optimizers","讲","algorithms","moment","给定","技巧","特殊","实际上","真的","mean","python","转换","都","进一步","推荐","tf","一段","像","根本","data","成效","名称","yes","模式","防止","满足","提示","是否","馈入","saveallresults","m","、","最低","cdot","地方","left","：","获取","重要","必定","秩是","momentum","“","31","收敛性","8600","估计","痛痒","适用","普遍","近似","=","件夹","plot","区域","里","epsilon","策略","partial","性相","类型","codes","23","一点","imshow","改动","36","强时","主要","数据字","不难看出","1234","46","地","表述","丢弃","有时候","相接","model","遗忘","负","formatname","等于","二值","才","从根本上","人","线性变换","很","大于","增加","适合","训练","argumenttypeerror","merged","lowrank","让","argumentdefaultshelpformatter","卓有成效","高级","可见","对应","b","advanced","title","后处理","知道","年","兴趣","而","l12","有着","？","noise","#","matrices","反而","同","效果","29","layout","则","8145","过高","sd","sze","后期","自定义","任何","低","none","testing","建议","就","并发","准确度","水准","排列","测试数据","ratio","重申","几乎","166","(","弊端","1983","取出","如果","机","依靠","特例","casefold","表明","那么","叙述","command","具体情况","提到","利用","减小","噪声","官方","auto","最大化","config","呢","marker","相若","运用","不服","6","有关","少则","两个","或","loss","亦","符合","还是","之间","当然","未免太","ba9132","list","已经","选用","同理","值过","最优","证明","dparser","陌生","它们","相应","究竟","快乐","outputs","压力","500","次","为何","…","成","允许","反向","gradient","更进一步","15","将要","迭代","无法","*","分别","一方","ldots2","惯性","逆","显然","做过","反映","st1","commands","推断出","功能","unconstrained","参考资料","灵活","range","传统","解来","result","24","值得注意","入","如此","注意","区分","│","bool","思想","per","compressed","methods","{","标量","message","41","随机噪声","relation","0","不为","不可或缺","看出","19","算法","path","为了","有人","换言之","learning","字段","路径","32","尤其","dp","理解","一对","高度","至少","lvert","w","形如","用户","成上","不加","onlyfirst","不好","理论","int","符号","真实","分类器","rms","sub","gb","}","testbatchnum","原生","使","f","splitext","用","论","多个","799","矩阵","显示","sim","数据文件","解析","机数","11","应用","savefig","配置","其一","探讨","编写","造成","以便","逐渐","dense1","节","关键字","这种","1s","10.0","慢","短接","无关","stddev","return","记","尽","请参阅","问题","默认","经过","原因","备受","t","充当","amin","numpy","大","从","重要性","绝大","查阅","def","机器","因为","with","shape","336","value","很大","7134","测试阶段","导数","iterator","其中","事实","单","seed","一种","setseed","所示","单独","adagrad","引起","由于","种","做法","post","join","adadelta","right","若","好后","不难","完完全全","上述","false","改正","使用","常","过程","opath","modify","器为","并非","至此","列","中能","黑色","后","不再","以后","三个","parameters","多则","因而","显明","cookbook","如何","不得","接下来","提出","line","展示","推断","__","speically","个别","add","封装","102","来说","37","可是","甚","论文","一定","令","强","添加","plt","假","文档","l21","只要","摘要","6665","算是","将近","不怎么","encountered","rvert","一下","换行","[","stystart","继","降到","而是","其二","对角线","usage","即使","起来","axis","第一次","该","from","具体","修改","有成","or","展开讨论","余弦","深度","适用性","测量","mergedlabel","各自","产生","note","增大","可知","平均","从式","顾名思义","alg","操作符","次序","这一","二阶","什么","9897","数十","决定","首尾","描述","39","仍","均会","str2bool","classdef","障碍","当","空行","点","一言","下面","提供","end","9579","nargs","94","mini","subgradient","自行","用作","1856","times","用法","有些","format","构建","overview","或者","尽可能","随机","decay","对比","预测值","这方面","就要","列举","construct","6678","因此","一组","show","steppe","代替","水平","”","自动","来","也就是说","不能","新","方阵","集来","mathcal","介绍","看到","只能","descent","求导","9835","noi","值得","设由","必须","由","上","default","连","没","min","生效","7719","关心","运行","出现","0.4","该字","结构","bar","本身","results","理器","0.6",".","目的","datakeys","分布","设置","远远不如","继上","便于","一","基本上","2459","收集","信噪比","尽可","constant","测试","复现","现在","png","诸多","值","├","i","线性","odot","─","k","将会","尽管","数据","去掉","sgd","特地","普通","选项","folder","中","bias","最为","带来","输","任意","计算","说明","ldots1","只有","could","顺序","keras","自然而然","继承","101","fileexistserror","yp","gca","初始化","有","红色","each","本节","程度","keys","相比","ε","关于","interpolation","领域","来者","导致","查","团队","delta","求取","足以"],"title":"线性回归","title_tokens":["线性","回归"]},{"location":"book-1-x/chapter-1/linear-regression/#_1","text":"摘要 本节介绍如何使用类模型(Model)来编写一个线性回归器，以拟合出一个线性模型。本节将第一次介绍如何编写一个带用户参数(选项)的Project，并且允许用户选择不同的优化器、对比验证不同优化算法的效果。","text_tokens":["器","拟合","验证","。","选项","、","一个","效果","优化","，","带","对比","不同","介绍","类","第一次","出","用户","本"," ","如何","编写","project","摘要","以","选择","一次","节","model","并且","的","本节","将","算法","回归","使用","(","第一","线性","来",")","参数","模型","允许"],"title":"线性回归","title_tokens":["线性","回归"]},{"location":"book-1-x/chapter-1/linear-regression/#_2","text":"","text_tokens":[],"title":"理论","title_tokens":["理论"]},{"location":"book-1-x/chapter-1/linear-regression/#_3","text":"设存在一个多输出方程 \\mathbf{y} = \\mathcal{F}(x) \\mathbf{y} = \\mathcal{F}(x) ，当然 \\mathcal{F} \\mathcal{F} 可以是非线性函数，那么我们可以考虑使用一个带可调参数的模型 \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}) \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}) 来模拟它，其中 \\boldsymbol{\\Theta} \\boldsymbol{\\Theta} 是可调的参数。于是，该问题可以被表述为 \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\boldsymbol{\\Theta}} &\\sum_{k=1}^N \\mathcal{L} \\left( y_k,~ \\mathbf{D}_{\\boldsymbol{\\Theta}}(\\mathbf{x}_k) \\right),\\\\ \\mathrm{s.t.}~&y_k = \\mathcal{F}(x_k). \\end{aligned} \\end{equation} 在我们不知道 \\mathcal{F} \\mathcal{F} 的情况下，我们的目的是使用大量的 x_k,~y_k x_k,~y_k 样本，来调整出一个最优的近似模型 \\mathbf{D}_{\\boldsymbol{\\Theta}} \\mathbf{D}_{\\boldsymbol{\\Theta}} 。由于 \\mathcal{F} \\mathcal{F} 是非线性的，这要求我们的 \\mathbf{D}_{\\boldsymbol{\\Theta}} \\mathbf{D}_{\\boldsymbol{\\Theta}} 也可以是非线性的。实际情况下，这样的问题往往不容易求解，尤其是信号的非线性性极强时，该问题很容易陷入局部最优解，从而对求得一个可接受的解造成很大的障碍。 这里 \\mathcal{L} \\mathcal{L} 是损失函数。在回归问题中，很多情况下我们都只能选择 均方误差 (Mean squared error, MSE) 作为损失函数，这是因为回归问题的目的是模拟出一组信号来，而这些信号的分布范围可能是任意的。在一些特别的应用里，例如，如果我们的信号全部为正值，那么我们可以考虑使用 信噪比 (Signal-to-noise ratio, SNR) 来作为我们的损失函数。","text_tokens":["于是","尤其","left","是","&","输出","方程","一个","特别","aligned","要求","arg","可以","带","拟出","模拟","出","这","均方","存在","范围","这样"," ","下","to","近似","-","=","一组","里","接受","例如","样本","非线性","}","mathbf","性极","很多","来","局部","f","大量","可能","强时","考虑","一些","当然","实际","模拟出","陷入","是因为","，","被","mathcal","可调","只能","应用","信号","表述","最优","signal","造成","对",",","sum","min","s","回归","也","这些","n","mse","很","从而","正值","问题",".","~","目的","。","分布","t","snr","函数","求得","在","该","一个多","begin","为","y","因为","往往","容易","limits","选择","全部","信噪比","很大","的","error","l","其中","mathrm","equation","_","知道","线性","不","d","k","boldsymbol","而","模型","^","损失","noise","情况","mean","中","由于","作为","解","都","任意","{","那么","right","squared","误差","1","障碍","求解","x","我们","调整","ratio","使用","theta","end","\\","(","如果",")","参数","设","可","这里","它"],"title":"一般回归问题","title_tokens":["问题","回归","一般"]},{"location":"book-1-x/chapter-1/linear-regression/#_4","text":"继上一节的学习，我们知道如何解一个定义为 \\begin{align} \\mathbf{y} = \\left\\{ \\begin{bmatrix}\\hat{y}_1 > 0 & \\hat{y}_2 > 0 & \\cdots & \\hat{y}_n > 0\\end{bmatrix}^T, ~ \\left| ~ \\hat{\\mathbf{y}} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon} \\right. \\right\\}, \\end{align} 的分类模型。在本节，让我们考虑一个更简单的模型： \\begin{align} \\mathbf{y} = \\mathbf{A} \\mathbf{x} + \\mathbf{c} + \\boldsymbol{\\varepsilon}. \\end{align} 现在， \\mathbf{y} \\mathbf{y} 是关乎 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 的一个仿射函数，并且我们仍然保留噪声函数 \\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon} 。由于这是一个线性模型，我们可以想象到，存在一个线性回归器， \\mathbf{W},~\\mathbf{b} \\mathbf{W},~\\mathbf{b} ，使得预测结果为 \\begin{align} \\tilde{\\mathbf{y}} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}. \\end{align} 类似上一节，假设我们的数据集 (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} (\\mathbf{x},~\\mathbf{y}) \\in \\mathbb{D} 符合 (3) (3) 定义的数据分布特征。我们的基本要求是，在我们不知道 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 的情况下，使用大量 (\\mathbf{x}_k,~\\mathbf{y}_k) \\in \\mathbb{D} (\\mathbf{x}_k,~\\mathbf{y}_k) \\in \\mathbb{D} 样本训练一个线性分类器，使得当我们给定任意一个新样本 \\mathbf{x} \\mathbf{x} 的时候，分类器能推断出其对应的 \\mathbf{y} \\mathbf{y} 。 故而，该问题可以描述为 \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\mathbf{W},~\\mathbf{b}} &\\sum_{k=1}^N \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathbf{W} \\mathbf{x}_k + \\mathbf{b} \\right), \\\\ \\mathcal{L} \\left( \\mathbf{y},~ \\tilde{\\mathbf{y}} \\right) &= \\lVert \\mathbf{y} - \\tilde{\\mathbf{y}} \\rVert_2^2. \\end{aligned} \\end{equation} 在本例中， \\mathbf{y} \\mathbf{y} 有正有负，因此我们使用均方误差来作为损失函数。","text_tokens":["left","：","是","&","保留","预测","一个","要求","aligned","arg","可以","噪声","lvert","cdots","其","均方","w","存在"," ","下","如何","-","=","分类","使得","分类器","因此","推断","|","仿射","正","数据分布","样本","in","}","mathbf","符合","时候","来","假设","大量","新","考虑","和","器","hat","varepsilon","，","到","mathcal","关乎","这是","c","上","负","类似","一节",",","sum","mathbb","min","rvert","回归","2","学习","align","特征","n","简单","能","问题",".","~","。","分布","t","函数","继上","定义","训练","在","该","begin","为","让","y","limits","的","现在","对应","b","tilde","3","l","equation","_","知道","推断出","线性","不","给定","集","d","k","a","boldsymbol","数据","模型","本例","^","损失","故而","情况","中","由于","作为","解","任意","描述","{","结果","right","并且","0","有","误差","1","x","本节","我们","当","仍然","使用","end","\\",">","想象","(","基本","+","更",")","bmatrix"],"title":"线性回归","title_tokens":["线性","回归"]},{"location":"book-1-x/chapter-1/linear-regression/#_5","text":"作为线性问题，该问题实际上可以写出其解析解。未免读者感到过于突兀，我们先从一个简单的问题开始入手： 例子：一次函数的线性回归 如果我们的矩阵 \\mathbf{A} \\mathbf{A} 退化为标量 a a ，向量 \\mathbf{c} \\mathbf{c} 退化为标量c，那么 (3) (3) 可以重新写为： \\begin{align} y = a x + c + \\varepsilon. \\end{align} 考虑我们拥有N个样本点 (x_k,~y_k) (x_k,~y_k) ，上述问题实际上可以求得解析解。设由这N个点构成了样本向量 \\mathbf{x}_d,~\\mathbf{y}_d \\mathbf{x}_d,~\\mathbf{y}_d (注意与前述的向量区分开来)，则问题可以写成 \\begin{align} \\arg \\min_\\limits{a,~c} \\lVert \\mathbf{y}_d - a \\mathbf{x}_d - c \\mathbf{1} \\rVert^2_2. \\end{align} 这就是附图所示的，拟合到直线的一次函数回归问题。将该损失函数展开，有 \\begin{equation} \\begin{aligned} \\mathcal{L}(a,~c) &= ( \\mathbf{y}_d - a \\mathbf{x}_d - c \\mathbf{1} )^T ( \\mathbf{y}_d - a \\mathbf{x}_d - c \\mathbf{1} )\\\\ &= \\mathbf{y}_d^T\\mathbf{y}_d + a^2 \\mathbf{x}_d^T \\mathbf{x}_d + c^2 + 2ac \\mathbf{1}^T \\mathbf{x}_d - 2 a \\mathbf{y}_d^T \\mathbf{x}_d - 2c \\mathbf{1}^T \\mathbf{y}_d. \\end{aligned} \\end{equation} 令 \\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial a}=0,~\\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial c}=0 \\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial a}=0,~\\dfrac{\\partial \\mathcal{L}(a,~c)}{\\partial c}=0 ，则我们得到一组二元一次方程组 \\begin{equation} \\left\\{ \\begin{aligned} a \\mathbf{x}_d^T \\mathbf{x}_d + c \\mathbf{1}^T \\mathbf{x}_d &= \\mathbf{y}_d^T \\mathbf{x}_d. \\\\ c + a \\mathbf{1}^T \\mathbf{x}_d &= \\mathbf{1}^T \\mathbf{y}_d. \\end{aligned} \\right. \\end{equation} 解之，得 \\begin{equation} \\left\\{ \\begin{aligned} a &= \\frac{ \\mathbf{y}_d^T \\mathbf{x}_d - ( \\mathbf{1}^T \\mathbf{y}_d ) ( \\mathbf{1}^T \\mathbf{x}_d ) }{ \\mathbf{x}_d^T \\mathbf{x}_d - (\\mathbf{1}^T \\mathbf{x}_d)^2 } = \\frac{ \\sum_k x_k y_k - \\sum_k y_k \\sum_k x_k }{ \\sum_k (x_k)^2 - \\left(\\sum_k x_k\\right)^2 }. \\\\ c &= \\mathbf{1}^T \\mathbf{y}_d - a ( \\mathbf{1}^T \\mathbf{x}_d ) = \\sum_k y_k - a \\left( \\sum_k x_k \\right). \\end{aligned} \\right. \\end{equation} 这个式子在诸多教材上都会出现，作为学生解回归问题的入门话题。可见，我们在本节讨论的问题并不是一个陌生的问题，相反，我们过去非常熟悉的一个问题，是这个问题的退化到标量下的特殊情况。另，计算该问题的相关系数，我们常使用 \\begin{align} \\rho = \\frac{ \\sum_k \\left(x_k - \\overline{x}\\right) \\left(y_k - \\overline{y}\\right) }{ \\sqrt{ \\sum_k \\left(x_k - \\overline{x}\\right)^2 \\sum_k \\left(y_k - \\overline{y}\\right)^2 } }, \\end{align} 其中 \\overline{x} = \\sum_k x_k ,~ \\overline{y} = \\sum_k y_k \\overline{x} = \\sum_k x_k ,~ \\overline{y} = \\sum_k y_k 。 有了解上述例子的基础，我们自然可以写出， \\begin{equation} \\begin{aligned} \\mathcal{L}(\\mathbf{A},~\\mathbf{c}) &= \\sum_k ( \\mathbf{y}_k - \\mathbf{A} \\mathbf{x}_k - \\mathbf{c} )^T ( \\mathbf{y}_k - \\mathbf{A} \\mathbf{x}_k - \\mathbf{c} )\\\\ &= \\sum_k \\left[ \\mathbf{y}_k^T\\mathbf{y}_k + \\mathbf{x}_k^T \\mathbf{A}^T\\mathbf{A} \\mathbf{x}_k + \\mathbf{c}^T \\mathbf{c} + 2 \\mathbf{c}^T \\mathbf{A} \\mathbf{x}_k - 2 \\mathbf{y}_k^T \\mathbf{A} \\mathbf{x}_k - 2 \\mathbf{y}_k^T \\mathbf{c} \\right]. \\end{aligned} \\end{equation} 提示 接下来的求导主要涉及单值对矩阵求导（导数仍是矩阵），单值对向量求导（导数仍是向量）。可以参考 The Matrix Cookbook 查到对应情况下的求导结果。 同理，令 \\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{A}}=0,~\\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{c}}=0 \\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{A}}=0,~\\dfrac{\\partial \\mathcal{L}(\\mathbf{A},~\\mathbf{c})}{\\partial \\mathbf{c}}=0 ，则我们得到一组二元一次方程组 \\begin{equation} \\left\\{ \\begin{aligned} \\sum_k \\left[ \\mathbf{A} \\mathbf{x}_k \\mathbf{x}_k^T + \\mathbf{c} \\mathbf{x}_k^T \\right] &= \\sum_k \\left[ \\mathbf{y}_k \\mathbf{x}_k^T \\right]. \\\\ \\sum_k \\left[ \\mathbf{c} + \\mathbf{A} \\mathbf{x}_k \\right] &= \\sum_k \\left[ \\mathbf{y}_k \\right]. \\end{aligned} \\right. \\end{equation} 解之，得 \\begin{equation} \\left\\{ \\begin{aligned} \\mathbf{A} &= \\left[ N \\sum_k \\left[ \\mathbf{y}_k \\mathbf{x}_k^T \\right] - \\sum_k \\left[ \\mathbf{y}_k \\right] \\sum_k \\left[ \\mathbf{x}_k^T \\right] \\right] \\left[ N \\sum_k \\left[ \\mathbf{x}_k \\mathbf{x}_k^T \\right] - \\sum_k \\left[ \\mathbf{x}_k \\right] \\sum_k \\left[ \\mathbf{x}_k^T \\right] \\right]^{-1} \\\\ \\mathbf{c} &= \\frac{1}{N} \\sum_k \\left[ \\mathbf{y}_k - \\mathbf{A} \\mathbf{x}_k \\right]. \\end{aligned} \\right. \\end{equation} 可见，当上式中的逆不存在时（即低秩的情况），该方程还是有可能解不唯一。 同时，相关系数的计算可以表示为 \\begin{align} \\rho = \\mathrm{mean} \\left[ \\frac{ \\sum_k \\left(\\mathbf{x}_k - \\overline{\\mathbf{x}}\\right) \\cdot \\left(\\mathbf{y}_k - \\overline{\\mathbf{y}}\\right) }{ \\sqrt{ \\sum_k \\left[ \\left(\\mathbf{x}_k - \\overline{\\mathbf{x}}\\right) \\cdot \\left(\\mathbf{x}_k - \\overline{\\mathbf{x}}\\right) \\right] \\sum_k \\left[ \\left(\\mathbf{y}_k - \\overline{\\mathbf{y}}\\right) \\cdot \\left(\\mathbf{y}_k - \\overline{\\mathbf{y}}\\right) \\right] } } \\right]. \\end{align} 这就是 皮尔森相关系数 (Pearson's correlation) 。其中 \\overline{\\mathbf{x}} = \\sum_k \\mathbf{x}_k ,~ \\overline{\\mathbf{y}} = \\sum_k \\mathbf{y}_k \\overline{\\mathbf{x}} = \\sum_k \\mathbf{x}_k ,~ \\overline{\\mathbf{y}} = \\sum_k \\mathbf{y}_k ， \\cdot \\cdot 表示的是两个向量按元素各自相乘。它是式 (11) (11) 在多变量问题上的推广。相当于对向量的每一个元素，分别从统计上求取皮尔森相关系数，然后对向量每个元素对应的皮尔森相关系数求取平均值。","text_tokens":["：","重新","拟合","left","式子","&","是","]","方程","退化","非常","另","一个","它","aligned","arg","可以","推广","lvert","其","开始","pearson","这","存在","当上"," ","例子","下","未免","cookbook","构成","-","sqrt","接下","=","不是","接下来","平均值","dfrac","然后","一组","熟悉","读者","样本","即","统计","展开","partial","写出","}","mathbf","得到","两个","相当于","可能","考虑","还是","过去","矩阵","主要","（","实际","varepsilon","，","到","解析","mathcal","11","感到","前述","令","单值","同理","变量","2c","求导","一次函数","拥有","附图","与","式","设由","c","上","对","下来","陌生",",","sum","参考","相反","min","the","s","）","2ac","rvert","方程组","回归","出现","多","2","align","每","二元","[","按","n","会","同时","简单","教材","唯一","问题",".","~","。","t","rho","函数","得","求得","在","从","该","begin","为","'","y","分别","frac","均值","limits","秩","开来","可见","并","解不","的","这个","自然","向量","写为","对应","逆","导数","诸多","各自","3","l","其中","mathrm","equation","相当","_","当于","讨论","线性","不","所示","d","k","话题","matrix","特殊","a","平均","实际上","入门","先","^","个点","损失","overline","情况","关系","mean","突兀","写成","作为","入手","直线","中","注意","时","解","区分","则","都","元素","{","计算","标量","解之","仍","结果","了","学生","right","就是","系数","低","一次","每个","0","有","1","correlation","表示","x","本节","上述","将","我们","涉及","过于","了解","点","相关系数","使用","end","个","常","\\","查","(","提示","如果","+","相乘",")","求取","相关","基础","皮尔森","那么","cdot"],"title":"求解问题","title_tokens":["求解","问题"]},{"location":"book-1-x/chapter-1/linear-regression/#_6","text":"接下来，我们要介绍几种最常见的优化算法。关于更多这方面的内容，可以查考Google团队编写的在线电子书 Deep Learning 。笔者打算在未来为此开辟专题写文，因此这里只是介绍几种常见的 一阶梯度下降 算法。传统优化领域里，单靠一阶梯度下降往往难以满足对准确度的需求，但深度学习(Deep learning)往往必须使用这些简单的一阶梯度下降算法，就连使用一阶梯度近似二阶梯度的算法 共轭梯度下降 ，在很多情况下都被认为是费用(cost)过高。这是由于一个深度网络，往往具有大量的参数需要训练，因此一个Model的参数少则数十MB，多则上GB。一阶梯度下降算法所需的计算量小，能确保我们一次迭代的过程能迅速完成，因而备受青睐。为了提升其性能，深度学习领域内也对其进行了诸多改进。 注意 其实，论到优化算法，往往不得不提到 反向传播 。不过实际上，一个Tensorflow的入门者，其实完全不需要学习如何推导反向传播的过程。下面我们的叙述也完全不会提及反向传播相关的内容。关于为何我们不需要了解反向传播，在下一节我们会论到。但是，在本教程后期，介绍高级技巧的时候，我们会详细展开。事实上，笔者认为，一个Tensorflow的用户，如果只是为了编写代码，反向传播与ta其实无关痛痒；但只有真正掌握反向传播，我们才算是真正入门了神经网络的理论。 我们在这里说到优化算法，是用在训练网络上的。事实上，只有几种个别的机器学习应用，需要我们在测试阶段执行 迭代算法 (iterative algorithm) 。一般来说，深度学习的训练过程可以被普遍地描述为：已知一个带可调参数 \\boldsymbol{\\Theta} \\boldsymbol{\\Theta} 的模型 \\mathcal{D}_{\\boldsymbol{\\Theta}} \\mathcal{D}_{\\boldsymbol{\\Theta}} ，已知一组数据集 (\\mathbf{x}_i,~\\mathbf{y}_i) \\in \\mathbb{D} (\\mathbf{x}_i,~\\mathbf{y}_i) \\in \\mathbb{D} ，则我们的训练目标为 \\begin{equation} \\begin{aligned} \\arg \\min_\\limits{\\boldsymbol{\\Theta}} \\mathbb{E}_{(\\mathbf{x}_i,~\\mathbf{y}_i) \\in \\mathbb{D}} \\left[ \\mathcal{L} \\left( \\mathbf{y}_i,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_i) \\right) \\right]. \\end{aligned} \\end{equation} 实际情况下，一般用均值估计来代替上式的期望函数。联系我们上一节的优化问题 (1) (1) 和本节的优化问题 (5) (5) ，都可以描述成上式的形式。也就是说，线性分类/回归器，是神经网络在解线性问题时的特例。","text_tokens":["叙述","e","：","left","方面","是","内","]","联系","一个","完成","提及","详细","aligned","提到","可以","arg","带","具有","写文","电子书","其","估计","这方","多则","因而"," ","下","完全","费用","改进","用户","痛痒","这方面","近似","如何","接下","迅速","不得","真正","接下来","青睐","理论","分类","无关痛痒","因此","一组","里","会论","不得不","代替","推导","/","in","gb","不过","个别","少则","但","要","展开","掌握","梯度","}","mathbf","很多","时候","来说","神经网","来","小","大量","5","也就是说","几种","用","论","需要","和","器","一般","实际","优化","，","被","上式","到","期望","mathcal","可调","普遍","介绍","认为","应用","deep","地","事实上","成上","编写","确保","阶梯","开辟","式","与","model","未来","必须","这是","单靠","上","不会","说","对","算是","连","下来","一节",",","mathbb","打算","min","ta","性能","无关","才","回归","也","多","传播","学习","这些","为何","度","[","共轭","神经网络","形式","会","一般来说","简单","目标","网络","反向","能","问题","电子",".","~","。","备受","函数","google","训练","在","mb","一","迭代","为","begin","为此","y","机器","cost","均值","往往","教程","高级","limits","代码","的","在线","测试","深度","需","测试阶段","诸多","已知","l","equation","事实","需求","进行","_","i","内容","不","线性","d","技巧","神经","集","实际上","boldsymbol","数据","入门","模型","难以","传统","下降","algorithm","情况","入门者","二阶","数十","由于","时","特例","其实","注意","则","解","准确","都","过高","描述","查考","就是说","后期","计算","执行","{","本","只有","了","right","就是","笔者","子书","就","一次","tensorflow","准确度","iterative","提升","1","x","本节","量","但是","我们","只是","算法","了解","；","阶段","为了","关于","下面","使用","theta","end","领域","\\","专题","常见","满足","(","过程","团队","learning","如果","更",")","参数","最","所","相关","这里"],"title":"优化算法","title_tokens":["优化","算法"]},{"location":"book-1-x/chapter-1/linear-regression/#_7","text":"接下来，让我们看看第一个算法， 随机梯度下降 (stochastic gradient descent, SGD) 。 随机梯度下降 记学习率为 \\epsilon \\epsilon ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\mathbf{g} 。 注意学习率一般需要设为一个较小的值，视情况而定。 由于梯度的期望满足 \\begin{equation} \\begin{aligned} \\mathbb{E} \\left[ \\mathbf{g} \\right] &= \\frac{1}{m} \\sum\\limits_{k=1}^m \\mathbb{E} \\left[ \\nabla_{\\boldsymbol{\\Theta}} \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\right] \\\\ &= \\mathbb{E} \\left[ \\nabla_{\\boldsymbol{\\Theta}} \\mathcal{L} \\left( \\mathbf{y},~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}) \\right) \\right] = \\nabla_{\\boldsymbol{\\Theta}} \\mathbb{E} \\left[ \\mathcal{L} \\left( \\mathbf{y},~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}) \\right) \\right]. \\end{aligned} \\end{equation} 可知随机抽取m个样本计算的梯度，在统计学上的期望等于全局梯度的期望。因此，这是一个有效的算法。 随机梯度下降存在明显的弊端，就是在收敛到（全局或局部）最优解的前提下，全局梯度为0，但通过随机选取batch得到的梯度（一般）可能不为0；并且，迭代受到个别极端样本梯度的影响较大，因此，我们有了第一个改进，即 带动量的随机梯度下降 (SGD with momentum) 。 带动量的随机梯度下降 参考文献 提出该算法的文章，可以在这里参考： On the momentum term in gradient descent learning algorithms. Neural Networks 记学习率为 \\epsilon \\epsilon ，惯性常数为 \\alpha \\alpha ，初始化动量 \\mathbf{v}=\\mathbf{v}_0 \\mathbf{v}=\\mathbf{v}_0 （不考虑继续训练的情况下 \\mathbf{v}_0 = \\mathbf{0} \\mathbf{v}_0 = \\mathbf{0} ），则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 计算带动量的更新值 \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} + \\mathbf{v} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} + \\mathbf{v} 。 显然，我们不难计算出， \\begin{equation} \\begin{aligned} \\mathbb{E} \\left[ \\mathbf{v} \\right] &= \\alpha \\mathbb{E} \\left[ \\mathbf{v} \\right] - \\epsilon \\mathbb{E} \\left[ \\mathbf{g} \\right]. \\\\ \\mathbb{E} \\left[ \\mathbf{v} \\right] &= - \\frac{\\epsilon}{1 - \\alpha} \\mathbb{E} \\left[ \\mathbf{g} \\right]. \\end{aligned} \\end{equation} 注意惯性通常需要设为 \\alpha \\in (0,~1) \\alpha \\in (0,~1) 。 这种改进的带来的好处是， 每次更新梯度时，上一次的梯度都会以指数衰减的形式残留在本次迭代中，从而确保新的梯度会被旧的梯度部分中和，避免极端梯度对更新参数影响过大； 当求解得到的梯度陷入局部最优时，如果该局部最优处的曲率较小，可以依靠动量的惯性，越过该局部最优解。 附图说明了使用这种算法的好处。黑色路径为SGD的更新轨迹，而红色路径为本算法的更新轨迹，可以看出随着迭代次数的增加，算法收敛的效果强于SGD。 有人从Nesterov在1983年的论文得到启发，提出了一个修正后的带动量随机梯度下降法，即 带Nesterov动量的随机梯度下降 (SGD with Nesterov momentum) 。 带Nesterov动量的随机梯度下降 参考文献 提出该算法的文章，可以在这里参考： A method for unconstrained convex minimization problem with the rate of convergence o\\left( \\frac{1}{k_2} \\right) o\\left( \\frac{1}{k_2} \\right) 记学习率为 \\epsilon \\epsilon ，惯性常数为 \\alpha \\alpha ，初始化动量 \\mathbf{v}=\\mathbf{v}_0 \\mathbf{v}=\\mathbf{v}_0 （不考虑继续训练的情况下 \\mathbf{v}_0 = \\mathbf{0} \\mathbf{v}_0 = \\mathbf{0} ），则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算惯性目标点的位置： \\boldsymbol{\\Theta}^{\\dagger} \\leftarrow \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} \\boldsymbol{\\Theta}^{\\dagger} \\leftarrow \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} ； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}^{\\dagger}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}^{\\dagger}} (\\mathbf{x}_k) \\right) ； 计算带动量的更新值 \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} \\mathbf{v} \\rightarrow \\alpha \\mathbf{v} - \\epsilon \\mathbf{g} ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta}^{\\dagger} - \\epsilon \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta}^{\\dagger} - \\epsilon \\mathbf{g} 。 其实，该方法的更新量期望与前一种方法一样， 显然，我们不难计算出， \\begin{equation} \\begin{aligned} \\mathbb{E} \\left[ \\mathbf{v} \\right] &= - \\frac{\\epsilon}{1 - \\alpha} \\mathbb{E} \\left[ \\mathbf{g} \\right]\\\\ &= \\frac{\\epsilon}{1 - \\alpha} \\nabla_{\\boldsymbol{\\Theta}} \\mathbb{E} \\left[ \\mathcal{L} \\left( \\mathbf{y},~ \\mathcal{D}_{\\boldsymbol{\\Theta} + \\alpha \\mathbf{v}} (\\mathbf{x}) \\right) \\right]. \\end{aligned} \\end{equation} 当收敛到最优解时， \\mathbf{v} \\rightarrow 0 \\mathbf{v} \\rightarrow 0 ，同时有 \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} \\rightarrow \\boldsymbol{\\Theta} \\boldsymbol{\\Theta} + \\alpha \\mathbf{v} \\rightarrow \\boldsymbol{\\Theta} 。我们在此不展开证明这个算法时能收敛的。但Nesterov的文献表明，它能将上面提到的带动量梯度下降算法的误差从 O\\left(\\frac{1}{K}\\right) O\\left(\\frac{1}{K}\\right) 下降到 O\\left(\\frac{1}{K^2}\\right) O\\left(\\frac{1}{K^2}\\right) 。其中 K K 为迭代次数。下图展示了这种方法的改进原理。它的梯度是在更新动量的惯性部分之后才计算出来的，因此新的梯度和之前的惯性是首尾相接的。实际实现时，按照上面的算法，每次迭代需要更新两次参数，计算一次梯度。合理调整算法的计算次序，可以改进为每次迭代更新一次参数，计算一次梯度。","text_tokens":["e","left","有效","：","是","]","&","它","momentum","视","一个","aligned","黑色","提到","可以","后","带","随机","看看","原理","stochastic","出","这","按照","存在","method"," ","下","改进","-","算出","接下","=","接下来","problem","指数","提出","minimization","因此","第一个","收敛","带动","展示","之前","前提","样本","残留","即","in","epsilon","统计","通过","但","}","或","mathbf","梯度","得到","个别","较大","为本","下图","leftarrow","每次","小","局部","可能","新","需要","考虑","论文","而定","和","（","一般","on","曲率","实际","选取","陷入","，","被","表明","期望","到","mathcal","文章","出来","修正","convergence","descent","前","次数","最优","确保","证明","附图","与","相接","这是","法","上","这种","对","部分","下来","明显",",","等于","sum","计学","上面","参考","mathbb","此","本次","dagger","the","nesterov","）","受到","更新","较","文献","才","for","学习","2","v","两次","记","[","形式","按","率为","会","g","同时","目标","动量","从而","能","位置","of",".","~","增加","首尾相接","。","neural","方法","gradient","衰减","处","训练","在","从","该","alpha","rightarrow","让","迭代","为","begin","y","frac","rate","with","limits","避免","影响","统计学","轨迹","极端","的","随着","batch","这个","集中","networks","convex","惯性","强于","展开","显然","algorithms","之后","l","其中","值","equation","时能","_","可知","年","unconstrained","term","一种","第一","不","d","k","好处","a","称","boldsymbol","数据","而","启发","一样","^","下降","sgd","次序","情况","实现","中","由于","抽取","时","常数","带来","注意","效果","则","解","旧","都","其实","首尾","{","计算","说明","越过","了","nabla","顺序","right","就是","以","一次","并且","0","有","不为","初始化","初始","1","不难","求解","红色","误差","排列","x","合理","量","参考文献","我们","过大","；","算法","当","看出","将","点","调整","解时","o","有人","使用","theta","个","end","全局","\\","满足","(","弊端","率","1983","learning","计算出来","如果","+",")","参数","路径","设","依靠","m","这里","通常","继续"],"title":"引入动量的优化算法","title_tokens":["引入","的","优化","算法","动量"]},{"location":"book-1-x/chapter-1/linear-regression/#_8","text":"上述几种算法共同的特点是，具有一个“学习率”。实际上，这个学习率非常不好处理，值过小时，收敛速度很慢；值过大时，在最优解附近又难以收敛。为了解决这一思路，我们可以令学习率可变。最简单的思路是，将学习率设为指数衰减的（当然也可以设置下界），这样当开始学习的时候，学习率较大；而即将收敛时，学习率又会较小。 但是，以上做法不过是一些小小的花招(trick)罢了，接下来介绍的几种算法，是根据当前计算出的梯度来自适应调整学习率的。理论上，使用这种算法，用户不再需要特别关注学习率对训练的影响，我们尽可以设置一个偏大的学习率，在训练过程中，它能被自适应调整到一个合适的区间上。 首先，我们来介绍一种初步的改进， Adagrad (Adaptive Subgradient) ， Adgrad 参考文献 提出该算法的文章，可以在这里参考： Adaptive Subgradient Methods for Online Learning and Stochastic Optimization 记学习率为 \\epsilon \\epsilon ，小量 \\delta \\delta ，初始化学习率参数对角矩阵为 \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新学习率为： \\mathbf{r} \\leftarrow \\mathbf{r} + \\mathrm{diag}(\\mathbf{g})^2 \\mathbf{r} \\leftarrow \\mathbf{r} + \\mathrm{diag}(\\mathbf{g})^2 ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} 。 注意文献中常用向量点积 \\odot \\odot 来表示学习率，这样学习率就不是对角矩阵而是向量了。我们这里不定义额外的符号，以便不熟悉相关定义的读者理解。 这一方法的思想是，学习率随着梯度的累计增大而逐渐减小，类似我们使用指数衰减的策略。所不同的是，在梯度小的地方，我们认为梯度平缓，所以学习率减小得慢，以便算法迅速地通过这一片区域；在梯度大地地方，由于梯度陡峭，为了防止我们因为学习率过大漏过该区域，学习率减小得快，以适应梯度的大小。 这个方法没有从根本上解决迭代次数过多时，梯度过小的问题。不难看出该算法学习率以 O\\left(\\frac{1}{\\mathbf{g}^T\\mathbf{g}}\\right) O\\left(\\frac{1}{\\mathbf{g}^T\\mathbf{g}}\\right) 的比率衰减，经验指出，这个算法在很多情况下是不好用的，只能解决一些比较特定的模型。 在这里，我们依然不给出收敛性的证明（或许在未来我们会在专题中讨论这一问题）。读者不必为这些算法的原理感到压力，我们只需要对其有一个直观的了解就好。 考虑到Adagrad学习率减小的速度未免太快了，我们可以考虑它的改进， RMSprop (root mean square proportion) ，注意它是另一个算法Adadelta的特例，不过在本节我们不会讨论Adadelta，有兴趣的读者可以自己去寻找参考资料。 RMSprop 参考文献 提出该算法的文章，可以在这里参考： Overview of mini-batch gradient descent 记学习率为 \\epsilon \\epsilon ，小量 \\delta \\delta ，衰减参数 \\rho \\rho ，初始化学习率参数对角矩阵为 \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新学习率为： \\mathbf{r} \\leftarrow \\rho \\mathbf{r} + (1 - \\rho) \\mathrm{diag}(\\mathbf{g})^2 \\mathbf{r} \\leftarrow \\rho \\mathbf{r} + (1 - \\rho) \\mathrm{diag}(\\mathbf{g})^2 ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} \\mathbf{g} 。 和上一个算法相比，它唯一的改变就是引入了一个衰减参数 \\rho \\rho ，以指数衰减将之前收集的学习率遗忘。如此就可以控制指数过大的问题，这个trick真是令人一言难尽。但是有趣的是，实际经验中，这个方法真的是卓有成效，是现在常用的神经网络优化算法之一。 最后让我们来介绍当前最实用的算法（之一）， Adam (adaptive momentum estimation) 。顾名思义，它的基本原理是基于对动量的可变估计。实际上，在上一节的Project中，我们选用的优化器就是Adam，Tensorflow的官方教程中，也将Adam作为默认推荐的优化器。 Adam 参考文献 提出该算法的文章，可以在这里参考： Adam: a Method for Stochastic Optimization 特别需要注意的是，Adam的收敛性证明已经被后来者推翻，指出其中存在一个错误。改正后的版本称为AMSGrad，Tensorflow的Keras API支持我们在设置Adam的时候开启AMSGrad模式。关于AMSGrad，我们不在此展开讨论，有兴趣的读者可以参考： On the Convergence of Adam and Beyond 记 k k 为迭代次数，学习率为 \\epsilon \\epsilon ，小量 \\delta \\delta ，衰减参数 \\rho_1,~\\rho_2 \\rho_1,~\\rho_2 ，初始化动量为 \\mathbf{s} = \\mathbf{0} \\mathbf{s} = \\mathbf{0} ，学习率参数对角矩阵为 \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) \\mathbf{r} = \\mathrm{diag}(\\mathbf{0}) ，则在每次迭代中 随机抽取（或从随机排列的数据集中按顺序抽取）m个样本 (\\mathbf{x}_k,~\\mathbf{y}_k) (\\mathbf{x}_k,~\\mathbf{y}_k) ，称这m个样本为一个batch； 计算梯度 \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) \\mathbf{g} = \\frac{1}{m} \\nabla_{\\boldsymbol{\\Theta}} \\sum\\limits_{k=1}^m \\mathcal{L} \\left( \\mathbf{y}_k,~ \\mathcal{D}_{\\boldsymbol{\\Theta}} (\\mathbf{x}_k) \\right) ； 更新动量为： \\mathbf{s} \\leftarrow \\rho_1 \\mathbf{s} + (1 - \\rho_1) \\mathbf{g} \\mathbf{s} \\leftarrow \\rho_1 \\mathbf{s} + (1 - \\rho_1) \\mathbf{g} ； 更新学习率为： \\mathbf{r} \\leftarrow \\rho_2 \\mathbf{r} + (1 - \\rho_2) \\mathrm{diag}(\\mathbf{g})^2 \\mathbf{r} \\leftarrow \\rho_2 \\mathbf{r} + (1 - \\rho_2) \\mathrm{diag}(\\mathbf{g})^2 ； 调整参数大小： \\hat{\\mathbf{s}} \\leftarrow \\dfrac{\\mathbf{s}}{1 - \\rho_1^k} \\hat{\\mathbf{s}} \\leftarrow \\dfrac{\\mathbf{s}}{1 - \\rho_1^k} , \\hat{\\mathbf{r}} \\leftarrow \\dfrac{\\mathbf{r}}{1 - \\rho_2^k} \\hat{\\mathbf{r}} \\leftarrow \\dfrac{\\mathbf{r}}{1 - \\rho_2^k} ； 更新参数 \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\dfrac{ \\hat{\\mathbf{s}} }{\\delta + \\sqrt{\\hat{\\mathbf{r}}}} \\boldsymbol{\\Theta} \\leftarrow \\boldsymbol{\\Theta} - \\epsilon \\dfrac{ \\hat{\\mathbf{s}} }{\\delta + \\sqrt{\\hat{\\mathbf{r}}}} 。 Adam不仅估计了学习率的可变性，还引入了可变的动量。这是迄今为止，我们见到的第一个将动量和可变学习率结合起来的算法。我们当然期望它能带来双份的 快乐 好处，可是…… 为什么会这样呢？ ，已经有文献指出，Adam存在原理上的失误，并提出了改正的算法AMSGrad，这正是我们未来将要在专题中讨论的内容。现在读者只需要知道，Adam的思路其实就是结合动量和可变学习率就行了。 注意 无论是我们没提到的Adadelta还是提到的Adam，其实都引入了动量的概念。那么一个自然而然的idea就是，使用Nesterov动量代替普通的动量。当然，毫无意外的是，已经有人做过了。例如，Adam的Nesterov动量版本叫Nadam，有兴趣的读者不妨去了解一下。","text_tokens":["非常","合适","一个","adaptive","提到","减小","stochastic","官方","开始","出","存在","未免","idea","小时","呢","root","dfrac","第一个","熟悉","读者","例如","通过","或","mathbf","很多","时候","leftarrow","神经网","一些","还是","器","当然","hat","on","未免太","小小","可变","，","被","期望","已经","选用","值过","最优","证明","参考","迄今","速度","实用","快乐","proportion","也","压力","卓有","这些","…","按","率为","又","或许","estimation","一言难尽","最后","有趣","方法","gradient","衰减","资料","定义","将要","迭代","不妨","依然","叫","limits","结合","并","的","batch","集中","向量","正是","做过","错误","第一","不","内容","神经","参考资料","过","后来者","模型","如此","抽取","注意","当前","思想","所以","methods","{","迄今为止","难看","以","tensorflow","0","1","表示","自己","看出","参考文献","但是","过大","算法","rmsprop","来自","区间","nadam","为了","有人","专题","关注","后来","learning","参数","所","理解","另","可以","用户","大时","不好","-","接下","理论","符号","推翻","样本","失误","展开","}","小","用","需要","考虑","和","矩阵","（","实际","优化","amsgrad","基于","感到","处理","下界","以便","逐渐","未来","附近","这种","累计","大地","下来","变性","此","慢","之一","s","文献","更新","学习","记","无论是","尽",":","g","网络","动量","唯一","问题","默认","t","对角","从","率设","为","y","因为","思路","影响","adam","这个","双份","adgrad","适应","trick","l","mathrm","其中","毫无","一种","讨论","a","为什么","难以","首先","adagrad","情况","由于","解","做法","adadelta","经验","了","nabla","基本原理","right","beyond","无论","不难","初始","上述","偏大","我们","了解","只","api","率以","开启","额外","改正","使用","theta","个","\\","过程","r","+","相关","这里","它","平缓","不必","后","不再","原理","这","下","直观","本原","sqrt","特点","接下来","提出","收敛","常用","共同","不过","较大","梯度","快","花招","square","可是","到","认为","convergence","令","给出","online","这是","大小","不会","对","类似","罢了","不仅","一节","即将","the","nesterov","）","一下","较","引入","小小的","for","2","optimization","神经网络","根据","去","概念","然而","能","而是","。","起来","在","该","令人","frac","可变性","project","教程","为止","有成","陡峭","随着","展开讨论","得慢","增大","顾名思义","好处","实际上","称","boldsymbol","真的","意外","and","^","这一","mean","什么","小量","作为","其实","都","控制","推荐","那么","根本","就是","成效","x","模式","当","调整","防止","o","真是","一言","mini","subgradient","基本",")","最","m","地方","：","left","是","overview","momentum","“","特别","收敛性","随机","具有","其","估计","这样","method"," ","改进","迅速","=","不是","就行了","指数","率过","之前","区域","代替","自","epsilon","策略","比率","”","来","每次","几种","一片","不难看出","大漏","初步","文章","mathcal","介绍","只能","称为","地","过多时","descent","次数","顾名","指出","遗忘","上",",","sum","没","从根本上","会","简单","很","of","~","支持","点积","rho","设置","得","寻找","训练","不同","让","收集","比较","卓有成效","没有","自然","现在","_","知道","解决","odot","d","k","兴趣","数据","而","？","普通","diag","中","时","带来","特定","则","计算","以上","顺序","keras","自然而然","版本","就","好","还","初始化","有","排列","本节","将","；","相比","关于","来者","(","见到","率","delta","特例","改变"],"title":"引入可变学习率的优化算法","title_tokens":["引入","的","学习","率","可变","优化","算法"]},{"location":"book-1-x/chapter-1/linear-regression/#_9","text":"","text_tokens":[],"title":"解线性回归问题","title_tokens":["线性","问题","回归","解"]},{"location":"book-1-x/chapter-1/linear-regression/#_10","text":"重申我们之前提到的，我们建议一个完整的工程应当包括 . ├─ data/ # where we store our data │ └─ ... ├─ tools.py # codes for post-processing and analyzing records. ├─ extension.py # codes for extending the tensorflow model. ├─ dparser.py # data parser └─ main.py # main module where we define our tensorflow model. 除了保存数据的文件夹，我们应当有三个子模块。其中 tool : 用来处理、分析生成的数据，通常与Tensorflow无关； extension : 用来扩展tensorflow，例如在这里自定义网络层和操作符； dparser : 数据处理器，用来读取并预处理送入网络的数据； main : 主模块，只定义跟Tensorflow模型有关的内容，需要引用 extension 和 dparser 。 视情况可以灵活调整结构，但建议将定义Tensorflow模型的代码单独放在主模块里，和其他外围代码分离。 在上一节中，我们没有定义 tool.py 和 extension.py ，这是因为我们的工程还很简单，不需要扩展Tensoflow模型，也不需要专门的数据处理代码。相应地，我们把数据的后处理代码直接集成在了主模块 lin-cls.py 里。在这一节，我们要开始构造一个真正严格按照这四部分分离的工程，并且在接下来的各个例子实现里，都会遵照这个模式，读者应当熟悉类似我们所推荐的、这样一个高度分离的模块化设计的思路。","text_tokens":["extension","tool","our","放在","视","一个","高度","提到","可以","三个","开始","extending","这","按照","这样"," ","真正","例子","主","-","接下","子","工程","跟","接下来","件夹","之前","里","熟悉","读者","例如","/","parser","有关","但","cls","要","main","codes","生成","需要","处理器","网络层","和","遵照","是因为","，","分析","调整结构","把","地","处理","读取","lin","与","model","上","集成","store","dparser","部分","下来","类似","一节","相应","外围","the","无关","...","模块","也","py","for",":","结构","会","简单","网络","文件夹","理器","很",".","这四","。","应当","定义","在","除了","专门","引用","因为","records","没有","送入","思路","代码","用来","并","预处理","的","模块化","这个","tensoflow","tools","分离","后处理","其中","设计","各个","├","内容","不","─","单独","数据","└","模型","灵活","操作符","module","and","#","情况","analyzing","实现","where","中","扩展","│","post","都","操作","直接","严格","推荐","define","自定义","数据处理","了","data","保存","建议","tensorflow","包括","其他","还","有","并且","文件","将","我们","we","；","模式","只","processing","调整","重申","完整","构造","自定","所","、","这里","通常"],"title":"代码规范","title_tokens":["规范","代码"]},{"location":"book-1-x/chapter-1/linear-regression/#_11","text":"此次是我们第一次写扩展模块，编写扩展模块的目的是，提供一个更复杂的支持库，以便我们能轻松地使用Tensorflow。因此，扩展模块编写地原则应当包括： 可适用性 : 它应当与我们某一个Project完全无关，就像我们自己基于Tensorflow编写一个扩展库一样，以后我们在任何项目都应该可以使用同一个扩展模块文件； 低依赖性 : 它应当最低限度地需要依赖库。 tensorflow 库本身当然是需要的，而 numpy ， matplotlib 甚或是读写数据的模块，都不宜出现在这里，以确保我们的扩展模块被其他任何模块调用时，依赖关系都是树状的； 强一致性 : 它的使用风格，应当尽可能和Tensorflow本身的API一致，使得一个之前不怎么接触它的人，也能快速上手。 在这个工程里，我们扩展的内容其实很简单，就是允许模型调用一个指定的优化器。让我们直接看以下代码： extension.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class AdvNetworkBase : ''' Base object of the advanced network APIs. ''' @staticmethod def optimizer ( name = 'adam' , l_rate = 0.01 , decay = 0.0 ): ''' Define the optimizer by default parameters except learning rate. Note that most of optimizers do not suggest users to modify their speically designed parameters. name: the name of optimizer (default='adam') (available: 'adam', 'amsgrad', 'adamax', 'nadam', 'adadelta', 'rms', 'adagrad', 'nmoment', 'sgd') l_rate: learning rate (default=0.01) decay: decay ratio ('adadeltaDA' do not support this option) ''' name = name . casefold () if name == 'adam' : return tf . keras . optimizers . Adam ( l_rate , decay = decay ) elif name == 'amsgrad' : return tf . keras . optimizers . Adam ( l_rate , decay = decay , amsgrad = True ) elif name == 'adamax' : return tf . keras . optimizers . Adamax ( l_rate , decay = decay ) elif name == 'nadam' : return tf . keras . optimizers . Nadam ( l_rate , schedule_decay = decay ) elif name == 'adadelta' : return tf . keras . optimizers . Adadelta ( l_rate , decay = decay ) elif name == 'rms' : return tf . keras . optimizers . RMSprop ( l_rate , decay = decay ) elif name == 'adagrad' : return tf . keras . optimizers . Adagrad ( l_rate , decay = decay ) elif name == 'nmoment' : return tf . keras . optimizers . SGD ( lr = l_rate , momentum = 0.6 , decay = decay , nesterov = True ) else : return tf . keras . optimizers . SGD ( l_rate , decay = decay ) 我们在这里几乎罗列了所有可能使用的优化器，全部来自Keras API。但我们也可以使用Tensorflow旧版API定义的优化器。目前Tensorflow允许使用两种API中的任意一种来定义，但是实验发现，旧版API系列的优化器要么已经在Keras中能找到对应的版本，要么就水土不服，无法正常调用。因此，上文提到的几种优化器，我们基本上全部在这里用Keras API定义出来。 优化器的参数尽可能应当选择默认参数，并且应当封装起来，不宜让用户自行操作。尤其是Adadelta，Adam这些优化器的 \\rho \\rho 变量，在 Keras文档 中，建议我们遵从默认值。 任何继承该类的子类，都可以通过 self . optimizer ( self . optimizerName , self . learning_rate ) 来将封装好的优化器API调用到主模块中。","text_tokens":["尤其","extension","：","8","同一个","是","指定","@","momentum","一个","object","中能","尽可能","31","可以","提到","that","库","一致性","decay","self","以后","两种","parameters","系列","their","用户"," ","适用","完全","optimizer","to","上文","主","工程","要么","=","此次","使得","因此","之前","do","里","rms","不服","6","罗列","speically","30","通过","但","正常","20","封装","10","23","33","5","一致","快速","来","实验","默认值","可能","36","几种","用","需要","限度","apis","和","当然","器","调用","优化","，","被","34","读写","基于","amsgrad","casefold","出来","available","已经","到","11","optimizername","if","地","强","变量","文档","依赖性","编写","确保","except","以便","与","18","suggest","name","上","default","35","旧版","依赖","不怎么","nmoment","lr",",","以下","the","nesterov","无关","模块","也","出现","py","人","2","这些","return","写","matplotlib",":","本身","option","简单","9","很","允许","能","原则","this","0.6","默认","of","水土","目的",".","。","支持","rho","else","甚或","应当","numpy","22","定义","15","在","起来","第一次","让","'","adamax","应该","def","无法","class","rate","基本上","找到","7","project","尽可","全部","选择","风格","28","0.01","代码","optimizers","遵从","adam","的","子类","users","这个","staticmethod","适用性","将","水土不服","true","看","对应","手","4","3","25","advanced","l","但是","note","建议","该类","_","26","所有","同一","17","一种","第一","内容","0.0","schedule","27","数据","而","模型","most","adagrad","一样","sgd","发现","support","adadeltada","关系","24","by","中","时","29","扩展","其实","21","都","操作","任意","复杂","接触","直接","define","tf","not","任何","14","像","adadelta","了","keras","就是","低","13","以","继承","版本","就","一次","tensorflow","项目","不宜","包括","其他","network","designed","并且","好","1","文件","自己","16","怎么","我们","nadam","；","19","rmsprop","来自","目前","api","advnetworkbase","elif","base","树状","提供","使用","某","ratio","几乎","\\","12","(","基本","自行","learning","轻松","modify","更",")","32","参数","可","最低","这里","它"],"title":"扩展模块","title_tokens":["模块","扩展"]},{"location":"book-1-x/chapter-1/linear-regression/#argparse","text":"本节将第一次引入 argparse 模块。该模块是python本身继承的原生模块，用来给代码提供启动选项。作为一个完整的Project，我们不希望为了调整参数而频繁地修改代码，因此 argparse 对我们是不可或缺的。在后面所有的Project中，我们都会通过 argparse 模块支持项目选项。 argparse 的官方文档可以在此查阅： argparse — Parser for command-line options, arguments and sub-commands 调用 argparse 的一开始，我们需要定义如下内容： Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import argparse def str2bool ( v ): if v . casefold () in ( 'yes' , 'true' , 't' , 'y' , '1' ): return True elif v . casefold () in ( 'no' , 'false' , 'f' , 'n' , '0' ): return False else : raise argparse . ArgumentTypeError ( 'Unsupported value encountered.' ) parser = argparse . ArgumentParser ( description = 'A demo for linear regression.' , formatter_class = argparse . ArgumentDefaultsHelpFormatter ) Output usage: tools.py [ -h ] A demo for linear regression. optional arguments: -h, --help show this help message and exit 我们首先定义了 str2bool 函数，用来支持用户提供布尔类型的选项；之后，我们初始化了 parser ，一般地初始化 parser 时，我们主要定义三个参数： description : 项目描述，展示在参数用法之前的一段字符串； formatter_class : 格式化器 ，我们一般调用的都是 ArgumentDefaultsHelpFormatter ，因为它能支持自动换行，并在每个参数用法后展示该参数的默认值； epilog : 后记 ，这一段说明文字出现在所有参数用法之后。我们一般不太需要这个功能，但是有时候我们可以使用该功能提供一些用法范例给用户。 现在，我们来介绍几种典型的 argparse 可以提供的参数类型。 字符串选项 1 2 3 4 5 6 7 parser . add_argument ( '-o' , '--optimizer' , default = 'adam' , metavar = 'str' , help = ''' \\ The optimizer we use to train the model (available: 'adam', 'amsgrad', 'adamax', 'nadam', 'adadelta', 'rms', 'adagrad', 'nmoment', 'sgd') ''' ) 在这里我们定义了一个字符串选项，这是最常用的一类选项。用户可以像 python codes.py -o amsgrad 或者 python codes.py --optimizer amsgrad 一样，通过添加参数来覆盖默认值(定义在 default 字段下)。 数值选项 1 2 3 4 5 6 parser . add_argument ( '-lr' , '--learningRate' , default = 0.001 , type = float , metavar = 'float' , help = ''' \\ The learning rate for training the model. ''' ) 这里添加的参数类型是一个浮点数，虽然用户在输入参数的时候输入的是一个字符串，但 metavar 字段告诉了用户应该输入浮点数， type 决定了用户输入的字符串会被自动转换为浮点数。类似地，将两个字段的 float 改为 int ，我们就能提供一个整数作为参数选项 布尔选项 1 2 3 4 5 6 parser . add_argument ( '-if' , '--importFlag' , type = str2bool , nargs = '?' , const = True , default = False , metavar = 'bool' , help = ''' \\ The flag of importing pre-trained model. ''' ) 这里添加的是一个二值选项，它的默认值是 False ，用户可以通过输入 ( 'yes' , 'true' , 't' , 'y' , '1' ) 中的任何一个来指定该选项为真，或通过 ( 'no' , 'false' , 'f' , 'n' , '0' ) 中的任何一个指定该选项为假，不区分大小写。该功能由我们之前定义的 str2bool 函数提供。 特别值得注意的是，这个布尔选项还可以有这样的用法，例如： python codes.py -if -o amsgrad 我们如果指派了 -if ，在不指定它任何值的情况下，该选项就会被开启（值为真）了；如果我们去掉这一行的 -if ，则该选项关闭（值为假）。 多值选项 1 2 3 4 5 6 parser . add_argument ( '-ml' , '--mergedLabel' , default = None , type = int , nargs = '+' , metavar = 'int' , help = ''' \\ The merged label settings. ''' ) 上面的设置提供了一个可以输入任意多个 int 型值的选项，用法如下： python codes.py -ml 1 3 4 0 2 -o amsgrad 上述的输入会被解析成一个值为 [ 1 , 3 , 4 , 0 , 2 ] 的列表。当然，我们也可以输入任意多的值，但是特别值得注意的是，由于在 nargs 字段指定了 + ，一旦我们指派该选项，就要至少输入一个值方可。 上面的几种范例，并不是每一种都需要用在Project中。实际设置选项的时候，应当参照实际情况来处理。例如，本例中，就只使用 字符串选项 和 数值选项 两种。更多关于 add_argument 的用法，请参阅官方文档： argparse — add_argument() 在所有参数都设置好后，调用 args = parser . parse_args () 即可使参数选项生效。用户输入的参数选项将返回到 args 中，例如，如果用户制定了 -o ( --optimizer )，那么我们可以调用 args.optimizer 来取出该字段的值。","text_tokens":["8","指定","输入","一个","command","官方","开始","制定","exit","to","import","后面","train","例如","6","通过","两个","或","10","时候","5","formatter","一些","器","当然","epilog","，","被","值为","频繁","const","数值","段","参照","不太","也","py","多","n","返回","成","importing","this","str","else","函数","改为","定义","即可","7","用来","并","的","4","ml","所有","或缺","commands","功能","importflag","给","第一","不","内容","值得注意","regression","格式化","真","注意","0.001","bool","区分","message","14","h","0","1","不可或缺","但是","nadam","为了","elif","12","告诉","learning","字段","参数","更","v","有时","可以","至少","用户","-","一行","int","description","rms","parser","sub","但","原生","使","f","用","需要","多个","和","（","实际","字符","amsgrad","解析","型值","11","处理","help","一旦","unsupported","lr","此","argumentparser","模块","return","每",":","请参阅","默认","t","布尔","启动","查阅","'","为","def","y","args","因为","use","value","linear","字符串","adam","这个","参阅","—","raise","optional","一种","字","float","a","浮点","首先","adagrad","一样","情况","由于","label","pre","adadelta","了","13","no","好后","项目","格式","初始","上述","我们","false","we","只","开启","使用","整数","\\","+","argument","type","这里","它","flag","]","后","三个","这","下","optimizer","parse","line","展示","常用","in","大小写","add","方可","到","available","if","添加","假","文档","大小","对","nmoment","类似","上面","the","）","encountered","引入","文字","换行","for","2","指派","[","9","能","。","argparse","典型","应当","usage","在","第一次","该","修改","rate","project","后记","learningrate","options","不可","mergedlabel","tools","?","本例","and","python","关闭","作为","转换","决定","都","描述","一段","像","一次","str2bool","yes","调整","o","提供","nargs",")","最","用法","：","是","如下","或者","特别","trained","两种","这样","点数"," ","就要","=","列表","不是","因此","之前","show","arguments","codes","类型","自动","来","默认值","几种","主要","一般","调用","虽然","介绍","地","有时候","值得","training","model","由","default",",","生效","二值","出现","该字","会","本身","浮点数","of",".","支持","设置","argumenttypeerror","merged","一","adamax","应该","argumentdefaultshelpformatter","class","代码","范例","true","现在","3","之后","值","_","demo","希望","而","去掉","sgd","选项","一类","中","时","output","metavar","则","任意","说明","任何","继承","none","就","每个","还","初始化","有","多值","本节","将","；","覆盖","settings","关于","小写","完整","(","取出","如果","casefold","那么"],"title":"项目选项：argparse","title_tokens":["选项","argparse","项目","："]},{"location":"book-1-x/chapter-1/linear-regression/#_12","text":"本节的数据也是自动生成出来的。参考上一节的数据生成器，重新定义数据生成类的迭代器： dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class TestDataRegSet ( TestDataSet ): ''' A generator of the data set for testing the linear regression model. ''' def next_train ( self ): ''' Get the next train batch: (x, y) ''' x = self . s_x * ( np . random . random ([ self . batch , self . len_x ]) - 0.5 ) y = np . matmul ( x , self . A ) + self . c if self . noise > 1e-3 : y = y + np . random . normal ( 0 , self . noise , size = y . shape ) else : np . random . normal ( 0 , self . noise , size = y . shape ) return x , y 提示 这里我们在没有噪声的情况下，仍然调用随机噪声函数，这是为了确保噪声函数被调用，使得随机数无论开关噪声，都能保持一致性。 该生成器同样是输入一组 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} ，以及相关配置，之后就可以通过 迭代器 (iterator) 或 方法 (method) 随机生成数据。与上一节不同的是，我们在本节可以尝试更进一步，令 \\mathbf{A} \\mathbf{A} 的 SVD分解 写作如下形式 \\begin{align} \\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^T. \\end{align} 其中， \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma} 是一个对角矩阵，对角线上的元素顺次排列，对应为矩阵 \\mathbf{A} \\mathbf{A} 的各个特征值。Numpy的库已经集成了 SVD分解 。我们知道，一个 M \\times N M \\times N 的矩阵经过SVD分解后，应当有 \\mathbf{U}_{M \\times M} \\mathbf{U}_{M \\times M} 和 \\mathbf{V}^T_{N \\times N} \\mathbf{V}^T_{N \\times N} 两个方阵。故而，矩阵 \\boldsymbol{\\Sigma}_{M \\times N} \\boldsymbol{\\Sigma}_{M \\times N} 并非方阵。由于它只有对角线上有元素，所以必定有多出来的空行或空列。因此，若我们设 K = \\min(M,~N) K = \\min(M,~N) ，则我们可以知道，SVD分解其实不需要矩阵 \\mathbf{U} \\mathbf{U} 和 \\mathbf{V}^T \\mathbf{V}^T 两个方阵都是方阵，因为当我们取矩阵 \\boldsymbol{\\Sigma}_{K \\times K} \\boldsymbol{\\Sigma}_{K \\times K} 这一对角部分后，可以只取部分行/列构成的矩阵 \\mathbf{U}_{M \\times K} \\mathbf{U}_{M \\times K} 和 \\mathbf{V}^T_{K \\times N} \\mathbf{V}^T_{K \\times N} 。这相当于我们略去了 \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma} 上的空行/空列，但是SVD分解仍然能保证恢复出原矩阵来。 在本例中，我们保留 \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma} 中的前 r r 个特征值，其后的特征值都丢弃，我们把这样的做法称为矩阵的低秩近似，于是有 dparser.py 1 2 3 4 5 6 7 8 9 10 11 12 def gen_lowrank ( A , r ): ''' Generate a low rank approximation to matrix A. A: input matrix. r: output rank. ''' sze = A . shape r_min = np . amin ( sze ) assert r <= r_min and r > 0 , 'r should in the range of [1, {0}]' . format ( r_min ) u , s , v = np . linalg . svd ( A , full_matrices = False ) s = np . diag ( s [: r ]) return np . matmul ( np . matmul ( u [:,: r ], s ), v [: r ,:]) 一个低秩近似的矩阵，其定义的仿射变换 (3) (3) 满足不同的 \\mathbf{x} \\mathbf{x} 对应同一个值 \\mathbf{y} \\mathbf{y} ；反之， \\mathbf{y} \\mathbf{y} 将会对应多个不同的解 \\mathbf{x} \\mathbf{x} 。如果我们训练的线性分类器模拟的是 (3) (3) 的逆过程，可能我们会无法模拟出合适的解来；但是，由于我们定义的 (4) (4) 仍是在拟合正过程，故而我们仍然可以把这个问题看成是有解的。在后续的内容中，我们会适当地讨论当问题 解不唯一 时，我们可以进行哪些工作来处理这类问题。 接下来，我们即可测试低秩近似的效果， dparser.py 1 2 3 4 5 6 7 8 9 def test_lowrank (): A = np . random . normal ( 0 , 10 , [ 10 , 6 ]) for r in range ( 1 , 7 ): A_ = gen_lowrank ( A , r ) RMS = np . sqrt ( np . mean ( np . square ( A - A_ ))) R = np . linalg . matrix_rank ( A_ ) print ( 'Rank = {0}, RMS={1}' . format ( R , RMS )) test_lowrank () Output Rank = 1 , RMS = 6.8600432267325955 Rank = 2 , RMS = 4.677152938185369 Rank = 3 , RMS = 3.216810970685858 Rank = 4 , RMS = 1.8380598782932136 Rank = 5 , RMS = 0.9348520972791058 Rank = 6 , RMS = 9.736224609164252e-15 可见，对于一个标准差为10的矩阵，低秩近似的残差仍然是不超过随机高斯矩阵本身的标准差的。这里的秩是我们在调用低秩近似函数后，使用 np.linalg.matrix_rank 测量的结果。","text_tokens":["于是","e","8","输入","合适","一个","库","模拟","噪声","一步","to","train","6","/","low","同样","通过","或","mathbf","两个","10","5","matmul","一致","生成器","可能","器","空列","，","被","已经","把","generate","linalg","与","dparser","部分","参考","也","py","多","n","generator","成器","标准","方法","else","函数","更进一步","定义","15","迭代","无法","*","即可","7","的","batch","残差","逆","4","normal","进行","当于","full","不","内容","matrix","range","故而","解来","写作","regression","test","元素","0.9348520972791058","所以","{","只取","14","随机噪声","0","特征值","1","svd","但是","为了","12","反之","角","v","一对","保留","可以","一致性","拟出","self","构成","-","仿射变换","接下","6.8600432267325955","分类","使得","分类器","size","rms","有解","}","尝试","取","生成","需要","多个","和","矩阵","机数","11","处理","随机数","配置","确保","下来","s","0.5","return","align",":","形式","特征","testdataregset","分解","唯一","问题","看成","经过","assert","t","对角","numpy","amin","'","begin","为","def","y","因为","set","shape","秩","linear","这个","iterator","其中","讨论","顺次","a","testdataset","略去","情况","next","1e","由于","保持","解","做法","结果","了","13","若","无论","我们","false","仍然","使用","个","\\","过程","r","+","并非","相关","这里","它","重新","列","拟合","]","后","这","下","1.8380598782932136","len","sqrt","以及","接下来","行","rank","仿射","in","相当于","u","square","高斯","变换","模拟出","9.736224609164252","出来","工作","if","令","前","开关","超过","这是","集成","其后","一节","the","2","for","[","定义数据","9","能","。","对角线","4.677152938185369","应当","在","类","该","测量","input","各个","boldsymbol","本例","and","^","mean","其实","都","进一步","仍","<","哪些","data","x","当","空行","end",">","满足","提示",")","times","设","m","format","gen","：","同一个","是","如下","必定","秩是","随机","其","method","这样"," ","近似","适当","=","should","因此","一组","正","approximation","自动","来","标准差","方阵","调用","称为","地","丢弃","model","c","上",",","min","print","会","本身","of",".","~","训练","不同","lowrank","class","没有","可见","解不","测试","np","对应","3","之后","相当","值","_","知道","sigma","同一","后续","线性","k","将会","对于","random","数据","noise","matrices","diag","中","时","output","效果","3.216810970685858","则","get","sze","只有","低","testing","就","出原","有","排列","本节","；","恢复","(","保证","如果"],"title":"数据生成","title_tokens":["生成","数据"]},{"location":"book-1-x/chapter-1/linear-regression/#_13","text":"类模型 (Model class) ，在官方文档中也称为函数式API，是Tensorflow-Keras的用户大多数情况下应当使用的模型。它支持一些灵活的操作，使得我们可以 多输入多输出 : 类模型的输入和输出层，都是通过函数定义的。类模型在构建的时候，只需要给定输入和输出即可； 跨层短接 : 由于类模型的各层都由函数定义，可以轻松将不同的层连接起来，通常通过 融合层 完成这一工作； 多优化器 : 可以通过复用同一层对应的对象，构建多个不同的类模型，并分别对它们使用不同的训练数据、损失函数、优化器，以实现多优化目标。 一个顺序模型大致可以描述为下图的模式： graph LR st1(输<br/>入<br/>1) --> l11[层<br/>1-1] l11 --> l21[层<br/>1-2] l21 --> l31[层<br/>1-3] l31 --> ldots1[层<br/>...] st2(输<br/>入<br/>2) --> l12[层<br/>2-1] l12 --> l22[层<br/>2-2] l22 --> l32[层<br/>2-3] l32 --> ldots2[层<br/>...] ldots1 --> l3[层<br/>3] ldots2 --> l3 l3 --> l4[层<br/>4] l4 --> ed1(输<br/>出<br/>1) l4 --> ed2(输<br/>出<br/>2) l22 --> ed3(输出3) l21 --> l3 classDef styStart fill:#FAE6A9,stroke:#BA9132; class st1,ed1,st2,ed2,ed3 styStart 在本节中，尽管我们开始使用类模型，但我们定义的仍然是一个单线路的线性回归模型，换言之，这样的模型完全可以通过 顺序模型 实现出来。我们从这一节开始，不再使用顺序模型，其一，是因为顺序模型都可以写成类模型的形式，其二，是希望读者能够熟悉、灵活运用类模型的优势。 我们定义一个继承自 extension.py 的类， class LinRegHandle ( ext . AdvNetworkBase ): 。与上一节的情况相若，这里我们不再赘述需要定义哪些方法。并且，我们也不会介绍一些改动不大、或者不重要的方法，详情请读者参阅源码。","text_tokens":["extension","：","是","构建","]","输入","输出","它","这里","重要","一个","完成","或者","可以","不再","官方","出","开始","这","用户","这样"," ","下","完全","l3","-","一层","相若","使得","l22","运用","st2","熟悉","读者","/","ed3","自","通过","但","fill","大多","下图","ed1","时候","跨层","linreghandle","源码","改动","stroke","需要","多个","一些","和","器","br","是因为","优化","，","ba9132","对象","工作","出来","介绍","称为","接起","其一","文档","l21","式","与","model","由","连接起来","上","不会","对","lr","l32",",","一节","它们","详情请","l11","短接","详情","...","回归","也","赘述","py","多","2",";","l4","[",":","形式","stystart","复用","目标","其二",".","。","支持","方法","函数","应当","大","定义","不同","在","起来","训练","类","从","大致","为","class","分别","因为","即可","并","ldots2","的","fae6a9","参阅","对应","优势","ext","4","3","st1","单","给定","线性","尽管","不","希望","数据","模型","灵活","l12","各层","连接","损失","这一","情况","#","ed2","实现","同","灵活运用","入","中","由于","写成","输","都","操作","描述","大多数","ldots1","<","顺序","keras","继承","哪些","以","tensorflow","能够","并且","classdef","1","本节","将","我们","层","；","模式","只","api","仍然","advnetworkbase","使用","换言之","线路",">","(","graph","轻松","l31",")","、","多数","通常","融合"],"title":"定义类模型","title_tokens":["类","定义","模型"]},{"location":"book-1-x/chapter-1/linear-regression/#_14","text":"首先，定义初始化方法： lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def __init__ ( self , learning_rate = 0.01 , epoch = 10 , steppe = 30 , optimizerName = 'adam' ): ''' Initialization and pass fixed parameters. learning_rate: the learning rate for optimizer. epoch: training epochs. steppe: steps per epoch optimizerName: the name of optimizer (available: 'adam', 'amsgrad', 'adamax', 'nadam', 'adadelta', 'rms', 'adagrad', 'nmoment', 'sgd') ''' self . lr = learning_rate self . epoch = epoch self . steppe = steppe self . optimizerName = optimizerName 与上一节相比，这里我们增加了一个参数， opmizerName ，用来指定我们选用的优化器名称，默认值为 adam 。","text_tokens":["：","8","指定","fixed","一个","self","parameters"," ","optimizer","-","linclshandle","=","rms","steppe","6","__","cls","10","5","默认值","器","优化","，","amsgrad","available","11","optimizername","选用","pass","lin","training","与","name","上","nmoment","lr",",","一节","the","py","2","for",":","9","默认","of",".","steps","增加","。","方法","epoch","init","定义","'","adamax","为","def","class","rate","7","0.01","用来","adam","的","4","3","_","首先","adagrad","and","sgd","per","initialization","14","adadelta","了","13","初始化","初始","1","名称","我们","nadam","相比","opmizername","12","(","learning",")","参数","epochs","30","这里"],"title":"初始化方法","title_tokens":["初始化","初始","方法"]},{"location":"book-1-x/chapter-1/linear-regression/#_15","text":"接下来定义网络构造 lin-cls.py: class LinClsHandle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def construct ( self ): ''' Construct a linear model and set the optimizer as Adam ''' # Construction input = tf . keras . Input ( shape = ( INPUT_SHAPE ,), dtype = tf . float32 ) dense1 = tf . keras . layers . Dense ( LABEL_SHAPE , use_bias = True , kernel_initializer = tf . keras . initializers . RandomNormal ( 0.0 , stddev = 10.0 ), bias_initializer = tf . keras . initializers . Constant ( 2 ), activation = None , name = 'dense1' )( input ) self . model = tf . keras . Model ( inputs = input , outputs = dense1 ) # Set optimizer self . model . compile ( optimizer = self . optimizer ( self . optimizerName , self . lr ), loss = tf . keras . losses . MeanSquaredError (), metrics = [ self . relation ] ) @staticmethod def relation ( y_true , y_pred ): m_y_true = tf . keras . backend . mean ( y_true , axis = 0 ) m_y_pred = tf . keras . backend . mean ( y_pred , axis = 0 ) s_y_true = tf . keras . backend . sqrt ( tf . keras . backend . mean ( tf . keras . backend . square ( y_true ), axis = 0 ) - tf . keras . backend . square ( m_y_true )) s_y_pred = tf . keras . backend . sqrt ( tf . keras . backend . mean ( tf . keras . backend . square ( y_pred ), axis = 0 ) - tf . keras . backend . square ( m_y_pred )) return tf . keras . backend . mean (( tf . keras . backend . mean ( y_true * y_pred , axis = 0 ) - m_y_true * m_y_pred ) / ( s_y_true * s_y_pred )) 使用类模型时，我们每定义一层，都调用对应的网络层函数，并返回层的输出结果。这就是为何它又叫“函数式API”。我们直接使用均方误差作为我们的损失函数，同时，我们还自行定义了一个评价函数， 皮尔森相关系数 ，该系数专门用来反映两组数据之间是否线性相关，上文我们已经叙述过它的定义。 注意 理想情况下，相关系数应当使用整个数据集来求取。但实际情况下做不到这一点，因此我们求取的相关系数只能看作是一个通过batch得到的估计。故此，我们可以发现，求相关系数要求我们每次输入的样本至少有2个。样本数目越多，相关系数的估计越准确。 注意 从式中可以发现，我们定义的皮尔森相关系数时，完全使用的时Tensorflow-Keras API，因此它当然可以用作我们的训练损失函数。但实际情况下，我们并不使用它。考虑一个反例，当两组数据的分布之间唯一的不同只是均值时，亦即 \\mathbf{y}_2 = \\mathbf{y}_1 + C \\mathbf{y}_2 = \\mathbf{y}_1 + C ，这种情况下皮尔森相关系数仍然为1。虽然我们可以考虑用 余弦相似度函数 (Cosine similarity) 来代替它，但经验显示，余弦相似度最大化到一定程度以后，其对应的均方误差反而上升。考虑另一个反例， \\mathbf{y}_2 = \\alpha \\mathbf{y}_1 \\mathbf{y}_2 = \\alpha \\mathbf{y}_1 ，显然 \\mathbf{y}_1 \\mathbf{y}_1 和 \\mathbf{y}_2 \\mathbf{y}_2 的余弦相似度是1。因此，实际应用中，无论是皮尔森相关系数还是余弦相似度，都适合用作评价函数而不是损失函数。 与上一节不同的是，由于这是一个线性回归器，我们不给它提供激活函数。","text_tokens":["叙述","8","是","]","@","输出","输入","“","另","一个","要求","可以","至少","self","以后","线性相关","其","losses","估计","这","均方"," ","下","完全","optimizer","上文","最大化","-","接下","sqrt","linclshandle","construct","相似","=","一层","接下来","不是","评价","因此","样本","理想","6","/","即","代替","上升","construction","通过","得到","性相","cls","但","20","loss","mathbf","}","10","”","一点","23","故此","5","每次","亦","来","数目","square","用","考虑","还是","之间","网络层","当然","和","器","显示","集来","实际","均","调用","虽然","backend","，","到","metrics","已经","11","optimizername","只能","一定","dtype","应用","两组","反例","lin","越","18","dense1","式","与","model","这是","c","name","上","这种","initializer","inputs","lr","下来",",","10.0","一节","最大","the","s","similarity","回归","outputs","py","as","2","stddev","多","return","每","为何","度","无论是","meansquarederror","[",":","返回","又","同时","9","网络","唯一",".","。","适合","分布","函数","22","应当","定义","15","axis","训练","不同","类","专门","该","看作","alpha","'","为","def","class","y","*","叫","7","均值","set","shape","激活","use","constant","linear","并","用来","adam","的","batch","余弦","staticmethod","true","对应","显然","4","反映","3","25","cosine","input","_","26","17","compile","给","0.0","线性","不","从式","求","a","过","数据","而","模型","and","损失","发现","#","情况","反而","mean","关系","24","中","bias","时","作为","由于","注意","pred","大化","21","都","float32","准确","{","直接","label","tf","方","14","结果","了","经验","系数","keras","就是","13","relation","none","tensorflow","做","无论","还","randomnormal","0","误差","dense","有","1","16","整个","程度","我们","层","19","当","只是","不到","api","仍然","相关系数","使用","提供","layers","个","\\","12","构造","(","kernel","自行","是否","用作","+",")","求取","m","相关","皮尔森","它","initializers","activation"],"title":"构造方法","title_tokens":["构造","方法","构造方法"]},{"location":"book-1-x/chapter-1/linear-regression/#_16","text":"类模型的 compile 、 fit 、 evaluate 、 predict 等API与顺序模型完全相同，详情请查看 Keras中文文档 - Model类 (函数式API) 。","text_tokens":["。","，","函数","evaluate","类","predict"," ","完全","文档","中文","顺序","keras","-","式","与","model","的","等","详情请","查看","fit","api","详情","compile","模型","(","完全相同",")","相同","、"],"title":"训练和测试方法","title_tokens":["和","测试","方法","训练","测试方法"]},{"location":"book-1-x/chapter-1/linear-regression/#_17","text":"上一节中，我们每次训练后，就当场显示分析结果。在本节中，我们会“再进一步”。即使用 tools.py 专门进行实验结果分析（后处理）。相对地，训练后，我们会讲 原始输出 (raw output) 保存到文件里。这是一种编写代码的思想，是为了便于我们批量分析测试数据。在后面的Project中，我们会看到，我们既会编写当场显示分析结果的测试代码，也会编写保存输出后使用 tools.py 分析的代码。究竟使用哪种方式分析数据，视具体情况而定。一般地，测试少量数据时，我们当场分析；批量测试大量数据时，或者需要比较不同选项（例如不同噪声）对结果的影响时，我们在 tools.py 中分析。本实验的情况属于后者。","text_tokens":["是","哪","输出","“","视","或者","后者","具体情况","后","噪声","一步"," ","方式","后面","再进一步","里","例如","当场","即","raw","”","每次","大量","实验","需要","而定","（","显示","一般","，","到","分析","看到","地","测试代码","处理","编写","这是","上","对","一节","究竟","）","也","py","会","既会",".","。","相对","训练","不同","在","专门","便于","具体","project","比较","影响","代码","少量","的","测试","讲","tools","后处理","进行","一种","数据","属于","情况","选项","中","时","output","种","思想","进一步","本","结果","保存","就","文件","本节","测试数据","我们","；","为了","使用","(",")","原始","批量"],"title":"调试","title_tokens":["调试"]},{"location":"book-1-x/chapter-1/linear-regression/#_18","text":"由于我们本次实验需要对比不同设置下的回归器性能，我们希望随机生成的矩阵 \\mathbf{A} \\mathbf{A} ，向量 \\mathbf{c} \\mathbf{c} 应当可复现；换言之，我们希望我们的结果是可复现的。 关于这一问题，Keras的文档给出的建议可以在这里查阅： 如何在 Keras 开发过程中获取可复现的结果？ 我们只需要使 argparse 添加一个选项 -sd ( --seed )，并通过该选项控制： 1 2 3 4 5 6 def setSeed ( seed ): np . random . seed ( seed ) random . seed ( seed + 12345 ) tf . set_random_seed ( seed + 1234 ) if args . seed is not None : # Set seed for reproductable results setSeed ( args . seed ) 其中， np.random.seed ， random.seed ， tf.set_random_seed 分别来自Numpy，python原生的random库，以及Tensorflow。将这三个库的 随机种子 (seed) 设为三个不同的值，即可保证我们每次指定 -sd 后，从程序运行开始，得到的所有随机数都是固定的随机序列。当然， Keras文档 指出，即使如此，我们还不能保证我们的结果完完全全是可复现的。因为多线程算法并发的先后顺序随机性、GPU运算带来的先后顺序随机性等干扰因素，均会导致我们每次得到的结果有细微的偏差。但这些因素对于本实验验证可复现数据的要求几乎没有什么影响。","text_tokens":["：","获取","是","指定","一个","要求","可以","后","随机","库","即使如此","对比","三个","开始","随机性","这"," ","下","完全","如何","-","以及","程序运行","偏差","6","12345","通过","得到","但","}","mathbf","生成","使","5","原生","每次","实验","不能","需要","器","矩阵","程序","当然","验证","，","1234","机数","if","线程","添加","随机数","文档","序列","指出","给出","c","先后","本次","性能","回归","运行","2","for","这些",":","is","results","问题",".","开发","。","argparse","干扰","固定","设置","细微","应当","numpy","即使","不同","在","从","该","查阅","为","def","args","分别","因为","即可","set","没有","影响","并","运算","的","等","向量","复现","np","种子","4","3","其中","值","_","所有","seed","setseed","reproductable","对于","希望","a","random","多线","因素","数据","？","这一","#","python","选项","什么","中","由于","如此","带来","先后顺序","都","控制","sd","{","tf","not","本","均会","结果","顺序","keras","none","建议","tensorflow","并发","还","有","1","完完全全","将","我们","；","算法","来自","只","关于","几乎","换言之","\\","导致","过程","(","多线程","保证","+",")","设","可","、","这里","gpu"],"title":"使实验结果可复现","title_tokens":["实验","复现","结果","使","可"]},{"location":"book-1-x/chapter-1/linear-regression/#_19","text":"首先，训练网络。我们同样随机生成 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 \\mathbf{x} \\mapsto \\mathbf{y}:~\\mathbb{R}^{10} \\mapsto \\mathbb{R}^6 的仿射变换，将该变换中的线性变换矩阵采用秩为4的低秩近似，并且设置好数据集，给定噪声扰动由用户决定。默认值下，噪声为 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,10)^6 \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0,10)^6 ，epoch为20个，每个epoch迭代500次，每次馈入32个样本构成的batch。我们将上一节的主函数输出部分修改成如下形式，并进行不加参数的调试： lin-cls.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # Initialization A = dp . gen_lowrank ( np . random . normal ( 0 , 10 , [ INPUT_SHAPE , LABEL_SHAPE ]), RANK ) c = np . random . uniform ( 1 , 3 , [ 1 , LABEL_SHAPE ]) dataSet = dp . TestDataRegSet ( 10 , A , c ) dataSet . config ( noise = args . noise ) # Generate a group of testing samples. if args . seed is not None : setSeed ( args . seed + 1000 ) dataSet . config ( batch = args . testBatchNum ) x , y = next ( dataSet ) # Set the data set for training. dataSet . config ( batch = args . trainBatchNum ) # Construct the model and train it. h = LinRegHandle ( learning_rate = args . learningRate , epoch = args . epoch , steppe = args . steppe , optimizerName = args . optimizer ) h . construct () print ( 'Begin to train:' ) print ( '---------------' ) record = h . train ( iter ( dataSet )) # Check the testing results print ( 'Begin to test:' ) print ( '---------------' ) yp , loss_p , corr_p = h . test ( x , y ) # Check the regressed values W , b = h . model . get_layer ( name = 'dense1' ) . get_weights () # Save if args . outputData is not None : np . savez_compressed ( args . outputData , epoch = record . epoch , loss = record . history [ 'loss' ], corr = record . history [ 'relation' ], test_x = x , test_y = y , pred_y = yp , pred_loss = loss_p , pred_corr = corr_p , W = W , b = b , A = A , c = c ) Output Begin to train: --------------- Epoch 1 /20 500 /500 [==============================] - 1s 2ms/step - loss: 29084 .6994 - relation: 0 .3472 Epoch 2 /20 500 /500 [==============================] - 1s 2ms/step - loss: 15669 .9579 - relation: 0 .5597 Epoch 3 /20 500 /500 [==============================] - 1s 2ms/step - loss: 8145 .8705 - relation: 0 .7134 Epoch 4 /20 500 /500 [==============================] - 1s 2ms/step - loss: 4000 .0838 - relation: 0 .8130 Epoch 5 /20 500 /500 [==============================] - 1s 2ms/step - loss: 1856 .1477 - relation: 0 .8801 Epoch 6 /20 500 /500 [==============================] - 1s 2ms/step - loss: 799 .4556 - relation: 0 .9354 Epoch 7 /20 500 /500 [==============================] - 1s 2ms/step - loss: 336 .8600 - relation: 0 .9700 Epoch 8 /20 500 /500 [==============================] - 1s 2ms/step - loss: 166 .5899 - relation: 0 .9813 Epoch 9 /20 500 /500 [==============================] - 1s 2ms/step - loss: 113 .2465 - relation: 0 .9831 Epoch 10 /20 500 /500 [==============================] - 1s 2ms/step - loss: 102 .0431 - relation: 0 .9834 Epoch 11 /20 500 /500 [==============================] - 1s 2ms/step - loss: 99 .6678 - relation: 0 .9838 Epoch 12 /20 500 /500 [==============================] - 1s 2ms/step - loss: 99 .8547 - relation: 0 .9833 Epoch 13 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .1278 - relation: 0 .9834 Epoch 14 /20 500 /500 [==============================] - 1s 2ms/step - loss: 99 .6048 - relation: 0 .9835 Epoch 15 /20 500 /500 [==============================] - 1s 2ms/step - loss: 101 .1930 - relation: 0 .9832 Epoch 16 /20 500 /500 [==============================] - 1s 2ms/step - loss: 101 .6636 - relation: 0 .9835 Epoch 17 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .6665 - relation: 0 .9834 Epoch 18 /20 500 /500 [==============================] - 1s 2ms/step - loss: 101 .2459 - relation: 0 .9832 Epoch 19 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .9701 - relation: 0 .9836 Epoch 20 /20 500 /500 [==============================] - 1s 2ms/step - loss: 100 .7719 - relation: 0 .9836 Begin to test: --------------- 10 /10 [==============================] - 0s 5ms/sample - loss: 94 .8883 - relation: 0 .9897 Evaluated loss ( losses.MeanSquaredError ) = 94 .88829040527344 Evaluated metric ( Pearson ' s correlation ) = 0 .9897396 以上结果是不加任何参数的前提下，直接以默认参数运行程序得到的。结果显明，MSE最后收敛在100左右，因为我们馈入的label添加了标准差为10的白噪声，对应的方差为100。可知，实验结果与预期一致。另一方面，我们可以看到，相关系数在这里可以充当类似准确度的作用，考虑到我们默认的噪声为10，这一相关系数的收敛结果是符合我们的预期的。 我们还可以注意到，这段代码中，生成测试集的代码被提前了，这是为了确保每次运行程序，只要指定了种子，生成的测试集总是一致的。 现在，我们可以导出生成数据了，首先，我们改变不同的优化器，其他参数全部一致，例如，学习率均为0.01（Adadelta除外，其初始参数一般推荐为1.0）。调用代码时的参数设置如下 python lin-reg.py -e 25 -sd 1 -do test/algorithm/ { optimizer } -o { optimizer } 其中我们用 {optimizer} 来指代我们选用的优化算法。同时，我们固定测试的epoch数量为25，这是因为有些算法的收敛速度不足以保证20个epoch收敛。 接下来，我们固定优化器为Adam，改变不同的噪声，分别令标准差为0, 1, 5, 10, 50, 100，产生多组结果。 python lin-reg.py -sd 1 -do test/noise/ { noise } -is { noise }","text_tokens":["e","8","指定","50","噪声","pearson","改变","to","config","train","do","例如","save","6","/","1.0","同样","得到","mathbf","loss","10","符合","5","一致","导出","器","是因为","，","被","1000","check","选用","generate","step","corr","18","与","部分","mathbb","段","作用","速度","预期","py","500","8883","次","提前","15669","n","成","最后","标准","8801","固定","函数","8130","15","迭代","分别","0838","7","全部","一方","0.01","并","的","batch","100","4","normal","trainbatchnum","进行","不","5ms","algorithm","关系","24","采用","test","9701","注意","compressed","{","1930","14","h","另一方面","relation","以","0","1","16","左右","19","算法","为了","相关系数","12","learning","参数","32","30","28","outputdata","dp","9354","方面","可以","扰动","w","用户","9834","不加","构成","-","仿射变换","接下","savez","6994","weights","不足以","前提","样本","2465","}","testbatchnum","生成","33","用","799","考虑","mapsto","9700","矩阵","5597","（","sim","优化","1278","0s","另一方","11","确保","uniform","dense1","name","layer","1s","下来","s","加","学习","meansquarederror",":","形式","testdataregset","同时","is","mse","网络","默认","充当","'","为","begin","y","args","因为","秩为","set","shape","336","秩","adam","7134","其中","17","seed","setseed","a","数量","首先","next","pred","21","label","initialization","结果","adadelta","了","dataset","13","values","evaluated","其他","初始","correlation","我们","率均","个","一方面","\\","r","+","器为","27","相关","这里","]","输出","多组","总是","losses","这","显明","下","optimizer","99","接下来","收敛","rank","仿射","cls","metric","20","29084","102","linreghandle","实验","37","变换","到","if","令","4000","添加","只要","lin","9831","6665","这是","类似","一节","the","）","2","for","[","3472","9","6048","。","epoch","在","4556","该","修改","rate","learningrate","除外","方差","种子","产生","25","input","regressed","26","可知","给定","集","boldsymbol","and","^","这一","9897","python","决定","准确","0431","直接","推荐","调试","not","系数","data","并且","x","2ms","o","9579","94","9836","馈入","1856",")","samples","有些","8547","gen","iter","：","如下","是","31","随机","8600","history","其"," ","主","近似","construct","=","1477","6678","steppe","23","来","每次","默认值","9897396","36","标准差","程序","一般","调用","varepsilon","34","mathcal","看到","optimizername","9813","9835","training","model","由","c","指代","上","35","p",",","9833","reg","7719","运行","print","不足","it","线性变换","results","of",".","~","设置","22","训练","sample","不同","lowrank","record","2459","6636","代码","测试","5899","np","对应","现在","b","3","group","113","_","88829040527344","线性","random","参数设置","数据","9832","noise","#","中","时","29","output","get","8145","sd","白","以上","任何","101","低","testing","none","yp","每个","好","准确度","还","将","8705","166","(","保证","9838","足以"],"title":"使实验代码保存输出","title_tokens":["输出","使","保存","实验","代码"]},{"location":"book-1-x/chapter-1/linear-regression/#toolspy","text":"首先，在 tools.py 中定义数据解析函数 tools.py 1 2 3 4 5 6 7 8 9 10 11 12 def parseData ( path , keys ): keys_list = dict (( k , []) for k in keys ) name_list = [] for f in os . scandir ( path ): if f . is_file (): name , _ = os . path . splitext ( f . name ) name_list . append ( name . replace ( '_' , ' ' )) data = np . load ( os . path . join ( path , f . name )) for key in keys : keys_list [ key ] . append ( data [ key ]) epoch = data [ 'epoch' ] return name_list , epoch , keys_list 该函数的作用是，给定保存输出文件的文件夹路径，能够自动读取文件夹下所有数据文件，并将不同文件的结果列在列表的不同元素中。 keys 关键字能帮助我们指派我们关心的数据字段。 接下来，我们通过如下代码，对比不同优化器条件下的损失函数和测度函数，对比不同噪声条件下的损失函数和测度函数，输出的曲线反映了对训练过程的跟踪。 tools.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def showCurves ( path , prefix = '{0}' , converter = str ): ''' Show curves from different tests in a same folder. ''' name_list , epoch , keys_list = parseData ( path , [ 'loss' , 'corr' ]) loss_list = keys_list [ 'loss' ] corr_list = keys_list [ 'corr' ] if ( not loss_list ) or ( not corr_list ): raise FileExistsError ( 'No data found, could not draw curves.' ) for i in range ( len ( loss_list )): plt . semilogy ( loss_list [ i ], label = prefix . format ( converter ( name_list [ i ])), marker = MARKERS [ i % 9 ], markevery = 3 ) plt . legend () plt . xlabel ( 'epoch' ), plt . ylabel ( 'MSE' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . tight_layout (), plt . show () for i in range ( len ( corr_list )): plt . plot ( corr_list [ i ], label = prefix . format ( converter ( name_list [ i ])), marker = MARKERS [ i % 9 ], markevery = 3 ) plt . legend () plt . xlabel ( 'epoch' ), plt . ylabel ( 'Pearson \\' s correlation' ) plt . gcf () . set_size_inches ( 5 , 5 ), plt . tight_layout (), plt . show () showCurves ( './test/algorithm' ) showCurves ( './test/noise' , prefix = 'ε=N(0,{0})' , converter = int ) Output (优化器) 损失函数 (MSE) 测度函数 (相关系数) Output (噪声) 损失函数 (MSE) 测度函数 (相关系数) 可见，损失曲线反映了训练的进度，而测度曲线反映了当前的准确度。我们可以得到如下结论： 令人意外的是，SGD和Nesterov动量法收敛速度最快。这是由于这两种方法没有引入对学习率的调整。我们使用的损失函数初始点梯度非常大，这使得简单的方法，形如SGD和动量法在一开头就取得了非常迅速的下降；而对那些需要调整学习率的算法而言，初始梯度在很大的情况下，会导致初始学习率被降到较小的水准。这就是为何Adagrad几乎不收敛的原因，因为一开始这一算法的学习率就被大梯度抑制到将近0的水平了，导致训练无法为继； 在调整学习率的算法里，收敛速度有 RMSprop > Adam = NAdam > Adamax = AMSgrad > Adadelta。从AMSgrad以上的这些算法都可资利用，Adadelta的原理和RMSprop几乎相同但效果相差甚巨，这是由于参数不同引起的，我们虽然将Adadelta的学习率特地设为 1.0 ，仍然远远不如RMSprop，可见一个合适的参数对算法的重要性。 噪声的输出结果并不令人意外，所有噪声条件下的MSE最后都收敛到对应的噪声方差上。 为了检查测试集的情况，我们通过以下函数来绘制比较不同样本在不同优化器、不同噪声条件下的RMSE（均方根误差）， tools.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def showBars ( path , prefix = '{0}' , converter = str , ylim = None ): ''' Show bar graphs for RMSE for each result ''' name_list , epoch , keys_list = parseData ( path , [ 'test_y' , 'pred_y' ]) #print(keys_list) ytrue_list = keys_list [ 'test_y' ] ypred_list = keys_list [ 'pred_y' ] def RMSE ( y_true , y_pred ): return np . sqrt ( np . mean ( np . square ( y_true - y_pred ), axis = 1 )) N = ytrue_list [ 0 ] . shape [ 0 ] NG = len ( ytrue_list ) for i in range ( NG ): plt . bar ([ 0.6 + j + 0.8 * i / NG + 0.4 / NG for j in range ( - 1 , 9 , 1 )], RMSE ( ytrue_list [ i ], ypred_list [ i ]), width = 0.8 / NG , label = prefix . format ( converter ( name_list [ i ]))) plt . legend ( ncol = 5 ) plt . xlabel ( 'sample' ), plt . ylabel ( 'RMSE' ) if ylim is not None : plt . ylim ([ 0 , ylim ]) plt . gcf () . set_size_inches ( 12 , 5 ), plt . tight_layout (), plt . show () showBars ( './test/algorithm' , ylim = 70 ) showBars ( './test/noise' , prefix = 'ε=N(0,{0})' , converter = int ) Output (优化器) Output (噪声) 上述结果反映了 测试结果和训练情况相仿，这是由于我们的训练集和测试机完全独立同分布； Adadelta和Adagrad还没有训练好，它们的误差明显大于其他算法。且Adagrad已经无法收敛，可见这种算法不实用。 再接下来，我们要分别展示不同测试下的输出。下面列举的所有输出由该函数所产生： tools.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def saveResults ( path , opath , oprefix , datakeys , title = '' , xlabel = None , ylabel = None , onlyFirst = False , plot = False , prefix = ' ({0})' , converter = str ): ''' Save result graphs to a folder. ''' name_list , _ , data_list = parseData ( path , datakeys ) if plot : # show curves c_list = data_list [ 'c' ] b_list = data_list [ 'b' ] NG = len ( b_list ) for i in range ( NG ): plt . plot ( c_list [ i ] . T , label = 'c' ) plt . plot ( b_list [ i ] . T , label = 'b' ) plt . legend () plt . gca () . set_title ( title + prefix . format ( converter ( name_list [ i ]))) if xlabel is not None : plt . xlabel ( xlabel ) if ylabel is not None : plt . ylabel ( ylabel ) plt . gcf () . set_size_inches ( 5 , 5 ) if onlyFirst : formatName = '' else : formatName = name_list [ i ] . replace ( ' ' , '_' ) plt . savefig ( os . path . join ( opath , oprefix + '{0}.svg' . format ( formatName ))) plt . close ( plt . gcf ()) if onlyFirst : return else : # show images data_list = data_list [ datakeys [ 0 ]] NG = len ( data_list ) for i in range ( NG ): plt . imshow ( data_list [ i ], interpolation = 'nearest' , aspect = 'auto' ), plt . colorbar (), plt . gca () . set_title ( title + prefix . format ( converter ( name_list [ i ]))) if xlabel is not None : plt . xlabel ( xlabel ) if ylabel is not None : plt . ylabel ( ylabel ) plt . gcf () . set_size_inches ( 6 , 5 ) if onlyFirst : formatName = '' else : formatName = name_list [ i ] . replace ( ' ' , '_' ) plt . savefig ( os . path . join ( opath , oprefix + '{0}.png' . format ( formatName ))) plt . close ( plt . gcf ()) if onlyFirst : return 测试代码 1 2 3 4 5 6 7 8 9 10 11 12 13 def saveAllResults (): saveResults ( './test/algorithm' , './record/algorithm' , 'alg-A-' , [ 'A' ], title = 'A' , prefix = '' , onlyFirst = True ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-yt-' , [ 'test_y' ], title = 'True values' , prefix = '' , onlyFirst = True ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-y-' , [ 'pred_y' ], title = 'Predicted values' ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-W-' , [ 'W' ], title = 'W' ) saveResults ( './test/algorithm' , './record/algorithm' , 'alg-cb-' , [ 'c' , 'b' ], title = 'Biases' , plot = True ) saveResults ( './test/noise' , './record/noise' , 'noi-A-' , [ 'A' ], title = 'A' , prefix = '' , onlyFirst = True ) saveResults ( './test/noise' , './record/noise' , 'noi-yt-' , [ 'test_y' ], title = 'True values' , prefix = ' (ε=N(0,{0}))' , converter = int ) saveResults ( './test/noise' , './record/noise' , 'noi-y-' , [ 'pred_y' ], title = 'Predicted values' , prefix = ' (ε=N(0,{0}))' , converter = int ) saveResults ( './test/noise' , './record/noise' , 'noi-W-' , [ 'W' ], title = 'W' , prefix = ' (ε=N(0,{0}))' , converter = int ) saveResults ( './test/noise' , './record/noise' , 'noi-cb-' , [ 'c' , 'b' ], title = 'Biases' , plot = True , prefix = ' (ε=N(0,{0}))' , converter = int ) saveAllResults () 首先考虑不同优化器的测试，在这些测试里，我们确保 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 对所有测试相同，且产生的随机数据真值 \\mathbf{y}_{\\mathrm{true}} \\mathbf{y}_{\\mathrm{true}} 对所有测试也相同，亦即： \\mathbf{A} \\mathbf{A} \\mathbf{y} \\mathbf{y} 的真实值 于是我们可得到所有的数据 优化器 \\mathbf{y} \\mathbf{y} 的预测值 \\mathbf{W} \\mathbf{W} \\mathbf{b} \\mathbf{b} 与 \\mathbf{c} \\mathbf{c} Adadelta Adagrad Adam Adamax AMSgrad Nesterov Adam Nesterov Moment RMSprop SGD 接下来考虑不同噪声的测试，在这些测试里，我们确保 \\mathbf{A},~\\mathbf{c} \\mathbf{A},~\\mathbf{c} 对所有测试相同，但由于噪声大小的不同，随机数据真值 \\mathbf{y}_{\\mathrm{true}} \\mathbf{y}_{\\mathrm{true}} 会有所偏差： \\mathbf{A} \\mathbf{A} 于是我们可得到所有的数据 \\boldsymbol{\\varepsilon} \\sim N(0,~?) \\boldsymbol{\\varepsilon} \\sim N(0,~?) \\mathbf{y} \\mathbf{y} 的真实值 \\mathbf{y} \\mathbf{y} 的预测值 \\mathbf{W} \\mathbf{W} \\mathbf{b} \\mathbf{b} 与 \\mathbf{c} \\mathbf{c} 0 1 5 10 50 100 我们最为看重的，其实是是否拟合出 \\mathbf{A} \\mathbf{A} 和 \\mathbf{c} \\mathbf{c} 。一系列实验表明， \\mathbf{W} \\mathbf{W} 和 \\mathbf{b} \\mathbf{b} 的拟合效果甚好。由于我们建立的仿射变换模型和原始仿射变换模型有着完全一致的结构，优化结果反映这一问题的解相当准确。至此，我们已经掌握了一个完整的Project应当具有的模块结构，以及对不同的优化器有了理论和实际的体验。在后续的章节里，除非有特别的应用，我们不再探讨不同的优化器对结果的影响，在绝大多数情况下，我们都将使用AMSgrad。","text_tokens":["parsedata","于是","8","取得","50","非常","合适","一个","利用","噪声","oprefix","auto","pearson","dict","开始","出","系列","一系列","to","showcurves","ypred","且","marker","save","6","/","跟踪","1.0","通过","得到","掌握","loss","mathbf","10","亦","5","一致","器","均","完全一致","，","被","list","已经","curves","测试代码","corr","18","与","明显","它们","段","作用","速度","same","实用","也","再","py","这些","为何","远远","n","str","最后","ncol","方法","else","函数","章节","定义","15","无法","markevery","*","分别","7","并","看重","的","j","100","4","原始","反映","不如","所有","不","建立","模型","range","algorithm","下降","关系","result","24","test","元素","当前","svg","{","saveresults","41","predicted","14","markers","保存","gcf","0","1","文件","16","nadam","19","算法","path","检查","ylabel","rmsprop","为了","相关系数","12","而言","参数","路径","32","30","所","可","28","40","legend","开头","预测","可以","形如","w","onlyfirst","完全","graphs","最快","-","仿射变换","接下","prefix","int","理论","有所","真实","使得","size","42","样本","但","}","images","大多","33","小","f","法在","splitext","需要","考虑","和","（","sim","数据文件","实际","优化","amsgrad","解析","11","应用","savefig","读取","探讨","semilogy","确保","关键字","name","这种","独立","下来","s","器有","close","found","模块","converter","学习","return",":","结论","is","mse","动量","问题","os","原因","可资利用","t","大","从","重要性","aspect","绝大","'","为","def","y","因为","nearest","set","shape","影响","adam","很大","raise","mathrm","17","a","首先","adagrad","引起","情况","帮助","由于","44","pred","解","21","join","file","大多数","label","ytrue","adadelta","结果","了","13","values","no","体验","其他","误差","初始","correlation","甚巨","上述","我们","false","仍然","使用","\\","showbars","过程","opath","+","27","相关","多数","偏差","replace","至此","列","load","拟合","]","输出","相仿","43","38","不再","tight","原理","这","下","len","相差","sqrt","以及","接下来","收敛","展示","真值","仿射","in","要","20","梯度","实验","37","square","甚","变换","到","0.8","if","plt","法","这是","大小","对","将近","nesterov","ng","）","较","引入","append","2","for","45","inches","指派","[","继","定义数据","9","文件夹","关键","远不如","进度","能","降到","。","epoch","应当","在","axis","该","from","令人","绘制","方根","project","different","测度","or","方差","tools","产生","25","moment","26","给定","集","rmse","alg","?","boldsymbol","意外","损失","这一","colorbar","mean","70","其实","准确","都","39","not","yt","系数","data","draw","就是","biases","key","点","调整","下面",">","是否","saveallresults",")","设","相同","、","format","：","是","如下","重要","由该","特别","31","随机","具有","对比","%","两种","预测值"," ","tests","迅速","列举","width","=","列表","件夹","plot","show","里","即","水平","imshow","自动","23","来","36","数据字","虽然","varepsilon","34","一系","46","noi","c","cb","上","35","formatname",",","以下","关心","print","0.4","结构","会","bar","简单","抑制","0.6",".","大于","datakeys","ylim","~","分布","远远不如","22","训练","不同","sample","一","adamax","record","比较","没有","可见","代码","测试","true","np","对应","png","b","3","title","相当","值","_","后续","i","scandir","k","数据","而","有着","sgd","特地","noise","#","除非","xlabel","同","folder","中","条件","layout","output","效果","29","最为","曲线","以上","那些","could","fileexistserror","none","绝大多数","就","能够","准确度","还","好","gca","有","水准","each","keys","将","；","ε","几乎","interpolation","导致","完整","(","率","机","表明"],"title":"在tools.py中分析比较结果","title_tokens":["py",".","结果","中","比较","tools","分析","在"]}]}