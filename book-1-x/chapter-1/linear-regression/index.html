



<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="本节介绍如何使用类模型(Model)来编写一个线性回归器，以拟合出一个线性模型。本节将第一次介绍如何编写一个带用户参数(选项)的Project，并且允许用户选择不同的优化器、对比验证不同优化算法的效果。">
      
      
        <link rel="canonical" href="https://cainmagi.github.io/tensorflow-guide/book-1-x/chapter-1/linear-regression/">
      
      
        <meta name="author" content="Yuchen Jin (cainmagi)">
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../../../assets/images/icons/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.0.1">
    
    
      
        <title>线性回归 - Tensorflow手札</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/application.982221ab.css">
      
        <link rel="stylesheet" href="../../../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#ff7043">
      
    
    
      <script src="../../../assets/javascripts/modernizr.1f0bcf2b.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif+SC:300,400,400i,600,700,900|Roboto+Mono">
        <style>body,input{font-family:"Noto Serif SC","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../../../stylesheets/main.css">
    
      <link rel="stylesheet" href="../../../stylesheets/extensions.css">
    
      <link rel="stylesheet" href="../../../stylesheets/simpleLightbox.min.css">
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "UA-119875813-2", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="deep-orange" data-md-color-accent="orange">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#_1" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://cainmagi.github.io/tensorflow-guide/" title="Tensorflow手札" class="md-header-nav__button md-logo">
          
            <img src="../../../assets/images/icons/Tensorflow.svg" width="24" height="24">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              Tensorflow手札
            </span>
            <span class="md-header-nav__topic">
              线性回归
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/cainmagi/tensorflow-guide" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    cainmagi/tensorflow-guide
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../.." title="TF 1.x" class="md-tabs__link md-tabs__link--active">
          TF 1.x
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://cainmagi.github.io/tensorflow-guide/" title="Tensorflow手札" class="md-nav__button md-logo">
      
        <img src="../../../assets/images/icons/Tensorflow.svg" width="48" height="48">
      
    </a>
    Tensorflow手札
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/cainmagi/tensorflow-guide" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    cainmagi/tensorflow-guide
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1" checked>
    
    <label class="md-nav__link" for="nav-1">
      TF 1.x
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        TF 1.x
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../.." title="扉页" class="md-nav__link">
      扉页
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1-2" type="checkbox" id="nav-1-2" checked>
    
    <label class="md-nav__link" for="nav-1-2">
      从线性问题入门
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-1-2">
        从线性问题入门
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../" title="本章总说" class="md-nav__link">
      本章总说
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../hello-world/" title="Hello world!" class="md-nav__link">
      Hello world!
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../linear-classification/" title="线性分类" class="md-nav__link">
      线性分类
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        线性回归
      </label>
    
    <a href="./" title="线性回归" class="md-nav__link md-nav__link--active">
      线性回归
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" title="理论" class="md-nav__link">
    理论
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" title="一般回归问题" class="md-nav__link">
    一般回归问题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" title="线性回归" class="md-nav__link">
    线性回归
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" title="求解问题" class="md-nav__link">
    求解问题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" title="优化算法" class="md-nav__link">
    优化算法
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_7" title="引入动量的优化算法" class="md-nav__link">
    引入动量的优化算法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" title="引入可变学习率的优化算法" class="md-nav__link">
    引入可变学习率的优化算法
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_9" title="解线性回归问题" class="md-nav__link">
    解线性回归问题
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_10" title="代码规范" class="md-nav__link">
    代码规范
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" title="扩展模块" class="md-nav__link">
    扩展模块
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#argparse" title="项目选项：argparse" class="md-nav__link">
    项目选项：argparse
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" title="数据生成" class="md-nav__link">
    数据生成
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" title="定义类模型" class="md-nav__link">
    定义类模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_14" title="初始化方法" class="md-nav__link">
    初始化方法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" title="构造方法" class="md-nav__link">
    构造方法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" title="训练和测试方法" class="md-nav__link">
    训练和测试方法
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" title="调试" class="md-nav__link">
    调试
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_18" title="使实验结果可复现" class="md-nav__link">
    使实验结果可复现
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" title="使实验代码保存输出" class="md-nav__link">
    使实验代码保存输出
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#toolspy" title="在tools.py中分析比较结果" class="md-nav__link">
    在tools.py中分析比较结果
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
        <li class="md-nav__item">
          <a href="#__source" title="来源" class="md-nav__link md-nav__link--active">
            来源
          </a>
        </li>
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">
            评论
          </a>
        </li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../nonlinear-regression/" title="非线性回归" class="md-nav__link">
      非线性回归
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../release-notes/" title="更新记录" class="md-nav__link">
      更新记录
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../licenses/" title="协议" class="md-nav__link">
      协议
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" title="理论" class="md-nav__link">
    理论
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" title="一般回归问题" class="md-nav__link">
    一般回归问题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" title="线性回归" class="md-nav__link">
    线性回归
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" title="求解问题" class="md-nav__link">
    求解问题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" title="优化算法" class="md-nav__link">
    优化算法
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_7" title="引入动量的优化算法" class="md-nav__link">
    引入动量的优化算法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" title="引入可变学习率的优化算法" class="md-nav__link">
    引入可变学习率的优化算法
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_9" title="解线性回归问题" class="md-nav__link">
    解线性回归问题
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_10" title="代码规范" class="md-nav__link">
    代码规范
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" title="扩展模块" class="md-nav__link">
    扩展模块
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#argparse" title="项目选项：argparse" class="md-nav__link">
    项目选项：argparse
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" title="数据生成" class="md-nav__link">
    数据生成
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" title="定义类模型" class="md-nav__link">
    定义类模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_14" title="初始化方法" class="md-nav__link">
    初始化方法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" title="构造方法" class="md-nav__link">
    构造方法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" title="训练和测试方法" class="md-nav__link">
    训练和测试方法
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" title="调试" class="md-nav__link">
    调试
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_18" title="使实验结果可复现" class="md-nav__link">
    使实验结果可复现
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" title="使实验代码保存输出" class="md-nav__link">
    使实验代码保存输出
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#toolspy" title="在tools.py中分析比较结果" class="md-nav__link">
    在tools.py中分析比较结果
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
        <li class="md-nav__item">
          <a href="#__source" title="来源" class="md-nav__link md-nav__link--active">
            来源
          </a>
        </li>
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">
            评论
          </a>
        </li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="_1">线性回归<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<div class="admonition abstract">
<p class="admonition-title">摘要</p>
<p>本节介绍如何使用类模型(Model)来编写一个线性回归器，以拟合出一个线性模型。本节将第一次介绍如何编写一个带用户参数(选项)的Project，并且允许用户选择不同的优化器、对比验证不同优化算法的效果。</p>
</div>
<h2 id="_2">理论<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<h3 id="_3">一般回归问题<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<p>设存在一个多输出方程<span><span class="MathJax_Preview">\mathbf{y} = \mathcal{F}(x)</span><script type="math/tex">\mathbf{y} = \mathcal{F}(x)</script></span>，当然<span><span class="MathJax_Preview">\mathcal{F}</span><script type="math/tex">\mathcal{F}</script></span>可以是非线性函数，那么我们可以考虑使用一个带可调参数的模型<span><span class="MathJax_Preview">\mathbf{D}_{\boldsymbol{\Theta}}(\mathbf{x})</span><script type="math/tex">\mathbf{D}_{\boldsymbol{\Theta}}(\mathbf{x})</script></span>来模拟它，其中<span><span class="MathJax_Preview">\boldsymbol{\Theta}</span><script type="math/tex">\boldsymbol{\Theta}</script></span>是可调的参数。于是，该问题可以被表述为</p>
<div class="overflow">\begin{equation}
    \begin{aligned}
        \arg \min_\limits{\boldsymbol{\Theta}} &amp;\sum_{k=1}^N \mathcal{L} \left( \mathbf{y}_k,~ \mathbf{D}_{\boldsymbol{\Theta}}(\mathbf{x}_k) \right),\\
        \mathrm{s.t.}~&amp;\mathbf{y}_k = \mathcal{F}(\mathbf{x}_k).
    \end{aligned}
\end{equation}</div>

<p>在我们不知道<span><span class="MathJax_Preview">\mathcal{F}</span><script type="math/tex">\mathcal{F}</script></span>的情况下，我们的目的是使用大量的<span><span class="MathJax_Preview">\mathbf{x}_k,~\mathbf{y}_k</span><script type="math/tex">\mathbf{x}_k,~\mathbf{y}_k</script></span>样本，来调整出一个最优的近似模型<span><span class="MathJax_Preview">\mathbf{D}_{\boldsymbol{\Theta}}</span><script type="math/tex">\mathbf{D}_{\boldsymbol{\Theta}}</script></span>。由于<span><span class="MathJax_Preview">\mathcal{F}</span><script type="math/tex">\mathcal{F}</script></span>是非线性的，这要求我们的<span><span class="MathJax_Preview">\mathbf{D}_{\boldsymbol{\Theta}}</span><script type="math/tex">\mathbf{D}_{\boldsymbol{\Theta}}</script></span>也可以是非线性的。实际情况下，这样的问题往往不容易求解，尤其是信号的非线性性极强时，该问题很容易陷入局部最优解，从而对求得一个可接受的解造成很大的障碍。</p>
<p>这里<span><span class="MathJax_Preview">\mathcal{L}</span><script type="math/tex">\mathcal{L}</script></span>是损失函数。在回归问题中，很多情况下我们都只能选择<span></span><strong>均方误差 (Mean squared error, MSE)</strong><span></span>作为损失函数，这是因为回归问题的目的是模拟出一组信号来，而这些信号的分布范围可能是任意的。在一些特别的应用里，例如，如果我们的信号全部为正值，那么我们可以考虑使用<span></span><strong>信噪比 (Signal-to-noise ratio, SNR)</strong><span></span>来作为我们的损失函数。</p>
<h3 id="_4">线性回归<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h3>
<p>继上一节的学习，我们知道如何解一个定义为</p>
<div class="overflow">\begin{align}
   \mathbf{y} = \left\{ \begin{bmatrix}\hat{y}_1 &gt; 0 &amp; \hat{y}_2 &gt; 0 &amp; \cdots &amp; \hat{y}_n &gt; 0\end{bmatrix}^T, ~ \left| ~ \hat{\mathbf{y}} = \mathbf{A} \mathbf{x} + \mathbf{c} + \boldsymbol{\varepsilon} \right. \right\},
\end{align}</div>

<p>的分类模型。在本节，让我们考虑一个更简单的模型：</p>
<div class="overflow">\begin{align}
   \mathbf{y} = \mathbf{A} \mathbf{x} + \mathbf{c} + \boldsymbol{\varepsilon}.
\end{align}</div>

<p>现在，<span><span class="MathJax_Preview">\mathbf{y}</span><script type="math/tex">\mathbf{y}</script></span>是关乎<span><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>和<span><span class="MathJax_Preview">\mathbf{c}</span><script type="math/tex">\mathbf{c}</script></span>的一个仿射函数，并且我们仍然保留噪声函数<span><span class="MathJax_Preview">\boldsymbol{\varepsilon}</span><script type="math/tex">\boldsymbol{\varepsilon}</script></span>。由于这是一个线性模型，我们可以想象到，存在一个线性回归器，<span><span class="MathJax_Preview">\mathbf{W},~\mathbf{b}</span><script type="math/tex">\mathbf{W},~\mathbf{b}</script></span>，使得预测结果为</p>
<div class="overflow">\begin{align}
   \tilde{\mathbf{y}} = \mathbf{W} \mathbf{x} + \mathbf{b}.
\end{align}</div>

<p>类似上一节，假设我们的数据集<span><span class="MathJax_Preview">(\mathbf{x},~\mathbf{y}) \in \mathbb{D}</span><script type="math/tex">(\mathbf{x},~\mathbf{y}) \in \mathbb{D}</script></span>符合<a href="#mjx-eqn-3"><span><span class="MathJax_Preview">(3)</span><script type="math/tex">(3)</script></span></a>定义的数据分布特征。我们的基本要求是，在我们不知道<span><span class="MathJax_Preview">\mathbf{A},~\mathbf{c}</span><script type="math/tex">\mathbf{A},~\mathbf{c}</script></span>的情况下，使用大量<span><span class="MathJax_Preview">(\mathbf{x}_k,~\mathbf{y}_k) \in \mathbb{D}</span><script type="math/tex">(\mathbf{x}_k,~\mathbf{y}_k) \in \mathbb{D}</script></span>样本训练一个线性分类器，使得当我们给定任意一个新样本<span><span class="MathJax_Preview">\mathbf{x}</span><script type="math/tex">\mathbf{x}</script></span>的时候，分类器能推断出其对应的<span><span class="MathJax_Preview">\mathbf{y}</span><script type="math/tex">\mathbf{y}</script></span>。</p>
<p>故而，该问题可以描述为</p>
<div class="overflow">\begin{equation}
    \begin{aligned}
        \arg \min_\limits{\mathbf{W},~\mathbf{b}} &amp;\sum_{k=1}^N \mathcal{L} \left( \mathbf{y}_k,~ \mathbf{W} \mathbf{x}_k + \mathbf{b} \right), \\
        \mathcal{L} \left( \mathbf{y},~ \tilde{\mathbf{y}} \right) &amp;= \lVert \mathbf{y} - \tilde{\mathbf{y}} \rVert_2^2.
    \end{aligned}
\end{equation}</div>

<p>在本例中，<span><span class="MathJax_Preview">\mathbf{y}</span><script type="math/tex">\mathbf{y}</script></span>有正有负，因此我们使用均方误差来作为损失函数。</p>
<h3 id="_5">求解问题<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<p>作为线性问题，该问题实际上可以写出其解析解。未免读者感到过于突兀，我们先从一个简单的问题开始入手：</p>
<div class="admonition example">
<p class="admonition-title">例子：一次函数的线性回归</p>
<p><div class='fl-right' style='width:40%'>
<img alt="一次函数线性回归" class="img-fluid" src="../../../assets/images/book-1-x/linreg-lsq.svg" tag="1" title="一次函数线性回归 (图出典自<a href='https://zh.wikipedia.org/wiki/線性回歸'><em>Wiki: 线性回归</em></a>)" />
</div></p>
<p>如果我们的矩阵<span><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>退化为标量<span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>，向量<span><span class="MathJax_Preview">\mathbf{c}</span><script type="math/tex">\mathbf{c}</script></span>退化为标量c，那么<a href="#mjx-eqn-3"><span><span class="MathJax_Preview">(3)</span><script type="math/tex">(3)</script></span></a>可以重新写为：</p>
<div class="overflow">\begin{align}
    y = a x + c + \varepsilon.
\end{align}</div>

<p>考虑我们拥有N个样本点<span><span class="MathJax_Preview">(x_k,~y_k)</span><script type="math/tex">(x_k,~y_k)</script></span>，上述问题实际上可以求得解析解。设由这N个点构成了样本向量<span><span class="MathJax_Preview">\mathbf{x}_d,~\mathbf{y}_d</span><script type="math/tex">\mathbf{x}_d,~\mathbf{y}_d</script></span> (注意与前述的向量区分开来)，则问题可以写成</p>
<div class="overflow">\begin{align}
    \arg \min_\limits{a,~c} \lVert \mathbf{y}_d - a \mathbf{x}_d - c \mathbf{1} \rVert^2_2.
\end{align}</div>

<p>这就是附图所示的，拟合到直线的一次函数回归问题。将该损失函数展开，有</p>
<div class="overflow">\begin{equation}
    \begin{aligned}
        \mathcal{L}(a,~c) &amp;= ( \mathbf{y}_d - a \mathbf{x}_d - c \mathbf{1} )^T ( \mathbf{y}_d - a \mathbf{x}_d - c \mathbf{1} )\\
        &amp;= \mathbf{y}_d^T\mathbf{y}_d + a^2 \mathbf{x}_d^T \mathbf{x}_d + c^2 + 2ac \mathbf{1}^T \mathbf{x}_d - 2 a \mathbf{y}_d^T \mathbf{x}_d - 2c \mathbf{1}^T \mathbf{y}_d.
    \end{aligned}
\end{equation}</div>

<p>令<span><span class="MathJax_Preview">\dfrac{\partial \mathcal{L}(a,~c)}{\partial a}=0,~\dfrac{\partial \mathcal{L}(a,~c)}{\partial c}=0</span><script type="math/tex">\dfrac{\partial \mathcal{L}(a,~c)}{\partial a}=0,~\dfrac{\partial \mathcal{L}(a,~c)}{\partial c}=0</script></span>，则我们得到一组二元一次方程组</p>
<div class="overflow">\begin{equation}
    \left\{
    \begin{aligned}
        a \mathbf{x}_d^T \mathbf{x}_d + c \mathbf{1}^T \mathbf{x}_d &amp;= \mathbf{y}_d^T \mathbf{x}_d. \\
        c + a \mathbf{1}^T \mathbf{x}_d &amp;= \mathbf{1}^T \mathbf{y}_d.
    \end{aligned}
    \right.
\end{equation}</div>

<p>解之，得</p>
<div class="overflow">\begin{equation}
    \left\{
    \begin{aligned}
        a &amp;= \frac{ \mathbf{y}_d^T \mathbf{x}_d - ( \mathbf{1}^T \mathbf{y}_d ) ( \mathbf{1}^T \mathbf{x}_d ) }{ \mathbf{x}_d^T \mathbf{x}_d - (\mathbf{1}^T \mathbf{x}_d)^2 } = \frac{ \sum_k x_k y_k - \sum_k y_k \sum_k x_k }{ \sum_k (x_k)^2 - \left(\sum_k x_k\right)^2 }. \\
        c &amp;= \mathbf{1}^T \mathbf{y}_d - a ( \mathbf{1}^T \mathbf{x}_d ) = \sum_k y_k - a \left( \sum_k x_k \right).
    \end{aligned}
    \right.
\end{equation}</div>

<p>这个式子在诸多教材上都会出现，作为学生解回归问题的入门话题。可见，我们在本节讨论的问题并不是一个陌生的问题，相反，我们过去非常熟悉的一个问题，是这个问题的退化到标量下的特殊情况。另，计算该问题的相关系数，我们常使用</p>
<div class="overflow">\begin{align}
    \rho = \frac{ \sum_k \left(x_k - \overline{x}\right) \left(y_k - \overline{y}\right) }{ \sqrt{ \sum_k \left(x_k - \overline{x}\right)^2 \sum_k \left(y_k - \overline{y}\right)^2 } },
\end{align}</div>

<p>其中<span><span class="MathJax_Preview">\overline{x} = \sum_k x_k ,~ \overline{y} = \sum_k y_k</span><script type="math/tex">\overline{x} = \sum_k x_k ,~ \overline{y} = \sum_k y_k</script></span>。</p>
</div>
<p>有了解上述例子的基础，我们自然可以写出，</p>
<div class="overflow">\begin{equation}
    \begin{aligned}
        \mathcal{L}(\mathbf{A},~\mathbf{c}) &amp;= \sum_k ( \mathbf{y}_k - \mathbf{A} \mathbf{x}_k - \mathbf{c} )^T ( \mathbf{y}_k - \mathbf{A} \mathbf{x}_k - \mathbf{c} )\\
        &amp;= \sum_k \left[ \mathbf{y}_k^T\mathbf{y}_k + \mathbf{x}_k^T \mathbf{A}^T\mathbf{A} \mathbf{x}_k + \mathbf{c}^T \mathbf{c} + 2 \mathbf{c}^T \mathbf{A} \mathbf{x}_k - 2 \mathbf{y}_k^T \mathbf{A} \mathbf{x}_k - 2 \mathbf{y}_k^T \mathbf{c} \right].
    \end{aligned}
\end{equation}</div>

<details class="tip" open="open"><summary>提示</summary><p>接下来的求导主要涉及单值对矩阵求导（导数仍是矩阵），单值对向量求导（导数仍是向量）。可以参考<a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a>查到对应情况下的求导结果。</p>
</details>
<p>同理，令<span><span class="MathJax_Preview">\dfrac{\partial \mathcal{L}(\mathbf{A},~\mathbf{c})}{\partial \mathbf{A}}=0,~\dfrac{\partial \mathcal{L}(\mathbf{A},~\mathbf{c})}{\partial \mathbf{c}}=0</span><script type="math/tex">\dfrac{\partial \mathcal{L}(\mathbf{A},~\mathbf{c})}{\partial \mathbf{A}}=0,~\dfrac{\partial \mathcal{L}(\mathbf{A},~\mathbf{c})}{\partial \mathbf{c}}=0</script></span>，则我们得到一组二元一次方程组</p>
<div class="overflow">\begin{equation}
    \left\{
    \begin{aligned}
        \sum_k \left[ \mathbf{A} \mathbf{x}_k \mathbf{x}_k^T + \mathbf{c} \mathbf{x}_k^T \right] &amp;= \sum_k \left[ \mathbf{y}_k \mathbf{x}_k^T \right]. \\
        \sum_k \left[ \mathbf{c} + \mathbf{A} \mathbf{x}_k \right] &amp;= \sum_k \left[ \mathbf{y}_k \right].
    \end{aligned}
    \right.
\end{equation}</div>

<p>解之，得</p>
<div class="overflow">\begin{equation}
    \left\{
    \begin{aligned}
        \mathbf{A} &amp;= \left[ N \sum_k \left[ \mathbf{y}_k \mathbf{x}_k^T \right] - \sum_k \left[ \mathbf{y}_k \right] \sum_k \left[ \mathbf{x}_k^T \right] \right] \left[ N \sum_k \left[ \mathbf{x}_k \mathbf{x}_k^T \right] - \sum_k \left[ \mathbf{x}_k \right] \sum_k \left[ \mathbf{x}_k^T \right] \right]^{-1} \\
        \mathbf{c} &amp;= \frac{1}{N} \sum_k \left[ \mathbf{y}_k - \mathbf{A} \mathbf{x}_k \right].
    \end{aligned}
    \right.
\end{equation}</div>

<p>可见，当上式中的逆不存在时（即低秩的情况），该方程还是有可能解不唯一。</p>
<p>同时，相关系数的计算可以表示为</p>
<div class="overflow">\begin{align}
    \rho = \mathrm{mean} \left[ \frac{ \sum_k \left(\mathbf{x}_k - \overline{\mathbf{x}}\right) \cdot \left(\mathbf{y}_k - \overline{\mathbf{y}}\right) }{ \sqrt{ \sum_k \left[ \left(\mathbf{x}_k - \overline{\mathbf{x}}\right) \cdot \left(\mathbf{x}_k - \overline{\mathbf{x}}\right) \right] \sum_k \left[ \left(\mathbf{y}_k - \overline{\mathbf{y}}\right) \cdot \left(\mathbf{y}_k - \overline{\mathbf{y}}\right) \right] } } \right].
\end{align}</div>

<p>这就是<a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient"><span></span><strong>皮尔森相关系数 (Pearson's correlation)</strong><span></span></a>。其中<span><span class="MathJax_Preview">\overline{\mathbf{x}} = \sum_k \mathbf{x}_k ,~ \overline{\mathbf{y}} = \sum_k \mathbf{y}_k</span><script type="math/tex">\overline{\mathbf{x}} = \sum_k \mathbf{x}_k ,~ \overline{\mathbf{y}} = \sum_k \mathbf{y}_k</script></span>，<span><span class="MathJax_Preview">\cdot</span><script type="math/tex">\cdot</script></span>表示的是两个向量按元素各自相乘。它是式<a href="#mjx-eqn-11"><span><span class="MathJax_Preview">(11)</span><script type="math/tex">(11)</script></span></a>在多变量问题上的推广。相当于对向量的每一个元素，分别从统计上求取皮尔森相关系数，然后对向量每个元素对应的皮尔森相关系数求取平均值。</p>
<h3 id="_6">优化算法<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h3>
<p>接下来，我们要介绍几种最常见的优化算法。关于更多这方面的内容，可以查考Google团队编写的在线电子书<a href="https://www.deeplearningbook.org/"><em>Deep Learning</em></a>。笔者打算在未来为此开辟专题写文，因此这里只是介绍几种常见的<span></span><strong>一阶梯度下降</strong><span></span>算法。传统优化领域里，单靠一阶梯度下降往往难以满足对准确度的需求，但深度学习(Deep learning)往往必须使用这些简单的一阶梯度下降算法，就连使用一阶梯度近似二阶梯度的算法<span></span><strong>共轭梯度下降</strong><span></span>，在很多情况下都被认为是费用(cost)过高。这是由于一个深度网络，往往具有大量的参数需要训练，因此一个Model的参数少则数十MB，多则上GB。一阶梯度下降算法所需的计算量小，能确保我们一次迭代的过程能迅速完成，因而备受青睐。为了提升其性能，深度学习领域内也对其进行了诸多改进。</p>
<details class="warning" open="open"><summary>注意</summary><p>其实，论到优化算法，往往不得不提到<span></span><strong>反向传播</strong><span></span>。不过实际上，一个Tensorflow的入门者，其实完全不需要学习如何推导反向传播的过程。下面我们的叙述也完全不会提及反向传播相关的内容。关于为何我们不需要了解反向传播，在下一节我们会论到。但是，在本教程后期，介绍高级技巧的时候，我们会详细展开。事实上，笔者认为，一个Tensorflow的用户，如果只是为了编写代码，反向传播与ta其实无关痛痒；但只有真正掌握反向传播，我们才算是真正入门了神经网络的理论。</p>
</details>
<p>我们在这里说到优化算法，是用在训练网络上的。事实上，只有几种个别的机器学习应用，需要我们在测试阶段执行<span></span><strong>迭代算法 (iterative algorithm)</strong><span></span>。一般来说，深度学习的训练过程可以被普遍地描述为：已知一个带可调参数<span><span class="MathJax_Preview">\boldsymbol{\Theta}</span><script type="math/tex">\boldsymbol{\Theta}</script></span>的模型<span><span class="MathJax_Preview">\mathcal{D}_{\boldsymbol{\Theta}}</span><script type="math/tex">\mathcal{D}_{\boldsymbol{\Theta}}</script></span>，已知一组数据集<span><span class="MathJax_Preview">(\mathbf{x}_i,~\mathbf{y}_i) \in \mathbb{D}</span><script type="math/tex">(\mathbf{x}_i,~\mathbf{y}_i) \in \mathbb{D}</script></span>，则我们的训练目标为</p>
<div class="overflow">\begin{equation}
    \begin{aligned}
        \arg \min_\limits{\boldsymbol{\Theta}} \mathbb{E}_{(\mathbf{x}_i,~\mathbf{y}_i) \in \mathbb{D}} \left[ \mathcal{L} \left( \mathbf{y}_i,~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}_i) \right) \right].
    \end{aligned}
\end{equation}</div>

<p>实际情况下，一般用均值估计来代替上式的期望函数。联系我们上一节的优化问题<a href="#mjx-eqn-1"><span><span class="MathJax_Preview">(1)</span><script type="math/tex">(1)</script></span></a>和本节的优化问题<a href="#mjx-eqn-5"><span><span class="MathJax_Preview">(5)</span><script type="math/tex">(5)</script></span></a>，都可以描述成上式的形式。也就是说，线性分类/回归器，是神经网络在解线性问题时的特例。</p>
<h4 id="_7">引入动量的优化算法<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h4>
<p>接下来，让我们看看第一个算法，<span></span><strong>随机梯度下降 (stochastic gradient descent, SGD)</strong><span></span>。</p>
<div class="admonition note">
<p class="admonition-title">随机梯度下降</p>
<dl>
<dt>记学习率为<span><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span>，则在每次迭代中</dt>
<dd>
<ol>
<li>随机抽取（或从随机排列的数据集中按顺序抽取）m个样本<span><span class="MathJax_Preview">(\mathbf{x}_k,~\mathbf{y}_k)</span><script type="math/tex">(\mathbf{x}_k,~\mathbf{y}_k)</script></span>，称这m个样本为一个batch；</li>
<li>计算梯度<span><span class="MathJax_Preview">\mathbf{g} = \frac{1}{m} \nabla_{\boldsymbol{\Theta}} \sum\limits_{k=1}^m \mathcal{L} \left( \mathbf{y}_k,~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}_k) \right)</span><script type="math/tex">\mathbf{g} = \frac{1}{m} \nabla_{\boldsymbol{\Theta}} \sum\limits_{k=1}^m \mathcal{L} \left( \mathbf{y}_k,~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}_k) \right)</script></span>；</li>
<li>更新参数<span><span class="MathJax_Preview">\boldsymbol{\Theta} \leftarrow \boldsymbol{\Theta} - \epsilon \mathbf{g}</span><script type="math/tex">\boldsymbol{\Theta} \leftarrow \boldsymbol{\Theta} - \epsilon \mathbf{g}</script></span>。</li>
</ol>
</dd>
</dl>
<p>注意学习率一般需要设为一个较小的值，视情况而定。</p>
<p>由于梯度的期望满足</p>
<div class="overflow">\begin{equation}
    \begin{aligned}
        \mathbb{E} \left[ \mathbf{g} \right] &amp;= \frac{1}{m} \sum\limits_{k=1}^m \mathbb{E} \left[ \nabla_{\boldsymbol{\Theta}} \mathcal{L} \left( \mathbf{y}_k,~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}_k) \right) \right] \\
        &amp;= \mathbb{E} \left[ \nabla_{\boldsymbol{\Theta}} \mathcal{L} \left( \mathbf{y},~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}) \right) \right] = \nabla_{\boldsymbol{\Theta}} \mathbb{E} \left[ \mathcal{L} \left( \mathbf{y},~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}) \right) \right].
    \end{aligned}
\end{equation}</div>

<p>可知随机抽取m个样本计算的梯度，在统计学上的期望等于全局梯度的期望。因此，这是一个有效的算法。</p>
</div>
<p>随机梯度下降存在明显的弊端，就是在收敛到（全局或局部）最优解的前提下，全局梯度为0，但通过随机选取batch得到的梯度（一般）可能不为0；并且，迭代受到个别极端样本梯度的影响较大，因此，我们有了第一个改进，即<span></span><strong>带动量的随机梯度下降 (SGD with momentum)</strong><span></span>。</p>
<div class="admonition note">
<p class="admonition-title">带动量的随机梯度下降</p>
<details class="info"><summary>参考文献</summary><p>提出该算法的文章，可以在这里参考：</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608098001166">On the momentum term in gradient descent learning algorithms. Neural Networks</a></p>
</details>
<dl>
<dt>记学习率为<span><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span>，惯性常数为<span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>，初始化动量<span><span class="MathJax_Preview">\mathbf{v}=\mathbf{v}_0</span><script type="math/tex">\mathbf{v}=\mathbf{v}_0</script></span>（不考虑继续训练的情况下<span><span class="MathJax_Preview">\mathbf{v}_0 = \mathbf{0}</span><script type="math/tex">\mathbf{v}_0 = \mathbf{0}</script></span>），则在每次迭代中</dt>
<dd>
<ol>
<li>随机抽取（或从随机排列的数据集中按顺序抽取）m个样本<span><span class="MathJax_Preview">(\mathbf{x}_k,~\mathbf{y}_k)</span><script type="math/tex">(\mathbf{x}_k,~\mathbf{y}_k)</script></span>，称这m个样本为一个batch；</li>
<li>计算梯度<span><span class="MathJax_Preview">\mathbf{g} = \frac{1}{m} \nabla_{\boldsymbol{\Theta}} \sum\limits_{k=1}^m \mathcal{L} \left( \mathbf{y}_k,~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}_k) \right)</span><script type="math/tex">\mathbf{g} = \frac{1}{m} \nabla_{\boldsymbol{\Theta}} \sum\limits_{k=1}^m \mathcal{L} \left( \mathbf{y}_k,~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}_k) \right)</script></span>；</li>
<li>计算带动量的更新值<span><span class="MathJax_Preview">\mathbf{v} \rightarrow \alpha \mathbf{v} - \epsilon \mathbf{g}</span><script type="math/tex">\mathbf{v} \rightarrow \alpha \mathbf{v} - \epsilon \mathbf{g}</script></span>；</li>
<li>更新参数<span><span class="MathJax_Preview">\boldsymbol{\Theta} \leftarrow \boldsymbol{\Theta} + \mathbf{v}</span><script type="math/tex">\boldsymbol{\Theta} \leftarrow \boldsymbol{\Theta} + \mathbf{v}</script></span>。</li>
</ol>
</dd>
</dl>
<p><div class='fl-right' style='width:40%'>
<img alt="带动量的随机梯度下降" class="img-fluid" src="../../../assets/images/book-1-x/linreg-moment-grad.png" tag="2" title="带动量的随机梯度下降 (图出典自<a href='https://www.deeplearningbook.org/'><em>Deep Learning</em></a>)" />
</div></p>
<p>显然，我们不难计算出，</p>
<div class="overflow">\begin{equation}
    \begin{aligned}
        \mathbb{E} \left[ \mathbf{v} \right] &amp;= \alpha \mathbb{E} \left[ \mathbf{v} \right] - \epsilon \mathbb{E} \left[ \mathbf{g} \right]. \\
        \mathbb{E} \left[ \mathbf{v} \right] &amp;= - \frac{\epsilon}{1 - \alpha} \mathbb{E} \left[ \mathbf{g} \right].
    \end{aligned}
\end{equation}</div>

<p>注意惯性通常需要设为<span><span class="MathJax_Preview">\alpha \in (0,~1)</span><script type="math/tex">\alpha \in (0,~1)</script></span>。</p>
<p>这种改进的带来的好处是，</p>
<ul>
<li>每次更新梯度时，上一次的梯度都会以指数衰减的形式残留在本次迭代中，从而确保新的梯度会被旧的梯度部分中和，避免极端梯度对更新参数影响过大；</li>
<li>当求解得到的梯度陷入局部最优时，如果该局部最优处的曲率较小，可以依靠动量的惯性，越过该局部最优解。</li>
</ul>
<p>附图说明了使用这种算法的好处。黑色路径为SGD的更新轨迹，而红色路径为本算法的更新轨迹，可以看出随着迭代次数的增加，算法收敛的效果强于SGD。</p>
</div>
<p>有人从Nesterov在1983年的论文得到启发，提出了一个修正后的带动量随机梯度下降法，即<span></span><strong>带Nesterov动量的随机梯度下降 (SGD with Nesterov momentum)</strong><span></span>。</p>
<div class="admonition note">
<p class="admonition-title">带Nesterov动量的随机梯度下降</p>
<details class="info"><summary>参考文献</summary><p>提出该算法的文章，可以在这里参考：</p>
<p><a href="http://mpawankumar.info/teaching/cdt-big-data/nesterov83.pdf">A method for unconstrained convex minimization problem with the rate of convergence <span><span class="MathJax_Preview">o\left( \frac{1}{k_2} \right)</span><script type="math/tex">o\left( \frac{1}{k_2} \right)</script></span></a></p>
</details>
<dl>
<dt>记学习率为<span><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span>，惯性常数为<span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>，初始化动量<span><span class="MathJax_Preview">\mathbf{v}=\mathbf{v}_0</span><script type="math/tex">\mathbf{v}=\mathbf{v}_0</script></span>（不考虑继续训练的情况下<span><span class="MathJax_Preview">\mathbf{v}_0 = \mathbf{0}</span><script type="math/tex">\mathbf{v}_0 = \mathbf{0}</script></span>），则在每次迭代中</dt>
<dd>
<ol>
<li>随机抽取（或从随机排列的数据集中按顺序抽取）m个样本<span><span class="MathJax_Preview">(\mathbf{x}_k,~\mathbf{y}_k)</span><script type="math/tex">(\mathbf{x}_k,~\mathbf{y}_k)</script></span>，称这m个样本为一个batch；</li>
<li>计算惯性目标点的位置：<span><span class="MathJax_Preview">\boldsymbol{\Theta}^{\dagger} \leftarrow \boldsymbol{\Theta} + \alpha \mathbf{v}</span><script type="math/tex">\boldsymbol{\Theta}^{\dagger} \leftarrow \boldsymbol{\Theta} + \alpha \mathbf{v}</script></span>；</li>
<li>计算梯度<span><span class="MathJax_Preview">\mathbf{g} = \frac{1}{m} \nabla_{\boldsymbol{\Theta}} \sum\limits_{k=1}^m \mathcal{L} \left( \mathbf{y}_k,~ \mathcal{D}_{\boldsymbol{\Theta}^{\dagger}} (\mathbf{x}_k) \right)</span><script type="math/tex">\mathbf{g} = \frac{1}{m} \nabla_{\boldsymbol{\Theta}} \sum\limits_{k=1}^m \mathcal{L} \left( \mathbf{y}_k,~ \mathcal{D}_{\boldsymbol{\Theta}^{\dagger}} (\mathbf{x}_k) \right)</script></span>；</li>
<li>计算带动量的更新值<span><span class="MathJax_Preview">\mathbf{v} \rightarrow \alpha \mathbf{v} - \epsilon \mathbf{g}</span><script type="math/tex">\mathbf{v} \rightarrow \alpha \mathbf{v} - \epsilon \mathbf{g}</script></span>；</li>
<li>更新参数<span><span class="MathJax_Preview">\boldsymbol{\Theta} \leftarrow \boldsymbol{\Theta}^{\dagger} - \epsilon \mathbf{g}</span><script type="math/tex">\boldsymbol{\Theta} \leftarrow \boldsymbol{\Theta}^{\dagger} - \epsilon \mathbf{g}</script></span>。</li>
</ol>
</dd>
</dl>
<p>其实，该方法的更新量期望与前一种方法一样，</p>
<p>显然，我们不难计算出，</p>
<div class="overflow">\begin{equation}
    \begin{aligned}
        \mathbb{E} \left[ \mathbf{v} \right] &amp;= - \frac{\epsilon}{1 - \alpha} \mathbb{E} \left[ \mathbf{g} \right]\\
        &amp;= \frac{\epsilon}{1 - \alpha} \nabla_{\boldsymbol{\Theta}} \mathbb{E} \left[ \mathcal{L} \left( \mathbf{y},~ \mathcal{D}_{\boldsymbol{\Theta} + \alpha \mathbf{v}} (\mathbf{x}) \right) \right].
    \end{aligned}
\end{equation}</div>

<p>当收敛到最优解时，<span><span class="MathJax_Preview">\mathbf{v} \rightarrow 0</span><script type="math/tex">\mathbf{v} \rightarrow 0</script></span>，同时有<span><span class="MathJax_Preview">\boldsymbol{\Theta} + \alpha \mathbf{v} \rightarrow \boldsymbol{\Theta}</span><script type="math/tex">\boldsymbol{\Theta} + \alpha \mathbf{v} \rightarrow \boldsymbol{\Theta}</script></span>。我们在此不展开证明这个算法是能收敛的。但Nesterov的文献表明，它能将上面提到的带动量梯度下降算法的误差从<span><span class="MathJax_Preview">O\left(\frac{1}{K}\right)</span><script type="math/tex">O\left(\frac{1}{K}\right)</script></span>下降到<span><span class="MathJax_Preview">O\left(\frac{1}{K^2}\right)</span><script type="math/tex">O\left(\frac{1}{K^2}\right)</script></span>。其中<span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>为迭代次数。下图展示了这种方法的改进原理。它的梯度是在更新动量的惯性部分之后才计算出来的，因此新的梯度和之前的惯性是首尾相接的。实际实现时，按照上面的算法，每次迭代需要更新两次参数，计算一次梯度。合理调整算法的计算次序，可以改进为每次迭代更新一次参数，计算一次梯度。</p>
<p><div style='margin:auto;width:70%'>
<img alt="Nesterov动量的合理性" class="img-fluid" src="../../../assets/images/book-1-x/linreg-nest-moment-grad.png" tag="3" title="Nesterov动量的合理性 (图出典自<a href='http://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_pauses.pdf'><em>Optimization for Deep Neural Networks</em></a>)" />
</div></p>
</div>
<h4 id="_8">引入可变学习率的优化算法<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h4>
<p>上述几种算法共同的特点是，具有一个“学习率”。实际上，这个学习率非常不好处理，值过小时，收敛速度很慢；值过大时，在最优解附近又难以收敛。为了解决这一思路，我们可以令学习率可变。最简单的思路是，将学习率设为指数衰减的（当然也可以设置下界），这样当开始学习的时候，学习率较大；而即将收敛时，学习率又会较小。</p>
<p>但是，以上做法不过是一些小小的花招(trick)罢了，接下来介绍的几种算法，是根据当前计算出的梯度来自适应调整学习率的。理论上，使用这种算法，用户不再需要特别关注学习率对训练的影响，我们尽可以设置一个偏大的学习率，在训练过程中，它能被自适应调整到一个合适的区间上。</p>
<p>首先，我们来介绍一种初步的改进，<span></span><strong>Adagrad (Adaptive Subgradient)</strong><span></span>，</p>
<div class="admonition note">
<p class="admonition-title">Adgrad</p>
<details class="info"><summary>参考文献</summary><p>提出该算法的文章，可以在这里参考：</p>
<p><a href="http://jmlr.org/papers/v12/duchi11a.html">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></p>
</details>
<dl>
<dt>记学习率为<span><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span>，小量<span><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span>，初始化学习率参数对角矩阵为<span><span class="MathJax_Preview">\mathbf{r} = \mathrm{diag}(\mathbf{0})</span><script type="math/tex">\mathbf{r} = \mathrm{diag}(\mathbf{0})</script></span>，则在每次迭代中</dt>
<dd>
<ol>
<li>随机抽取（或从随机排列的数据集中按顺序抽取）m个样本<span><span class="MathJax_Preview">(\mathbf{x}_k,~\mathbf{y}_k)</span><script type="math/tex">(\mathbf{x}_k,~\mathbf{y}_k)</script></span>，称这m个样本为一个batch；</li>
<li>计算梯度<span><span class="MathJax_Preview">\mathbf{g} = \frac{1}{m} \nabla_{\boldsymbol{\Theta}} \sum\limits_{k=1}^m \mathcal{L} \left( \mathbf{y}_k,~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}_k) \right)</span><script type="math/tex">\mathbf{g} = \frac{1}{m} \nabla_{\boldsymbol{\Theta}} \sum\limits_{k=1}^m \mathcal{L} \left( \mathbf{y}_k,~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}_k) \right)</script></span>；</li>
<li>更新学习率为：<span><span class="MathJax_Preview">\mathbf{r} \leftarrow \mathbf{r} + \mathrm{diag}(\mathbf{g})^2</span><script type="math/tex">\mathbf{r} \leftarrow \mathbf{r} + \mathrm{diag}(\mathbf{g})^2</script></span>；</li>
<li>更新参数<span><span class="MathJax_Preview">\boldsymbol{\Theta} \leftarrow \boldsymbol{\Theta} - \dfrac{\epsilon}{\delta + \sqrt{\mathbf{r}}} \mathbf{g}</span><script type="math/tex">\boldsymbol{\Theta} \leftarrow \boldsymbol{\Theta} - \dfrac{\epsilon}{\delta + \sqrt{\mathbf{r}}} \mathbf{g}</script></span>。</li>
</ol>
</dd>
</dl>
<p>注意文献中常用向量点积<span><span class="MathJax_Preview">\odot</span><script type="math/tex">\odot</script></span>来表示学习率，这样学习率就不是对角矩阵而是向量了。我们这里不定义额外的符号，以便不熟悉相关定义的读者理解。</p>
<p>这一方法的思想是，学习率随着梯度的累计增大而逐渐减小，类似我们使用指数衰减的策略。所不同的是，在梯度小的地方，我们认为梯度平缓，所以学习率减小得慢，以便算法迅速地通过这一片区域；在梯度大地地方，由于梯度陡峭，为了防止我们因为学习率过大漏过该区域，学习率减小得快，以适应梯度的大小。</p>
<p>这个方法没有从根本上解决迭代次数过多时，梯度过小的问题。不难看出该算法学习率以<span><span class="MathJax_Preview">O\left(\frac{1}{\mathbf{g}^T\mathbf{g}}\right)</span><script type="math/tex">O\left(\frac{1}{\mathbf{g}^T\mathbf{g}}\right)</script></span>的比率衰减，经验指出，这个算法在很多情况下是不好用的，只能解决一些比较特定的模型。</p>
<p>在这里，我们依然不给出收敛性的证明（或许在未来我们会在专题中讨论这一问题）。读者不必为这些算法的原理感到压力，我们只需要对其有一个直观的了解就好。</p>
</div>
<p>考虑到Adagrad学习率减小的速度未免太快了，我们可以考虑它的改进，<span></span><strong>RMSprop (root mean square proportion)</strong><span></span>，注意它是另一个算法Adadelta的特例，不过在本节我们不会讨论Adadelta，有兴趣的读者可以自己去寻找参考资料。</p>
<div class="admonition note">
<p class="admonition-title">RMSprop</p>
<details class="info"><summary>参考文献</summary><p>提出该算法的文章，可以在这里参考：</p>
<p><a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">Overview of mini-batch gradient descent</a></p>
</details>
<dl>
<dt>记学习率为<span><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span>，小量<span><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span>，衰减参数<span><span class="MathJax_Preview">\rho</span><script type="math/tex">\rho</script></span>，初始化学习率参数对角矩阵为<span><span class="MathJax_Preview">\mathbf{r} = \mathrm{diag}(\mathbf{0})</span><script type="math/tex">\mathbf{r} = \mathrm{diag}(\mathbf{0})</script></span>，则在每次迭代中</dt>
<dd>
<ol>
<li>随机抽取（或从随机排列的数据集中按顺序抽取）m个样本<span><span class="MathJax_Preview">(\mathbf{x}_k,~\mathbf{y}_k)</span><script type="math/tex">(\mathbf{x}_k,~\mathbf{y}_k)</script></span>，称这m个样本为一个batch；</li>
<li>计算梯度<span><span class="MathJax_Preview">\mathbf{g} = \frac{1}{m} \nabla_{\boldsymbol{\Theta}} \sum\limits_{k=1}^m \mathcal{L} \left( \mathbf{y}_k,~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}_k) \right)</span><script type="math/tex">\mathbf{g} = \frac{1}{m} \nabla_{\boldsymbol{\Theta}} \sum\limits_{k=1}^m \mathcal{L} \left( \mathbf{y}_k,~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}_k) \right)</script></span>；</li>
<li>更新学习率为：<span><span class="MathJax_Preview">\mathbf{r} \leftarrow \rho \mathbf{r} + (1 - \rho) \mathrm{diag}(\mathbf{g})^2</span><script type="math/tex">\mathbf{r} \leftarrow \rho \mathbf{r} + (1 - \rho) \mathrm{diag}(\mathbf{g})^2</script></span>；</li>
<li>更新参数<span><span class="MathJax_Preview">\boldsymbol{\Theta} \leftarrow \boldsymbol{\Theta} - \dfrac{\epsilon}{\delta + \sqrt{\mathbf{r}}} \mathbf{g}</span><script type="math/tex">\boldsymbol{\Theta} \leftarrow \boldsymbol{\Theta} - \dfrac{\epsilon}{\delta + \sqrt{\mathbf{r}}} \mathbf{g}</script></span>。</li>
</ol>
</dd>
</dl>
<p>和上一个算法相比，它唯一的改变就是引入了一个衰减参数<span><span class="MathJax_Preview">\rho</span><script type="math/tex">\rho</script></span>，以指数衰减将之前收集的学习率遗忘。如此就可以控制指数过大的问题，这个trick真是令人一言难尽。但是有趣的是，实际经验中，这个方法真的是卓有成效，是现在常用的神经网络优化算法之一。</p>
</div>
<p>最后让我们来介绍当前最实用的算法（之一），<span></span><strong>Adam (adaptive momentum estimation)</strong><span></span>。顾名思义，它的基本原理是基于对动量的可变估计。实际上，在上一节的Project中，我们选用的优化器就是Adam，Tensorflow的官方教程中，也将Adam作为默认推荐的优化器。</p>
<div class="admonition note">
<p class="admonition-title">Adam</p>
<details class="info"><summary>参考文献</summary><p>提出该算法的文章，可以在这里参考：</p>
<p><a href="https://arxiv.org/abs/1412.6980">Adam: a Method for Stochastic Optimization</a></p>
<p>特别需要注意的是，Adam的收敛性证明已经被后来者推翻，指出其中存在一个错误。改正后的版本称为AMSGrad，Tensorflow的Keras API支持我们在设置Adam的时候开启AMSGrad模式。关于AMSGrad，我们不在此展开讨论，有兴趣的读者可以参考：</p>
<p><a href="https://openreview.net/forum?id=ryQu7f-RZ">On the Convergence of Adam and Beyond</a></p>
</details>
<dl>
<dt>记<span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>为迭代次数，学习率为<span><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span>，小量<span><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span>，衰减参数<span><span class="MathJax_Preview">\rho_1,~\rho_2</span><script type="math/tex">\rho_1,~\rho_2</script></span>，初始化动量为<span><span class="MathJax_Preview">\mathbf{s} = \mathbf{0}</span><script type="math/tex">\mathbf{s} = \mathbf{0}</script></span>，学习率参数对角矩阵为<span><span class="MathJax_Preview">\mathbf{r} = \mathrm{diag}(\mathbf{0})</span><script type="math/tex">\mathbf{r} = \mathrm{diag}(\mathbf{0})</script></span>，则在每次迭代中</dt>
<dd>
<ol>
<li>随机抽取（或从随机排列的数据集中按顺序抽取）m个样本<span><span class="MathJax_Preview">(\mathbf{x}_k,~\mathbf{y}_k)</span><script type="math/tex">(\mathbf{x}_k,~\mathbf{y}_k)</script></span>，称这m个样本为一个batch；</li>
<li>计算梯度<span><span class="MathJax_Preview">\mathbf{g} = \frac{1}{m} \nabla_{\boldsymbol{\Theta}} \sum\limits_{k=1}^m \mathcal{L} \left( \mathbf{y}_k,~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}_k) \right)</span><script type="math/tex">\mathbf{g} = \frac{1}{m} \nabla_{\boldsymbol{\Theta}} \sum\limits_{k=1}^m \mathcal{L} \left( \mathbf{y}_k,~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}_k) \right)</script></span>；</li>
<li>更新动量为：<span><span class="MathJax_Preview">\mathbf{s} \leftarrow \rho_1 \mathbf{s} + (1 - \rho_1) \mathbf{g}</span><script type="math/tex">\mathbf{s} \leftarrow \rho_1 \mathbf{s} + (1 - \rho_1) \mathbf{g}</script></span>；</li>
<li>更新学习率为：<span><span class="MathJax_Preview">\mathbf{r} \leftarrow \rho_2 \mathbf{r} + (1 - \rho_2) \mathrm{diag}(\mathbf{g})^2</span><script type="math/tex">\mathbf{r} \leftarrow \rho_2 \mathbf{r} + (1 - \rho_2) \mathrm{diag}(\mathbf{g})^2</script></span>；</li>
<li>调整参数大小：<span><span class="MathJax_Preview">\hat{\mathbf{s}} \leftarrow \dfrac{\mathbf{s}}{1 - \rho_1^k}</span><script type="math/tex">\hat{\mathbf{s}} \leftarrow \dfrac{\mathbf{s}}{1 - \rho_1^k}</script></span>, <span><span class="MathJax_Preview">\hat{\mathbf{r}} \leftarrow \dfrac{\mathbf{r}}{1 - \rho_2^k}</span><script type="math/tex">\hat{\mathbf{r}} \leftarrow \dfrac{\mathbf{r}}{1 - \rho_2^k}</script></span>；</li>
<li>更新参数<span><span class="MathJax_Preview">\boldsymbol{\Theta} \leftarrow \boldsymbol{\Theta} - \epsilon \dfrac{ \hat{\mathbf{s}} }{\delta + \sqrt{\hat{\mathbf{r}}}}</span><script type="math/tex">\boldsymbol{\Theta} \leftarrow \boldsymbol{\Theta} - \epsilon \dfrac{ \hat{\mathbf{s}} }{\delta + \sqrt{\hat{\mathbf{r}}}}</script></span>。</li>
</ol>
</dd>
</dl>
<p>Adam不仅估计了学习率的可变性，还引入了可变的动量。这是迄今为止，我们见到的第一个将动量和可变学习率结合起来的算法。我们当然期望它能带来双份的<del>快乐</del>好处，可是……<del>为什么会这样呢？</del>，已经有文献指出，Adam存在原理上的失误，并提出了改正的算法AMSGrad，这正是我们未来将要在专题中讨论的内容。现在读者只需要知道，Adam的思路其实就是结合动量和可变学习率就行了。</p>
<details class="warning" open="open"><summary>注意</summary><p>无论是我们没提到的Adadelta还是提到的Adam，其实都引入了动量的概念。那么一个自然而然的idea就是，使用Nesterov动量代替普通的动量。当然，毫无意外的是，已经有人做过了。例如，Adam的Nesterov动量版本叫Nadam，有兴趣的读者不妨去了解一下。</p>
</details>
</div>
<h2 id="_9">解线性回归问题<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h2>
<h3 id="_10">代码规范<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h3>
<p>重申我们之前提到的，我们建议一个完整的工程应当包括</p>
<div class="codehilite"><pre><span></span>.
├─ data/           <span class="c1"># where we store our data</span>
│  └─ ...
├─ tools.py        <span class="c1"># codes for post-processing and analyzing records.</span>
├─ extension.py    <span class="c1"># codes for extending the tensorflow model.</span>
├─ dparser.py      <span class="c1"># data parser</span>
└─ main.py         <span class="c1"># main module where we define our tensorflow model.</span>
</pre></div>

<p>除了保存数据的文件夹，我们应当有三个子模块。其中</p>
<ul>
<li><code>tool</code>: 用来处理、分析生成的数据，通常与Tensorflow无关；</li>
<li><code>extension</code>: 用来扩展tensorflow，例如在这里自定义网络层和操作符；</li>
<li><code>dparser</code>: 数据处理器，用来读取并预处理送入网络的数据；</li>
<li><code>main</code>: 主模块，只定义跟Tensorflow模型有关的内容，需要引用<code>extension</code>和<code>dparser</code>。</li>
</ul>
<p>视情况可以灵活调整结构，但建议将定义Tensorflow模型的代码单独放在主模块里，和其他外围代码分离。</p>
<p>在上一节中，我们没有定义<code>tool.py</code>和<code>extension.py</code>，这是因为我们的工程还很简单，不需要扩展Tensoflow模型，也不需要专门的数据处理代码。相应地，我们把数据的后处理代码直接集成在了主模块<code>lin-cls.py</code>里。在这一节，我们要开始构造一个真正严格按照这四部分分离的工程，并且在接下来的各个例子实现里，都会遵照这个模式，读者应当熟悉类似我们所推荐的、这样一个高度分离的模块化设计的思路。</p>
<h3 id="_11">扩展模块<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h3>
<p>此次是我们第一次写扩展模块，编写扩展模块的目的是，提供一个更复杂的支持库，以便我们能轻松地使用Tensorflow。因此，扩展模块编写地原则应当包括：</p>
<ul>
<li><span></span><strong>可适用性</strong><span></span>: 它应当与我们某一个Project完全无关，就像我们自己基于Tensorflow编写一个扩展库一样，以后我们在任何项目都应该可以使用同一个扩展模块文件；</li>
<li><span></span><strong>低依赖性</strong><span></span>: 它应当最低限度地需要依赖库。<code>tensorflow</code>库本身当然是需要的，而<code>numpy</code>，<code>matplotlib</code>甚或是读写数据的模块，都不宜出现在这里，以确保我们的扩展模块被其他任何模块调用时，依赖关系都是树状的；</li>
<li><span></span><strong>强一致性</strong><span></span>: 它的使用风格，应当尽可能和Tensorflow本身的API一致，使得一个之前不怎么接触它的人，也能快速上手。</li>
</ul>
<p>在这个工程里，我们扩展的内容其实很简单，就是允许模型调用一个指定的优化器。让我们直接看以下代码：</p>
<div class="superfences-tabs">
<input name="__tabs_1" type="radio" id="__tab_1_0" checked="checked" />
<label for="__tab_1_0">extension.py</label>
<div class="superfences-content"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">AdvNetworkBase</span><span class="p">:</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Base object of the advanced network APIs.</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">optimizer</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">l_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Define the optimizer by default parameters except learning rate.</span>
<span class="sd">        Note that most of optimizers do not suggest users to modify their</span>
<span class="sd">        speically designed parameters.</span>
<span class="sd">        name: the name of optimizer (default=&#39;adam&#39;) (available: &#39;adam&#39;, </span>
<span class="sd">              &#39;amsgrad&#39;, &#39;adamax&#39;, &#39;nadam&#39;, &#39;adadelta&#39;, &#39;rms&#39;, &#39;adagrad&#39;,</span>
<span class="sd">              &#39;nmoment&#39;, &#39;sgd&#39;)</span>
<span class="sd">        l_rate: learning rate (default=0.01)</span>
<span class="sd">        decay: decay ratio (&#39;adadeltaDA&#39; do not support this option)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">casefold</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;adam&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">l_rate</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="n">decay</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;amsgrad&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">l_rate</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="n">decay</span><span class="p">,</span> <span class="n">amsgrad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;adamax&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adamax</span><span class="p">(</span><span class="n">l_rate</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="n">decay</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;nadam&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Nadam</span><span class="p">(</span><span class="n">l_rate</span><span class="p">,</span> <span class="n">schedule_decay</span><span class="o">=</span><span class="n">decay</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;adadelta&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adadelta</span><span class="p">(</span><span class="n">l_rate</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="n">decay</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;rms&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">l_rate</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="n">decay</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;adagrad&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adagrad</span><span class="p">(</span><span class="n">l_rate</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="n">decay</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;nmoment&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">l_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="n">decay</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">l_rate</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="n">decay</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
</div>
<p>我们在这里几乎罗列了所有可能使用的优化器，全部来自Keras API。但我们也可以使用Tensorflow旧版API定义的优化器。目前Tensorflow允许使用两种API中的任意一种来定义，但是实验发现，旧版API系列的优化器要么已经在Keras中能找到对应的版本，要么就水土不服，无法正常调用。因此，上文提到的几种优化器，我们基本上全部在这里用Keras API定义出来。</p>
<p>优化器的参数尽可能应当选择默认参数，并且应当封装起来，不宜让用户自行操作。尤其是Adadelta，Adam这些优化器的<span><span class="MathJax_Preview">\rho</span><script type="math/tex">\rho</script></span>变量，在<a href="https://keras-zh.readthedocs.io/optimizers/">Keras文档</a>中，建议我们遵从默认值。</p>
<p>任何继承该类的子类，都可以通过</p>
<div class="codehilite"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizerName</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>

<p>来将封装好的优化器API调用到主模块中。</p>
<h3 id="argparse">项目选项：argparse<a class="headerlink" href="#argparse" title="Permanent link">&para;</a></h3>
<p>本节将第一次引入<code>argparse</code>模块。该模块是python本身具有的原生模块，用来给代码提供启动选项。作为一个完整的Project，我们不希望为了调整参数而频繁地修改代码，因此<code>argparse</code>对我们是不可或缺的。在后面所有的Project中，我们都会通过<code>argparse</code>模块支持项目选项。<code>argparse</code>的官方文档可以在此查阅：</p>
<p><a href="https://docs.python.org/3/library/argparse.html">argparse — Parser for command-line options, arguments and sub-commands</a></p>
<p>调用<code>argparse</code>的一开始，我们需要定义如下内容：</p>
<div class="superfences-tabs">
<input name="__tabs_2" type="radio" id="__tab_2_0" checked="checked" />
<label for="__tab_2_0">Codes</label>
<div class="superfences-content"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>

<span class="k">def</span> <span class="nf">str2bool</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">casefold</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;yes&#39;</span><span class="p">,</span> <span class="s1">&#39;true&#39;</span><span class="p">,</span> <span class="s1">&#39;t&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">True</span>
    <span class="k">elif</span> <span class="n">v</span><span class="o">.</span><span class="n">casefold</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;no&#39;</span><span class="p">,</span> <span class="s1">&#39;false&#39;</span><span class="p">,</span> <span class="s1">&#39;f&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;0&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentTypeError</span><span class="p">(</span><span class="s1">&#39;Unsupported value encountered.&#39;</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span>
    <span class="n">description</span><span class="o">=</span><span class="s1">&#39;A demo for linear regression.&#39;</span><span class="p">,</span>
    <span class="n">formatter_class</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentDefaultsHelpFormatter</span>
<span class="p">)</span>
</pre></div>
</td></tr></table></div>
<input name="__tabs_2" type="radio" id="__tab_2_1" />
<label for="__tab_2_1">Output</label>
<div class="superfences-content"><div class="codehilite"><pre><span></span>usage: tools.py <span class="o">[</span>-h<span class="o">]</span>

A demo <span class="k">for</span> linear regression.

optional arguments:
  -h, --help  show this <span class="nb">help</span> message and <span class="nb">exit</span>
</pre></div></div>
</div>
<p>我们首先定义了<code>str2bool</code>函数，用来支持用户提供布尔类型的选项；之后，我们初始化了<code>parser</code>，一般地初始化<code>parser</code>时，我们主要定义三个参数：</p>
<ul>
<li><code>description</code>: 项目描述，展示在参数用法之前的一段字符串；</li>
<li><code>formatter_class</code>: <a href="https://docs.python.org/3/library/argparse.html#formatter-class">格式化器</a>，我们一般调用的都是<code>ArgumentDefaultsHelpFormatter</code>，因为它能支持自动换行，并在每个参数用法后展示该参数的默认值；</li>
<li><code>epilog</code>: <a href="https://docs.python.org/3/library/argparse.html#epilog">后记</a>，这一段说明文字出现在所有参数用法之后。我们一般不太需要这个功能，但是有时候我们可以使用该功能提供一些用法范例给用户。</li>
</ul>
<p>现在，我们来介绍几种典型的<code>argparse</code>可以提供的参数类型。</p>
<div class="admonition note">
<p class="admonition-title">字符串选项</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="s1">&#39;--optimizer&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;str&#39;</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;&#39;</span><span class="se">\</span>
<span class="s1">    The optimizer we use to train the model (available: &#39;adam&#39;, &#39;amsgrad&#39;, &#39;adamax&#39;, </span>
<span class="s1">    &#39;nadam&#39;, &#39;adadelta&#39;, &#39;rms&#39;, &#39;adagrad&#39;, &#39;nmoment&#39;, &#39;sgd&#39;)</span>
<span class="s1">    &#39;&#39;&#39;</span>
<span class="p">)</span>
</pre></div>
</td></tr></table>

<p>在这里我们定义了一个字符串选项，这是最常用的一类选项。用户可以像<code class="codehilite">python codes.py -o amsgrad</code>或者<code class="codehilite">python codes.py --optimizer amsgrad</code>一样，通过添加参数来覆盖默认值(定义在<code>default</code>字段下)。</p>
</div>
<div class="admonition note">
<p class="admonition-title">数值选项</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s1">&#39;-lr&#39;</span><span class="p">,</span> <span class="s1">&#39;--learningRate&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;float&#39;</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;&#39;</span><span class="se">\</span>
<span class="s1">    The learning rate for training the model.</span>
<span class="s1">    &#39;&#39;&#39;</span>
<span class="p">)</span>
</pre></div>
</td></tr></table>

<p>这里添加的参数类型是一个浮点数，虽然用户在输入参数的时候输入的是一个字符串，但<code>metavar</code>字段告诉了用户应该输入浮点数，<code>type</code>决定了用户输入的字符串会被自动转换为浮点数。类似地，将两个字段的<code>float</code>改为<code>int</code>，我们就能提供一个整数作为参数选项</p>
</div>
<div class="admonition note">
<p class="admonition-title">布尔选项</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s1">&#39;-if&#39;</span><span class="p">,</span> <span class="s1">&#39;--importFlag&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span> <span class="n">nargs</span><span class="o">=</span><span class="s1">&#39;?&#39;</span><span class="p">,</span> <span class="n">const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;bool&#39;</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;&#39;</span><span class="se">\</span>
<span class="s1">    The flag of importing pre-trained model.</span>
<span class="s1">    &#39;&#39;&#39;</span>
<span class="p">)</span>
</pre></div>
</td></tr></table>

<p>这里添加的是一个二值选项，它的默认值是<code class="codehilite"><span class="bp">False</span></code>，用户可以通过输入<code class="codehilite"><span class="p">(</span><span class="s1">&#39;yes&#39;</span><span class="p">,</span> <span class="s1">&#39;true&#39;</span><span class="p">,</span> <span class="s1">&#39;t&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">)</span></code>中的任何一个来指定该选项为真，或通过<code class="codehilite"><span class="p">(</span><span class="s1">&#39;no&#39;</span><span class="p">,</span> <span class="s1">&#39;false&#39;</span><span class="p">,</span> <span class="s1">&#39;f&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;0&#39;</span><span class="p">)</span></code>中的任何一个指定该选项为假，不区分大小写。该功能由我们之前定义的<code>str2bool</code>函数提供。</p>
<p>特别值得注意的是，这个布尔选项还可以有这样的用法，例如：</p>
<div class="codehilite"><pre><span></span>python codes.py -if -o amsgrad
</pre></div>

<p>我们如果指派了<code>-if</code>，在不指定它任何值的情况下，该选项就会被开启（值为真）了；如果我们去掉这一行的<code>-if</code>，则该选项关闭（值为假）。</p>
</div>
<div class="admonition note">
<p class="admonition-title">多值选项</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s1">&#39;-ml&#39;</span><span class="p">,</span> <span class="s1">&#39;--mergedLabel&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">nargs</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;int&#39;</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;&#39;</span><span class="se">\</span>
<span class="s1">    The merged label settings.</span>
<span class="s1">    &#39;&#39;&#39;</span>
<span class="p">)</span>
</pre></div>
</td></tr></table>

<p>上面的设置提供了一个可以输入任意多个<code class="codehilite"><span class="nb">int</span></code>型值的选项，用法如下：</p>
<div class="codehilite"><pre><span></span>python codes.py -ml <span class="m">1</span> <span class="m">3</span> <span class="m">4</span> <span class="m">0</span> <span class="m">2</span> -o amsgrad
</pre></div>

<p>上述的输入会被解析成一个值为<code class="codehilite"><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span></code>的列表。当然，我们也可以输入任意多的值，但是特别值得注意的是，由于在<code>nargs</code>字段指定了<code>+</code>，一旦我们指派该选项，就要至少输入一个值方可。</p>
</div>
<p>上面的几种范例，并不是每一种都需要用在Project中。实际设置选项的时候，应当参照实际情况来处理。例如，本例中，就只使用<span></span><em>字符串选项</em><span></span>和<span></span><em>数值选项</em><span></span>两种。更多关于<code>add_argument</code>的用法，请参阅官方文档：</p>
<p><a href="https://docs.python.org/3/library/argparse.html#argparse.ArgumentParser.add_argument">argparse — add_argument()</a></p>
<p>在所有参数都设置好后，调用</p>
<div class="codehilite"><pre><span></span><span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</pre></div>

<p>即可使参数选项生效。用户输入的参数选项将返回到<code>args</code>中，例如，如果用户制定了<code>-o</code>(<code>--optimizer</code>)，那么我们可以调用<code>args.optimizer</code>来取出该字段的值。</p>
<h3 id="_12">数据生成<a class="headerlink" href="#_12" title="Permanent link">&para;</a></h3>
<p>本节的数据也是自动生成出来的。参考上一节的数据生成器，重新定义数据生成类的迭代器：</p>
<div class="superfences-tabs">
<input name="__tabs_3" type="radio" id="__tab_3_0" checked="checked" />
<label for="__tab_3_0">dparser.py</label>
<div class="superfences-content"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">TestDataRegSet</span><span class="p">(</span><span class="n">TestDataSet</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    A generator of the data set for testing the linear regression model.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">next_train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Get the next train batch: (x, y)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_x</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_x</span><span class="p">])</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">&gt;</span> <span class="mf">1e-3</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</td></tr></table></div>
</div>
<details class="tip" open="open"><summary>提示</summary><p>这里我们在没有噪声的情况下，仍然调用随机噪声函数，这是为了确保噪声函数被调用，使得随机数无论开关噪声，都能保持一致性。</p>
</details>
<p>该生成器同样是输入一组<span><span class="MathJax_Preview">\mathbf{A},~\mathbf{c}</span><script type="math/tex">\mathbf{A},~\mathbf{c}</script></span>，以及相关配置，之后就可以通过<span></span><strong>迭代器 (iterator)</strong><span></span>或<span></span><strong>方法 (method)</strong><span></span>随机生成数据。与上一节不同的是，我们在本节可以尝试更进一步，令<span><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>的<a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">SVD分解</a>写作如下形式</p>
<div class="overflow">\begin{align}
    \mathbf{A} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^T.
\end{align}</div>

<p>其中，<span><span class="MathJax_Preview">\boldsymbol{\Sigma}</span><script type="math/tex">\boldsymbol{\Sigma}</script></span>是一个对角矩阵，对角线上的元素顺次排列，对应为矩阵<span><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>的各个特征值。Numpy的库已经集成了<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html">SVD分解</a>。我们知道，一个<span><span class="MathJax_Preview">M \times N</span><script type="math/tex">M \times N</script></span>的矩阵经过SVD分解后，应当有<span><span class="MathJax_Preview">\mathbf{U}_{M \times M}</span><script type="math/tex">\mathbf{U}_{M \times M}</script></span>和<span><span class="MathJax_Preview">\mathbf{V}^T_{N \times N}</span><script type="math/tex">\mathbf{V}^T_{N \times N}</script></span>两个方阵。故而，矩阵<span><span class="MathJax_Preview">\boldsymbol{\Sigma}_{M \times N}</span><script type="math/tex">\boldsymbol{\Sigma}_{M \times N}</script></span>并非方阵。由于它只有对角线上有元素，所以必定有多出来的空行或空列。因此，若我们设<span><span class="MathJax_Preview">K = \min(M,~N)</span><script type="math/tex">K = \min(M,~N)</script></span>，则我们可以知道，SVD分解其实不需要矩阵<span><span class="MathJax_Preview">\mathbf{U}</span><script type="math/tex">\mathbf{U}</script></span>和<span><span class="MathJax_Preview">\mathbf{V}^T</span><script type="math/tex">\mathbf{V}^T</script></span>两个方阵都是方阵，因为当我们取矩阵<span><span class="MathJax_Preview">\boldsymbol{\Sigma}_{K \times K}</span><script type="math/tex">\boldsymbol{\Sigma}_{K \times K}</script></span>这一对角部分后，可以只取部分行/列构成的矩阵<span><span class="MathJax_Preview">\mathbf{U}_{M \times K}</span><script type="math/tex">\mathbf{U}_{M \times K}</script></span>和<span><span class="MathJax_Preview">\mathbf{V}^T_{K \times N}</span><script type="math/tex">\mathbf{V}^T_{K \times N}</script></span>。这相当于我们略去了<span><span class="MathJax_Preview">\boldsymbol{\Sigma}</span><script type="math/tex">\boldsymbol{\Sigma}</script></span>上的空行/空列，但是SVD分解仍然能保证恢复出原矩阵来。</p>
<p>在本例中，我们保留<span><span class="MathJax_Preview">\boldsymbol{\Sigma}</span><script type="math/tex">\boldsymbol{\Sigma}</script></span>中的前<span><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span>个特征值，其后的特征值都丢弃，我们把这样的做法称为矩阵的低秩近似，于是有</p>
<div class="superfences-tabs">
<input name="__tabs_4" type="radio" id="__tab_4_0" checked="checked" />
<label for="__tab_4_0">dparser.py</label>
<div class="superfences-content"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">gen_lowrank</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Generate a low rank approximation to matrix A.</span>
<span class="sd">        A: input matrix.</span>
<span class="sd">        r: output rank.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">sze</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">r_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="n">sze</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">r</span> <span class="o">&lt;=</span> <span class="n">r_min</span> <span class="ow">and</span> <span class="n">r</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;r should in the range of [1, {0}]&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">r_min</span><span class="p">)</span>
    <span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="n">r</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">u</span><span class="p">[:,:</span><span class="n">r</span><span class="p">],</span> <span class="n">s</span><span class="p">),</span> <span class="n">v</span><span class="p">[:</span><span class="n">r</span><span class="p">,:])</span>
</pre></div>
</td></tr></table></div>
</div>
<p>一个低秩近似的矩阵，其定义的仿射变换<a href="#mjx-eqn-3"><span><span class="MathJax_Preview">(3)</span><script type="math/tex">(3)</script></span></a>满足不同的<span><span class="MathJax_Preview">\mathbf{x}</span><script type="math/tex">\mathbf{x}</script></span>对应同一个值<span><span class="MathJax_Preview">\mathbf{y}</span><script type="math/tex">\mathbf{y}</script></span>；反之，<span><span class="MathJax_Preview">\mathbf{y}</span><script type="math/tex">\mathbf{y}</script></span>将会对应多个不同的解<span><span class="MathJax_Preview">\mathbf{x}</span><script type="math/tex">\mathbf{x}</script></span>。如果我们训练的线性分类器模拟的是<a href="#mjx-eqn-3"><span><span class="MathJax_Preview">(3)</span><script type="math/tex">(3)</script></span></a>的逆过程，可能我们会无法模拟出合适的解来；但是，由于我们定义的<a href="#mjx-eqn-4"><span><span class="MathJax_Preview">(4)</span><script type="math/tex">(4)</script></span></a>仍是在拟合正过程，故而我们仍然可以把这个问题看成是有解的。在后续的内容中，我们会适当地讨论当问题<span></span><strong>解不唯一</strong><span></span>时，我们可以进行哪些工作来处理这类问题。</p>
<p>接下来，我们即可测试低秩近似的效果，</p>
<div class="superfences-tabs">
<input name="__tabs_5" type="radio" id="__tab_5_0" checked="checked" />
<label for="__tab_5_0">dparser.py</label>
<div class="superfences-content"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7
8
9</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">test_lowrank</span><span class="p">():</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">7</span><span class="p">):</span>
        <span class="n">A_</span> <span class="o">=</span> <span class="n">gen_lowrank</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
        <span class="n">RMS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">A</span> <span class="o">-</span> <span class="n">A_</span><span class="p">)))</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">A_</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Rank = {0}, RMS={1}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">RMS</span><span class="p">))</span>

<span class="n">test_lowrank</span><span class="p">()</span>
</pre></div>
</td></tr></table></div>
<input name="__tabs_5" type="radio" id="__tab_5_1" />
<label for="__tab_5_1">Output</label>
<div class="superfences-content"><div class="codehilite"><pre><span></span><span class="n">Rank</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">RMS</span><span class="o">=</span><span class="mf">6.8600432267325955</span>
<span class="n">Rank</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">RMS</span><span class="o">=</span><span class="mf">4.677152938185369</span>
<span class="n">Rank</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">RMS</span><span class="o">=</span><span class="mf">3.216810970685858</span>
<span class="n">Rank</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">RMS</span><span class="o">=</span><span class="mf">1.8380598782932136</span>
<span class="n">Rank</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">RMS</span><span class="o">=</span><span class="mf">0.9348520972791058</span>
<span class="n">Rank</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="n">RMS</span><span class="o">=</span><span class="mf">9.736224609164252e-15</span>
</pre></div></div>
</div>
<p>可见，对于一个标准差为10的矩阵，低秩近似的残差仍然是不超过随机高斯矩阵本身的标准差的。这里的秩是我们在调用低秩近似函数后，使用<code>np.linalg.matrix_rank</code>测量的结果。</p>
<h3 id="_13">定义类模型<a class="headerlink" href="#_13" title="Permanent link">&para;</a></h3>
<p><span></span><strong>类模型 (Model class)</strong><span></span>，在官方文档中也称为函数式API，是Tensorflow-Keras的用户大多数情况下应当使用的模型。它支持一些灵活的操作，使得我们可以</p>
<ul>
<li><span></span><strong>多输入多输出</strong><span></span>: 类模型的输入和输出层，都是通过函数定义的。类模型在构建的时候，只需要给定输入和输出即可；</li>
<li><span></span><strong>跨层短接</strong><span></span>: 由于类模型的各层都由函数定义，可以轻松将不同的层连接起来，通常通过<a href="https://keras-zh.readthedocs.io/layers/merge/">融合层</a>完成这一工作；</li>
<li><span></span><strong>多优化器</strong><span></span>: 可以通过复用同一层对应的对象，构建多个不同的类模型，并分别对它们使用不同的训练数据、损失函数、优化器，以实现多优化目标。</li>
</ul>
<p>一个顺序模型大致可以描述为下图的模式：</p>
<div class="mermaid">graph LR
st1(输&lt;br/&gt;入&lt;br/&gt;1) --&gt; l11[层&lt;br/&gt;1-1]
l11 --&gt; l21[层&lt;br/&gt;1-2]
l21 --&gt; l31[层&lt;br/&gt;1-3]
l31 --&gt; ldots1[层&lt;br/&gt;...]

st2(输&lt;br/&gt;入&lt;br/&gt;2) --&gt; l12[层&lt;br/&gt;2-1]
l12 --&gt; l22[层&lt;br/&gt;2-2]
l22 --&gt; l32[层&lt;br/&gt;2-3]
l32 --&gt; ldots2[层&lt;br/&gt;...]

ldots1 --&gt; l3[层&lt;br/&gt;3]
ldots2 --&gt; l3
l3 --&gt; l4[层&lt;br/&gt;4]
l4 --&gt; ed1(输&lt;br/&gt;出&lt;br/&gt;1)
l4 --&gt; ed2(输&lt;br/&gt;出&lt;br/&gt;2)
l22 --&gt; ed3(输出3)
l21 --&gt; l3

classDef styStart fill:#FAE6A9,stroke:#BA9132;
class st1,ed1,st2,ed2,ed3 styStart</div>

<p>在本节中，尽管我们开始使用类模型，但我们定义的仍然是一个单线路的线性回归模型，换言之，这样的模型完全可以通过<span></span><strong>顺序模型</strong><span></span>实现出来。我们从这一节开始，不再使用顺序模型，其一，是因为顺序模型都可以写成类模型的形式，其二，是希望读者能够熟悉、灵活运用类模型的优势。</p>
<p>我们定义一个继承自<code>extension.py</code>的类，<code class="codehilite"><span class="k">class</span> <span class="nc">LinRegHandle</span><span class="p">(</span><span class="n">ext</span><span class="o">.</span><span class="n">AdvNetworkBase</span><span class="p">):</span></code>。与上一节的情况相若，这里我们不再赘述需要定义哪些方法。并且，我们也不会介绍一些改动不大、或者不重要的方法，详情请读者参阅源码。</p>
<h4 id="_14">初始化方法<a class="headerlink" href="#_14" title="Permanent link">&para;</a></h4>
<p>首先，定义初始化方法：</p>
<div class="superfences-tabs">
<input name="__tabs_6" type="radio" id="__tab_6_0" checked="checked" />
<label for="__tab_6_0">lin-cls.py: class LinRegHandle</label>
<div class="superfences-content"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">steppe</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">optimizerName</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Initialization and pass fixed parameters.</span>
<span class="sd">        learning_rate: the learning rate for optimizer.</span>
<span class="sd">        epoch:         training epochs.</span>
<span class="sd">        steppe:        steps per epoch</span>
<span class="sd">        optimizerName: the name of optimizer (available: &#39;adam&#39;, &#39;amsgrad&#39;, </span>
<span class="sd">                        &#39;adamax&#39;, &#39;nadam&#39;, &#39;adadelta&#39;, &#39;rms&#39;, &#39;adagrad&#39;, </span>
<span class="sd">                        &#39;nmoment&#39;, &#39;sgd&#39;)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">=</span> <span class="n">epoch</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">steppe</span> <span class="o">=</span> <span class="n">steppe</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizerName</span> <span class="o">=</span> <span class="n">optimizerName</span>
</pre></div>
</td></tr></table></div>
</div>
<p>与上一节相比，这里我们增加了一个参数，<code>opmizerName</code>，用来指定我们选用的优化器名称，默认值为<code>adam</code>。</p>
<h4 id="_15">构造方法<a class="headerlink" href="#_15" title="Permanent link">&para;</a></h4>
<p>接下来定义网络构造</p>
<div class="superfences-tabs">
<input name="__tabs_7" type="radio" id="__tab_7_0" checked="checked" />
<label for="__tab_7_0">lin-cls.py: class LinRegHandle</label>
<div class="superfences-content"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Construct a linear model and set the optimizer as Adam</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Construction</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">INPUT_SHAPE</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">dense1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">LABEL_SHAPE</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                                <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">10.0</span><span class="p">),</span> 
                                <span class="n">bias_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> 
                                <span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dense1&#39;</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>

    <span class="c1"># Set optimizer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizerName</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">),</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">MeanSquaredError</span><span class="p">(),</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">relation</span><span class="p">]</span>
    <span class="p">)</span>

<span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">relation</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">m_y_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">m_y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">s_y_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_true</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">m_y_true</span><span class="p">))</span>
    <span class="n">s_y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">m_y_pred</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">m_y_true</span> <span class="o">*</span> <span class="n">m_y_pred</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">s_y_true</span> <span class="o">*</span> <span class="n">s_y_pred</span><span class="p">))</span>
</pre></div>
</td></tr></table></div>
</div>
<p>使用类模型时，我们每定义一层，都调用对应的网络层函数，并返回层的输出结果。这就是为何它又叫“函数式API”。我们直接使用均方误差作为我们的损失函数，同时，我们还自行定义了一个评价函数，<a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient"><span></span><strong>皮尔森相关系数</strong><span></span></a>，该系数专门用来反映两组数据之间是否线性相关，上文我们已经叙述过它的定义。</p>
<details class="warning" open="open"><summary>注意</summary><p>理想情况下，相关系数应当使用整个数据集来求取。但实际情况下做不到这一点，因此我们求取的相关系数只能看作是一个通过batch得到的估计。故此，我们可以发现，求相关系数要求我们每次输入的样本至少有2个。样本数目越多，相关系数的估计越准确。</p>
</details>
<details class="warning" open="open"><summary>注意</summary><p>从式中可以发现，我们定义的皮尔森相关系数时，完全使用的时Tensorflow-Keras API，因此它当然可以用作我们的训练损失函数。但实际情况下，我们并不使用它。考虑一个反例，当两组数据的分布之间唯一的不同只是均值时，亦即<span><span class="MathJax_Preview">\mathbf{y}_2 = \mathbf{y}_1 + C</span><script type="math/tex">\mathbf{y}_2 = \mathbf{y}_1 + C</script></span>，这种情况下皮尔森相关系数仍然为1。虽然我们可以考虑用<a href="https://en.wikipedia.org/wiki/Cosine_similarity">余弦相似度函数 (Cosine similarity)</a>来代替它，但经验显示，余弦相似度最大化到一定程度以后，其对应的均方误差反而上升。考虑另一个反例，<span><span class="MathJax_Preview">\mathbf{y}_2 = \alpha \mathbf{y}_1</span><script type="math/tex">\mathbf{y}_2 = \alpha \mathbf{y}_1</script></span>，显然<span><span class="MathJax_Preview">\mathbf{y}_1</span><script type="math/tex">\mathbf{y}_1</script></span>和<span><span class="MathJax_Preview">\mathbf{y}_2</span><script type="math/tex">\mathbf{y}_2</script></span>的余弦相似度是1。因此，实际应用中，无论是皮尔森相关系数还是余弦相似度，都适合用作评价函数而不是损失函数。</p>
</details>
<p>与上一节不同的是，由于这是一个线性回归器，我们不给它提供激活函数。</p>
<h4 id="_16">训练和测试方法<a class="headerlink" href="#_16" title="Permanent link">&para;</a></h4>
<p>类模型的<code>compile</code>、<code>fit</code>、<code>evaluate</code>、<code>predict</code>等API与顺序模型完全相同，详情请查看：</p>
<p><a href="https://keras-zh.readthedocs.io/models/model/">Model类 (函数式API) - Keras中文文档</a></p>
<h3 id="_17">调试<a class="headerlink" href="#_17" title="Permanent link">&para;</a></h3>
<p>上一节中，我们每次训练后，就当场显示分析结果。在本节中，我们会“再进一步”。即使用<code>tools.py</code>专门进行实验结果分析（后处理）。相对地，训练后，我们会讲<span></span><strong>原始输出 (raw output)</strong><span></span>保存到文件里。这是一种编写代码的思想，是为了便于我们批量分析测试数据。在后面的Project中，我们会看到，我们既会编写当场显示分析结果的测试代码，也会编写保存输出后使用<code>tools.py</code>分析的代码。究竟使用哪种方式分析数据，视具体情况而定。一般地，测试少量数据时，我们当场分析；批量测试大量数据时，或者需要比较不同选项（例如不同噪声）对结果的影响时，我们在<code>tools.py</code>中分析。本实验的情况属于后者。</p>
<h4 id="_18">使实验结果可复现<a class="headerlink" href="#_18" title="Permanent link">&para;</a></h4>
<p>由于我们本次实验需要对比不同设置下的回归器性能，我们希望随机生成的矩阵<span><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>，向量<span><span class="MathJax_Preview">\mathbf{c}</span><script type="math/tex">\mathbf{c}</script></span>应当可复现；换言之，我们希望我们的结果是可复现的。</p>
<p>关于这一问题，Keras的文档给出的建议可以在这里查阅：</p>
<p><a href="https://keras-zh.readthedocs.io/getting-started/faq/#keras_5">如何在 Keras 开发过程中获取可复现的结果？ - Keras中文文档</a></p>
<p>我们只需要使<code>argparse</code>添加一个选项<code>-sd</code>(<code>--seed</code>)，并通过该选项控制：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">setSeed</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">+</span><span class="mi">12345</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">+</span><span class="mi">1234</span><span class="p">)</span>
<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span> <span class="c1"># Set seed for reproductable results</span>
    <span class="n">setSeed</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>其中，<code>np.random.seed</code>，<code>random.seed</code>，<code>tf.set_random_seed</code>分别来自Numpy，python原生的random库，以及Tensorflow。将这三个库的<span></span><strong>随机种子 (seed)</strong><span></span>设为三个不同的值，即可保证我们每次指定<code>-sd</code>后，从程序运行开始，得到的所有随机数都是固定的随机序列。当然，<a href="https://keras-zh.readthedocs.io/getting-started/faq/#keras_5">Keras文档</a>指出，即使如此，我们还不能保证我们的结果完完全全是可复现的。因为多线程算法并发的先后顺序随机性、GPU运算带来的先后顺序随机性等干扰因素，均会导致我们每次得到的结果有细微的偏差。但这些因素对于本实验验证可复现数据的要求几乎没有什么影响。</p>
<h4 id="_19">使实验代码保存输出<a class="headerlink" href="#_19" title="Permanent link">&para;</a></h4>
<p>首先，训练网络。我们同样随机生成<span><span class="MathJax_Preview">\mathbf{x} \mapsto \mathbf{y}:~\mathbb{R}^{10} \mapsto \mathbb{R}^6</span><script type="math/tex">\mathbf{x} \mapsto \mathbf{y}:~\mathbb{R}^{10} \mapsto \mathbb{R}^6</script></span>的仿射变换，将该变换中的线性变换矩阵采用秩为4的低秩近似，并且设置好数据集，给定噪声扰动由用户决定。默认值下，噪声为<span><span class="MathJax_Preview">\boldsymbol{\varepsilon} \sim \mathcal{N}(0,10)^6</span><script type="math/tex">\boldsymbol{\varepsilon} \sim \mathcal{N}(0,10)^6</script></span>，epoch为20个，每个epoch迭代500次，每次馈入32个样本构成的batch。我们将上一节的主函数输出部分修改成如下形式，并进行不加参数的调试：</p>
<div class="superfences-tabs">
<input name="__tabs_8" type="radio" id="__tab_8_0" checked="checked" />
<label for="__tab_8_0">lin-cls.py</label>
<div class="superfences-content"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Initialization</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dp</span><span class="o">.</span><span class="n">gen_lowrank</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">[</span><span class="n">INPUT_SHAPE</span><span class="p">,</span> <span class="n">LABEL_SHAPE</span><span class="p">]),</span> <span class="n">RANK</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">LABEL_SHAPE</span><span class="p">])</span>
<span class="n">dataSet</span> <span class="o">=</span> <span class="n">dp</span><span class="o">.</span><span class="n">TestDataRegSet</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="n">dataSet</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">noise</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">noise</span><span class="p">)</span>
<span class="c1"># Generate a group of testing samples.</span>
<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">setSeed</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="o">+</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">dataSet</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">batch</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">testBatchNum</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">dataSet</span><span class="p">)</span>
<span class="c1"># Set the data set for training.</span>
<span class="n">dataSet</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">batch</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">trainBatchNum</span><span class="p">)</span>
<span class="c1"># Construct the model and train it.</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">LinRegHandle</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">learningRate</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> 
                    <span class="n">steppe</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">steppe</span><span class="p">,</span> <span class="n">optimizerName</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">h</span><span class="o">.</span><span class="n">construct</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Begin to train:&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;---------------&#39;</span><span class="p">)</span>
<span class="n">record</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataSet</span><span class="p">))</span>

<span class="c1"># Check the testing results</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Begin to test:&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;---------------&#39;</span><span class="p">)</span>
<span class="n">yp</span><span class="p">,</span> <span class="n">loss_p</span><span class="p">,</span> <span class="n">corr_p</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Check the regressed values</span>
<span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;dense1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>

<span class="c1"># Save</span>
<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">outputData</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">savez_compressed</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">outputData</span><span class="p">,</span> 
        <span class="n">epoch</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">],</span> <span class="n">corr</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;relation&#39;</span><span class="p">],</span> 
        <span class="n">test_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred_y</span> <span class="o">=</span> <span class="n">yp</span><span class="p">,</span> 
        <span class="n">pred_loss</span> <span class="o">=</span> <span class="n">loss_p</span><span class="p">,</span> <span class="n">pred_corr</span> <span class="o">=</span> <span class="n">corr_p</span><span class="p">,</span>
        <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">A</span><span class="o">=</span><span class="n">A</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span>
    <span class="p">)</span>
</pre></div>
</td></tr></table></div>
<input name="__tabs_8" type="radio" id="__tab_8_1" />
<label for="__tab_8_1">Output</label>
<div class="superfences-content"><div class="codehilite"><pre><span></span>Begin to train:
---------------
Epoch <span class="m">1</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">29084</span>.6994 - relation: <span class="m">0</span>.3472
Epoch <span class="m">2</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">15669</span>.9579 - relation: <span class="m">0</span>.5597
Epoch <span class="m">3</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">8145</span>.8705 - relation: <span class="m">0</span>.7134
Epoch <span class="m">4</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">4000</span>.0838 - relation: <span class="m">0</span>.8130
Epoch <span class="m">5</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">1856</span>.1477 - relation: <span class="m">0</span>.8801
Epoch <span class="m">6</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">799</span>.4556 - relation: <span class="m">0</span>.9354
Epoch <span class="m">7</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">336</span>.8600 - relation: <span class="m">0</span>.9700
Epoch <span class="m">8</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">166</span>.5899 - relation: <span class="m">0</span>.9813
Epoch <span class="m">9</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">113</span>.2465 - relation: <span class="m">0</span>.9831
Epoch <span class="m">10</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">102</span>.0431 - relation: <span class="m">0</span>.9834
Epoch <span class="m">11</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">99</span>.6678 - relation: <span class="m">0</span>.9838
Epoch <span class="m">12</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">99</span>.8547 - relation: <span class="m">0</span>.9833
Epoch <span class="m">13</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">100</span>.1278 - relation: <span class="m">0</span>.9834
Epoch <span class="m">14</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">99</span>.6048 - relation: <span class="m">0</span>.9835
Epoch <span class="m">15</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">101</span>.1930 - relation: <span class="m">0</span>.9832
Epoch <span class="m">16</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">101</span>.6636 - relation: <span class="m">0</span>.9835
Epoch <span class="m">17</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">100</span>.6665 - relation: <span class="m">0</span>.9834
Epoch <span class="m">18</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">101</span>.2459 - relation: <span class="m">0</span>.9832
Epoch <span class="m">19</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">100</span>.9701 - relation: <span class="m">0</span>.9836
Epoch <span class="m">20</span>/20
<span class="m">500</span>/500 <span class="o">[==============================]</span> - 1s 2ms/step - loss: <span class="m">100</span>.7719 - relation: <span class="m">0</span>.9836
Begin to test:
---------------
<span class="m">10</span>/10 <span class="o">[==============================]</span> - 0s 5ms/sample - loss: <span class="m">94</span>.8883 - relation: <span class="m">0</span>.9897
Evaluated loss <span class="o">(</span>losses.MeanSquaredError<span class="o">)</span> <span class="o">=</span> <span class="m">94</span>.88829040527344
Evaluated metric <span class="o">(</span>Pearson<span class="err">&#39;</span>s correlation<span class="o">)</span> <span class="o">=</span> <span class="m">0</span>.9897396
</pre></div></div>
</div>
<p>以上结果是不加任何参数的前提下，直接以默认参数运行程序得到的。结果显明，MSE最后收敛在100左右，因为我们馈入的label添加了标准差为10的白噪声，对应的方差为100。可知，实验结果与预期一致。另一方面，我们可以看到，相关系数在这里可以充当类似准确度的作用，考虑到我们默认的噪声为10，这一相关系数的收敛结果是符合我们的预期的。</p>
<p>我们还可以注意到，这段代码中，生成测试集的代码被提前了，这是为了确保每次运行程序，只要指定了种子，生成的测试集总是一致的。</p>
<p>现在，我们可以导出生成数据了，首先，我们改变不同的优化器，其他参数全部一致，例如，学习率均为0.01（Adadelta除外，其初始参数一般推荐为1.0）。调用代码时的参数设置如下</p>
<div class="codehilite"><pre><span></span>python lin-reg.py -e <span class="m">25</span> -sd <span class="m">1</span> -do test/algorithm/<span class="o">{</span>optimizer<span class="o">}</span> -o <span class="o">{</span>optimizer<span class="o">}</span>
</pre></div>

<p>其中我们用<code>{optimizer}</code>来指代我们选用的优化算法。同时，我们固定测试的epoch数量为25，这是因为有些算法的收敛速度不足以保证20个epoch收敛。</p>
<p>接下来，我们固定优化器为Adam，改变不同的噪声，分别令标准差为0, 1, 5, 10, 50, 100，产生多组结果。</p>
<div class="codehilite"><pre><span></span>python lin-reg.py -sd <span class="m">1</span> -do test/noise/<span class="o">{</span>noise<span class="o">}</span> -is <span class="o">{</span>noise<span class="o">}</span>
</pre></div>

<h4 id="toolspy">在<code>tools.py</code>中分析比较结果<a class="headerlink" href="#toolspy" title="Permanent link">&para;</a></h4>
<p>首先，在<code>tools.py</code>中定义数据解析函数</p>
<div class="superfences-tabs">
<input name="__tabs_9" type="radio" id="__tab_9_0" checked="checked" />
<label for="__tab_9_0">tools.py</label>
<div class="superfences-content"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">parseData</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">keys</span><span class="p">):</span>
    <span class="n">keys_list</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="p">[])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">)</span>
    <span class="n">name_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">scandir</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
            <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">name_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">))</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">:</span>
                <span class="n">keys_list</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
            <span class="n">epoch</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">name_list</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">keys_list</span>
</pre></div>
</td></tr></table></div>
</div>
<p>该函数的作用是，给定保存输出文件的文件夹路径，能够自动读取文件夹下所有数据文件，并将不同文件的结果列在列表的不同元素中。<code>keys</code>关键字能帮助我们指派我们关心的数据字段。</p>
<p>接下来，我们通过如下代码，对比不同优化器条件下的损失函数和测度函数，对比不同噪声条件下的损失函数和测度函数，输出的曲线反映了对训练过程的跟踪。</p>
<div class="superfences-tabs">
<input name="__tabs_10" type="radio" id="__tab_10_0" checked="checked" />
<label for="__tab_10_0">tools.py</label>
<div class="superfences-content"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">showCurves</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;{0}&#39;</span><span class="p">,</span> <span class="n">converter</span><span class="o">=</span><span class="nb">str</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Show curves from different tests in a same folder.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">name_list</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">keys_list</span> <span class="o">=</span> <span class="n">parseData</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="s1">&#39;corr&#39;</span><span class="p">])</span>
    <span class="n">loss_list</span> <span class="o">=</span> <span class="n">keys_list</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
    <span class="n">corr_list</span> <span class="o">=</span> <span class="n">keys_list</span><span class="p">[</span><span class="s1">&#39;corr&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">loss_list</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">corr_list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="n">FileExistsError</span><span class="p">(</span><span class="s1">&#39;No data found, could not draw curves.&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">loss_list</span><span class="p">)):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">loss_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">prefix</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">converter</span><span class="p">(</span><span class="n">name_list</span><span class="p">[</span><span class="n">i</span><span class="p">])),</span> <span class="n">marker</span><span class="o">=</span><span class="n">MARKERS</span><span class="p">[</span><span class="n">i</span><span class="o">%</span><span class="mi">9</span><span class="p">],</span> <span class="n">markevery</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;MSE&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(),</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corr_list</span><span class="p">)):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">corr_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">prefix</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">converter</span><span class="p">(</span><span class="n">name_list</span><span class="p">[</span><span class="n">i</span><span class="p">])),</span> <span class="n">marker</span><span class="o">=</span><span class="n">MARKERS</span><span class="p">[</span><span class="n">i</span><span class="o">%</span><span class="mi">9</span><span class="p">],</span> <span class="n">markevery</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Pearson</span><span class="se">\&#39;</span><span class="s1">s correlation&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(),</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">showCurves</span><span class="p">(</span><span class="s1">&#39;./test/algorithm&#39;</span><span class="p">)</span>
<span class="n">showCurves</span><span class="p">(</span><span class="s1">&#39;./test/noise&#39;</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;ε=N(0,{0})&#39;</span><span class="p">,</span> <span class="n">converter</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<input name="__tabs_10" type="radio" id="__tab_10_1" />
<label for="__tab_10_1">Output (优化器)</label>
<div class="superfences-content"><div class="overflow"><table style='width:90%;margin:auto'>
<thead><tr>
<th>损失函数 (MSE)</th>
<th>测度函数 (相关系数)</th>
</tr></thead>
<tbody><tr>
<td><img alt="损失函数 (MSE)" style="min-width:none" class="img-fluid" src="../../../assets/images/book-1-x/linreg-alg-loss.svg" tag="4" title="损失函数 (MSE)" /></td>
<td><img alt="测度函数 (相关系数)" style="min-width:none" class="img-fluid" src="../../../assets/images/book-1-x/linreg-alg-corr.svg" tag="4" title="测度函数 (相关系数)" /></td>
</tr></tbody>
</table></div></div>
<input name="__tabs_10" type="radio" id="__tab_10_2" />
<label for="__tab_10_2">Output (噪声)</label>
<div class="superfences-content"><div class="overflow"><table style='width:90%;margin:auto'>
<thead><tr>
<th>损失函数 (MSE)</th>
<th>测度函数 (相关系数)</th>
</tr></thead>
<tbody><tr>
<td><img alt="损失函数 (MSE)" style="min-width:none" class="img-fluid" src="../../../assets/images/book-1-x/linreg-noi-loss.svg" tag="5" title="损失函数 (MSE)" /></td>
<td><img alt="测度函数 (相关系数)" style="min-width:none" class="img-fluid" src="../../../assets/images/book-1-x/linreg-noi-corr.svg" tag="5" title="测度函数 (相关系数)" /></td>
</tr></tbody>
</table></div></div>
</div>
<p>可见，损失曲线反映了训练的进度，而测度曲线反映了当前的准确度。我们可以得到如下结论：</p>
<ul>
<li>令人意外的是，SGD和Nesterov动量法收敛速度最快。这是由于这两种方法没有引入对学习率的调整。我们使用的损失函数初始点梯度非常大，这使得简单的方法，形如SGD和动量法在一开头就取得了非常迅速的下降；而对那些需要调整学习率的算法而言，初始梯度在很大的情况下，会导致初始学习率被降到较小的水准。这就是为何Adagrad几乎不收敛的原因，因为一开始这一算法的学习率就被大梯度抑制到将近0的水平了，导致训练无法为继；</li>
<li>在调整学习率的算法里，收敛速度有 RMSprop &gt; Adam = NAdam &gt; Adamax = AMSgrad &gt; Adadelta。从AMSgrad以上的这些算法都可资利用，Adadelta的原理和RMSprop几乎相同但效果相差甚巨，这是由于参数不同引起的，我们虽然将Adadelta的学习率特地设为<code>1.0</code>，仍然远远不如RMSprop，可见一个合适的参数对算法的重要性。</li>
<li>噪声的输出结果并不令人意外，所有噪声条件下的MSE最后都收敛到对应的噪声方差上。</li>
</ul>
<p>为了检查测试集的情况，我们通过以下函数来绘制比较不同样本在不同优化器、不同噪声条件下的RMSE（均方根误差），</p>
<div class="superfences-tabs">
<input name="__tabs_11" type="radio" id="__tab_11_0" checked="checked" />
<label for="__tab_11_0">tools.py</label>
<div class="superfences-content"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">showBars</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;{0}&#39;</span><span class="p">,</span> <span class="n">converter</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Show bar graphs for RMSE for each result</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">name_list</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">keys_list</span> <span class="o">=</span> <span class="n">parseData</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;test_y&#39;</span><span class="p">,</span> <span class="s1">&#39;pred_y&#39;</span><span class="p">])</span>
    <span class="c1">#print(keys_list)</span>
    <span class="n">ytrue_list</span> <span class="o">=</span> <span class="n">keys_list</span><span class="p">[</span><span class="s1">&#39;test_y&#39;</span><span class="p">]</span>
    <span class="n">ypred_list</span> <span class="o">=</span> <span class="n">keys_list</span><span class="p">[</span><span class="s1">&#39;pred_y&#39;</span><span class="p">]</span>
    <span class="k">def</span> <span class="nf">RMSE</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">ytrue_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">NG</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ytrue_list</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NG</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="mf">0.6</span><span class="o">+</span><span class="n">j</span><span class="o">+</span><span class="mf">0.8</span><span class="o">*</span><span class="n">i</span><span class="o">/</span><span class="n">NG</span><span class="o">+</span><span class="mf">0.4</span><span class="o">/</span><span class="n">NG</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">1</span><span class="p">)],</span> <span class="n">RMSE</span><span class="p">(</span><span class="n">ytrue_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ypred_list</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.8</span><span class="o">/</span><span class="n">NG</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">prefix</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">converter</span><span class="p">(</span><span class="n">name_list</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">ncol</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;sample&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;RMSE&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ylim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="n">ylim</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(),</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">showBars</span><span class="p">(</span><span class="s1">&#39;./test/algorithm&#39;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="mi">70</span><span class="p">)</span>
<span class="n">showBars</span><span class="p">(</span><span class="s1">&#39;./test/noise&#39;</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;ε=N(0,{0})&#39;</span><span class="p">,</span> <span class="n">converter</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<input name="__tabs_11" type="radio" id="__tab_11_1" />
<label for="__tab_11_1">Output (优化器)</label>
<div class="superfences-content"><div class="overflow"><p><img alt="均方根误差 (RMSE)" class="img-fluid" src="../../../assets/images/book-1-x/linreg-alg-rmse.svg" tag="5b" title="均方根误差 (RMSE)" /></p></div></div>
<input name="__tabs_11" type="radio" id="__tab_11_2" />
<label for="__tab_11_2">Output (噪声)</label>
<div class="superfences-content"><div class="overflow"><p><img alt="均方根误差 (RMSE)" class="img-fluid" src="../../../assets/images/book-1-x/linreg-noi-rmse.svg" tag="5b" title="均方根误差 (RMSE)" /></p></div></div>
</div>
<p>上述结果反映了</p>
<ul>
<li>测试结果和训练情况相仿，这是由于我们的训练集和测试机完全独立同分布；</li>
<li>Adadelta和Adagrad还没有训练好，它们的误差明显大于其他算法。且Adagrad已经无法收敛，可见这种算法不实用。</li>
</ul>
<p>再接下来，我们要分别展示不同测试下的输出。下面列举的所有输出由该函数所产生：</p>
<div class="superfences-tabs">
<input name="__tabs_12" type="radio" id="__tab_12_0" checked="checked" />
<label for="__tab_12_0">tools.py</label>
<div class="superfences-content"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">saveResults</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">opath</span><span class="p">,</span> <span class="n">oprefix</span><span class="p">,</span> <span class="n">datakeys</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">onlyFirst</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39; ({0})&#39;</span><span class="p">,</span> <span class="n">converter</span><span class="o">=</span><span class="nb">str</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Save result graphs to a folder.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">name_list</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">data_list</span> <span class="o">=</span> <span class="n">parseData</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">datakeys</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span> <span class="c1"># show curves</span>
        <span class="n">c_list</span> <span class="o">=</span> <span class="n">data_list</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">]</span>
        <span class="n">b_list</span> <span class="o">=</span> <span class="n">data_list</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
        <span class="n">NG</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">b_list</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NG</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">c_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">b_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="o">+</span><span class="n">prefix</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">converter</span><span class="p">(</span><span class="n">name_list</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
            <span class="k">if</span> <span class="n">xlabel</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">ylabel</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">onlyFirst</span><span class="p">:</span>
                <span class="n">formatName</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">formatName</span> <span class="o">=</span> <span class="n">name_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">opath</span><span class="p">,</span> <span class="n">oprefix</span><span class="o">+</span><span class="s1">&#39;{0}.svg&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">formatName</span><span class="p">)))</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">onlyFirst</span><span class="p">:</span>
                <span class="k">return</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1"># show images</span>
        <span class="n">data_list</span> <span class="o">=</span> <span class="n">data_list</span><span class="p">[</span><span class="n">datakeys</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">NG</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_list</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NG</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">data_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(),</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="o">+</span><span class="n">prefix</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">converter</span><span class="p">(</span><span class="n">name_list</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
            <span class="k">if</span> <span class="n">xlabel</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">ylabel</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">onlyFirst</span><span class="p">:</span>
                <span class="n">formatName</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">formatName</span> <span class="o">=</span> <span class="n">name_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">opath</span><span class="p">,</span> <span class="n">oprefix</span><span class="o">+</span><span class="s1">&#39;{0}.png&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">formatName</span><span class="p">)))</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">onlyFirst</span><span class="p">:</span>
                <span class="k">return</span>
</pre></div>
</td></tr></table></div>
<input name="__tabs_12" type="radio" id="__tab_12_1" />
<label for="__tab_12_1">测试代码</label>
<div class="superfences-content"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">saveAllResults</span><span class="p">():</span>
    <span class="n">saveResults</span><span class="p">(</span><span class="s1">&#39;./test/algorithm&#39;</span><span class="p">,</span> <span class="s1">&#39;./record/algorithm&#39;</span><span class="p">,</span> <span class="s1">&#39;alg-A-&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">onlyFirst</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">saveResults</span><span class="p">(</span><span class="s1">&#39;./test/algorithm&#39;</span><span class="p">,</span> <span class="s1">&#39;./record/algorithm&#39;</span><span class="p">,</span> <span class="s1">&#39;alg-yt-&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;test_y&#39;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;True values&#39;</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">onlyFirst</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">saveResults</span><span class="p">(</span><span class="s1">&#39;./test/algorithm&#39;</span><span class="p">,</span> <span class="s1">&#39;./record/algorithm&#39;</span><span class="p">,</span> <span class="s1">&#39;alg-y-&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;pred_y&#39;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Predicted values&#39;</span><span class="p">)</span>
    <span class="n">saveResults</span><span class="p">(</span><span class="s1">&#39;./test/algorithm&#39;</span><span class="p">,</span> <span class="s1">&#39;./record/algorithm&#39;</span><span class="p">,</span> <span class="s1">&#39;alg-W-&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;W&#39;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">)</span>
    <span class="n">saveResults</span><span class="p">(</span><span class="s1">&#39;./test/algorithm&#39;</span><span class="p">,</span> <span class="s1">&#39;./record/algorithm&#39;</span><span class="p">,</span> <span class="s1">&#39;alg-cb-&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Biases&#39;</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">saveResults</span><span class="p">(</span><span class="s1">&#39;./test/noise&#39;</span><span class="p">,</span> <span class="s1">&#39;./record/noise&#39;</span><span class="p">,</span> <span class="s1">&#39;noi-A-&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">onlyFirst</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">saveResults</span><span class="p">(</span><span class="s1">&#39;./test/noise&#39;</span><span class="p">,</span> <span class="s1">&#39;./record/noise&#39;</span><span class="p">,</span> <span class="s1">&#39;noi-yt-&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;test_y&#39;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;True values&#39;</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39; (ε=N(0,{0}))&#39;</span><span class="p">,</span> <span class="n">converter</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">saveResults</span><span class="p">(</span><span class="s1">&#39;./test/noise&#39;</span><span class="p">,</span> <span class="s1">&#39;./record/noise&#39;</span><span class="p">,</span> <span class="s1">&#39;noi-y-&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;pred_y&#39;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Predicted values&#39;</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39; (ε=N(0,{0}))&#39;</span><span class="p">,</span> <span class="n">converter</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">saveResults</span><span class="p">(</span><span class="s1">&#39;./test/noise&#39;</span><span class="p">,</span> <span class="s1">&#39;./record/noise&#39;</span><span class="p">,</span> <span class="s1">&#39;noi-W-&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;W&#39;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39; (ε=N(0,{0}))&#39;</span><span class="p">,</span> <span class="n">converter</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">saveResults</span><span class="p">(</span><span class="s1">&#39;./test/noise&#39;</span><span class="p">,</span> <span class="s1">&#39;./record/noise&#39;</span><span class="p">,</span> <span class="s1">&#39;noi-cb-&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Biases&#39;</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39; (ε=N(0,{0}))&#39;</span><span class="p">,</span> <span class="n">converter</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

<span class="n">saveAllResults</span><span class="p">()</span>
</pre></div>
</td></tr></table></div>
</div>
<p>首先考虑不同优化器的测试，在这些测试里，我们确保<span><span class="MathJax_Preview">\mathbf{A},~\mathbf{c}</span><script type="math/tex">\mathbf{A},~\mathbf{c}</script></span>对所有测试相同，且产生的随机数据真值<span><span class="MathJax_Preview">\mathbf{y}_{\mathrm{true}}</span><script type="math/tex">\mathbf{y}_{\mathrm{true}}</script></span>对所有测试也相同，亦即：</p>
<table>
<thead>
<tr>
<th><span><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span></th>
<th><span><span class="MathJax_Preview">\mathbf{y}</span><script type="math/tex">\mathbf{y}</script></span>的真实值</th>
</tr>
</thead>
<tbody>
<tr>
<td><img alt="A" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-A.png" tag="6" title="A" /></td>
<td><img alt="y (true)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-yt.png" tag="6" title="y (true)" /></td>
</tr>
</tbody>
</table>
<p>于是我们可得到所有的数据</p>
<table>
<thead>
<tr>
<th>优化器</th>
<th><span><span class="MathJax_Preview">\mathbf{y}</span><script type="math/tex">\mathbf{y}</script></span>的预测值</th>
<th><span><span class="MathJax_Preview">\mathbf{W}</span><script type="math/tex">\mathbf{W}</script></span></th>
<th><span><span class="MathJax_Preview">\mathbf{b}</span><script type="math/tex">\mathbf{b}</script></span>与<span><span class="MathJax_Preview">\mathbf{c}</span><script type="math/tex">\mathbf{c}</script></span></th>
</tr>
</thead>
<tbody>
<tr>
<td>Adadelta</td>
<td><img alt="y (Adadelta)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-y-Adadelta.png" tag="7" title="y (Adadelta)" /></td>
<td><img alt="W (Adadelta)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-W-Adadelta.png" tag="8" title="W (Adadelta)" /></td>
<td><img alt="c &amp; b (Adadelta)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-cb-Adadelta.svg" tag="9" title="c &amp; b (Adadelta)" /></td>
</tr>
<tr>
<td>Adagrad</td>
<td><img alt="y (Adagrad)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-y-Adagrad.png" tag="7" title="y (Adagrad)" /></td>
<td><img alt="W (Adagrad)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-W-Adagrad.png" tag="8" title="W (Adagrad)" /></td>
<td><img alt="c &amp; b (Adagrad)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-cb-Adagrad.svg" tag="9" title="c &amp; b (Adagrad)" /></td>
</tr>
<tr>
<td>Adam</td>
<td><img alt="y (Adam)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-y-Adam.png" tag="7" title="y (Adam)" /></td>
<td><img alt="W (Adam)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-W-Adam.png" tag="8" title="W (Adam)" /></td>
<td><img alt="c &amp; b (Adam)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-cb-Adam.svg" tag="9" title="c &amp; b (Adam)" /></td>
</tr>
<tr>
<td>Adamax</td>
<td><img alt="y (Adamax)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-y-Adamax.png" tag="7" title="y (Adamax)" /></td>
<td><img alt="W (Adamax)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-W-Adamax.png" tag="8" title="W (Adamax)" /></td>
<td><img alt="c &amp; b (Adamax)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-cb-Adamax.svg" tag="9" title="c &amp; b (Adamax)" /></td>
</tr>
<tr>
<td>AMSgrad</td>
<td><img alt="y (AMSgrad)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-y-AMSgrad.png" tag="7" title="y (AMSgrad)" /></td>
<td><img alt="W (AMSgrad)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-W-AMSgrad.png" tag="8" title="W (AMSgrad)" /></td>
<td><img alt="c &amp; b (AMSgrad)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-cb-AMSgrad.svg" tag="9" title="c &amp; b (AMSgrad)" /></td>
</tr>
<tr>
<td>Nesterov Adam</td>
<td><img alt="y (Nesterov Adam)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-y-Nesterov_Adam.png" tag="7" title="y (Nesterov Adam)" /></td>
<td><img alt="W (Nesterov Adam)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-W-Nesterov_Adam.png" tag="8" title="W (Nesterov Adam)" /></td>
<td><img alt="c &amp; b (Nesterov Adam)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-cb-Nesterov_Adam.svg" tag="9" title="c &amp; b (Nesterov Adam)" /></td>
</tr>
<tr>
<td>Nesterov Moment</td>
<td><img alt="y (Nesterov Moment)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-y-Nesterov_Moment.png" tag="7" title="y (Nesterov Moment)" /></td>
<td><img alt="W (Nesterov Moment)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-W-Nesterov_Moment.png" tag="8" title="W (Nesterov Moment)" /></td>
<td><img alt="c &amp; b (Nesterov Moment)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-cb-Nesterov_Moment.svg" tag="9" title="c &amp; b (Nesterov Moment)" /></td>
</tr>
<tr>
<td>RMSprop</td>
<td><img alt="y (RMSprop)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-y-RMSprop.png" tag="7" title="y (RMSprop)" /></td>
<td><img alt="W (RMSprop)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-W-RMSprop.png" tag="8" title="W (RMSprop)" /></td>
<td><img alt="c &amp; b (RMSprop)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-cb-RMSprop.svg" tag="9" title="c &amp; b (RMSprop)" /></td>
</tr>
<tr>
<td>SGD</td>
<td><img alt="y (SGD)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-y-SGD.png" tag="7" title="y (SGD)" /></td>
<td><img alt="W (SGD)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-W-SGD.png" tag="8" title="W (SGD)" /></td>
<td><img alt="c &amp; b (SGD)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/alg-cb-SGD.svg" tag="9" title="c &amp; b (SGD)" /></td>
</tr>
</tbody>
</table>
<p>接下来考虑不同噪声的测试，在这些测试里，我们确保<span><span class="MathJax_Preview">\mathbf{A},~\mathbf{c}</span><script type="math/tex">\mathbf{A},~\mathbf{c}</script></span>对所有测试相同，但由于噪声大小的不同，随机数据真值<span><span class="MathJax_Preview">\mathbf{y}_{\mathrm{true}}</span><script type="math/tex">\mathbf{y}_{\mathrm{true}}</script></span>会有所偏差：</p>
<div style="width:60%;margin:auto">
    <table>
    <thead><tr><th><span><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span></th></tr></thead>
    <tbody><tr><td><img alt="A" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-A.png" tag="10" title="A" /></td></tr></tbody>
    </table>
</div>

<p>于是我们可得到所有的数据</p>
<table>
<thead>
<tr>
<th><span><span class="MathJax_Preview">\boldsymbol{\varepsilon} \sim N(0,~?)</span><script type="math/tex">\boldsymbol{\varepsilon} \sim N(0,~?)</script></span></th>
<th><span><span class="MathJax_Preview">\mathbf{y}</span><script type="math/tex">\mathbf{y}</script></span>的真实值</th>
<th><span><span class="MathJax_Preview">\mathbf{y}</span><script type="math/tex">\mathbf{y}</script></span>的预测值</th>
<th><span><span class="MathJax_Preview">\mathbf{W}</span><script type="math/tex">\mathbf{W}</script></span></th>
<th><span><span class="MathJax_Preview">\mathbf{b}</span><script type="math/tex">\mathbf{b}</script></span>与<span><span class="MathJax_Preview">\mathbf{c}</span><script type="math/tex">\mathbf{c}</script></span></th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td><img alt="sigma = 0" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-yt-000.png" tag="11" title="y<sub>true</sub> (sigma = 0)" /></td>
<td><img alt="sigma = 0" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-y-000.png" tag="11" title="y (sigma = 0)" /></td>
<td><img alt="W (sigma = 0)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-W-000.png" tag="13" title="W (sigma = 0)" /></td>
<td><img alt="c &amp; b (sigma = 0)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-cb-000.svg" tag="14" title="c &amp; b (sigma = 0)" /></td>
</tr>
<tr>
<td>1</td>
<td><img alt="sigma = 1" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-yt-001.png" tag="11" title="y<sub>true</sub> (sigma = 1)" /></td>
<td><img alt="sigma = 1" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-y-001.png" tag="11" title="y (sigma = 1)" /></td>
<td><img alt="W (sigma = 1)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-W-001.png" tag="13" title="W (sigma = 1)" /></td>
<td><img alt="c &amp; b (sigma = 1)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-cb-001.svg" tag="14" title="c &amp; b (sigma = 1)" /></td>
</tr>
<tr>
<td>5</td>
<td><img alt="sigma = 5" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-yt-005.png" tag="11" title="y<sub>true</sub> (sigma = 5)" /></td>
<td><img alt="sigma = 5" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-y-005.png" tag="11" title="y (sigma = 5)" /></td>
<td><img alt="W (sigma = 5)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-W-005.png" tag="13" title="W (sigma = 5)" /></td>
<td><img alt="c &amp; b (sigma = 5)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-cb-005.svg" tag="14" title="c &amp; b (sigma = 5)" /></td>
</tr>
<tr>
<td>10</td>
<td><img alt="sigma = 10" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-yt-010.png" tag="11" title="y<sub>true</sub> (sigma = 10)" /></td>
<td><img alt="sigma = 10" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-y-010.png" tag="11" title="y (sigma = 10)" /></td>
<td><img alt="W (sigma = 10)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-W-010.png" tag="13" title="W (sigma = 10)" /></td>
<td><img alt="c &amp; b (sigma = 10)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-cb-010.svg" tag="14" title="c &amp; b (sigma = 10)" /></td>
</tr>
<tr>
<td>50</td>
<td><img alt="sigma = 50" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-yt-050.png" tag="11" title="y<sub>true</sub> (sigma = 50)" /></td>
<td><img alt="sigma = 50" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-y-050.png" tag="11" title="y (sigma = 50)" /></td>
<td><img alt="W (sigma = 50)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-W-050.png" tag="13" title="W (sigma = 50)" /></td>
<td><img alt="c &amp; b (sigma = 50)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-cb-050.svg" tag="14" title="c &amp; b (sigma = 50)" /></td>
</tr>
<tr>
<td>100</td>
<td><img alt="sigma = 100" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-yt-100.png" tag="11" title="y<sub>true</sub> (sigma = 100)" /></td>
<td><img alt="sigma = 100" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-y-100.png" tag="11" title="y (sigma = 100)" /></td>
<td><img alt="W (sigma = 100)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-W-100.png" tag="13" title="W (sigma = 100)" /></td>
<td><img alt="c &amp; b (sigma = 100)" class="img-fluid" src="../../../assets/images/book-1-x/linreg/noi-cb-100.svg" tag="14" title="c &amp; b (sigma = 100)" /></td>
</tr>
</tbody>
</table>
<p>我们最为看重的，其实是是否拟合出<span><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>和<span><span class="MathJax_Preview">\mathbf{c}</span><script type="math/tex">\mathbf{c}</script></span>。一系列实验表明，<span><span class="MathJax_Preview">\mathbf{W}</span><script type="math/tex">\mathbf{W}</script></span>和<span><span class="MathJax_Preview">\mathbf{b}</span><script type="math/tex">\mathbf{b}</script></span>的拟合效果甚好。由于我们建立的仿射变换模型和原始仿射变换模型有着完全一致的结构，优化结果反映这一问题的解相当准确。至此，我们已经掌握了一个完整的Project应当具有的模块结构，以及对不同的优化器有了理论和实际的体验。在后续的章节里，除非有特别的应用，我们不再探讨不同的优化器对结果的影响，在绝大多数情况下，我们都将使用AMSgrad。</p>
                
                  
                    <h2 id="__source">来源</h2>
                    
                    
                    
                    
                    <a href="https://github.com/cainmagi/tensorflow-guide/tree//1-2-linear-regression" title="1-2-linear-regression" class="md-source-file">
                      1-2-linear-regression
                    </a>
                  
                
              
              
                


  <h2 id="__comments">评论</h2>
  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = "https://cainmagi.github.io/tensorflow-guide/book-1-x/chapter-1/linear-regression/";
      this.page.identifier =
        "book-1-x/chapter-1/linear-regression/";
    };
    (function() {
      var d = document, s = d.createElement("script");
      s.src = "//tensorflow-guide.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>

              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../linear-classification/" title="线性分类" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                线性分类
              </span>
            </div>
          </a>
        
        
          <a href="../nonlinear-regression/" title="非线性回归" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  前进
                </span>
                非线性回归
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 Yuchen Jin
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../../assets/fonts/font-awesome.css">
    
      <a href="https://cainmagi.github.io/" class="md-footer-social__link fa fa-globe"></a>
    
      <a href="mailto:cainmagi@gmail.com" class="md-footer-social__link fa fa-envelope"></a>
    
      <a href="https://github.com/cainmagi" class="md-footer-social__link fa fa-github-alt"></a>
    
      <a href="https://twitter.com/squidfunk" class="md-footer-social__link fa fa-steam"></a>
    
      <a href="https://weibo.com/u/5885093621" class="md-footer-social__link fa fa-weibo"></a>
    
      <a href="https://www.youtube.com/channel/UCzqpNK5qFMy5_cI1i0Z1nQw" class="md-footer-social__link fa fa-youtube-play"></a>
    
      <a href="https://music.163.com/#/user/home?id=276304206" class="md-footer-social__link fa fa-music"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/application.43ad2ac2.js"></script>
      
        
        
          
          <script src="../../../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../../../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../../../assets/javascripts/lunr/lunr.jp.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:"../../.."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js"></script>
      
        <script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
      
        <script src="../../../javascripts/simpleLightbox.min.js"></script>
      
        <script src="../../../javascripts/extensions.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML"></script>
      
    
  </body>
</html>